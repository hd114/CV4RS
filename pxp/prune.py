from collections import OrderedDict

import torch
import torch.nn.utils.prune as prune

from pxp.utils import ModelLayerUtils


class LocalPruningOperations:
    def __init__(self):
        pass

    def bind_mask_to_module(
        self,
        model,
        layer_name,
        pruning_mask,
        weight_or_bias,
        remove_re_parametrization=True,
    ):
        """
        Make the pruning mask permanent by applying it to the model and removing the pruning hook
        generated by the prune method of PyTorch

        Args:
            model (torch.module): the model
            layer_name (str): the name of the submodel that pruning has
                            to be applied to, i.e. "features.20" or "classifier.0" in VGG
            pruning_mask (dict): the binary pruning mask
            weight_or_bias (str): whether to prune the "weight" or "bias" of the layer
            remove_re_parametrization (bool, optional): whether to remove the re-parametrization
        """
        prune.custom_from_mask(
            ModelLayerUtils.get_module_from_name(model, layer_name),
            weight_or_bias,
            mask=pruning_mask,
        )
        if remove_re_parametrization:
            prune.remove(
                ModelLayerUtils.get_module_from_name(model, layer_name), weight_or_bias
            )

    def fit_pruning_mask(self, model, layer_name, pruning_mask):
        """
        Apply the pruning mask to the model and fix it

        Args:
            model (torch.nn.module): the model to prune
            layer_name (str): the layer which the pruning mask is applied to
            pruning_mask (dict): dictionary of binary mask of the concepts to prune for each layer
        """
        mask_keys = list(pruning_mask.keys())
        batch_norm_order_flag = False
        if "BatchNorm2d" in mask_keys:
            if ModelLayerUtils.is_batchnorm2d_after_conv2d(model):
                batch_norm_order_flag = True
                conv_bn_layers = ModelLayerUtils.get_layer_names(
                    model, [torch.nn.Conv2d, torch.nn.BatchNorm2d]
                )
                bn_layer_name = conv_bn_layers[conv_bn_layers.index(layer_name) + 1]

        for layer_type in mask_keys:
            if layer_type == "BatchNorm2d" and batch_norm_order_flag == True:
                layer_name_to_prune = bn_layer_name
            else:
                layer_name_to_prune = layer_name

            # Prune the weights first
            self.bind_mask_to_module(
                model,
                layer_name_to_prune,
                pruning_mask[layer_type]["weight"],
                weight_or_bias="weight",
                remove_re_parametrization=True,
            )

            # For bias, prune if they exist
            if ModelLayerUtils.get_module_from_name(model, layer_name).bias is not None:
                self.bind_mask_to_module(
                    model,
                    layer_name_to_prune,
                    pruning_mask[layer_type]["bias"],
                    weight_or_bias="bias",
                    remove_re_parametrization=True,
                )

    def generate_local_pruning_mask(
            self,
            pruning_mask_shape,
            pruning_indices,
            subsequent_layer_pruning="Conv2d",  # Standardwert wie im ursprünglichen Projekt
            device="cpu",
    ):
        """
        Generate a binary pruning mask for the specified layer types.

        Args:
            pruning_mask_shape (tuple): Shape of the parameter to prune.
            pruning_indices (list): Indices of the parameters to prune.
            subsequent_layer_pruning (str): Specifies which layers to prune.
                Options: ["Conv2d", "Both", "BatchNorm2d", "Linear", "Softmax"].
                Default is "Conv2d" (standard pruning).
            device (str): Device to allocate the mask ("cuda" or "cpu").

        Returns:
            dict: Binary pruning mask for the specified layer types.
        """
        print(f"[DEBUG] Generiere lokale Pruning-Maske...")
        print(f"Masken-Shape: {pruning_mask_shape}, Pruning-Indizes: {pruning_indices}")

        # Initialize the final mask dictionary
        final_pruning_mask = {}

        # Handle "Conv2d" pruning (default behavior)
        if subsequent_layer_pruning in ["Conv2d", "Both"]:
            conv_weight_mask = torch.ones(pruning_mask_shape).to(device)
            conv_bias_mask = torch.ones(pruning_mask_shape[0]).to(device)
            # Apply pruning
            conv_weight_mask[pruning_indices] = 0
            conv_bias_mask[pruning_indices] = 0
            final_pruning_mask["Conv2d"] = {
                "weight": conv_weight_mask,
                "bias": conv_bias_mask,
            }

        # Handle "BatchNorm2d" pruning (if specified)
        if subsequent_layer_pruning in ["BatchNorm2d", "Both"]:
            bn_weight_mask = torch.ones(pruning_mask_shape[0]).to(device)
            bn_bias_mask = torch.ones(pruning_mask_shape[0]).to(device)
            # Apply pruning
            bn_weight_mask[pruning_indices] = 0
            bn_bias_mask[pruning_indices] = 0
            final_pruning_mask["BatchNorm2d"] = {
                "weight": bn_weight_mask,
                "bias": bn_bias_mask,
            }

        # Handle "Linear" pruning
        if subsequent_layer_pruning == "Linear":
            linear_weight_mask = torch.ones(pruning_mask_shape).to(device)
            linear_bias_mask = torch.ones(pruning_mask_shape[0]).to(device)
            # Apply pruning
            linear_weight_mask[pruning_indices] = 0
            linear_bias_mask[pruning_indices] = 0
            final_pruning_mask["Linear"] = {
                "weight": linear_weight_mask,
                "bias": linear_bias_mask,
            }

        # Handle "Softmax" pruning
        if subsequent_layer_pruning == "Softmax":
            softmax_weight_mask = torch.ones(pruning_mask_shape).to(device)
            # Apply pruning
            softmax_weight_mask[pruning_indices] = 0
            final_pruning_mask["Softmax"] = {"weight": softmax_weight_mask}

        # Ensure at least one mask is generated
        if not final_pruning_mask:
            raise ValueError(
                f"Unsupported option for subsequent_layer_pruning: {subsequent_layer_pruning}"
            )
        print(f"[DEBUG] Generierte Masken: {final_pruning_mask}")
        return final_pruning_mask


class GlobalPruningOperations(LocalPruningOperations):
    def __init__(self, target_layer, layer_names):
        self.target_layer = target_layer
        self.layer_names = layer_names

    def generate_global_pruning_mask(global_concept_maps, layer_order, pruning_rate):
        global_pruning_mask = {}
        for layer_name in layer_order:
            print(f"[DEBUG] Überprüfe Layer '{layer_name}' in global_concept_maps...")
            if layer_name == 'conv1':
                print(f"[WARNING] Layer '{layer_name}' wird übersprungen (Testfall).")
                continue

            if isinstance(global_concept_maps[layer_name], dict):
                if 'Conv2d' in global_concept_maps[layer_name]:
                    relevance_map = global_concept_maps[layer_name]['Conv2d']['weight']
                else:
                    print(f"[ERROR] Layer '{layer_name}' hat keine 'Conv2d'-Maske.")
                    continue
            else:
                relevance_map = global_concept_maps[layer_name]

            # Debug: Ausgabe der Shape-Informationen
            print(f"[DEBUG] Layer '{layer_name}' hat Relevance-Map mit Shape: {relevance_map.shape}")

            # Generiere Pruning-Maske
            try:
                mask = torch.ones_like(relevance_map)
                threshold = torch.quantile(relevance_map, pruning_rate)
                mask[relevance_map < threshold] = 0
                global_pruning_mask[layer_name] = {'weight': mask}
                print(f"[DEBUG] Layer '{layer_name}' - Gewicht-Maske erstellt.")
            except Exception as e:
                print(f"[ERROR] Fehler beim Generieren der Maske für Layer '{layer_name}': {str(e)}")

        return global_pruning_mask

    '''def generate_global_pruning_mask(
            self,
            model,
            global_concept_maps,
            pruning_percentage,
            subsequent_layer_pruning="Conv2d",
            least_relevant_first=True,
            device="cuda",
    ):
        """
        Generate a global pruning mask for the model based on LRP relevances.

        Args:
            model (torch.nn.Module): The model to prune.
            global_concept_maps (dict): Relevance maps for each layer.
            pruning_percentage (float): Percentage of parameters to prune.
            subsequent_layer_pruning (str): Layer types to include in subsequent pruning. Options: ["Conv2d", "BatchNorm2d", "Both"].
            least_relevant_first (bool): Prune the least relevant parameters first.
            device (str): Device to perform computations (e.g., "cuda" or "cpu").

        Returns:
            dict: A dictionary containing pruning masks for each layer.
        """
        global_pruning_masks = {}
        cumulative_mask = None

        for layer_name, relevance_map in global_concept_maps.items():
            relevance_map = relevance_map.to(device)
            num_elements = relevance_map.numel()
            num_prune = int(pruning_percentage * num_elements)

            # Flatten the relevance map and get indices to prune
            flat_relevance = relevance_map.view(-1)
            _, prune_indices = torch.topk(flat_relevance, k=num_prune, largest=not least_relevant_first)

            # Generate a local pruning mask
            pruning_mask = self.generate_local_pruning_mask(
                pruning_mask_shape=relevance_map.shape,
                pruning_indices=prune_indices,
                subsequent_layer_pruning=subsequent_layer_pruning,
                device=device,
            )

            # Combine with cumulative mask if needed
            if cumulative_mask is not None and subsequent_layer_pruning in ["Both", "BatchNorm2d"]:
                pruning_mask = {
                    key: pruning_mask[key] * cumulative_mask.get(key, 1)
                    for key in pruning_mask.keys()
                }

            global_pruning_masks[layer_name] = pruning_mask

            # Update the cumulative mask for subsequent layers
            if subsequent_layer_pruning in ["Both", "BatchNorm2d"]:
                cumulative_mask = pruning_mask

        return global_pruning_masks'''

    def generate_global_pruning_indices(
        self,
        global_concept_maps,
        interval_indices,
        pruning_percentage,
        least_relevant_first,
    ):
        """
        Generate the indices of concepts/filters to prune from each layer

        Args:
            global_concept_maps (dict): summed of concept relevances for each layer
            interval_indices (dict): interval indices of the concepts/filters for each layer
            pruning_percentage (float): percentage of concepts/filters to prune
            least_relevant_first (bool): whether to prune the least or most relevant concepts/filters

        Returns:
            dict: Dictionary of indices of concepts/filters to prune for each layer
        """
        # Flatten relevances for each layer into a single tensor
        flattened_concept_relevances = torch.cat(
            [value.flatten() for value in global_concept_maps.values()]
        )

        # Total number of concepts/filters to prune
        total_num_candidates = int(
            flattened_concept_relevances.shape[0] * pruning_percentage
        )

        # Sort the concepts/filters by their relevances and get the indices
        _, pruning_indices = flattened_concept_relevances.topk(
            total_num_candidates,
            largest=not least_relevant_first,
        )

        # Assign the sorted indices to the corresponding layers
        # ,stating the filters/concepts to prune from each layer
        global_pruning_indices = OrderedDict([])
        for layer_name, _ in global_concept_maps.items():
            start_index, end_index = interval_indices[layer_name]
            global_pruning_indices[layer_name] = (
                pruning_indices[
                    (pruning_indices >= start_index) & (pruning_indices <= end_index)
                ]
                - start_index
            )

        return global_pruning_indices

    def fit_pruning_mask(self, model, global_pruning_mask):
        """
        Apply the global pruning mask to the model and fixing it

        Args:
            model (torch.nn.Module): the model to prune
            global_pruning_mask (dict): pruning mask for each layer
        """
        # Debug-Ausgabe
        print(f"[DEBUG] Starte Anwendung der Pruning-Maske auf {len(global_pruning_mask)} Layer...")
        if self.target_layer != torch.nn.Softmax:
            for layer_name, layer_pruning_mask in global_pruning_mask.items():
                #print(f"[DEBUG] Prüfe Layer '{layer_name}'...")
                if ModelLayerUtils.get_module_from_name(model, layer_name) is None:
                    print(f"[ERROR] Layer '{layer_name}' existiert nicht im Modell!")
                    continue
                try:
                    super(GlobalPruningOperations, self).fit_pruning_mask(
                        model, layer_name, layer_pruning_mask
                    )
                    print(f"[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer '{layer_name}'.")
                except Exception as e:
                    print(f"[ERROR] Fehler beim Anwenden der Pruning-Maske auf Layer '{layer_name}': {e}")
        else:
            for layer_name, layer_pruning_indices in global_pruning_mask.items():
                print(f"[DEBUG] Registriere Hook für Softmax-Layer '{layer_name}'...")
                try:
                    hook_handles = self.mask_attention_head(
                        model, layer_name, layer_pruning_indices
                    )
                    print(f"[DEBUG] Hook erfolgreich registriert für Softmax-Layer '{layer_name}'.")
                except Exception as e:
                    print(f"[ERROR] Fehler bei Hook-Registrierung für Layer '{layer_name}': {e}")
            return hook_handles

    @staticmethod
    def mask_attention_head(model, layer_names, head_indices):
        hook_handles = []
        act_layer_softmax = {}

        def generate_set_forward_hook_softmax(layer_name, head_indices):
            def set_out_activations(module, input, output):
                output[:, head_indices] = 0

            return set_out_activations

        def generate_get_forward_hook_softmax(layer_name):
            def get_out_activations(module, input, output):
                act_layer_softmax[layer_name] = output

            return get_out_activations

        # Hook für jeden angegebenen Layer registrieren
        for name, layer in model.named_modules():
            if name == layer_names:
                print(f"[DEBUG] Registriere Hook für Layer: {name}")
                hook_handles.append(
                    layer.register_forward_hook(
                        generate_set_forward_hook_softmax(name, head_indices)
                    )
                )
            else:
                print(f"[WARNING] Layer '{layer_names}' nicht im Modell gefunden.")

        if not hook_handles:
            print(f"[ERROR] Keine Hooks registriert für '{layer_names}'. Überprüfe Layer-Namen.")
        return hook_handles



