from collections import OrderedDict

import torch
import torch.nn.utils.prune as prune

from pxp.utils import ModelLayerUtils


class LocalPruningOperations:
    class LocalPruningOperations:
        def __init__(self, device=None):
            """
            Konstruktor für LocalPruningOperations.
            Args:
                device (str, optional): Das zu verwendende Gerät ("cuda" oder "cpu").
            """
            self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
            print(f"[INFO] Using device: {self.device}")

    def bind_mask_to_module(
        self,
        model,
        layer_name,
        pruning_mask,
        weight_or_bias,
        remove_re_parametrization=True,
    ):
        """
        Make the pruning mask permanent by applying it to the model and removing the pruning hook
        generated by the prune method of PyTorch

        Args:
            model (torch.module): the model
            layer_name (str): the name of the submodel that pruning has
                            to be applied to, i.e. "features.20" or "classifier.0" in VGG
            pruning_mask (dict): the binary pruning mask
            weight_or_bias (str): whether to prune the "weight" or "bias" of the layer
            remove_re_parametrization (bool, optional): whether to remove the re-parametrization
        """
        prune.custom_from_mask(
            ModelLayerUtils.get_module_from_name(model, layer_name),
            weight_or_bias,
            mask=pruning_mask,
        )
        if remove_re_parametrization:
            prune.remove(
                ModelLayerUtils.get_module_from_name(model, layer_name), weight_or_bias
            )

    def fit_pruning_mask(self, model, layer_name, pruning_mask):
        """
        Apply the pruning mask to the model and fix it

        Args:
            model (torch.nn.module): the model to prune
            layer_name (str): the layer which the pruning mask is applied to
            pruning_mask (dict): dictionary of binary mask of the concepts to prune for each layer
        """
        mask_keys = list(pruning_mask.keys())
        batch_norm_order_flag = False
        if "BatchNorm2d" in mask_keys:
            if ModelLayerUtils.is_batchnorm2d_after_conv2d(model):
                batch_norm_order_flag = True
                conv_bn_layers = ModelLayerUtils.get_layer_names(
                    model, [torch.nn.Conv2d, torch.nn.BatchNorm2d]
                )
                bn_layer_name = conv_bn_layers[conv_bn_layers.index(layer_name) + 1]

        for layer_type in mask_keys:
            if layer_type == "BatchNorm2d" and batch_norm_order_flag == True:
                layer_name_to_prune = bn_layer_name
            else:
                layer_name_to_prune = layer_name

            # Prune the weights first
            self.bind_mask_to_module(
                model,
                layer_name_to_prune,
                pruning_mask[layer_type]["weight"],
                weight_or_bias="weight",
                remove_re_parametrization=True,
            )

            # For bias, prune if they exist
            if ModelLayerUtils.get_module_from_name(model, layer_name).bias is not None:
                self.bind_mask_to_module(
                    model,
                    layer_name_to_prune,
                    pruning_mask[layer_type]["bias"],
                    weight_or_bias="bias",
                    remove_re_parametrization=True,
                )

    def generate_local_pruning_mask(
        self,
        pruning_mask_shape,
        pruning_indices,
        subsequent_layer_pruning="Conv2d",
    ):
        """
        Generate a binary pruning mask for the specified layer types.
        """
        final_pruning_mask = {}

        if "Conv2d" in subsequent_layer_pruning or subsequent_layer_pruning == "Both":
            if len(pruning_mask_shape) != 0:  # Valid shape
                conv_weight_mask = torch.ones(pruning_mask_shape).to(self.device)
                conv_weight_mask[pruning_indices] = 0
                final_pruning_mask["Conv2d"] = {
                    "weight": conv_weight_mask,
                    "bias": torch.ones(pruning_mask_shape[0]).to(self.device),  # Default bias mask
                }
            else:
                print(f"[WARNING] Invalid mask shape for Conv2d: {pruning_mask_shape}. Skipping layer.")

        if "BatchNorm2d" in subsequent_layer_pruning or subsequent_layer_pruning == "Both":
            bn_weight_mask = torch.ones(pruning_mask_shape[0]).to(self.device)
            bn_weight_mask[pruning_indices] = 0
            final_pruning_mask["BatchNorm2d"] = {
                "weight": bn_weight_mask,
                "bias": bn_weight_mask.clone(),
            }

        return final_pruning_mask


class GlobalPruningOperations(LocalPruningOperations):
    def __init__(self, target_layer, layer_names, device=None):
        """
        Konstruktor für GlobalPruningOperations.
        Args:
            target_layer: Ziel-Layer, z. B. torch.nn.Conv2d.
            layer_names (list): Namen der Layer, die geprunt werden sollen.
            device (str, optional): Das zu verwendende Gerät ("cuda" oder "cpu").
        """
        super().__init__(device=device)  # Initialisiere die Basisklasse
        self.target_layer = target_layer
        self.layer_names = layer_names


    def generate_global_pruning_mask(
            self,
            model,
            global_concept_maps,
            pruning_percentage,
            subsequent_layer_pruning="Conv2d",
            least_relevant_first=True,
            device="cuda",
    ):
        """
        Generate a global pruning mask for the model based on the LRP relevances.

        Args:
            model (torch.nn.Module): The model to prune.
            global_concept_maps (dict): Dictionary of relevance maps for each layer.
            pruning_percentage (float): Percentage of the concepts/filters to prune.
            subsequent_layer_pruning (str, optional): Specifies whether to include subsequent layers in pruning.
                Options are ["Conv2d", "BatchNorm2d", "Both"]. Defaults to "Conv2d".
            least_relevant_first (bool, optional): Whether to prune the least relevant parameters first.
                Defaults to True.
            device (str, optional): Device to run on. Defaults to "cuda".

        Returns:
            dict: Dictionary of binary pruning masks or pruning indices, depending on the layer type.
        """
        print("[DEBUG] Validierung von global_concept_maps vor Beginn der Maskengenerierung...")
        for layer_name, value in global_concept_maps.items():
            if value is None:
                print(f"[ERROR] Layer '{layer_name}' hat None-Wert vor Maskengenerierung.")
            elif isinstance(value, dict):
                print(f"[DEBUG] Layer '{layer_name}' enthält Keys: {list(value.keys())}")
            else:
                print(f"[DEBUG] Layer '{layer_name}' hat Typ: {type(value)}.")

        # Verarbeite verschachtelte Dictionaries
        for layer_name in global_concept_maps.keys():
            print(f"[DEBUG] Überprüfe Layer '{layer_name}' in global_concept_maps...")
            if isinstance(global_concept_maps[layer_name], dict):
                if "relevance" in global_concept_maps[layer_name]:
                    global_concept_maps[layer_name] = global_concept_maps[layer_name]["relevance"]
                elif "Conv2d" in global_concept_maps[layer_name]:
                    if isinstance(global_concept_maps[layer_name]["Conv2d"], dict):
                        if "weight" in global_concept_maps[layer_name]["Conv2d"]:
                            print(
                                f"[DEBUG] Layer '{layer_name}' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.")
                            global_concept_maps[layer_name] = global_concept_maps[layer_name]["Conv2d"]["weight"]
                        else:
                            raise KeyError(
                                f"[ERROR] Layer '{layer_name}' enthält 'Conv2d', aber kein 'weight'-Key. Verfügbare Keys: {list(global_concept_maps[layer_name]['Conv2d'].keys())}"
                            )
                    else:
                        global_concept_maps[layer_name] = global_concept_maps[layer_name]["Conv2d"]
                elif "BatchNorm2d" in global_concept_maps[layer_name]:
                    print(f"[DEBUG] Layer '{layer_name}' enthält 'BatchNorm2d'-Key. Verwende als Relevanz.")
                    global_concept_maps[layer_name] = global_concept_maps[layer_name]["BatchNorm2d"]
                else:
                    raise KeyError(
                        f"[ERROR] Layer '{layer_name}' enthält keine unterstützten Keys. Verfügbare Keys: {list(global_concept_maps[layer_name].keys())}"
                    )

            # Sicherstellen, dass der Wert ein Tensor ist
            if not isinstance(global_concept_maps[layer_name], torch.Tensor):
                raise ValueError(
                    f"[ERROR] Unerwartete Struktur in Layer '{layer_name}'. Erwartet Tensor, aber {type(global_concept_maps[layer_name])} gefunden."
                )

        print(f"[DEBUG] Überprüfung abgeschlossen. Alle relevanten Layer sind jetzt Tensoren.")

        # Initialisiere Intervalle für Pruning-Indizes
        interval_indices = OrderedDict([])
        old_start_index = 0
        for layer_name, relevance_map in global_concept_maps.items():
            if layer_name not in interval_indices.keys():
                interval_indices[layer_name] = (
                    old_start_index,
                    old_start_index + relevance_map.shape[0] - 1,
                )
                old_start_index += relevance_map.shape[0]

        print(f"[DEBUG] Intervallindizes initialisiert: {interval_indices}")

        # Generiere Pruning-Indizes
        global_pruning_indices = self.generate_global_pruning_indices(
            global_concept_maps,
            interval_indices,
            pruning_percentage,
            least_relevant_first,
        )

        print("[DEBUG] Generierte Pruning-Indizes überprüft:")
        for layer_name, indices in global_pruning_indices.items():
            print(f"  Layer: {layer_name} | Indizes: {indices.shape}")

        # Validierung von subsequent_layer_pruning
        valid_options = {"Conv2d", "BatchNorm2d", "Both"}
        if isinstance(subsequent_layer_pruning, bool):
            print(
                f"[WARNING] 'subsequent_layer_pruning' war bool: {subsequent_layer_pruning}. Konvertiere zu 'Conv2d'.")
            subsequent_layer_pruning = "Conv2d" if subsequent_layer_pruning else "None"
        elif subsequent_layer_pruning not in valid_options:
            raise ValueError(
                f"[ERROR] Unsupported option for subsequent_layer_pruning: {subsequent_layer_pruning}. "
                f"Valid options are: {valid_options}"
            )

        print(f"[DEBUG] Verwende 'subsequent_layer_pruning': {subsequent_layer_pruning}")

        # Generiere Pruning-Masken
        global_pruning_mask = OrderedDict([])
        for layer_name, layer_pruning_indices in global_pruning_indices.items():
            print(f"[DEBUG] Generiere Pruning-Maske für Layer '{layer_name}'...")
            layer = ModelLayerUtils.get_module_from_name(model, layer_name)
            mask_shape = layer.weight.shape
            global_pruning_mask[layer_name] = self.generate_local_pruning_mask(
                mask_shape,
                layer_pruning_indices,
                subsequent_layer_pruning=subsequent_layer_pruning,
                device=device,
            )

        # Debugging: Überprüfe die erstellten Masken
        #for layer_name, mask in global_pruning_mask.items():
        #    print(f"[DEBUG] Layer {layer_name} - Weight mask non-zero elements: {torch.sum(mask['weight'] > 0)}")

        return global_pruning_mask

    def generate_global_pruning_indices(
        self,
        global_concept_maps,
        interval_indices,
        pruning_percentage,
        least_relevant_first,
    ):
        """
        Generate the indices of concepts/filters to prune from each layer

        Args:
            global_concept_maps (dict): summed of concept relevances for each layer
            interval_indices (dict): interval indices of the concepts/filters for each layer
            pruning_percentage (float): percentage of concepts/filters to prune
            least_relevant_first (bool): whether to prune the least or most relevant concepts/filters

        Returns:
            dict: Dictionary of indices of concepts/filters to prune for each layer
        """
        # Flatten relevances for each layer into a single tensor
        flattened_concept_relevances = torch.cat(
            [value.flatten() for value in global_concept_maps.values()]
        )

        # Total number of concepts/filters to prune
        total_num_candidates = int(
            flattened_concept_relevances.shape[0] * pruning_percentage
        )

        # Sort the concepts/filters by their relevances and get the indices
        _, pruning_indices = flattened_concept_relevances.topk(
            total_num_candidates,
            largest=not least_relevant_first,
        )

        # Assign the sorted indices to the corresponding layers
        # ,stating the filters/concepts to prune from each layer
        global_pruning_indices = OrderedDict([])
        for layer_name, _ in global_concept_maps.items():
            start_index, end_index = interval_indices[layer_name]
            global_pruning_indices[layer_name] = (
                pruning_indices[
                    (pruning_indices >= start_index) & (pruning_indices <= end_index)
                ]
                - start_index
            )

        return global_pruning_indices

    def fit_pruning_mask(self, model, global_pruning_mask):
        """
        Apply the global pruning mask to the model and fix it.

        Args:
            model (torch.nn.Module): The model to prune.
            global_pruning_mask (dict): Pruning mask for each layer.
        """
        print(f"[DEBUG] Applying pruning mask to {len(global_pruning_mask)} layers...")
        for layer_name, layer_pruning_mask in global_pruning_mask.items():
            # Validate layer existence
            layer = ModelLayerUtils.get_module_from_name(model, layer_name)
            if layer is None:
                print(f"[ERROR] Layer '{layer_name}' does not exist in the model. Skipping...")
                continue

            # Validate pruning mask
            if "weight" not in layer_pruning_mask:
                print(f"[ERROR] Missing 'weight' in pruning mask for layer '{layer_name}'. Skipping...")
                continue

            try:
                # Apply weight mask
                self.bind_mask_to_module(
                    model, layer_name, layer_pruning_mask["weight"], "weight"
                )
                # Apply bias mask if exists
                if "bias" in layer_pruning_mask and layer.bias is not None:
                    self.bind_mask_to_module(
                        model, layer_name, layer_pruning_mask["bias"], "bias"
                    )
                print(f"[DEBUG] Pruning mask successfully applied to layer '{layer_name}'.")
            except Exception as e:
                print(f"[ERROR] Failed to apply pruning mask to layer '{layer_name}': {e}")

    @staticmethod
    def mask_attention_head(model, layer_names, head_indices):
        hook_handles = []
        act_layer_softmax = {}

        def generate_set_forward_hook_softmax(layer_name, head_indices):
            def set_out_activations(module, input, output):
                output[:, head_indices] = 0

            return set_out_activations

        def generate_get_forward_hook_softmax(layer_name):
            def get_out_activations(module, input, output):
                act_layer_softmax[layer_name] = output

            return get_out_activations

        # Hook für jeden angegebenen Layer registrieren
        for name, layer in model.named_modules():
            if name == layer_names:
                print(f"[DEBUG] Registriere Hook für Layer: {name}")
                hook_handles.append(
                    layer.register_forward_hook(
                        generate_set_forward_hook_softmax(name, head_indices)
                    )
                )
            else:
                print(f"[WARNING] Layer '{layer_names}' nicht im Modell gefunden.")

        if not hook_handles:
            print(f"[ERROR] Keine Hooks registriert für '{layer_names}'. Überprüfe Layer-Namen.")
        return hook_handles



