Lines that potentially need to be canonized 309
Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initializing LRP Pruning...
LRP initialized successfully.
=== Runde 1/5 ===
Training and communication for Round 1...
Epoch 1/1
----------
Batch 0:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 1:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 2:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 3:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 4:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 5:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 6:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 7:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 8:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 9:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 10:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 11:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 12:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 13:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 14:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 15:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 16:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 17:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 18:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 19:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 20:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 21:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 22:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 23:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 24:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 25:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 26:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 27:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 28:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 29:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 30:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 31:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 32:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 33:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 34:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 35:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 36:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 37:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 38:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 39:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 40:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 41:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 42:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 43:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 44:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 45:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 46:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 47:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 48:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 49:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 50:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 51:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 52:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 53:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 54:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 55:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 56:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 57:
  Data shape: torch.Size([213, 10, 120, 120])
  Labels shape: torch.Size([213, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Starting validation after Round 1...
Bug fix for empty classification report.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.5678
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.3397

F端hre LRP-Pruning in Runde 1 durch...
F端hre LRP-Pruning in Runde 1 durch...
Berechne LRP-Pruning-Maske f端r Land: Finland
Erstelle DataLoader f端r Land: Finland
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([5])
Unexpected input shape for LRP: torch.Size([5]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 5])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 5
Input shape before relevance computation: torch.Size([5, 10, 1, 5])
Model output shape: torch.Size([5, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([5, 19]) to 5 for one-hot encoding.
Targets shape: torch.Size([5])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([5, 19])
Grad output shape: torch.Size([5, 19])
Relevance computation successful. Relevance shape: torch.Size([5, 10, 1, 5])
Relevanzkarten berechnet: ['conv1', 'encoder.4.0.conv1', 'encoder.4.0.conv2', 'encoder.4.0.conv3', 'encoder.4.0.downsample.0', 'encoder.4.1.conv1', 'encoder.4.1.conv2', 'encoder.4.1.conv3', 'encoder.4.2.conv1', 'encoder.4.2.conv2', 'encoder.4.2.conv3', 'encoder.5.0.conv1', 'encoder.5.0.conv2', 'encoder.5.0.conv3', 'encoder.5.0.downsample.0', 'encoder.5.1.conv1', 'encoder.5.1.conv2', 'encoder.5.1.conv3', 'encoder.5.2.conv1', 'encoder.5.2.conv2', 'encoder.5.2.conv3', 'encoder.5.3.conv1', 'encoder.5.3.conv2', 'encoder.5.3.conv3', 'encoder.6.0.conv1', 'encoder.6.0.conv2', 'encoder.6.0.conv3', 'encoder.6.0.downsample.0', 'encoder.6.1.conv1', 'encoder.6.1.conv2', 'encoder.6.1.conv3', 'encoder.6.2.conv1', 'encoder.6.2.conv2', 'encoder.6.2.conv3', 'encoder.6.3.conv1', 'encoder.6.3.conv2', 'encoder.6.3.conv3', 'encoder.6.4.conv1', 'encoder.6.4.conv2', 'encoder.6.4.conv3', 'encoder.6.5.conv1', 'encoder.6.5.conv2', 'encoder.6.5.conv3', 'encoder.7.0.conv1', 'encoder.7.0.conv2', 'encoder.7.0.conv3', 'encoder.7.0.downsample.0', 'encoder.7.1.conv1', 'encoder.7.1.conv2', 'encoder.7.1.conv3', 'encoder.7.2.conv1', 'encoder.7.2.conv2', 'encoder.7.2.conv3']
Calling generate_global_pruning_mask with pruning_rate: 0.3
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.2818e+19, 2.0114e+19, 5.5602e+19, 1.3660e+19, 1.4093e+19, 1.9060e+19,
        2.4925e+19, 1.9575e+19, 2.6075e+19, 3.1076e+19, 1.3115e+19, 2.0112e+19,
        2.1217e+19, 1.2646e+19, 6.1285e+18, 3.1565e+19, 1.6853e+19, 3.3460e+19,
        1.6298e+19, 2.7242e+19, 2.2452e+19, 2.2872e+19, 8.6996e+18, 1.5787e+19,
        1.2081e+19, 1.9704e+19, 2.9040e+19, 2.6592e+19, 9.4947e+18, 2.5389e+19,
        1.7014e+19, 9.9617e+18, 1.4579e+19, 1.9568e+19, 1.4810e+19, 2.8122e+19,
        1.2977e+19, 2.5237e+19, 3.6733e+19, 2.7780e+19, 1.2758e+19, 2.4817e+19,
        9.6189e+18, 3.5282e+19, 2.9442e+19, 2.1105e+19, 2.6919e+19, 2.9079e+19,
        3.5800e+19, 3.1566e+19, 1.8003e+19, 2.0651e+19, 6.1676e+18, 3.4463e+19,
        1.3747e+19, 9.8343e+18, 2.3329e+19, 2.7056e+19, 3.4222e+19, 1.4692e+19,
        2.8715e+19, 2.7403e+19, 1.8288e+19, 2.6784e+19])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.7170e+18, 1.4498e+19, 2.5809e+18, 1.0324e+19, 8.5743e+18, 2.8200e+18,
        5.5041e+18, 1.4012e+19, 3.4905e+18, 6.7154e+18, 9.3505e+18, 7.1911e+18,
        1.0880e+19, 6.5535e+18, 1.4291e+19, 1.5055e+19, 8.2301e+18, 1.2706e+19,
        1.1521e+19, 7.7853e+18, 5.5454e+18, 9.3215e+18, 4.3919e+18, 5.1948e+18,
        1.2689e+19, 7.9019e+18, 1.2646e+19, 9.3414e+18, 1.6555e+19, 1.2714e+19,
        1.0589e+19, 8.9191e+18, 5.6445e+18, 6.6444e+18, 1.6594e+19, 6.0562e+18,
        2.8666e+18, 7.7378e+18, 1.1450e+19, 1.1119e+19, 1.3434e+19, 6.9348e+18,
        1.8506e+19, 1.3092e+19, 6.3564e+18, 7.4470e+18, 1.2163e+19, 5.8508e+18,
        9.6108e+18, 7.4274e+18, 1.1245e+19, 1.1660e+19, 1.2420e+19, 1.2316e+19,
        1.8568e+19, 1.1910e+19, 7.3830e+18, 1.6863e+19, 8.0862e+18, 6.8212e+18,
        1.2258e+19, 5.5524e+18, 8.1305e+18, 1.3097e+19])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0588e+19, 1.0833e+19, 6.9811e+18, 1.1891e+19, 1.1237e+19, 6.9592e+18,
        1.2008e+19, 9.9228e+18, 1.8012e+19, 9.8775e+18, 7.6148e+18, 8.3679e+18,
        8.2982e+18, 1.2202e+19, 1.0284e+19, 2.1667e+19, 2.4611e+19, 1.4941e+19,
        2.1022e+19, 1.0857e+19, 1.1918e+19, 1.8260e+19, 1.1245e+19, 1.1025e+19,
        1.5391e+19, 4.6130e+18, 2.0031e+19, 1.5292e+19, 1.1635e+19, 5.9353e+18,
        8.0048e+18, 1.8827e+19, 3.7787e+18, 1.1933e+19, 2.5005e+19, 4.1507e+18,
        7.7529e+18, 1.6179e+19, 5.2444e+18, 8.3824e+18, 1.0367e+19, 1.5413e+19,
        2.1184e+19, 1.2275e+19, 1.8793e+19, 1.1500e+19, 6.2186e+18, 1.2204e+19,
        1.1079e+19, 1.7760e+19, 9.4696e+18, 8.8008e+18, 1.7241e+19, 1.1261e+19,
        1.5458e+19, 7.8941e+18, 1.0181e+19, 6.9686e+18, 1.1623e+19, 1.0114e+19,
        1.7100e+19, 8.5631e+18, 1.6054e+19, 8.7780e+18])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0204e+18, 1.2476e+18, 2.7141e+16, 3.4766e+17, 2.7991e+17, 2.2435e+18,
        1.7856e+17, 5.6398e+17, 1.9012e+17, 4.5407e+16, 2.2083e+17, 1.7004e+18,
        2.7554e+17, 1.4369e+17, 1.3458e+17, 1.2518e+17, 2.6245e+17, 1.8163e+18,
        3.6573e+16, 1.5269e+17, 2.8675e+17, 3.8741e+17, 7.7843e+16, 4.7332e+17,
        3.1558e+17, 3.8455e+17, 2.4213e+18, 1.4426e+18, 7.1664e+17, 2.9295e+16,
        5.2711e+17, 5.6938e+17, 1.4110e+18, 1.9200e+17, 4.0208e+16, 2.6286e+17,
        1.3633e+18, 1.1273e+18, 6.9497e+17, 8.0140e+16, 5.6998e+17, 5.0699e+16,
        9.6086e+16, 3.5627e+17, 1.2862e+18, 2.3096e+17, 9.6023e+16, 1.2563e+17,
        1.2694e+17, 7.1691e+17, 4.4048e+16, 1.5047e+17, 2.5115e+16, 7.8364e+17,
        1.9942e+17, 1.9517e+17, 6.1370e+16, 2.6255e+17, 1.7951e+17, 1.3332e+17,
        1.7332e+16, 6.1559e+17, 3.7069e+17, 5.1030e+17, 2.6775e+17, 1.7988e+17,
        2.5433e+17, 1.1610e+17, 4.2141e+16, 2.0579e+17, 1.2942e+17, 1.1900e+17,
        1.5473e+17, 7.7539e+17, 7.4562e+17, 1.8446e+17, 6.6180e+16, 9.5331e+17,
        9.3119e+16, 1.4584e+17, 1.1853e+17, 4.0570e+16, 5.0232e+16, 4.9458e+17,
        8.3138e+17, 1.2185e+17, 1.3417e+19, 4.0261e+16, 7.4300e+16, 1.5237e+17,
        6.7140e+17, 9.2539e+16, 5.0741e+18, 3.3428e+17, 3.9437e+17, 1.3113e+17,
        4.8689e+16, 1.3641e+17, 1.1931e+17, 2.6106e+17, 4.4811e+16, 9.6206e+16,
        4.3307e+17, 1.0733e+17, 1.8674e+17, 8.2675e+17, 8.3661e+17, 6.9684e+17,
        3.3621e+17, 1.5073e+17, 4.6217e+17, 8.8592e+16, 9.9598e+16, 2.2454e+17,
        5.4207e+16, 3.1044e+17, 1.5955e+17, 3.8718e+16, 2.2023e+17, 5.0287e+17,
        2.9151e+19, 8.4008e+17, 3.1880e+17, 7.1646e+16, 1.7239e+17, 1.6761e+17,
        1.2425e+18, 5.4237e+17, 1.3449e+18, 3.5597e+17, 4.4336e+16, 6.5720e+17,
        1.9559e+16, 1.4845e+18, 7.5559e+16, 3.9999e+17, 3.9542e+17, 2.7767e+16,
        5.8874e+16, 8.3999e+17, 2.6890e+17, 7.4326e+17, 3.9264e+17, 8.4250e+16,
        3.4172e+17, 4.8934e+17, 3.8144e+16, 1.0380e+18, 1.5967e+17, 1.0999e+18,
        2.1031e+17, 1.3607e+17, 3.3583e+17, 1.4716e+19, 2.5937e+17, 8.8637e+17,
        1.0990e+17, 2.3826e+16, 9.4608e+17, 4.3624e+16, 2.8166e+17, 2.7799e+17,
        3.5980e+17, 1.9599e+18, 1.5600e+17, 8.7974e+17, 4.3644e+16, 1.0477e+17,
        4.4193e+17, 1.9422e+19, 1.1683e+17, 1.2474e+17, 1.5211e+17, 1.4138e+17,
        1.0418e+18, 7.0841e+16, 1.1559e+17, 5.9745e+17, 2.7256e+18, 4.7913e+17,
        1.9432e+17, 4.1438e+17, 5.7742e+17, 8.7548e+17, 1.4069e+18, 1.2861e+17,
        2.4580e+17, 2.5999e+17, 1.5629e+17, 1.6574e+17, 2.5491e+17, 1.3310e+17,
        1.1204e+17, 1.9449e+17, 1.7165e+18, 3.9090e+17, 5.2216e+16, 2.8183e+17,
        3.9602e+17, 1.1410e+18, 4.0751e+17, 8.7890e+17, 8.3195e+17, 3.4039e+17,
        1.1046e+18, 3.5553e+16, 1.6621e+17, 1.5068e+18, 1.8256e+17, 1.8474e+17,
        8.5753e+16, 4.3536e+17, 5.1018e+16, 9.7416e+16, 1.7292e+17, 1.8572e+17,
        1.2178e+17, 2.4126e+18, 1.9570e+17, 4.4587e+17, 1.7161e+17, 1.0924e+16,
        7.5985e+17, 1.5853e+17, 2.9654e+17, 2.1008e+17, 1.2146e+17, 5.8735e+17,
        1.0636e+17, 1.8550e+17, 8.9397e+17, 9.9130e+16, 2.7290e+17, 6.6694e+18,
        5.0237e+17, 1.2196e+18, 6.5649e+16, 9.0043e+16, 1.6287e+16, 1.0591e+17,
        1.1304e+17, 9.8952e+16, 2.2637e+17, 3.2802e+16, 4.3922e+16, 4.6154e+17,
        1.2182e+17, 7.4379e+17, 1.1432e+17, 2.3313e+17, 2.3979e+18, 1.0056e+17,
        2.7775e+17, 5.0779e+16, 1.3079e+17, 1.7901e+17])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.2264e+17, 1.1752e+18, 2.2665e+17, 3.6581e+17, 4.8984e+17, 2.3779e+18,
        5.3945e+16, 6.4486e+17, 1.0331e+17, 1.4449e+17, 1.2751e+17, 1.3488e+18,
        1.6390e+17, 1.9761e+17, 2.6459e+17, 2.6231e+17, 3.0652e+17, 2.0570e+18,
        6.8340e+16, 1.5297e+17, 2.6700e+17, 2.5682e+17, 2.1307e+17, 5.0690e+17,
        4.0723e+17, 3.5618e+17, 2.3472e+18, 1.3180e+18, 8.8446e+17, 1.0258e+17,
        6.6415e+17, 2.4354e+17, 1.7316e+18, 1.6193e+16, 1.0922e+17, 1.2489e+17,
        1.2736e+18, 1.0178e+18, 7.1181e+17, 1.7721e+17, 6.1628e+17, 8.3303e+16,
        7.0214e+16, 3.5078e+17, 1.1919e+18, 1.0753e+17, 1.7989e+17, 1.4527e+17,
        6.0261e+16, 1.0289e+18, 3.4582e+16, 7.0210e+16, 8.4408e+16, 6.9588e+17,
        2.6791e+16, 1.4627e+17, 1.1852e+17, 2.7964e+17, 2.1049e+17, 2.5908e+17,
        2.7208e+17, 5.3319e+17, 5.2795e+17, 4.6359e+17, 2.8055e+17, 1.5689e+17,
        1.9281e+17, 1.3222e+17, 7.7515e+16, 1.9266e+17, 9.5982e+16, 2.4237e+17,
        1.4292e+17, 6.3568e+17, 4.6303e+17, 1.6841e+17, 1.1100e+17, 1.0880e+18,
        2.2254e+17, 2.6770e+17, 3.6141e+17, 8.0382e+16, 7.2260e+16, 4.7081e+17,
        9.4353e+17, 1.6960e+17, 1.3131e+19, 1.4548e+17, 2.4941e+17, 3.8094e+16,
        3.7104e+17, 1.9895e+17, 5.0644e+18, 2.3670e+17, 2.9071e+17, 2.0941e+17,
        5.3533e+16, 9.3582e+16, 8.2714e+16, 2.5926e+17, 1.1367e+17, 2.4319e+17,
        1.0083e+17, 3.9971e+16, 1.6087e+17, 1.1111e+18, 6.9677e+17, 8.0508e+17,
        3.9877e+17, 1.2526e+17, 3.9018e+17, 1.0519e+17, 4.3100e+16, 1.0844e+17,
        1.0529e+17, 3.7629e+17, 2.6599e+16, 2.0337e+17, 2.6053e+17, 5.2171e+17,
        2.9365e+19, 6.9497e+17, 2.8498e+17, 4.8744e+16, 2.0586e+17, 3.7308e+17,
        1.3666e+18, 2.6561e+17, 9.3172e+17, 2.9083e+17, 1.8260e+17, 4.4614e+17,
        5.9943e+16, 1.4643e+18, 2.7784e+16, 2.5329e+17, 1.2791e+18, 5.1150e+16,
        1.0726e+17, 1.1412e+18, 2.7890e+17, 8.4844e+17, 4.1341e+17, 1.1432e+17,
        2.7211e+17, 4.2398e+17, 1.9899e+17, 8.9709e+17, 1.3875e+17, 1.1565e+18,
        7.0289e+16, 7.1582e+16, 1.5098e+17, 1.4912e+19, 9.2474e+16, 9.6278e+17,
        8.4478e+16, 1.4762e+17, 9.5158e+17, 1.6609e+17, 3.0310e+17, 4.7023e+17,
        1.8144e+17, 1.9427e+18, 1.3434e+17, 8.6175e+17, 1.2087e+17, 1.1345e+17,
        6.0640e+17, 1.9436e+19, 1.5717e+17, 8.9468e+16, 2.9937e+17, 5.7750e+16,
        8.7061e+17, 3.5109e+16, 9.6425e+16, 4.5575e+17, 2.6065e+18, 3.8631e+17,
        2.0825e+17, 2.1816e+17, 4.9356e+17, 1.0639e+18, 1.5117e+18, 2.5172e+17,
        1.1797e+17, 7.9143e+16, 1.0552e+17, 3.5681e+17, 9.6086e+16, 1.3032e+17,
        6.8785e+16, 5.3252e+17, 2.0184e+18, 5.3979e+17, 1.7874e+17, 4.0171e+17,
        4.0845e+17, 1.1911e+18, 2.1412e+17, 1.0062e+18, 9.4801e+17, 3.0149e+17,
        1.2458e+18, 1.4130e+17, 6.7528e+16, 1.5661e+18, 2.3510e+17, 1.5378e+17,
        7.4840e+16, 5.1762e+17, 9.7125e+16, 7.0235e+16, 6.1445e+16, 2.6111e+17,
        2.5028e+16, 2.0962e+18, 2.3888e+17, 3.3861e+17, 8.5423e+16, 7.2384e+16,
        6.6128e+17, 7.2464e+16, 4.5292e+17, 2.4982e+17, 1.1066e+17, 4.1902e+17,
        9.9687e+16, 9.8909e+16, 7.3609e+17, 2.2454e+17, 1.4436e+17, 6.3497e+18,
        3.2040e+17, 1.2041e+18, 1.6008e+17, 9.6048e+16, 6.2872e+16, 6.0780e+16,
        2.4900e+17, 1.8313e+17, 2.0853e+17, 1.8208e+17, 6.6907e+16, 7.2209e+17,
        4.4447e+16, 8.1324e+17, 1.6273e+17, 2.7886e+17, 2.3255e+18, 1.0509e+17,
        9.5132e+16, 1.4762e+17, 7.2239e+16, 1.8297e+17])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2864e+17, 2.9653e+17, 2.9653e+17, 5.6115e+17, 1.5465e+17, 3.6369e+17,
        3.8163e+17, 3.0177e+17, 1.4843e+17, 2.4867e+17, 1.2260e+17, 1.7176e+17,
        3.0397e+17, 3.5457e+17, 4.2858e+17, 1.7748e+17, 4.4750e+17, 6.0821e+17,
        3.6046e+17, 5.6507e+17, 2.3206e+17, 1.5540e+17, 1.3729e+17, 1.5234e+17,
        5.3716e+17, 4.1082e+17, 2.7231e+17, 3.8911e+17, 4.3312e+17, 2.2378e+17,
        2.0452e+17, 1.0306e+17, 2.6811e+17, 2.0161e+17, 2.6527e+17, 9.8225e+16,
        2.5792e+17, 3.8358e+17, 1.2410e+17, 2.7674e+17, 3.4779e+17, 1.9369e+17,
        1.3316e+17, 2.2710e+17, 2.4850e+17, 1.7596e+17, 3.2414e+17, 3.7466e+17,
        2.0940e+17, 1.9658e+17, 1.0043e+17, 2.4770e+17, 1.5918e+17, 2.5697e+17,
        3.3754e+17, 1.9260e+17, 2.4741e+17, 2.4076e+17, 2.0308e+17, 5.3173e+17,
        6.6377e+17, 3.7048e+17, 2.5188e+17, 2.4985e+17])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.8930e+17, 5.1519e+17, 6.2213e+17, 4.2261e+17, 2.6110e+17, 3.0764e+17,
        3.6435e+17, 2.1456e+17, 3.5716e+17, 3.0297e+17, 3.9903e+17, 3.4267e+17,
        1.5361e+17, 5.8646e+17, 1.9481e+17, 3.5708e+17, 1.4735e+17, 1.5676e+17,
        6.4050e+17, 2.7388e+17, 3.5712e+17, 1.2526e+17, 5.7291e+17, 3.0896e+17,
        4.0914e+17, 4.8680e+17, 2.2265e+17, 2.2792e+17, 4.2441e+17, 2.4469e+17,
        4.2153e+17, 2.7366e+17, 3.8870e+17, 3.9484e+17, 2.5607e+17, 4.2034e+17,
        2.7532e+17, 3.0471e+17, 5.8102e+17, 2.7095e+17, 5.1492e+17, 1.9506e+17,
        3.6804e+17, 1.5389e+17, 4.7099e+17, 2.2332e+17, 1.9688e+17, 2.3684e+17,
        4.4295e+17, 4.4326e+17, 6.1214e+17, 4.9498e+17, 6.8857e+17, 3.1513e+17,
        2.7008e+17, 2.5095e+17, 1.9703e+17, 3.6702e+17, 4.3431e+17, 3.2644e+17,
        4.7595e+17, 2.5275e+17, 2.4056e+17, 5.3345e+17])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.6084e+15, 2.1121e+16, 8.2756e+15, 5.7978e+15, 1.4060e+16, 1.0552e+16,
        2.3422e+15, 1.3362e+16, 8.0608e+15, 1.7017e+15, 5.3126e+15, 1.0916e+16,
        4.4154e+16, 1.1733e+16, 4.2173e+15, 4.1379e+16, 8.0330e+15, 8.4207e+15,
        2.8537e+15, 5.0079e+16, 6.6625e+15, 7.6313e+15, 4.4960e+15, 1.1517e+16,
        6.1058e+15, 7.1331e+15, 5.8177e+15, 1.1584e+16, 1.1518e+16, 8.3708e+15,
        5.8585e+15, 3.8187e+15, 9.4477e+15, 7.8908e+15, 7.1485e+15, 4.4040e+16,
        8.6782e+15, 1.4025e+16, 1.5505e+16, 1.3785e+15, 8.7049e+15, 1.1124e+16,
        1.1361e+16, 2.4669e+15, 1.9739e+16, 6.6387e+15, 4.8604e+15, 5.7617e+16,
        1.3153e+16, 1.4835e+16, 1.2838e+15, 1.7540e+15, 5.1485e+15, 1.7892e+16,
        3.8970e+16, 1.1563e+16, 8.8332e+16, 7.5993e+15, 1.3924e+16, 1.9127e+16,
        7.7959e+15, 1.0454e+16, 2.8765e+16, 1.1820e+16, 7.3527e+15, 5.0739e+15,
        1.1086e+16, 1.1210e+16, 1.1738e+16, 1.4275e+16, 1.7945e+15, 1.8500e+16,
        4.9741e+15, 1.2263e+16, 2.3670e+15, 6.2610e+16, 6.5077e+15, 1.8288e+16,
        1.4648e+16, 7.1552e+15, 6.3632e+15, 1.0901e+15, 9.8021e+15, 8.4642e+15,
        3.6424e+16, 1.1226e+16, 5.8042e+15, 1.9019e+15, 7.2653e+15, 9.9797e+15,
        9.6775e+15, 2.8168e+16, 4.4566e+16, 9.4997e+15, 4.7976e+16, 6.5397e+15,
        1.1593e+16, 7.6585e+15, 7.0693e+15, 1.2534e+16, 7.2224e+15, 6.5016e+15,
        4.6366e+15, 2.7273e+15, 1.0947e+16, 8.3284e+16, 1.4829e+16, 7.8620e+15,
        1.1871e+16, 1.2075e+16, 1.1676e+16, 7.3090e+15, 8.3193e+15, 4.6181e+15,
        1.0413e+16, 1.3116e+16, 1.9666e+15, 5.6233e+16, 6.9507e+16, 1.1684e+17,
        2.0387e+17, 5.0996e+16, 1.0333e+16, 7.1248e+15, 1.7268e+16, 7.4653e+15,
        3.2262e+16, 5.5351e+15, 1.4824e+16, 2.1436e+16, 1.4271e+16, 1.0341e+16,
        4.4744e+15, 3.8550e+16, 2.5233e+15, 8.1970e+15, 1.5489e+18, 1.3975e+16,
        1.0984e+16, 5.3755e+15, 8.2185e+15, 1.2184e+16, 1.0106e+16, 6.4613e+15,
        1.5552e+16, 1.1158e+16, 2.0752e+15, 5.0562e+15, 3.7204e+15, 7.3052e+15,
        5.8691e+15, 1.0749e+16, 5.4054e+15, 4.3015e+15, 2.9878e+16, 1.3795e+16,
        1.5774e+16, 1.0570e+16, 6.8341e+15, 1.6523e+16, 1.0121e+16, 1.9086e+16,
        1.2341e+16, 1.1444e+16, 8.2636e+15, 9.7087e+15, 1.3705e+16, 2.3620e+15,
        4.7356e+15, 7.9882e+15, 3.8090e+16, 1.9766e+16, 7.9936e+15, 7.4528e+15,
        2.4283e+16, 1.1922e+16, 1.3024e+16, 7.5818e+15, 3.9869e+15, 4.7650e+16,
        6.6143e+15, 4.4906e+16, 5.9039e+15, 9.9481e+15, 4.7403e+16, 9.5894e+15,
        1.4856e+16, 1.1670e+16, 1.4371e+16, 7.6301e+15, 6.0033e+15, 5.8012e+16,
        1.6603e+15, 6.6593e+16, 1.1271e+17, 9.2048e+15, 6.1122e+15, 5.2552e+15,
        8.7781e+15, 6.8499e+15, 2.8005e+15, 4.9013e+15, 9.7875e+15, 1.4084e+16,
        4.6405e+15, 1.4739e+16, 4.6500e+15, 8.8798e+15, 2.4743e+16, 5.1022e+15,
        1.2132e+15, 1.4191e+16, 1.0254e+16, 8.4298e+15, 9.7076e+15, 5.3897e+15,
        9.1065e+15, 6.3252e+15, 5.9618e+15, 2.4038e+15, 1.5116e+15, 9.6617e+15,
        1.6792e+16, 7.3593e+15, 8.6191e+15, 4.4869e+16, 2.2221e+15, 3.7889e+16,
        2.2818e+16, 7.0857e+15, 1.2799e+16, 7.0406e+15, 1.2337e+16, 6.5278e+16,
        6.6415e+15, 1.0246e+16, 2.0727e+15, 1.0704e+17, 1.2617e+16, 7.8864e+15,
        8.6444e+15, 7.8158e+15, 3.8220e+16, 1.0621e+16, 3.9418e+15, 2.0700e+16,
        9.3861e+15, 2.5821e+16, 7.2363e+15, 7.1188e+15, 1.3467e+16, 1.2769e+16,
        5.2916e+15, 2.4278e+16, 4.9985e+16, 1.4015e+16])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3798e+16, 2.7959e+16, 1.9524e+16, 1.3093e+16, 1.6400e+16, 1.3850e+16,
        1.2625e+16, 1.5825e+16, 1.9546e+16, 1.4782e+16, 1.5186e+16, 1.1637e+16,
        2.1929e+16, 2.2330e+16, 1.9261e+16, 2.4211e+16, 2.2405e+16, 1.4637e+16,
        3.1853e+16, 2.8392e+16, 2.0437e+16, 1.6307e+16, 1.7004e+16, 2.6195e+16,
        1.1190e+16, 2.4010e+16, 2.2511e+16, 1.5986e+16, 1.0958e+16, 1.4970e+16,
        1.7050e+16, 1.1073e+16, 1.3982e+16, 2.0402e+16, 2.2466e+16, 8.8747e+15,
        1.3164e+16, 1.5637e+16, 1.9214e+16, 2.5796e+16, 2.9637e+16, 1.5911e+16,
        1.4728e+16, 1.7979e+16, 1.4936e+16, 2.4744e+16, 1.3785e+16, 1.0932e+16,
        1.6721e+16, 2.8144e+16, 1.1492e+16, 2.2167e+16, 2.4376e+16, 1.6871e+16,
        1.6888e+16, 2.3745e+16, 1.7443e+16, 2.9521e+16, 1.8800e+16, 1.4606e+16,
        2.4026e+16, 1.9355e+16, 3.0276e+16, 2.2470e+16])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.4645e+16, 1.8411e+16, 1.4577e+16, 2.2868e+16, 3.2407e+16, 2.5865e+16,
        1.0290e+16, 1.7194e+16, 1.4459e+16, 2.4218e+16, 2.4180e+16, 1.6879e+16,
        1.8210e+16, 2.0676e+16, 2.5968e+16, 1.5896e+16, 1.6482e+16, 2.1420e+16,
        2.3449e+16, 2.2748e+16, 2.7907e+16, 1.4281e+16, 2.8253e+16, 3.3255e+16,
        2.2820e+16, 2.5058e+16, 1.9315e+16, 1.6380e+16, 1.2358e+16, 1.8606e+16,
        1.8565e+16, 1.9704e+16, 1.4189e+16, 2.5078e+16, 1.8821e+16, 1.1857e+16,
        2.1461e+16, 2.9746e+16, 2.2007e+16, 2.4986e+16, 2.3148e+16, 1.8300e+16,
        1.8397e+16, 1.2337e+16, 2.4806e+16, 1.5404e+16, 2.1019e+16, 2.0118e+16,
        2.2459e+16, 2.9748e+16, 1.3667e+16, 2.9652e+16, 1.6638e+16, 3.1263e+16,
        1.5236e+16, 2.0946e+16, 1.6355e+16, 2.0875e+16, 1.7556e+16, 2.2064e+16,
        1.9836e+16, 2.2227e+16, 2.0012e+16, 1.9627e+16])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.1617e+14, 8.5882e+14, 1.3728e+14, 5.6219e+14, 6.8262e+14, 2.6310e+15,
        2.1612e+14, 5.0919e+15, 1.2764e+15, 4.8216e+13, 8.5880e+14, 6.6037e+14,
        3.9605e+14, 2.6606e+14, 8.8656e+14, 1.2371e+15, 3.3697e+14, 6.1370e+14,
        1.6395e+14, 7.6153e+14, 4.1134e+14, 5.1300e+14, 1.6548e+14, 3.0444e+15,
        3.3066e+14, 3.7662e+14, 6.6781e+14, 9.4597e+14, 3.7285e+14, 1.0905e+14,
        2.5433e+14, 2.8615e+14, 1.1839e+14, 2.9189e+14, 9.2185e+14, 1.5769e+15,
        1.4394e+15, 3.2400e+15, 1.1666e+15, 7.7389e+13, 1.3288e+15, 3.2639e+14,
        4.4921e+14, 9.5061e+13, 5.1563e+14, 2.1957e+14, 2.2290e+15, 3.6374e+15,
        3.8375e+14, 3.8039e+14, 1.6722e+14, 8.1660e+14, 9.6086e+14, 8.1442e+14,
        1.9393e+15, 3.1319e+14, 2.6699e+16, 3.9420e+14, 2.1018e+15, 2.5854e+15,
        1.5780e+14, 1.3147e+14, 2.2715e+16, 7.6955e+14, 2.0293e+14, 1.9341e+14,
        5.6269e+14, 1.4063e+14, 5.0278e+14, 9.3745e+14, 1.3944e+14, 1.5858e+14,
        1.3589e+14, 7.2033e+14, 4.7546e+14, 1.7440e+15, 1.3608e+15, 3.9200e+14,
        1.3627e+15, 7.7435e+14, 5.4184e+14, 2.2552e+14, 2.7961e+14, 8.4234e+14,
        2.3332e+14, 3.5040e+14, 1.4389e+14, 4.3118e+13, 5.3795e+14, 5.0065e+14,
        7.8281e+14, 3.1750e+14, 7.1086e+14, 7.8513e+14, 1.1287e+15, 1.0002e+14,
        4.6042e+14, 2.7031e+14, 4.4314e+14, 1.7459e+14, 5.6679e+14, 4.6687e+14,
        4.3461e+14, 1.0279e+14, 7.0257e+14, 2.8607e+14, 1.9096e+14, 5.7617e+14,
        2.6449e+14, 8.0826e+14, 4.1116e+14, 2.1590e+14, 2.9355e+14, 1.0464e+15,
        8.5373e+14, 4.3559e+15, 1.0514e+14, 9.6940e+13, 2.5605e+14, 7.1659e+14,
        1.7773e+15, 1.0255e+15, 1.1419e+14, 1.5000e+14, 6.7254e+14, 1.9964e+14,
        1.8085e+16, 2.5438e+14, 2.8886e+14, 1.0979e+15, 1.7087e+14, 4.9679e+14,
        7.0859e+13, 2.8889e+14, 2.7740e+14, 3.7387e+14, 3.5341e+15, 6.7209e+14,
        6.7620e+14, 2.2100e+14, 3.1641e+14, 2.9166e+14, 1.8121e+16, 6.7735e+14,
        4.7631e+14, 3.8636e+14, 6.0724e+13, 2.5271e+14, 4.2866e+13, 1.2528e+15,
        6.0994e+14, 2.2160e+14, 8.3080e+15, 1.1744e+15, 1.8324e+14, 1.7356e+14,
        1.9262e+14, 1.3698e+14, 1.6937e+14, 2.1258e+15, 2.6500e+14, 2.9879e+14,
        4.7300e+14, 2.7029e+14, 9.0841e+14, 1.2544e+15, 6.1753e+14, 1.0677e+15,
        1.3348e+14, 1.0768e+15, 1.7973e+14, 4.1608e+15, 3.2156e+14, 3.4239e+14,
        8.7020e+15, 4.8012e+14, 2.4592e+14, 1.9496e+14, 2.2605e+14, 5.3215e+14,
        1.7360e+14, 2.9913e+15, 3.4639e+14, 9.0878e+14, 3.7604e+14, 9.6996e+14,
        5.9643e+15, 9.3283e+14, 4.0234e+14, 2.0586e+14, 2.5453e+14, 5.0232e+14,
        6.7732e+13, 1.6984e+14, 3.9955e+14, 4.1734e+14, 4.9553e+14, 2.9588e+14,
        2.4774e+14, 4.5136e+14, 2.0759e+14, 7.2356e+13, 1.7013e+15, 2.7342e+14,
        2.4397e+14, 4.9719e+14, 2.1884e+15, 3.4738e+14, 1.1061e+14, 7.2143e+14,
        1.8210e+14, 2.3143e+14, 4.7118e+14, 1.8484e+14, 7.3628e+14, 4.9081e+14,
        2.7413e+14, 4.3238e+14, 5.0761e+14, 1.6796e+15, 1.3441e+14, 1.3027e+15,
        1.8528e+14, 2.4760e+14, 1.9458e+15, 4.9251e+14, 8.4313e+13, 4.6694e+14,
        1.4045e+16, 3.3383e+14, 1.2924e+14, 4.6034e+14, 1.8141e+14, 7.4791e+14,
        4.3120e+14, 4.7394e+14, 4.3556e+13, 3.8197e+14, 2.2753e+15, 3.2662e+14,
        8.5324e+14, 1.2469e+14, 2.1600e+15, 4.8783e+14, 4.8508e+13, 2.9831e+14,
        5.0864e+14, 3.6325e+14, 4.2815e+14, 4.4132e+14, 6.5769e+15, 6.4821e+14,
        4.4922e+14, 1.6928e+14, 3.5583e+14, 2.2457e+14])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.7671e+14, 5.6175e+14, 5.9625e+14, 7.3938e+14, 4.2746e+14, 4.1679e+14,
        3.8792e+14, 3.4937e+14, 5.7138e+14, 7.6129e+14, 9.5771e+14, 3.6716e+14,
        5.3046e+14, 5.3917e+14, 4.7610e+14, 7.5167e+14, 3.7638e+14, 8.7751e+14,
        6.1509e+14, 4.9388e+14, 7.1108e+14, 3.3791e+14, 4.2515e+14, 4.1516e+14,
        4.2866e+14, 6.3520e+14, 3.7423e+14, 6.0589e+14, 7.1312e+14, 4.3346e+14,
        3.7231e+14, 5.7057e+14, 2.9141e+14, 5.4105e+14, 5.2365e+14, 5.3252e+14,
        3.3523e+14, 5.5965e+14, 8.1563e+14, 4.9748e+14, 4.0626e+14, 3.4752e+14,
        3.3385e+14, 6.5309e+14, 5.8122e+14, 2.2889e+14, 7.2669e+14, 2.5934e+14,
        5.4525e+14, 3.5634e+14, 7.4629e+14, 5.7928e+14, 3.5939e+14, 7.0660e+14,
        7.2135e+14, 6.7933e+14, 3.2225e+14, 5.6765e+14, 6.2715e+14, 5.3891e+14,
        8.0493e+14, 4.7640e+14, 5.4192e+14, 4.4536e+14, 5.4780e+14, 6.6662e+14,
        6.7839e+14, 4.3244e+14, 4.7018e+14, 3.1492e+14, 5.5037e+14, 5.1207e+14,
        4.9540e+14, 6.1687e+14, 5.3262e+14, 8.2688e+14, 6.8931e+14, 5.6491e+14,
        4.2442e+14, 4.9396e+14, 4.4340e+14, 6.1585e+14, 6.3504e+14, 6.3792e+14,
        6.7025e+14, 4.3326e+14, 1.0484e+15, 6.4569e+14, 8.7903e+14, 5.4772e+14,
        7.7384e+14, 6.3682e+14, 6.3138e+14, 3.7814e+14, 6.0016e+14, 9.3960e+14,
        4.6514e+14, 5.8211e+14, 4.3913e+14, 4.2443e+14, 3.6956e+14, 3.6778e+14,
        4.8964e+14, 8.3605e+14, 8.8996e+14, 3.7749e+14, 3.5132e+14, 9.0525e+14,
        5.5956e+14, 6.6601e+14, 4.5258e+14, 9.9509e+14, 2.3252e+14, 3.3908e+14,
        4.0754e+14, 3.5161e+14, 6.2773e+14, 4.9808e+14, 5.8618e+14, 6.8835e+14,
        4.6927e+14, 4.8802e+14, 2.9485e+14, 9.0466e+14, 6.2083e+14, 6.2954e+14,
        4.1948e+14, 5.0957e+14])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.1886e+14, 5.1707e+14, 1.1706e+15, 3.2945e+14, 7.2519e+14, 7.2227e+14,
        1.0045e+15, 7.6982e+14, 1.0491e+15, 5.5900e+14, 1.4201e+15, 6.9303e+14,
        9.1189e+14, 8.1931e+14, 5.8406e+14, 7.8221e+14, 7.2077e+14, 9.8582e+14,
        6.1614e+14, 6.3382e+14, 4.7939e+14, 8.2206e+14, 8.0980e+14, 7.9082e+14,
        4.4453e+14, 8.0138e+14, 6.8404e+14, 8.0950e+14, 7.4923e+14, 6.5533e+14,
        5.2627e+14, 6.3421e+14, 4.5419e+14, 9.8093e+14, 6.4523e+14, 6.5666e+14,
        6.8364e+14, 7.5459e+14, 4.3623e+14, 3.9665e+14, 8.7401e+14, 7.2001e+14,
        8.6287e+14, 3.4754e+14, 5.1919e+14, 4.7859e+14, 7.9804e+14, 8.4608e+14,
        4.7112e+14, 5.2615e+14, 5.6314e+14, 7.5696e+14, 9.8993e+14, 5.3787e+14,
        5.8614e+14, 5.5422e+14, 1.0422e+15, 8.4578e+14, 8.4079e+14, 7.8651e+14,
        6.2347e+14, 1.4892e+15, 3.8978e+14, 4.8853e+14, 8.7485e+14, 6.7793e+14,
        4.0676e+14, 3.0096e+14, 4.7197e+14, 7.5487e+14, 4.9011e+14, 1.3987e+15,
        6.4938e+14, 5.2003e+14, 1.1395e+15, 6.6368e+14, 1.0834e+15, 5.5209e+14,
        5.6269e+14, 6.0090e+14, 7.3164e+14, 4.3978e+14, 3.8418e+14, 6.3650e+14,
        2.4938e+14, 4.8615e+14, 1.0682e+15, 6.9279e+14, 9.2169e+14, 1.1135e+15,
        8.1951e+14, 6.5825e+14, 1.1593e+15, 8.0921e+14, 1.2163e+15, 4.4002e+14,
        5.6117e+14, 4.0598e+14, 7.4097e+14, 4.5241e+14, 7.2006e+14, 5.4682e+14,
        9.1588e+14, 8.2033e+14, 1.0220e+15, 4.0785e+14, 2.9067e+14, 6.5555e+14,
        6.9386e+14, 6.5554e+14, 6.5530e+14, 1.2792e+15, 5.8822e+14, 4.3940e+14,
        7.8440e+14, 9.6792e+14, 5.2763e+14, 3.7347e+14, 6.3054e+14, 9.5178e+14,
        6.0538e+14, 6.5170e+14, 8.4442e+14, 6.6167e+14, 8.8780e+14, 8.3374e+14,
        5.9686e+14, 6.3852e+14])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.3351e+12, 4.1380e+13, 4.1445e+13, 1.1573e+14, 3.4976e+12, 8.0044e+12,
        2.6008e+12, 2.6798e+12, 5.5341e+12, 3.7938e+13, 9.4780e+13, 2.1585e+13,
        3.1568e+13, 1.1127e+13, 1.1446e+13, 4.2262e+13, 1.9806e+13, 3.4983e+13,
        1.9307e+13, 5.9349e+12, 6.5544e+12, 1.0549e+13, 1.5616e+12, 2.5426e+12,
        7.9416e+11, 2.6683e+14, 1.2804e+13, 3.0398e+13, 9.9835e+12, 3.5857e+14,
        2.4201e+13, 3.9371e+13, 9.9414e+13, 1.6840e+13, 3.9374e+13, 1.5754e+13,
        5.4158e+12, 6.6415e+12, 4.4678e+12, 4.8454e+13, 5.0507e+12, 2.0626e+13,
        3.8196e+13, 5.7464e+14, 3.4235e+13, 4.3411e+12, 4.3591e+13, 2.8572e+13,
        5.8861e+12, 3.7484e+13, 7.1160e+12, 6.9940e+12, 1.6253e+12, 8.1961e+12,
        8.9771e+13, 1.6898e+13, 2.3775e+13, 2.3783e+13, 3.3841e+12, 1.5318e+13,
        9.6936e+12, 1.8220e+13, 2.8762e+13, 3.7734e+13, 1.9249e+13, 4.5956e+12,
        7.2210e+12, 7.9509e+14, 6.0819e+12, 2.9995e+12, 1.9665e+13, 9.9718e+12,
        1.6463e+13, 1.9852e+13, 5.8971e+13, 2.9093e+12, 3.6237e+12, 2.0745e+13,
        1.8881e+13, 8.6386e+12, 7.5145e+12, 1.2472e+13, 1.6108e+13, 1.9443e+13,
        1.4484e+13, 1.9211e+13, 5.0396e+12, 5.5233e+13, 6.4104e+13, 3.1192e+13,
        1.0035e+14, 9.5297e+12, 5.7802e+12, 3.8359e+13, 3.7077e+13, 1.1802e+14,
        8.2019e+12, 1.2674e+13, 5.1544e+13, 8.4240e+12, 6.6091e+14, 6.0150e+12,
        9.7908e+13, 1.7010e+13, 2.2718e+13, 9.2355e+12, 1.5994e+14, 2.3311e+14,
        1.4017e+14, 1.8535e+12, 1.4869e+12, 6.3752e+12, 1.5014e+13, 5.3880e+12,
        2.5325e+13, 3.5927e+12, 3.9218e+13, 1.1008e+13, 2.5856e+12, 4.9105e+12,
        2.4084e+12, 1.0972e+13, 1.0920e+13, 1.7974e+12, 9.4099e+12, 9.2898e+13,
        7.7720e+13, 8.3535e+12, 1.1967e+12, 3.7612e+13, 1.9000e+13, 2.8241e+13,
        1.6606e+13, 4.1739e+12, 7.0659e+12, 8.0521e+12, 6.4780e+12, 3.7172e+13,
        9.1677e+13, 9.0516e+12, 7.1609e+13, 1.7466e+12, 3.0451e+14, 1.2486e+13,
        1.2346e+13, 1.8114e+13, 2.8392e+13, 1.7533e+14, 8.6244e+12, 2.3054e+13,
        1.5099e+13, 4.1456e+13, 3.9213e+12, 3.5553e+14, 7.3867e+12, 4.0795e+13,
        2.5680e+13, 3.3927e+13, 8.7722e+12, 2.1042e+13, 5.4945e+13, 5.3447e+12,
        1.0604e+13, 1.7975e+13, 2.9676e+13, 2.2810e+13, 9.8444e+12, 2.1285e+13,
        8.1488e+12, 2.2923e+12, 7.2639e+12, 5.6354e+12, 1.3712e+13, 5.7634e+13,
        3.1745e+13, 6.0213e+13, 8.1185e+13, 4.7118e+12, 9.7744e+13, 1.0022e+13,
        2.0382e+13, 9.8383e+12, 3.8997e+13, 5.7952e+13, 1.6727e+13, 2.8953e+13,
        3.5475e+12, 1.0129e+13, 6.3531e+12, 2.9982e+13, 7.8227e+12, 1.1855e+14,
        8.8312e+13, 1.6680e+13, 5.6875e+12, 2.8540e+13, 1.1003e+13, 3.8627e+13,
        8.4394e+13, 9.5094e+12, 9.0052e+12, 4.4026e+12, 1.9726e+12, 2.4178e+12,
        5.4176e+12, 8.2659e+12, 4.0570e+12, 5.7786e+13, 7.3649e+13, 1.3363e+12,
        2.1725e+13, 2.0613e+13, 2.2949e+12, 6.7447e+12, 1.0053e+12, 8.5381e+13,
        1.2588e+13, 4.6130e+12, 1.3523e+13, 2.0334e+13, 2.8226e+12, 1.0825e+13,
        1.4026e+13, 1.6887e+13, 8.1840e+12, 3.4512e+13, 3.7472e+12, 6.8817e+12,
        2.1369e+13, 1.1167e+13, 5.2277e+13, 2.6242e+13, 9.7659e+12, 1.3261e+14,
        4.1361e+13, 2.1587e+13, 6.3540e+12, 5.0233e+13, 3.3352e+13, 7.8638e+13,
        4.4020e+12, 8.2096e+13, 5.5072e+12, 3.7742e+12, 1.5548e+13, 4.1040e+13,
        3.7542e+12, 5.1634e+12, 6.4740e+12, 1.9818e+14, 8.9557e+12, 1.1546e+13,
        6.6487e+12, 2.2237e+13, 3.5212e+12, 7.1192e+12, 3.8636e+12, 1.1950e+14,
        8.0183e+13, 3.1382e+12, 3.6494e+13, 3.1437e+13, 1.7740e+12, 2.7897e+13,
        3.4001e+12, 1.9033e+12, 9.3384e+12, 2.2958e+13, 3.5107e+13, 1.7369e+12,
        4.7678e+13, 9.3081e+12, 9.4037e+11, 2.5586e+13, 4.2687e+14, 1.7940e+13,
        1.0514e+14, 6.2206e+13, 1.8058e+13, 1.3773e+13, 1.1301e+14, 3.9914e+12,
        7.2797e+12, 2.4050e+13, 6.0119e+12, 5.5003e+12, 1.8362e+13, 2.6640e+12,
        2.6974e+12, 3.3091e+12, 1.1871e+13, 6.7692e+13, 7.2861e+12, 9.7893e+12,
        2.4384e+13, 5.9215e+13, 3.0473e+12, 1.0806e+13, 1.3122e+13, 7.2978e+13,
        2.8960e+14, 2.0634e+13, 8.4436e+12, 3.9014e+12, 2.1386e+13, 3.6326e+13,
        6.3336e+14, 5.1452e+13, 1.2633e+14, 2.1749e+13, 4.5404e+12, 4.0948e+13,
        1.7762e+14, 2.2287e+13, 3.5109e+13, 1.7331e+14, 3.3832e+13, 8.4482e+12,
        2.9823e+12, 4.5852e+13, 4.9243e+13, 1.0057e+13, 1.4279e+13, 3.3358e+13,
        4.6918e+12, 3.9402e+12, 4.6119e+13, 2.0105e+13, 3.3704e+13, 8.5673e+14,
        1.2739e+13, 3.9674e+12, 1.3665e+13, 2.3769e+12, 1.3683e+13, 1.2831e+13,
        2.9788e+13, 9.6084e+13, 1.1936e+12, 4.1537e+12, 1.4120e+13, 8.8397e+12,
        1.5324e+13, 2.3877e+13, 6.2305e+13, 1.2088e+13, 1.0358e+14, 4.3023e+13,
        3.5132e+12, 6.9249e+12, 1.9766e+13, 7.6193e+12, 1.8030e+13, 3.0395e+13,
        2.4683e+13, 1.0249e+13, 1.1555e+13, 4.0958e+13, 8.7170e+13, 1.2603e+14,
        5.5132e+13, 5.0112e+12, 7.2069e+12, 8.7597e+12, 4.1712e+13, 4.0297e+13,
        6.0804e+12, 3.8964e+12, 3.8258e+12, 6.7260e+12, 1.2562e+13, 5.7694e+13,
        2.8763e+13, 5.8116e+12, 3.3148e+13, 4.7533e+13, 1.0549e+14, 1.4445e+13,
        1.2599e+13, 4.7065e+13, 1.0459e+15, 7.7150e+12, 1.0902e+13, 1.6973e+13,
        7.2088e+12, 1.8938e+12, 1.0410e+14, 4.1132e+13, 5.0306e+13, 6.3170e+13,
        3.7694e+13, 3.8080e+13, 2.5052e+13, 5.3100e+12, 1.2090e+13, 3.6744e+12,
        4.9419e+13, 2.5267e+13, 1.8633e+13, 4.4691e+12, 1.9137e+13, 3.8692e+13,
        2.7398e+12, 2.3633e+13, 1.3534e+13, 2.5447e+13, 1.9549e+13, 1.8846e+13,
        7.3221e+12, 1.0429e+15, 1.0094e+12, 1.3550e+13, 9.2851e+13, 1.5227e+13,
        4.1028e+12, 2.9158e+13, 1.8507e+13, 8.9996e+12, 4.0424e+13, 2.7908e+13,
        1.0168e+13, 4.0641e+12, 2.9229e+13, 6.8852e+12, 2.8537e+13, 4.2206e+12,
        8.5491e+13, 1.1460e+13, 7.5556e+13, 1.5094e+13, 2.2962e+13, 8.6015e+12,
        8.7485e+12, 4.9138e+13, 1.4728e+13, 6.4144e+12, 6.3125e+13, 1.0506e+13,
        3.5458e+13, 3.0474e+13, 4.4379e+13, 1.0123e+13, 6.4143e+12, 5.9313e+12,
        1.4989e+13, 4.3664e+13, 9.5920e+13, 3.5009e+13, 2.4265e+13, 3.4238e+13,
        4.8710e+13, 9.1051e+11, 2.1635e+13, 1.3510e+13, 9.4493e+12, 2.1857e+13,
        8.2048e+12, 1.0707e+13, 4.1070e+12, 3.5419e+12, 6.6382e+13, 1.1923e+13,
        1.3307e+13, 1.3158e+13, 1.3429e+13, 3.1173e+13, 3.5333e+12, 4.8478e+12,
        4.6644e+12, 1.2018e+14, 4.5695e+12, 2.0185e+13, 3.8020e+12, 1.3294e+13,
        5.1537e+13, 1.1833e+12, 9.1829e+14, 9.5560e+12, 1.4061e+13, 3.4942e+13,
        7.3537e+12, 1.4982e+13, 4.3352e+12, 1.8957e+12, 1.3979e+13, 1.4725e+13,
        2.1134e+13, 3.0203e+12, 4.2947e+12, 3.1311e+13, 6.3124e+12, 1.4041e+13,
        8.2245e+12, 1.6057e+13, 2.5898e+13, 1.3964e+13, 8.4781e+12, 5.6618e+12,
        3.6261e+12, 1.0928e+13, 1.4695e+13, 3.6659e+13, 3.8232e+13, 1.0953e+13,
        6.5892e+13, 1.8801e+14, 1.0867e+12, 8.4194e+12, 1.4188e+13, 1.1845e+13,
        1.8232e+12, 4.4827e+13])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.4994e+12, 6.1406e+13, 5.5910e+13, 1.3558e+14, 1.5923e+13, 1.8838e+13,
        2.0747e+13, 1.6238e+13, 1.7291e+13, 3.7536e+13, 9.5163e+13, 2.9730e+12,
        2.4681e+13, 1.8151e+13, 7.5286e+12, 4.9272e+13, 9.2137e+12, 4.2642e+13,
        1.0695e+13, 1.8552e+13, 1.6581e+13, 2.2424e+13, 2.3722e+13, 7.2070e+12,
        7.5858e+12, 2.5397e+14, 1.5611e+13, 5.4937e+13, 1.7577e+13, 3.7450e+14,
        1.6065e+13, 2.2218e+13, 1.0323e+14, 7.4210e+12, 4.7033e+13, 1.0598e+13,
        3.1801e+13, 1.4822e+13, 1.9391e+13, 5.6479e+13, 5.8292e+12, 2.7683e+13,
        6.5182e+13, 5.7818e+14, 2.8482e+13, 6.3356e+12, 2.9301e+13, 1.4834e+13,
        1.0074e+13, 4.0052e+13, 2.0362e+13, 7.8677e+12, 7.5355e+12, 1.3191e+13,
        8.6993e+13, 1.4982e+13, 1.4961e+13, 3.2163e+13, 7.7707e+12, 2.0932e+13,
        2.4483e+13, 1.9918e+13, 1.9428e+13, 2.9602e+13, 2.3820e+13, 9.2198e+12,
        1.0510e+13, 7.7615e+14, 2.5624e+13, 5.6602e+12, 1.5541e+13, 1.3322e+13,
        1.8517e+13, 4.6778e+13, 4.4013e+13, 1.0189e+13, 1.8030e+13, 1.4383e+13,
        1.7913e+13, 1.8424e+13, 1.6259e+13, 1.8710e+13, 1.1491e+13, 5.4368e+12,
        3.2932e+13, 3.3907e+13, 8.9211e+12, 9.3406e+13, 6.1426e+13, 1.5819e+13,
        1.0235e+14, 8.2268e+12, 1.5846e+13, 5.3600e+13, 7.4751e+12, 1.1270e+14,
        1.0451e+13, 1.1218e+13, 8.6186e+13, 3.8027e+12, 6.5389e+14, 9.3104e+12,
        9.8943e+13, 3.0455e+13, 4.6769e+13, 2.0495e+13, 1.6457e+14, 2.3097e+14,
        1.3967e+14, 1.0539e+13, 1.2101e+13, 7.9704e+12, 6.5584e+12, 1.6021e+12,
        1.4194e+13, 4.9407e+12, 3.5325e+13, 1.1321e+13, 1.4954e+13, 1.1437e+13,
        4.9946e+13, 4.6606e+13, 8.4482e+12, 1.9544e+13, 1.4641e+13, 8.6722e+13,
        8.5978e+13, 1.4041e+13, 4.9958e+12, 6.5668e+13, 3.8934e+13, 3.9490e+13,
        4.5790e+13, 2.6144e+13, 1.8987e+13, 2.3211e+13, 1.4302e+13, 3.0748e+13,
        5.9539e+13, 2.0374e+13, 8.7656e+13, 6.2520e+12, 3.1657e+14, 1.8451e+13,
        1.7061e+13, 2.3498e+13, 2.2458e+13, 1.7469e+14, 1.8794e+13, 3.5052e+13,
        7.0781e+12, 4.4327e+13, 1.1086e+13, 3.6208e+14, 7.1187e+12, 3.8316e+13,
        2.7773e+13, 2.3585e+13, 2.1588e+13, 5.5453e+13, 7.5268e+13, 1.1856e+13,
        1.6274e+13, 1.1911e+13, 4.3848e+13, 2.8206e+13, 1.1508e+13, 2.4490e+13,
        1.0318e+13, 1.1945e+13, 2.5689e+13, 1.1434e+13, 1.9517e+13, 6.8411e+13,
        3.1979e+13, 6.1573e+13, 7.8103e+13, 5.8155e+12, 1.1620e+14, 1.6691e+13,
        2.8453e+13, 5.9568e+12, 4.0664e+13, 6.8686e+13, 5.0440e+12, 3.3561e+13,
        1.2808e+13, 2.0339e+13, 2.4980e+13, 3.9753e+13, 3.2844e+13, 1.1897e+14,
        3.7741e+13, 1.5824e+13, 5.6273e+12, 3.3908e+13, 1.1151e+13, 5.1621e+13,
        7.3379e+13, 6.9837e+12, 1.1623e+13, 2.2988e+13, 1.5751e+13, 1.5381e+13,
        1.1153e+13, 7.8817e+12, 1.9419e+13, 8.7285e+13, 6.3461e+13, 1.7934e+13,
        2.4712e+13, 2.6958e+13, 1.0986e+13, 1.1019e+13, 1.6325e+13, 7.5786e+13,
        3.5493e+13, 1.3843e+13, 1.6881e+13, 2.3631e+13, 1.0128e+13, 2.1782e+13,
        3.1756e+12, 1.2178e+13, 6.9969e+12, 4.0787e+13, 1.9084e+13, 9.9415e+12,
        1.2450e+13, 1.4127e+13, 5.2985e+13, 2.7293e+13, 1.0093e+13, 1.0005e+14,
        3.8357e+13, 2.9149e+13, 1.1333e+13, 6.3830e+13, 4.4032e+13, 1.0286e+14,
        9.1295e+12, 8.6408e+13, 3.2486e+13, 1.7895e+13, 2.9746e+13, 3.9656e+13,
        1.5494e+13, 6.3994e+12, 1.5346e+13, 3.1397e+14, 9.9567e+12, 2.4761e+13,
        1.6770e+13, 2.5459e+13, 8.8031e+12, 1.5731e+13, 9.9624e+12, 1.2369e+14,
        7.6929e+13, 3.2323e+13, 4.4187e+13, 7.0078e+13, 7.9267e+12, 1.4959e+13,
        2.5300e+13, 1.5498e+13, 5.8621e+12, 1.6058e+13, 3.4685e+13, 1.6024e+13,
        5.0504e+13, 1.0210e+13, 2.2694e+12, 2.4216e+13, 4.4953e+14, 2.6649e+13,
        1.1881e+14, 8.0876e+13, 3.4083e+13, 1.0049e+13, 1.4643e+14, 1.6278e+13,
        2.1097e+13, 2.0521e+13, 1.0939e+13, 5.8170e+12, 1.4097e+13, 6.7104e+12,
        7.6777e+12, 6.7886e+12, 1.0085e+13, 6.8793e+13, 1.0461e+13, 1.4571e+13,
        1.5866e+13, 4.2291e+13, 1.0385e+13, 1.8562e+13, 2.8577e+13, 8.8169e+13,
        2.7248e+14, 3.3809e+13, 8.1130e+12, 2.4300e+13, 2.9826e+13, 3.5252e+13,
        6.4029e+14, 4.5203e+13, 1.2958e+14, 1.7597e+13, 1.2478e+13, 4.7666e+13,
        2.1481e+14, 2.2456e+13, 4.2644e+13, 1.8282e+14, 2.7660e+13, 1.0548e+13,
        7.1443e+12, 5.0178e+13, 3.7053e+13, 2.2081e+13, 3.9306e+13, 3.2286e+13,
        1.5407e+13, 3.1126e+13, 2.6433e+13, 1.9847e+13, 3.0739e+13, 8.5244e+14,
        1.3695e+13, 1.1240e+13, 2.1465e+13, 2.0785e+13, 3.1623e+13, 8.4938e+12,
        4.5721e+13, 9.4634e+13, 1.2255e+13, 1.2744e+13, 1.1527e+13, 1.3924e+13,
        3.2302e+13, 2.6384e+13, 6.5817e+13, 1.6434e+13, 1.3251e+14, 5.2004e+13,
        1.1002e+13, 2.2440e+13, 2.8062e+13, 1.9068e+13, 2.1045e+13, 3.3365e+13,
        8.9301e+12, 2.4546e+13, 1.4571e+13, 3.9822e+13, 1.4819e+14, 1.4220e+14,
        4.8055e+13, 2.4230e+13, 1.3328e+13, 1.7847e+13, 5.5338e+13, 4.7500e+13,
        7.5078e+12, 1.6503e+13, 7.2271e+12, 5.2987e+12, 2.6283e+13, 4.9143e+13,
        3.9886e+12, 1.0158e+13, 3.3849e+13, 3.8292e+13, 8.1875e+13, 1.9451e+13,
        7.4964e+12, 3.5219e+13, 1.0616e+15, 2.4199e+13, 2.3005e+13, 7.9294e+12,
        1.4084e+13, 8.4179e+12, 1.0822e+14, 4.8864e+13, 5.5943e+13, 6.3737e+13,
        4.9468e+13, 6.3732e+13, 4.8119e+13, 1.0654e+13, 8.6107e+12, 9.9203e+12,
        5.5252e+13, 2.5697e+13, 1.3360e+13, 1.1688e+13, 1.5843e+13, 4.3062e+13,
        1.3940e+13, 4.5782e+13, 2.6358e+13, 6.6569e+12, 2.0419e+13, 2.5655e+13,
        1.4987e+13, 1.0356e+15, 5.3378e+12, 2.1172e+13, 6.4039e+13, 2.5079e+13,
        1.9745e+13, 3.5540e+13, 2.9593e+13, 9.9879e+12, 6.6421e+13, 7.6219e+12,
        1.6867e+13, 9.1705e+12, 9.5805e+12, 2.5878e+13, 2.5149e+13, 3.1191e+13,
        1.1080e+14, 8.8665e+12, 8.0458e+13, 1.4262e+13, 1.7241e+13, 1.4167e+13,
        2.1386e+13, 4.9601e+13, 2.8685e+13, 5.1339e+12, 7.3364e+13, 1.3820e+13,
        2.6931e+13, 4.2084e+13, 5.4491e+13, 2.6693e+13, 7.2533e+12, 1.0824e+13,
        2.7643e+13, 5.0897e+13, 1.1645e+14, 5.7604e+13, 4.4643e+13, 3.6468e+13,
        8.3462e+13, 9.2444e+12, 3.9169e+13, 2.0799e+13, 8.0424e+12, 2.1999e+13,
        2.9822e+13, 1.7553e+13, 6.0378e+12, 4.1397e+12, 5.0494e+13, 1.4039e+13,
        1.6151e+13, 1.6450e+13, 6.6693e+12, 3.3248e+13, 1.1435e+13, 1.2397e+13,
        7.8348e+12, 1.2887e+14, 1.9893e+13, 3.6687e+13, 3.3290e+13, 1.2430e+13,
        6.3743e+13, 8.7933e+12, 9.0911e+14, 5.1467e+12, 8.8675e+12, 3.7049e+13,
        1.9675e+13, 2.2568e+13, 5.9171e+12, 1.8832e+13, 1.3305e+13, 5.0719e+13,
        1.7891e+13, 7.8506e+12, 2.5659e+13, 2.4126e+13, 1.4700e+13, 2.7929e+13,
        2.4169e+13, 2.1387e+13, 5.1934e+13, 2.1166e+13, 2.7346e+13, 5.5560e+12,
        1.2081e+13, 1.1515e+13, 3.2329e+13, 4.5237e+13, 4.3176e+13, 1.9555e+13,
        5.5524e+13, 1.8144e+14, 6.9532e+12, 9.6310e+12, 1.7258e+13, 2.4757e+13,
        1.1115e+13, 5.4156e+13])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.3079e+13, 2.6592e+13, 2.6515e+13, 1.4410e+13, 2.2435e+13, 2.8255e+13,
        2.2579e+13, 4.2220e+13, 1.8935e+13, 6.4159e+13, 3.1991e+13, 5.2664e+13,
        1.5867e+13, 2.7392e+13, 1.6284e+13, 2.5493e+13, 3.8506e+13, 3.0876e+13,
        1.1848e+13, 1.3168e+13, 5.3837e+13, 1.1284e+13, 1.4267e+13, 5.3820e+13,
        3.3034e+13, 3.5464e+13, 2.6168e+13, 3.8223e+13, 1.8515e+13, 2.4372e+13,
        4.4701e+13, 2.6037e+13, 3.7508e+13, 2.7674e+13, 5.0531e+13, 1.4861e+13,
        6.3257e+13, 2.5585e+13, 2.9038e+13, 4.2229e+13, 2.2492e+13, 2.6165e+13,
        1.2240e+13, 1.4818e+13, 2.0725e+13, 2.5989e+13, 2.5308e+13, 3.3649e+13,
        1.2082e+13, 1.2797e+13, 4.2060e+13, 3.4695e+13, 3.8883e+13, 3.3568e+13,
        1.6311e+13, 2.4312e+13, 4.0489e+13, 1.8580e+13, 4.2027e+13, 1.2740e+13,
        3.9215e+13, 7.3980e+12, 2.7472e+13, 7.5150e+12, 2.4487e+13, 4.1659e+13,
        3.1817e+13, 2.6273e+13, 1.8397e+13, 2.0952e+13, 4.2242e+13, 1.3502e+13,
        1.7526e+13, 1.9226e+13, 5.5915e+13, 2.9525e+13, 5.7931e+13, 2.5739e+13,
        1.3142e+13, 1.3970e+13, 4.7027e+13, 1.4388e+13, 2.6407e+13, 7.6029e+13,
        3.0237e+13, 3.8231e+13, 2.4300e+13, 7.0487e+13, 3.1288e+13, 1.2068e+13,
        3.0019e+13, 1.3399e+13, 2.3956e+13, 2.4641e+13, 2.5700e+13, 1.5484e+13,
        2.8694e+13, 2.0754e+13, 5.2546e+13, 2.4947e+13, 2.5077e+13, 2.2565e+13,
        1.7785e+13, 1.8564e+13, 1.9293e+13, 1.7599e+13, 3.1781e+13, 4.0985e+13,
        4.5969e+13, 2.2982e+13, 2.8645e+13, 1.7425e+13, 1.9325e+13, 2.0809e+13,
        2.1484e+13, 4.0917e+13, 1.2021e+13, 4.9458e+13, 4.6483e+13, 4.4879e+13,
        3.3332e+13, 5.5937e+13, 1.6531e+13, 1.1196e+13, 2.7681e+13, 3.0341e+13,
        1.2592e+13, 4.7987e+13])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.9226e+13, 3.7854e+13, 4.2547e+13, 2.7922e+13, 4.4651e+13, 1.1175e+14,
        6.0750e+13, 4.6015e+13, 7.8236e+13, 2.4713e+13, 4.2778e+13, 6.8934e+13,
        5.6322e+13, 4.4870e+13, 2.3491e+13, 6.1102e+13, 4.0799e+13, 2.3944e+13,
        7.2488e+13, 5.7676e+13, 3.2161e+13, 2.6037e+13, 3.8690e+13, 3.2639e+13,
        2.8152e+13, 4.7274e+13, 3.8935e+13, 2.0006e+13, 5.4888e+13, 6.4962e+13,
        4.3583e+13, 6.7419e+13, 1.8051e+13, 5.7263e+13, 3.3431e+13, 3.3228e+13,
        2.5162e+13, 3.1477e+13, 4.7641e+13, 3.1360e+13, 4.5352e+13, 8.0859e+13,
        3.1558e+13, 3.4203e+13, 3.2340e+13, 2.8471e+13, 5.3234e+13, 2.8146e+13,
        6.6015e+13, 5.2994e+13, 2.6508e+13, 5.7685e+13, 5.1602e+13, 7.2450e+13,
        2.7417e+13, 4.5647e+13, 2.8187e+13, 5.0080e+13, 2.5786e+13, 2.1464e+13,
        4.1279e+13, 1.8921e+13, 7.3653e+13, 4.4117e+13, 5.3784e+13, 5.6400e+13,
        3.1077e+13, 2.7680e+13, 5.7053e+13, 3.9380e+13, 5.4726e+13, 5.2359e+13,
        2.7741e+13, 1.6351e+13, 5.2121e+13, 8.0660e+13, 6.5377e+13, 3.3270e+13,
        7.5193e+13, 7.3819e+13, 4.8124e+13, 3.6695e+13, 6.4924e+13, 1.3629e+13,
        3.0696e+13, 2.2518e+13, 4.7399e+13, 7.2008e+13, 2.4438e+13, 2.6035e+13,
        5.5438e+13, 4.7135e+13, 5.5260e+13, 7.2386e+13, 3.1851e+13, 6.2057e+13,
        5.4339e+13, 4.3808e+13, 4.5363e+13, 2.8508e+13, 3.6493e+13, 4.0180e+13,
        2.1720e+13, 2.1637e+13, 4.5678e+13, 1.7289e+13, 4.4327e+13, 1.9144e+13,
        1.4283e+13, 7.2465e+13, 3.7475e+13, 4.5097e+13, 4.1595e+13, 3.3305e+13,
        2.6065e+13, 3.0546e+13, 4.6795e+13, 3.6244e+13, 8.6616e+13, 4.7560e+13,
        2.2053e+13, 3.0628e+13, 4.2389e+13, 3.0350e+13, 1.8249e+13, 3.4495e+13,
        4.6952e+13, 4.9894e+13])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.7103e+12, 1.4841e+12, 5.0960e+12, 6.3609e+12, 5.9229e+11, 4.8046e+12,
        3.1474e+12, 4.0525e+12, 4.0913e+12, 2.9160e+12, 1.1214e+13, 5.3430e+12,
        1.7177e+12, 9.2557e+11, 1.8609e+12, 2.8152e+12, 2.3224e+12, 1.4844e+12,
        2.5189e+12, 2.7327e+12, 3.8886e+12, 2.9684e+12, 1.9054e+11, 8.9292e+10,
        7.1702e+11, 1.6552e+12, 3.8140e+12, 1.4162e+12, 6.4518e+12, 1.0848e+12,
        2.6497e+12, 1.3336e+13, 6.9996e+11, 1.5085e+12, 4.5089e+12, 7.3131e+12,
        1.8783e+12, 4.3165e+12, 2.1407e+12, 9.0491e+12, 5.4282e+12, 6.7673e+12,
        2.0488e+12, 2.8586e+12, 2.4607e+12, 1.6922e+12, 1.9931e+12, 3.5847e+12,
        3.4830e+12, 8.8851e+12, 2.5541e+12, 4.5514e+12, 4.0302e+12, 4.1084e+12,
        1.8100e+12, 1.3221e+12, 4.1778e+12, 3.6753e+12, 9.6289e+12, 7.0467e+11,
        3.2094e+13, 3.1954e+12, 1.8036e+12, 3.6384e+12, 2.3659e+12, 2.7833e+12,
        3.1827e+12, 2.9753e+12, 2.2726e+13, 4.8772e+12, 2.2704e+12, 4.2778e+12,
        5.6921e+12, 5.3678e+12, 2.6759e+12, 4.5733e+12, 5.1019e+12, 6.9235e+12,
        4.8204e+12, 7.0138e+11, 1.1621e+12, 1.7228e+12, 1.2299e+12, 5.1121e+11,
        3.5953e+12, 1.6591e+12, 7.1737e+12, 1.7835e+12, 1.9984e+12, 2.5134e+12,
        3.6863e+12, 5.9801e+12, 4.3137e+12, 2.5921e+12, 2.6109e+13, 9.3567e+11,
        2.3467e+12, 4.4936e+12, 2.3306e+13, 1.3806e+11, 4.1150e+12, 8.0160e+11,
        1.0894e+12, 2.5794e+12, 6.1538e+12, 1.0164e+12, 5.1286e+12, 4.6607e+12,
        3.7085e+12, 1.5568e+12, 4.6760e+11, 1.6846e+12, 4.2455e+12, 6.6660e+11,
        4.5098e+12, 2.6648e+12, 2.0819e+12, 3.0126e+12, 3.0150e+12, 1.1015e+12,
        4.3451e+13, 2.5590e+13, 7.8586e+11, 5.8363e+11, 7.2409e+11, 3.5143e+12,
        2.5807e+12, 8.4738e+12, 3.6337e+12, 2.1207e+12, 7.7756e+12, 1.4999e+12,
        2.7139e+12, 4.9244e+11, 1.4733e+12, 3.2184e+12, 4.4875e+13, 5.7047e+12,
        5.9509e+13, 5.5062e+11, 2.3430e+12, 4.4895e+12, 2.9651e+12, 2.9895e+12,
        2.1502e+12, 3.5901e+12, 1.7217e+12, 3.1592e+12, 5.2060e+11, 1.7920e+13,
        2.1128e+12, 7.7336e+11, 8.3002e+11, 4.2653e+12, 2.9946e+12, 5.2738e+12,
        2.9356e+12, 7.5692e+11, 1.2151e+12, 3.2787e+12, 1.8522e+13, 6.0427e+11,
        4.6340e+12, 6.1590e+12, 1.4956e+13, 4.8220e+12, 4.8106e+12, 1.3719e+12,
        2.0839e+12, 7.8850e+11, 2.7688e+12, 2.9416e+12, 3.1494e+12, 2.3819e+12,
        2.2497e+12, 3.2889e+12, 3.1645e+12, 2.0247e+12, 2.4382e+12, 5.3466e+12,
        7.2128e+12, 1.9723e+12, 1.2191e+12, 3.8915e+12, 3.7777e+12, 3.8198e+12,
        1.9485e+12, 3.0077e+12, 4.1937e+12, 3.9789e+12, 1.0833e+12, 3.8073e+12,
        5.9514e+13, 1.0060e+12, 3.6190e+12, 1.7069e+12, 2.0789e+12, 2.3798e+12,
        3.0369e+12, 6.2338e+11, 2.8200e+12, 3.0018e+12, 3.7934e+12, 1.8472e+11,
        2.9899e+12, 1.7960e+12, 4.8900e+12, 1.7833e+13, 5.4059e+12, 2.8217e+12,
        5.1367e+12, 3.7228e+12, 1.0966e+12, 4.0097e+12, 3.0078e+11, 2.4496e+12,
        1.9110e+12, 3.9868e+12, 2.5707e+12, 1.3850e+12, 2.6808e+11, 1.6177e+12,
        3.8208e+11, 1.9038e+12, 2.6114e+12, 3.7315e+12, 2.5579e+12, 1.1755e+12,
        2.3421e+12, 4.4546e+12, 2.2417e+12, 1.6496e+12, 3.5431e+11, 2.5027e+12,
        1.4752e+12, 3.6198e+12, 2.7645e+12, 2.3947e+12, 2.7911e+12, 2.3235e+12,
        1.5247e+12, 1.4470e+12, 2.4480e+11, 5.0290e+12, 1.0614e+12, 3.2534e+12,
        6.2422e+12, 3.4747e+12, 1.0822e+12, 1.1470e+14, 4.1370e+12, 7.9174e+12,
        2.3773e+12, 2.9964e+12, 7.5644e+12, 2.9790e+11, 4.4997e+12, 1.0949e+13,
        5.3718e+12, 1.3087e+12, 1.5606e+12, 9.2694e+13, 4.7581e+11, 3.5780e+12,
        6.4469e+12, 2.2070e+12, 4.4397e+11, 6.3004e+12, 1.9733e+12, 3.0748e+12,
        6.7804e+11, 2.9003e+12, 2.6802e+12, 4.6480e+12, 2.6106e+12, 1.9231e+12,
        4.3323e+12, 2.2188e+12, 3.8852e+12, 2.4041e+12, 1.1817e+13, 2.5497e+11,
        2.4875e+12, 1.0664e+12, 3.5844e+12, 6.8147e+11, 4.3766e+12, 1.3721e+11,
        4.8548e+11, 2.9783e+12, 1.0856e+12, 2.2141e+12, 4.7546e+12, 1.3408e+12,
        3.8439e+12, 4.7525e+12, 1.8459e+12, 4.4995e+12, 6.8437e+12, 9.1959e+12,
        1.2695e+12, 1.1279e+12, 4.1983e+12, 1.9714e+12, 2.4774e+12, 2.3002e+12,
        1.5561e+12, 2.6499e+12, 4.0141e+12, 5.1775e+12, 2.2194e+12, 4.3328e+12,
        4.5528e+13, 2.5480e+12, 5.9711e+12, 1.0447e+13, 2.2957e+12, 4.0808e+12,
        5.7359e+12, 2.0541e+12, 2.6940e+12, 2.0272e+12, 1.5263e+12, 4.7769e+12,
        1.3699e+12, 5.9715e+11, 5.9689e+12, 3.9015e+12, 4.1395e+12, 1.6821e+12,
        8.1298e+12, 1.7238e+11, 3.3192e+12, 2.6420e+12, 1.7804e+12, 3.1508e+12,
        1.4182e+12, 1.1053e+12, 3.8699e+12, 6.2386e+11, 1.6332e+12, 4.2457e+12,
        5.6207e+11, 4.7428e+12, 2.3166e+12, 2.7602e+12, 2.5891e+13, 4.0670e+12,
        3.0396e+13, 3.8122e+12, 1.8046e+12, 3.5362e+12, 3.3010e+12, 3.2241e+12,
        4.2415e+12, 7.3089e+12, 3.4759e+12, 1.9010e+12, 4.7489e+13, 4.2332e+12,
        2.3782e+11, 5.9016e+12, 3.6555e+12, 9.6954e+11, 7.3075e+12, 4.4598e+12,
        3.3120e+12, 5.5285e+12, 2.3111e+12, 9.7111e+11, 3.9867e+12, 5.8802e+12,
        1.1877e+12, 1.2700e+12, 3.6849e+12, 1.7056e+12, 1.5184e+12, 9.3018e+12,
        2.9593e+12, 2.7380e+12, 2.8996e+12, 6.8327e+11, 7.1245e+12, 2.6481e+12,
        1.6649e+12, 4.6082e+12, 2.8836e+12, 2.8666e+12, 2.5443e+12, 1.2524e+12,
        1.5771e+12, 3.0150e+12, 1.8986e+12, 1.1823e+13, 3.8405e+12, 3.0893e+11,
        6.5482e+12, 5.7355e+12, 3.3767e+12, 6.7406e+11, 5.0667e+11, 4.4105e+12,
        2.6228e+12, 3.8028e+12, 5.1787e+12, 7.2816e+12, 5.5123e+11, 7.6921e+12,
        4.3052e+12, 4.5317e+12, 5.2081e+11, 1.4335e+12, 1.9210e+12, 2.2894e+12,
        1.7789e+12, 1.6656e+12, 4.7908e+12, 4.5794e+11, 1.5481e+13, 1.1091e+12,
        7.4364e+11, 9.3990e+12, 5.2569e+12, 6.1947e+12, 1.8898e+12, 4.6712e+12,
        1.7718e+12, 1.0374e+13, 8.3716e+12, 3.7326e+12, 1.8263e+12, 2.0400e+12,
        5.0339e+12, 5.2422e+12, 2.3285e+12, 3.7313e+12, 5.4092e+12, 2.2185e+12,
        6.7127e+12, 4.7913e+12, 1.8407e+12, 6.7160e+11, 8.0357e+12, 9.5015e+11,
        1.6088e+12, 7.7569e+12, 2.7907e+12, 1.6542e+12, 3.2634e+12, 3.5786e+12,
        1.5403e+12, 4.1048e+12, 3.4657e+12, 2.5641e+13, 2.0194e+11, 4.7422e+12,
        4.7841e+12, 1.1343e+12, 4.2389e+11, 8.0251e+12, 5.9885e+12, 2.2568e+12,
        5.8395e+12, 2.9359e+12, 5.0682e+12, 1.0701e+12, 1.9115e+12, 3.6929e+12,
        2.5374e+12, 1.8884e+12, 5.2253e+11, 1.5274e+12, 3.0573e+12, 1.4794e+12,
        2.9968e+12, 1.8474e+12, 6.8061e+12, 2.9524e+12, 4.7934e+12, 1.8083e+12,
        1.2730e+12, 2.1934e+12, 2.3576e+12, 1.3080e+12, 3.3172e+12, 2.7164e+12,
        6.9402e+11, 4.0612e+12, 5.6654e+12, 1.4823e+13, 3.6185e+12, 2.2510e+12,
        4.6288e+11, 2.6005e+12, 1.2589e+12, 1.1483e+12, 3.0998e+12, 7.9276e+11,
        3.4760e+12, 2.7872e+12, 4.4351e+12, 2.5455e+12, 1.0290e+12, 4.1435e+12,
        8.6341e+12, 6.9581e+12, 1.7884e+12, 2.4686e+12, 3.6744e+12, 8.5728e+11,
        2.8719e+12, 3.3284e+12])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.2969e+12, 6.9214e+12, 1.1926e+13, 6.8091e+12, 1.1293e+13, 9.1642e+12,
        9.3357e+12, 6.5885e+12, 1.2681e+13, 4.8075e+12, 4.0456e+12, 5.7170e+12,
        1.4802e+13, 7.9588e+12, 3.7890e+12, 9.6600e+12, 6.5565e+12, 1.0748e+13,
        7.4180e+12, 3.8644e+12, 1.0409e+13, 9.0855e+12, 1.8989e+12, 5.7633e+12,
        1.2922e+13, 3.8720e+12, 6.8906e+12, 7.0936e+12, 5.8784e+12, 8.7931e+12,
        8.6784e+12, 5.3537e+12, 6.3701e+12, 5.5355e+12, 1.1372e+13, 7.1501e+12,
        8.0029e+12, 5.5991e+12, 1.6265e+13, 1.0236e+13, 8.4499e+12, 1.4738e+13,
        1.0335e+13, 1.0193e+13, 9.8700e+12, 3.9424e+12, 1.6578e+13, 4.3074e+12,
        3.0218e+12, 9.4359e+12, 2.0003e+13, 1.4963e+13, 1.0834e+13, 1.6764e+13,
        1.1631e+13, 6.2426e+12, 2.7980e+12, 8.4919e+12, 1.0632e+13, 3.6382e+12,
        9.4137e+12, 5.8321e+12, 6.1863e+12, 2.6815e+12, 8.1803e+12, 5.3897e+12,
        5.4027e+12, 4.6926e+12, 8.7449e+12, 8.5792e+12, 3.0437e+12, 1.4604e+13,
        1.3915e+13, 4.1805e+12, 1.2097e+13, 8.1415e+12, 4.6810e+12, 3.5544e+12,
        1.6631e+13, 7.1292e+12, 6.0866e+12, 1.5036e+13, 8.4219e+12, 9.2208e+12,
        4.3899e+12, 5.1682e+12, 1.0265e+13, 5.0570e+12, 4.2951e+12, 7.2018e+12,
        8.5519e+12, 5.0562e+12, 1.2319e+13, 1.3469e+13, 5.0761e+12, 1.4904e+13,
        4.8844e+12, 4.4251e+12, 1.0253e+13, 1.0563e+13, 7.8513e+12, 5.4044e+12,
        8.1181e+12, 3.5106e+12, 4.9977e+12, 2.5280e+12, 1.7219e+13, 4.9904e+12,
        6.5353e+12, 6.8195e+12, 1.3278e+13, 6.4154e+12, 5.1722e+12, 1.6176e+13,
        1.7473e+13, 4.4851e+12, 7.1472e+12, 1.2953e+13, 8.8481e+12, 3.3014e+12,
        5.7499e+12, 6.7992e+12, 6.1343e+12, 1.1951e+13, 1.1463e+13, 8.6720e+12,
        4.0726e+12, 7.8253e+12])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6026e+13, 1.9338e+13, 2.0603e+13, 1.7392e+13, 9.1327e+12, 1.6557e+13,
        1.3085e+13, 9.0355e+12, 2.0139e+13, 1.5907e+13, 1.0590e+13, 1.1344e+13,
        6.6261e+12, 1.0295e+13, 2.2699e+13, 4.6958e+12, 8.1265e+12, 7.3926e+12,
        4.1718e+12, 1.1099e+13, 1.2453e+13, 5.0192e+12, 1.7820e+13, 1.0618e+13,
        1.3285e+13, 8.9817e+12, 3.9707e+12, 1.0083e+13, 2.1148e+13, 1.1242e+13,
        1.5693e+13, 3.5790e+12, 1.1272e+13, 1.0217e+13, 1.9334e+13, 1.1096e+13,
        9.2807e+12, 9.9539e+12, 2.1845e+13, 3.6767e+13, 1.7902e+13, 1.9485e+13,
        7.0497e+12, 2.0109e+13, 2.1849e+13, 5.6243e+12, 1.0513e+13, 9.8648e+12,
        2.3828e+13, 8.0768e+12, 1.1661e+13, 6.4072e+12, 1.2339e+13, 1.1585e+13,
        7.3271e+12, 6.5882e+12, 1.8403e+13, 1.0015e+13, 7.7919e+12, 4.2519e+12,
        1.9636e+13, 1.2240e+13, 1.6736e+13, 2.2093e+13, 1.0455e+13, 7.9163e+12,
        1.4606e+13, 1.6643e+13, 6.7296e+12, 1.2097e+13, 1.0531e+13, 1.1169e+13,
        2.0038e+13, 3.4716e+12, 1.2381e+13, 8.8744e+12, 3.6105e+12, 1.4392e+13,
        2.1048e+13, 5.7474e+12, 5.2149e+12, 3.1845e+12, 8.4760e+12, 8.7748e+12,
        1.5121e+13, 5.3588e+12, 1.5071e+13, 7.2430e+12, 1.8802e+13, 1.0718e+13,
        8.5444e+12, 1.2886e+13, 7.7378e+12, 9.6799e+12, 1.6838e+13, 6.6759e+12,
        1.2944e+13, 1.6529e+13, 1.1805e+13, 1.5726e+13, 5.2730e+12, 1.5974e+13,
        7.9094e+12, 7.3125e+12, 8.0066e+12, 1.9690e+13, 8.7971e+12, 6.2093e+12,
        6.3955e+12, 9.0973e+12, 1.0010e+13, 9.0665e+12, 1.6305e+13, 6.3115e+12,
        1.5866e+13, 1.3603e+13, 1.8532e+13, 1.0971e+13, 1.0823e+13, 1.4512e+13,
        2.0500e+13, 9.2901e+12, 5.0621e+12, 7.9432e+12, 1.7349e+13, 7.0739e+12,
        9.9822e+12, 1.3154e+13])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1369e+11, 3.5076e+10, 8.6237e+10, 2.4349e+11, 3.0260e+10, 1.8240e+12,
        8.8375e+10, 2.0879e+11, 5.4533e+11, 1.8654e+10, 9.3820e+10, 2.5052e+11,
        1.8104e+11, 9.8817e+09, 1.9232e+11, 9.8765e+10, 2.2870e+11, 1.7045e+11,
        5.6120e+10, 1.3208e+11, 6.6387e+10, 1.0502e+12, 2.0586e+10, 3.4389e+09,
        6.6698e+10, 3.2491e+10, 7.1802e+10, 1.0816e+12, 1.5191e+11, 2.8978e+10,
        2.1207e+10, 4.5203e+10, 1.0166e+11, 1.7796e+10, 2.7675e+10, 5.4528e+10,
        4.6342e+10, 6.3093e+10, 3.5548e+10, 1.1497e+11, 4.9930e+10, 4.9840e+11,
        5.8737e+10, 9.7115e+10, 1.8475e+11, 9.1384e+10, 1.6584e+11, 1.9502e+11,
        3.0245e+10, 3.1744e+12, 9.2290e+10, 4.4642e+11, 1.9602e+11, 2.1147e+11,
        5.9810e+10, 1.8520e+11, 1.2591e+11, 9.6099e+10, 3.2836e+11, 5.7721e+10,
        2.8082e+13, 1.8116e+11, 4.7913e+11, 7.2049e+10, 5.2729e+10, 1.4128e+11,
        1.5356e+11, 1.0817e+11, 3.3491e+10, 8.6832e+10, 8.8107e+10, 5.9908e+10,
        5.1282e+10, 7.2249e+10, 7.2589e+10, 7.0555e+10, 2.7394e+10, 2.4821e+10,
        6.5663e+10, 9.5095e+10, 1.4454e+11, 2.7457e+12, 1.8989e+11, 3.9435e+10,
        3.3245e+11, 1.6836e+10, 4.0363e+12, 4.9523e+10, 1.4219e+11, 9.1070e+10,
        6.6996e+10, 6.8096e+10, 6.0439e+10, 1.0136e+11, 1.5923e+11, 2.1858e+11,
        1.3608e+11, 4.0125e+11, 8.4559e+10, 1.9937e+10, 2.3505e+11, 3.6060e+10,
        1.0968e+11, 1.6017e+11, 9.2366e+10, 3.7038e+10, 7.0125e+11, 4.2169e+10,
        4.2326e+10, 5.4592e+10, 1.3331e+10, 4.3499e+10, 9.7238e+11, 1.1627e+10,
        9.8515e+10, 1.5742e+11, 9.8201e+10, 3.0192e+10, 1.5685e+11, 1.9229e+10,
        3.4734e+11, 2.0434e+13, 6.7373e+10, 7.4242e+09, 4.2721e+10, 1.1041e+11,
        1.7499e+11, 4.6206e+11, 2.0885e+11, 1.7266e+11, 9.3026e+11, 2.1955e+11,
        6.1175e+10, 1.5500e+10, 2.1059e+11, 1.7529e+12, 4.6326e+13, 4.0860e+11,
        1.3562e+11, 4.7300e+10, 1.7137e+11, 1.3163e+11, 3.9613e+11, 7.1994e+10,
        1.3044e+11, 6.2093e+10, 2.9267e+10, 4.9018e+10, 2.5529e+10, 4.2979e+10,
        4.7466e+10, 1.1495e+11, 1.0235e+11, 9.0860e+10, 1.3499e+11, 1.1739e+11,
        1.5431e+11, 2.3347e+10, 1.9469e+10, 1.3610e+11, 9.4347e+10, 3.8609e+10,
        3.1407e+11, 1.1757e+11, 5.3067e+10, 5.4375e+10, 4.1796e+11, 6.2419e+10,
        4.7090e+10, 2.7824e+11, 1.7637e+11, 9.9831e+10, 2.7951e+10, 2.3498e+11,
        2.2229e+11, 9.2288e+10, 4.7039e+10, 4.6713e+10, 7.9840e+10, 2.5982e+10,
        1.2366e+10, 1.0643e+11, 1.1400e+11, 9.6307e+10, 1.0362e+11, 1.3214e+11,
        9.1461e+10, 1.0840e+11, 1.2311e+11, 1.9634e+11, 1.2918e+10, 9.8040e+10,
        2.7049e+11, 2.3727e+11, 1.1493e+10, 7.1572e+10, 7.8235e+10, 3.5203e+10,
        9.9632e+10, 7.3690e+10, 1.0125e+11, 7.9389e+10, 2.1249e+11, 2.2241e+10,
        5.6615e+10, 6.7243e+10, 1.0960e+11, 6.5453e+10, 1.1358e+11, 1.2219e+11,
        1.9043e+11, 6.9437e+11, 2.2609e+10, 5.0218e+09, 2.6418e+10, 4.0375e+11,
        5.1078e+11, 1.0582e+11, 4.8573e+11, 7.7001e+11, 6.8962e+10, 5.2387e+10,
        4.1287e+11, 2.8873e+10, 4.5956e+11, 1.5142e+11, 4.7097e+10, 2.1936e+11,
        4.4004e+10, 7.9840e+10, 8.4804e+10, 1.4716e+11, 4.0915e+09, 9.1695e+10,
        1.8013e+11, 4.2893e+10, 2.5766e+11, 3.3687e+10, 1.7041e+11, 1.2889e+11,
        1.7903e+11, 2.6794e+12, 8.8762e+09, 9.1416e+10, 3.1732e+10, 1.2090e+11,
        6.2825e+10, 9.3389e+10, 1.0398e+11, 7.2832e+10, 9.9632e+10, 1.9054e+11,
        2.6548e+11, 4.5534e+10, 9.0387e+10, 2.4934e+09, 2.1224e+10, 1.3007e+11,
        9.6972e+10, 9.1349e+10, 2.1620e+11, 3.6929e+10, 8.9906e+09, 1.1624e+11,
        3.0308e+11, 5.9650e+10, 3.9314e+10, 1.2392e+11, 7.4543e+10, 5.4758e+10,
        5.1940e+10, 1.5078e+11, 2.9284e+11, 8.8049e+10, 3.1557e+10, 8.2937e+10,
        6.2427e+10, 3.1489e+12, 9.5929e+10, 2.2610e+10, 1.2043e+11, 1.7453e+10,
        4.2814e+10, 7.5736e+10, 1.8240e+11, 4.6276e+10, 1.3042e+11, 1.8261e+10,
        2.3557e+10, 6.4743e+10, 2.0488e+10, 7.7129e+10, 1.8590e+11, 8.8234e+10,
        1.2783e+11, 1.8981e+11, 7.0784e+09, 1.4429e+11, 9.5439e+10, 4.8199e+10,
        1.0516e+11, 5.5557e+10, 5.5994e+10, 1.5560e+10, 2.2965e+10, 1.3639e+11,
        1.2270e+11, 2.4499e+11, 1.4006e+11, 1.8710e+11, 4.5773e+10, 8.9288e+10,
        6.5451e+10, 3.2359e+10, 4.9983e+10, 4.1126e+11, 9.1035e+10, 1.1265e+11,
        3.7437e+10, 8.8307e+10, 1.6536e+10, 4.8754e+11, 6.7087e+10, 8.8031e+10,
        8.7974e+10, 1.2100e+11, 5.4080e+10, 8.0761e+10, 4.2728e+10, 1.0206e+11,
        1.2044e+11, 4.9652e+10, 4.5266e+10, 5.7559e+10, 5.2332e+10, 1.0606e+11,
        1.3403e+11, 2.3081e+11, 1.4596e+11, 1.2493e+11, 3.5683e+11, 1.2295e+11,
        1.6743e+11, 6.1387e+11, 1.3324e+12, 1.6101e+11, 1.3348e+11, 2.8595e+11,
        2.7146e+13, 1.2803e+11, 5.3444e+10, 9.2965e+10, 2.6618e+10, 8.5132e+10,
        2.6851e+10, 9.0039e+10, 1.3695e+11, 8.5957e+10, 1.0753e+11, 9.0206e+11,
        1.7221e+11, 6.5064e+10, 1.4974e+11, 8.2764e+10, 1.3542e+11, 2.0119e+11,
        6.9869e+10, 7.7696e+10, 2.0127e+11, 1.4355e+10, 1.0869e+11, 3.1768e+10,
        3.6605e+10, 1.8031e+10, 3.8680e+10, 2.2262e+10, 8.1369e+10, 2.9367e+11,
        1.0364e+11, 6.6807e+11, 5.6491e+11, 1.7905e+11, 1.6493e+10, 4.1313e+10,
        3.4845e+11, 8.5114e+10, 1.2107e+11, 1.3178e+11, 6.5905e+10, 5.0484e+10,
        1.1657e+11, 1.1253e+11, 1.0145e+11, 4.5653e+12, 1.6575e+11, 2.2900e+10,
        6.4647e+10, 1.0077e+11, 7.5802e+10, 1.0731e+10, 5.8048e+10, 8.9987e+10,
        9.5791e+10, 9.4026e+10, 5.3746e+10, 4.6034e+11, 4.8278e+10, 5.4749e+11,
        1.3419e+11, 5.9798e+10, 5.1000e+09, 2.8822e+10, 4.1683e+10, 3.0347e+10,
        4.0344e+10, 5.1427e+10, 1.2027e+11, 4.6628e+10, 7.9854e+10, 4.5782e+11,
        2.8661e+11, 1.0644e+12, 1.3402e+11, 9.1438e+10, 5.7003e+11, 1.7822e+11,
        2.5205e+10, 5.2732e+10, 5.9300e+10, 2.5493e+11, 2.6454e+10, 2.7642e+11,
        8.8779e+10, 4.8355e+10, 6.3527e+10, 1.0700e+11, 3.6145e+10, 6.1817e+10,
        6.5933e+10, 1.8138e+11, 3.4161e+10, 4.0969e+11, 4.9179e+12, 3.5370e+10,
        7.9248e+10, 1.4804e+11, 3.9238e+10, 3.8381e+10, 5.0347e+11, 9.3606e+09,
        7.2692e+10, 1.4827e+11, 2.0608e+11, 4.0743e+10, 8.1099e+10, 2.6778e+10,
        7.1280e+10, 1.2575e+11, 1.0098e+11, 1.4227e+11, 4.4229e+11, 8.4592e+10,
        7.9472e+10, 9.0470e+10, 1.4416e+11, 1.7815e+10, 3.2303e+11, 5.8154e+10,
        1.1648e+11, 5.2468e+11, 1.4909e+10, 8.2991e+10, 8.3028e+10, 8.3122e+10,
        9.2726e+10, 4.1232e+10, 9.4151e+10, 6.2353e+11, 5.9042e+10, 1.4009e+11,
        6.0008e+10, 2.5147e+11, 5.1495e+10, 2.0967e+11, 3.0246e+10, 5.5201e+10,
        9.3393e+10, 1.5395e+11, 3.2209e+10, 7.4434e+10, 2.0293e+11, 1.9221e+10,
        2.2150e+10, 3.3542e+10, 9.5991e+10, 9.4360e+10, 8.8639e+10, 7.7963e+10,
        3.6608e+10, 4.6466e+10, 1.9687e+11, 1.4768e+11, 8.5446e+10, 4.6383e+11,
        1.1534e+11, 1.6212e+11, 2.1890e+10, 2.0096e+11, 4.1187e+10, 5.1025e+10,
        6.0799e+10, 1.8138e+11])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.3146e+11, 1.8475e+11, 2.3003e+11, 1.3949e+11, 2.1868e+11, 1.8482e+11,
        1.6394e+11, 3.1880e+11, 3.9597e+11, 2.9721e+11, 2.0494e+11, 2.1033e+11,
        1.3268e+11, 1.6283e+11, 1.8694e+11, 2.4922e+11, 3.9189e+11, 2.9019e+11,
        2.8127e+11, 1.0094e+11, 1.3209e+11, 2.8416e+11, 1.6600e+11, 2.5326e+11,
        1.6034e+11, 1.2795e+11, 4.6371e+11, 2.6112e+11, 1.1988e+11, 1.2192e+11,
        3.2163e+11, 1.2914e+11, 1.2752e+11, 1.9729e+11, 3.5198e+11, 1.9237e+11,
        1.5702e+11, 1.5474e+11, 2.5581e+11, 2.8958e+11, 1.3847e+11, 3.2857e+11,
        1.8984e+11, 1.5698e+11, 2.2761e+11, 4.1203e+11, 1.4146e+11, 5.5869e+11,
        1.6092e+11, 2.6793e+11, 1.8081e+11, 2.4926e+11, 2.2949e+11, 3.1111e+11,
        2.7659e+11, 1.5447e+11, 4.5804e+11, 9.1416e+10, 3.0603e+11, 1.8131e+11,
        1.9466e+11, 3.3306e+11, 1.1709e+11, 2.0283e+11, 3.2676e+11, 2.5995e+11,
        1.5527e+11, 1.1677e+11, 1.3345e+11, 5.4891e+11, 3.9103e+11, 1.7094e+11,
        4.8649e+11, 6.0551e+11, 3.6032e+11, 2.5309e+11, 2.6828e+11, 3.5972e+11,
        3.9495e+11, 2.5438e+11, 2.6557e+11, 2.3615e+11, 1.1272e+11, 3.4988e+11,
        2.1684e+11, 1.7391e+11, 2.2518e+11, 1.4858e+11, 1.5021e+11, 1.4835e+11,
        2.4336e+11, 3.1635e+11, 3.7402e+11, 1.3167e+11, 3.4727e+11, 2.6114e+11,
        3.9473e+11, 2.0700e+11, 3.9158e+11, 3.2771e+11, 3.3355e+11, 2.5172e+11,
        3.8966e+11, 2.4746e+11, 2.8296e+11, 3.7832e+11, 2.3222e+11, 1.6512e+11,
        1.1697e+11, 3.0529e+11, 4.2196e+11, 2.7241e+11, 2.3055e+11, 1.1574e+11,
        1.9550e+11, 2.1446e+11, 5.3978e+11, 1.1698e+11, 1.6154e+11, 1.0943e+11,
        2.7533e+11, 1.1419e+11, 2.1608e+11, 1.8683e+11, 1.8236e+11, 1.8634e+11,
        1.7729e+11, 4.1162e+11])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.5510e+11, 2.8154e+11, 2.1390e+11, 3.3704e+11, 3.0601e+11, 2.7445e+11,
        2.2687e+11, 1.2813e+11, 3.8187e+11, 6.1302e+11, 3.9211e+11, 2.2391e+11,
        2.8483e+11, 2.7102e+11, 2.9170e+11, 2.6212e+11, 2.4097e+11, 1.6779e+11,
        2.6860e+11, 3.3122e+11, 2.1880e+11, 4.2815e+11, 2.1117e+11, 6.2261e+11,
        3.8395e+11, 3.4446e+11, 6.3924e+11, 2.5681e+11, 7.9403e+11, 1.1013e+11,
        2.5848e+11, 4.5165e+11, 2.0477e+11, 4.0632e+11, 4.4541e+11, 5.2989e+11,
        2.0530e+11, 5.5418e+11, 2.8837e+11, 6.0262e+11, 3.3328e+11, 4.1411e+11,
        1.7955e+11, 3.9505e+11, 3.9322e+11, 3.8092e+11, 2.4055e+11, 3.8105e+11,
        3.2048e+11, 6.8198e+11, 6.5936e+11, 2.9749e+11, 3.3001e+11, 3.8994e+11,
        8.7742e+11, 6.8602e+11, 2.6699e+11, 6.0641e+11, 4.2750e+11, 2.8947e+11,
        3.0954e+11, 2.5287e+11, 2.5329e+11, 1.3858e+11, 2.4000e+11, 3.4507e+11,
        4.3099e+11, 3.4734e+11, 3.1930e+11, 2.4988e+11, 3.1615e+11, 4.6738e+11,
        3.6208e+11, 6.3752e+11, 1.5962e+11, 4.7524e+11, 1.4517e+11, 1.7585e+11,
        3.7516e+11, 1.5428e+11, 3.6503e+11, 2.4597e+11, 1.8858e+11, 5.4732e+11,
        4.0908e+11, 7.6364e+11, 1.4425e+11, 4.2095e+11, 4.2087e+11, 9.4105e+11,
        3.2304e+11, 4.5514e+11, 4.2798e+11, 2.3955e+11, 5.6134e+11, 1.8315e+11,
        1.6589e+11, 6.1302e+11, 4.3643e+11, 6.6372e+11, 3.4327e+11, 1.6766e+11,
        3.7438e+11, 2.4029e+11, 5.7210e+11, 3.0720e+11, 3.3413e+11, 4.0878e+11,
        6.8610e+11, 3.6759e+11, 3.1579e+11, 1.7736e+11, 5.9708e+11, 3.9066e+11,
        3.8763e+11, 3.2166e+11, 5.1089e+11, 1.9953e+11, 1.4081e+11, 2.8774e+11,
        5.0254e+11, 7.2938e+11, 1.7994e+11, 3.0381e+11, 5.0164e+11, 2.5042e+11,
        2.6532e+11, 2.2103e+11])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0642e+10, 1.3343e+10, 6.2235e+10, 3.4042e+10, 1.7846e+09, 1.4579e+10,
        3.1366e+09, 2.8001e+10, 5.2257e+09, 2.0896e+10, 4.5083e+10, 7.3346e+09,
        1.7273e+10, 3.0895e+10, 6.1074e+09, 9.4331e+09, 1.9670e+09, 8.6561e+10,
        4.0701e+09, 6.1275e+09, 4.4887e+09, 7.9577e+09, 5.1728e+09, 2.2154e+09,
        8.2869e+09, 1.0144e+10, 1.6125e+10, 1.0327e+10, 5.5158e+09, 2.2852e+09,
        6.7749e+09, 9.1295e+09, 1.1584e+10, 4.2857e+09, 2.2958e+09, 1.1900e+10,
        1.4331e+10, 4.9479e+09, 7.4087e+09, 5.2785e+09, 2.1881e+09, 3.0695e+09,
        7.7370e+10, 2.5971e+10, 6.3727e+09, 1.5392e+10, 1.4133e+09, 1.5067e+10,
        2.3809e+09, 6.5014e+09, 1.0994e+10, 3.3819e+11, 6.0212e+09, 1.4779e+10,
        1.3311e+10, 7.8955e+08, 1.4939e+10, 1.1182e+10, 5.8400e+09, 4.0583e+10,
        2.1988e+10, 2.2065e+10, 4.3428e+11, 3.6485e+09, 2.3579e+09, 1.0073e+10,
        3.8210e+09, 2.1048e+09, 1.9625e+09, 3.9383e+09, 3.8783e+09, 6.5343e+09,
        1.6578e+10, 4.6405e+09, 1.0154e+10, 2.7077e+09, 4.5288e+09, 1.7885e+09,
        3.9395e+09, 1.2966e+10, 1.7085e+10, 1.1596e+10, 1.2343e+10, 7.3195e+08,
        1.7985e+10, 1.3476e+09, 7.5880e+09, 3.4682e+09, 3.9996e+09, 2.0421e+10,
        3.9646e+09, 3.5342e+10, 5.6205e+09, 5.9903e+09, 2.1430e+10, 3.0327e+10,
        2.5458e+10, 8.4348e+09, 3.8640e+09, 8.1970e+08, 1.3354e+11, 1.0781e+09,
        4.0439e+10, 2.7044e+10, 6.0975e+09, 5.3696e+09, 8.9839e+09, 4.9373e+09,
        3.2643e+10, 2.1294e+09, 8.9356e+08, 8.5508e+10, 7.8311e+09, 8.1253e+08,
        1.4825e+10, 1.1160e+10, 2.0832e+10, 1.6032e+09, 1.2719e+10, 8.7934e+08,
        1.3309e+10, 6.0916e+08, 1.5456e+09, 3.5876e+08, 9.1121e+08, 1.7799e+10,
        2.5589e+10, 4.6587e+11, 1.1672e+10, 9.8271e+09, 1.7613e+10, 2.2844e+10,
        2.1024e+10, 4.0900e+09, 8.7890e+09, 7.5431e+09, 1.4324e+10, 5.5625e+09,
        2.0345e+10, 4.9898e+09, 4.4584e+10, 2.3276e+11, 2.3701e+10, 8.7312e+10,
        2.4375e+10, 6.8022e+09, 2.1286e+11, 4.2991e+09, 6.4780e+09, 9.4819e+09,
        1.9336e+10, 3.7015e+09, 4.7198e+09, 1.1047e+10, 2.6066e+10, 1.2558e+10,
        1.1965e+10, 2.2511e+09, 3.4210e+09, 5.0004e+09, 1.1263e+10, 4.5910e+09,
        1.0319e+10, 1.1070e+10, 9.1821e+09, 4.9782e+09, 8.1371e+09, 4.0110e+09,
        6.5783e+09, 6.6207e+09, 8.3719e+09, 4.6486e+09, 1.8583e+09, 3.4923e+10,
        1.7307e+10, 2.8864e+09, 5.6239e+10, 1.3780e+10, 3.9401e+09, 2.0820e+09,
        2.5676e+09, 1.4097e+10, 7.6851e+09, 9.1529e+09, 8.6209e+09, 1.6075e+10,
        1.0088e+10, 6.6513e+09, 6.9195e+10, 9.7157e+09, 4.2134e+08, 3.2039e+09,
        1.0982e+10, 8.2813e+09, 7.5950e+09, 3.1704e+09, 6.2537e+09, 1.8535e+10,
        1.1449e+10, 1.1762e+10, 3.8570e+09, 2.3127e+09, 4.1405e+09, 4.8265e+08,
        2.1333e+10, 2.8290e+09, 5.9181e+09, 8.4865e+09, 4.6296e+09, 9.9319e+09,
        4.1639e+09, 8.9141e+09, 1.2339e+09, 6.3526e+09, 1.1093e+09, 5.2450e+09,
        4.6348e+09, 2.6816e+09, 1.7462e+10, 1.1201e+10, 9.9924e+09, 1.5188e+09,
        9.5661e+11, 4.5225e+10, 2.0848e+10, 8.4697e+09, 1.0696e+09, 9.9084e+09,
        5.4448e+09, 2.9367e+09, 7.8640e+09, 2.4947e+10, 5.6405e+08, 1.6679e+10,
        1.3645e+10, 1.1790e+10, 1.0262e+10, 3.9809e+10, 6.2749e+09, 4.8945e+09,
        7.3303e+09, 3.4681e+09, 2.4586e+09, 9.6042e+09, 1.8633e+10, 8.7044e+10,
        8.6732e+09, 5.4652e+09, 2.3800e+09, 6.7747e+09, 3.2136e+09, 3.3073e+10,
        4.8749e+10, 1.8222e+10, 2.0007e+10, 5.8186e+09, 1.6387e+09, 1.3671e+12,
        7.1882e+09, 6.3334e+09, 1.1285e+10, 6.9674e+09, 1.1915e+09, 1.6484e+10,
        5.0638e+09, 9.1384e+09, 1.0119e+10, 1.4949e+10, 2.0473e+09, 4.7817e+09,
        1.2600e+10, 1.0053e+10, 3.3086e+10, 1.8216e+10, 1.3053e+10, 5.7010e+09,
        3.2019e+09, 1.2198e+10, 4.0591e+09, 2.6415e+10, 1.4905e+10, 2.7357e+09,
        3.3695e+09, 4.1438e+09, 1.9309e+10, 2.5451e+09, 8.2026e+09, 2.8764e+09,
        7.2531e+10, 1.7486e+10, 3.1430e+09, 3.0671e+10, 1.3257e+10, 5.2264e+10,
        8.6950e+10, 5.0727e+09, 8.1318e+08, 6.1096e+09, 3.7138e+09, 1.0686e+09,
        6.1154e+09, 4.6864e+09, 1.6664e+09, 1.5561e+09, 1.0610e+09, 9.7752e+09,
        6.5129e+09, 3.4785e+10, 5.9180e+09, 1.2355e+11, 7.0514e+09, 1.3893e+10,
        3.5346e+10, 4.0428e+09, 7.5319e+09, 1.0634e+10, 3.3777e+09, 8.3051e+09,
        5.0745e+09, 1.8838e+10, 6.2347e+09, 1.1860e+10, 1.0235e+10, 5.2549e+09,
        1.7613e+10, 7.3243e+09, 3.7005e+09, 3.6825e+10, 2.9971e+09, 2.6722e+09,
        1.8157e+10, 6.0543e+08, 1.9318e+10, 1.3645e+10, 5.9720e+09, 6.1424e+09,
        3.9253e+10, 3.8343e+09, 4.7518e+09, 6.6093e+09, 1.2528e+10, 1.1304e+10,
        4.0137e+10, 7.5582e+09, 7.1604e+09, 1.6011e+11, 9.6069e+09, 2.8813e+09,
        1.1671e+10, 6.5071e+09, 3.7448e+09, 4.4362e+09, 4.4359e+09, 4.8232e+09,
        8.1645e+09, 9.6192e+09, 3.4174e+10, 2.0156e+10, 1.2276e+10, 7.5229e+09,
        1.5758e+10, 1.6640e+10, 1.6127e+10, 1.1454e+10, 9.8288e+09, 1.0849e+10,
        5.0235e+09, 2.5998e+10, 7.3061e+09, 5.9959e+09, 7.4628e+09, 4.2991e+09,
        3.0529e+09, 1.0119e+09, 3.6155e+09, 2.7238e+09, 6.8858e+09, 8.4983e+09,
        1.8912e+10, 7.3427e+09, 6.9775e+09, 1.9574e+10, 3.7349e+10, 5.0193e+09,
        4.8873e+09, 1.6634e+10, 1.6282e+10, 2.6928e+10, 6.6474e+10, 6.0333e+09,
        1.5289e+10, 5.5794e+09, 5.8239e+09, 2.3891e+10, 1.3033e+10, 1.6062e+09,
        2.3321e+09, 7.1400e+09, 3.5693e+09, 6.7579e+08, 1.4246e+10, 4.8887e+09,
        2.5565e+09, 1.1664e+10, 1.1184e+10, 8.5462e+09, 7.7592e+09, 5.4907e+09,
        1.4679e+10, 6.7232e+09, 1.3724e+10, 6.2484e+09, 1.0959e+10, 2.7364e+09,
        5.8115e+09, 1.4948e+10, 7.2425e+09, 3.4387e+09, 6.6613e+09, 1.3384e+10,
        3.4913e+09, 4.2853e+10, 1.5287e+10, 1.0420e+10, 4.5952e+09, 2.0353e+10,
        1.6298e+09, 2.7482e+09, 9.8964e+09, 3.8795e+09, 5.0865e+09, 1.7690e+10,
        1.2976e+10, 2.2903e+09, 6.4005e+09, 2.2192e+10, 5.0865e+09, 3.2950e+09,
        1.9814e+10, 1.2405e+10, 1.6667e+09, 8.1204e+09, 1.9422e+10, 3.6673e+09,
        4.3549e+09, 5.5603e+09, 2.8109e+09, 2.6957e+10, 4.4667e+09, 1.4712e+09,
        1.0664e+10, 2.8173e+09, 1.8442e+10, 1.5088e+10, 1.5501e+10, 5.2805e+09,
        4.9289e+09, 3.4425e+10, 4.9105e+09, 5.8131e+09, 1.0825e+10, 3.9057e+09,
        3.7013e+10, 5.7742e+09, 9.7746e+09, 8.5927e+09, 1.3326e+10, 1.3996e+10,
        9.2135e+09, 1.6463e+10, 1.9841e+09, 4.1354e+09, 4.0131e+09, 6.8839e+09,
        2.3003e+09, 6.8200e+08, 2.5716e+10, 3.3566e+09, 7.3324e+09, 1.6194e+09,
        1.0389e+10, 3.3291e+10, 6.9629e+09, 2.9664e+10, 1.3576e+09, 7.3151e+09,
        1.7392e+09, 1.5051e+09, 5.6255e+09, 4.6623e+09, 1.4770e+10, 3.0143e+09,
        1.6835e+09, 6.0010e+09, 6.9708e+09, 2.7284e+10, 1.6366e+10, 3.2347e+09,
        1.6360e+10, 1.2459e+10, 8.6389e+09, 3.7579e+09, 8.0333e+10, 8.6904e+09,
        5.0737e+09, 4.0698e+09, 2.1641e+09, 1.6035e+10, 6.0983e+09, 2.2615e+09,
        1.5710e+10, 3.7680e+09])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.5172e+09, 1.7353e+10, 6.5530e+09, 2.3516e+10, 1.0074e+10, 3.6435e+10,
        2.5561e+10, 9.8101e+09, 1.2374e+10, 3.0590e+10, 1.0812e+10, 8.4839e+09,
        1.6756e+10, 1.6602e+10, 2.0312e+10, 1.6474e+10, 1.2971e+10, 1.7724e+10,
        1.6185e+10, 5.5315e+09, 1.1077e+10, 1.2662e+10, 1.6838e+10, 2.2384e+10,
        1.6747e+10, 1.4279e+10, 4.9171e+09, 7.2368e+09, 1.0185e+10, 1.3349e+10,
        1.1567e+10, 5.0917e+09, 1.8802e+10, 2.1943e+10, 9.7261e+09, 1.4445e+10,
        1.0527e+10, 5.7289e+09, 1.1224e+10, 1.3708e+10, 2.2535e+10, 1.2658e+10,
        1.7165e+10, 7.4447e+09, 1.4922e+10, 1.3638e+10, 3.0966e+10, 1.9816e+10,
        9.2225e+09, 7.7892e+09, 8.0628e+09, 1.6967e+10, 8.0267e+09, 3.7246e+09,
        1.7411e+10, 5.6169e+09, 7.3710e+09, 9.4103e+09, 8.1456e+09, 2.1480e+10,
        1.2423e+10, 1.2330e+10, 1.3751e+10, 9.4544e+09, 2.6200e+10, 1.7792e+10,
        1.0955e+10, 7.0563e+09, 2.8207e+10, 9.4510e+09, 1.5749e+10, 3.4847e+09,
        5.0527e+09, 3.9129e+10, 1.0500e+10, 1.8148e+10, 1.5314e+10, 1.5473e+10,
        1.3893e+10, 1.1055e+10, 1.4812e+10, 1.6153e+10, 2.8514e+10, 1.6519e+10,
        1.1308e+10, 2.7726e+10, 2.2574e+10, 1.2082e+10, 6.9385e+09, 1.1019e+10,
        1.6460e+10, 5.3379e+09, 2.3183e+10, 2.6885e+10, 7.5232e+09, 4.7265e+09,
        2.3263e+10, 1.2087e+10, 2.7708e+10, 1.1723e+10, 1.3034e+10, 2.2907e+10,
        7.6341e+09, 1.0931e+10, 1.4745e+10, 1.4299e+10, 8.9754e+09, 1.0513e+10,
        3.7365e+09, 1.5288e+10, 1.9830e+10, 2.7573e+10, 1.6942e+10, 6.6576e+09,
        2.0107e+10, 1.8529e+10, 1.2165e+10, 1.4977e+10, 8.6863e+09, 1.1645e+10,
        2.0127e+10, 1.4385e+10, 4.4910e+09, 7.0494e+09, 3.0405e+10, 2.0783e+10,
        9.3304e+09, 8.9088e+09, 7.8639e+09, 8.3328e+09, 1.6384e+10, 7.3655e+09,
        6.4966e+09, 7.8948e+09, 8.9871e+09, 1.0011e+10, 4.0707e+09, 7.1711e+09,
        1.1170e+10, 2.5065e+10, 7.8420e+09, 1.0646e+10, 9.5501e+09, 1.5652e+10,
        7.0375e+09, 7.0565e+09, 9.4772e+09, 1.1596e+10, 3.4021e+09, 1.3367e+10,
        2.0435e+10, 2.2972e+10, 8.5512e+09, 2.6560e+10, 8.1576e+09, 1.1374e+10,
        2.9381e+10, 5.7150e+09, 1.7121e+10, 1.0067e+10, 1.0097e+10, 9.4608e+09,
        8.9061e+09, 7.4990e+09, 7.3317e+09, 8.3060e+09, 2.2060e+10, 8.5619e+09,
        2.2685e+10, 1.4530e+10, 3.2875e+10, 1.7833e+10, 1.7042e+10, 6.0112e+09,
        8.7347e+09, 5.6571e+09, 7.7836e+09, 2.0085e+10, 1.2678e+10, 1.9039e+10,
        5.7129e+09, 9.5602e+09, 1.1594e+10, 5.0008e+09, 2.3899e+10, 1.5725e+10,
        3.2672e+10, 1.5880e+10, 1.5831e+10, 2.2941e+10, 2.3816e+10, 7.9884e+09,
        2.1127e+10, 3.0335e+10, 4.8212e+09, 7.2496e+09, 1.7055e+10, 6.3956e+09,
        1.2550e+10, 5.8140e+09, 9.3416e+09, 1.1037e+10, 1.2255e+10, 2.2683e+10,
        9.3561e+09, 1.7654e+10, 8.4566e+09, 1.4991e+10, 1.7036e+10, 3.6391e+10,
        6.3140e+09, 7.2004e+09, 1.4393e+10, 6.6287e+09, 7.5040e+09, 7.2884e+09,
        1.6406e+10, 1.4065e+10, 3.5988e+09, 1.5088e+10, 2.1982e+10, 9.1069e+09,
        1.2924e+10, 7.2761e+09, 1.9267e+10, 6.6036e+09, 3.2340e+10, 1.6022e+10,
        9.7003e+09, 6.7379e+09, 1.6038e+10, 4.5299e+09, 1.0105e+10, 1.7501e+10,
        3.6239e+09, 8.7914e+09, 1.3276e+10, 2.0131e+10, 1.3555e+10, 1.5487e+10,
        1.0544e+10, 9.3786e+09, 6.1383e+09, 4.8896e+09, 1.7497e+10, 1.8863e+10,
        2.1821e+10, 2.0988e+10, 1.2693e+10, 9.7189e+09, 5.0917e+09, 2.0912e+10,
        8.0001e+09, 6.2193e+09, 1.5488e+10, 1.0919e+10])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.2055e+10, 1.6379e+10, 1.3735e+10, 6.3122e+09, 1.0596e+10, 2.0514e+10,
        1.5164e+10, 1.0619e+10, 2.2865e+10, 1.8299e+10, 9.5525e+09, 2.7775e+10,
        8.0263e+09, 1.5564e+10, 3.6726e+10, 1.8391e+10, 2.2193e+10, 2.7657e+10,
        3.6383e+10, 2.7516e+10, 1.0970e+10, 1.4502e+10, 1.4459e+10, 5.5188e+09,
        2.6348e+10, 2.1294e+10, 2.0526e+10, 1.3384e+10, 1.3733e+10, 3.1108e+10,
        9.0627e+09, 7.5634e+09, 3.4637e+10, 2.3941e+10, 2.9432e+10, 1.0092e+10,
        2.0850e+10, 3.8725e+10, 1.7753e+10, 1.4278e+10, 4.2331e+10, 4.5178e+10,
        2.3793e+10, 1.6330e+10, 8.0820e+09, 7.9509e+09, 2.2663e+10, 9.0100e+09,
        1.5100e+10, 4.7227e+10, 1.0670e+10, 2.1798e+10, 4.2748e+10, 3.2566e+10,
        2.0120e+10, 1.6816e+10, 9.0134e+09, 2.2541e+10, 1.6679e+10, 2.7049e+10,
        2.5142e+10, 1.6042e+10, 1.9261e+10, 3.3171e+10, 1.1209e+10, 1.4662e+10,
        4.8912e+10, 4.7986e+10, 9.2010e+09, 4.1715e+10, 2.0058e+10, 2.3380e+10,
        9.7983e+09, 1.2723e+10, 1.3718e+10, 3.1910e+10, 1.3414e+10, 3.3274e+10,
        2.4387e+10, 2.0757e+10, 1.5133e+10, 1.5642e+10, 1.3435e+10, 3.2833e+10,
        1.6626e+10, 2.7818e+10, 1.4254e+10, 3.5289e+10, 3.7335e+10, 4.4068e+10,
        2.8674e+10, 1.0415e+10, 8.2732e+09, 3.4190e+10, 1.4334e+10, 3.1107e+10,
        2.1728e+10, 1.6806e+10, 8.3891e+09, 1.7249e+10, 1.7138e+10, 1.5518e+10,
        3.2027e+10, 1.1766e+10, 3.1648e+10, 1.4608e+10, 1.5093e+10, 1.6207e+10,
        1.6156e+10, 4.2165e+10, 3.1892e+10, 2.7324e+10, 3.0602e+10, 3.5266e+10,
        9.4097e+09, 4.5368e+10, 1.6710e+10, 9.5049e+09, 5.9477e+09, 7.7732e+09,
        2.1126e+10, 1.3728e+10, 2.0419e+10, 1.9911e+10, 1.3952e+10, 3.7229e+10,
        1.2762e+10, 1.1682e+10, 1.1315e+10, 9.3567e+09, 3.5229e+10, 1.9222e+10,
        1.0531e+10, 2.4254e+10, 1.6918e+10, 3.9281e+10, 3.6705e+10, 2.6045e+10,
        1.3007e+10, 7.7795e+09, 1.0193e+10, 1.1462e+10, 2.9757e+10, 8.6624e+09,
        1.4426e+10, 3.8701e+10, 1.0653e+10, 3.5605e+10, 9.4484e+09, 2.7559e+10,
        2.1120e+10, 3.6433e+10, 1.4115e+10, 1.6505e+10, 8.2441e+09, 1.9206e+10,
        3.2551e+10, 3.3313e+10, 4.2660e+10, 3.0064e+10, 1.3427e+10, 8.1889e+09,
        2.0782e+10, 3.2766e+10, 2.9230e+10, 3.0590e+10, 2.2592e+10, 3.7560e+10,
        2.8861e+10, 1.5365e+10, 9.4398e+09, 2.5730e+10, 4.0918e+10, 1.6692e+10,
        2.4406e+10, 2.0745e+10, 1.1764e+10, 1.3257e+10, 1.6387e+10, 5.6523e+10,
        2.7394e+10, 3.6185e+10, 6.2981e+09, 1.1630e+10, 2.6185e+10, 1.7937e+10,
        1.4928e+10, 3.0418e+10, 1.6065e+10, 2.5815e+10, 1.8831e+10, 2.4969e+10,
        1.2847e+10, 2.3412e+10, 2.9568e+10, 3.4107e+10, 2.6198e+10, 1.3245e+10,
        1.2675e+10, 1.6623e+10, 3.7553e+10, 3.2630e+10, 3.1226e+10, 5.7742e+10,
        2.1454e+10, 4.1059e+10, 1.1514e+10, 3.0151e+10, 2.5260e+10, 1.6930e+10,
        2.8931e+10, 8.2456e+09, 9.9803e+09, 8.0982e+09, 3.2691e+10, 2.9018e+10,
        3.3255e+10, 1.8873e+10, 2.4369e+10, 2.9537e+10, 1.3604e+10, 2.2790e+10,
        8.4945e+09, 2.5072e+10, 1.9186e+10, 3.5225e+10, 1.6282e+10, 1.0766e+10,
        2.9087e+10, 1.3237e+10, 4.2525e+10, 2.3084e+10, 1.5024e+10, 1.2829e+10,
        3.0509e+10, 2.6323e+10, 2.6787e+10, 1.3459e+10, 1.6613e+10, 3.0142e+10,
        3.2635e+10, 4.8506e+10, 4.1261e+10, 4.5380e+10, 1.5392e+10, 1.3815e+10,
        3.7126e+10, 3.2154e+10, 3.6546e+10, 3.4131e+10, 1.1115e+10, 1.5066e+10,
        2.3241e+10, 3.0065e+10, 4.1922e+10, 1.1481e+10])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.3938e+07, 1.2453e+08, 1.0318e+08,  ..., 2.5731e+07, 1.6586e+11,
        2.7105e+08])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3354e+08, 2.9252e+08, 1.6196e+08,  ..., 1.3066e+08, 1.6615e+11,
        1.7576e+08])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6976e+08, 3.9939e+08, 6.5920e+08, 6.4630e+08, 1.3850e+09, 1.5079e+09,
        3.8971e+08, 4.2840e+08, 8.3512e+08, 7.1819e+08, 9.7086e+08, 2.4389e+08,
        3.2774e+08, 1.9784e+08, 1.5543e+08, 3.7929e+08, 6.0427e+08, 2.8313e+08,
        1.5751e+09, 3.1800e+08, 3.0520e+08, 6.7466e+08, 5.0550e+08, 2.7443e+08,
        3.7037e+08, 2.7316e+08, 3.0915e+08, 2.9214e+08, 3.1878e+08, 1.0054e+09,
        6.9458e+08, 8.7496e+08, 3.8679e+08, 1.8608e+08, 3.5514e+08, 8.0212e+08,
        5.2239e+08, 5.5833e+08, 8.8414e+08, 1.0662e+09, 6.3222e+08, 2.8539e+08,
        4.9371e+08, 7.0307e+08, 3.2802e+08, 4.4445e+08, 2.1911e+08, 1.4863e+09,
        4.3225e+08, 5.5165e+08, 3.5933e+08, 5.5753e+08, 4.2525e+08, 1.7961e+08,
        5.0841e+08, 3.2916e+08, 3.0280e+08, 9.1070e+08, 1.0374e+09, 9.3763e+08,
        4.6940e+08, 3.2694e+08, 8.0625e+08, 5.0268e+08, 7.3131e+08, 9.2620e+08,
        3.7175e+08, 4.1037e+08, 4.7488e+08, 2.3737e+08, 9.9009e+08, 8.7948e+08,
        3.6813e+08, 8.9025e+08, 4.0690e+08, 1.7429e+08, 1.0842e+09, 5.4726e+08,
        5.7209e+08, 7.8176e+08, 6.4273e+08, 2.3133e+08, 1.6311e+09, 6.0432e+08,
        5.6456e+08, 6.9759e+08, 1.9814e+08, 3.8713e+08, 4.9296e+08, 8.0952e+08,
        4.4201e+08, 1.7991e+08, 5.9225e+08, 2.1422e+08, 3.5599e+08, 2.9051e+08,
        6.0213e+08, 7.9084e+08, 8.5649e+08, 3.0403e+08, 6.7474e+08, 2.2942e+08,
        6.7708e+08, 8.5290e+08, 5.8329e+08, 6.5641e+08, 9.5879e+08, 1.9512e+08,
        3.9867e+08, 4.1659e+08, 3.4811e+08, 4.3253e+08, 2.9452e+08, 5.3968e+08,
        2.6362e+08, 7.7890e+08, 2.3073e+08, 1.1212e+09, 2.1778e+08, 4.2697e+08,
        5.7838e+08, 5.6893e+08, 1.3264e+09, 3.7528e+08, 2.5708e+08, 7.3742e+08,
        4.0816e+08, 6.2895e+08, 1.8099e+08, 4.8891e+08, 1.5794e+09, 5.0105e+08,
        3.5235e+08, 2.1516e+08, 1.0249e+09, 1.4174e+09, 1.2014e+09, 1.0264e+09,
        3.8476e+08, 3.8254e+08, 2.4481e+08, 1.0483e+09, 4.0795e+08, 5.6890e+08,
        1.3609e+08, 1.9030e+08, 1.2559e+08, 3.1617e+08, 9.5066e+08, 3.8601e+08,
        4.5842e+08, 2.1393e+08, 3.3348e+08, 4.3315e+08, 2.0849e+08, 4.8795e+08,
        5.2884e+08, 1.1944e+09, 5.1850e+08, 2.3464e+08, 2.0581e+08, 2.7846e+08,
        1.6986e+08, 3.6161e+08, 4.3622e+08, 3.7014e+08, 6.1591e+08, 5.8957e+08,
        1.6370e+08, 3.7807e+08, 9.3105e+08, 4.5039e+08, 5.7824e+08, 2.1389e+08,
        4.4639e+08, 9.7644e+08, 4.1030e+08, 4.8139e+08, 6.7704e+08, 6.0757e+08,
        1.7993e+08, 1.9718e+08, 6.8949e+08, 5.3408e+08, 1.4064e+09, 1.7271e+08,
        5.4041e+08, 1.1172e+09, 4.9368e+08, 2.7167e+08, 9.0334e+08, 1.7238e+08,
        7.7411e+08, 3.4006e+08, 4.4583e+08, 3.6803e+08, 5.1996e+08, 9.3339e+08,
        7.7483e+08, 1.1551e+09, 2.9109e+08, 2.9739e+08, 1.3324e+09, 4.0475e+08,
        2.8329e+08, 5.7913e+08, 1.5916e+09, 8.9319e+08, 6.0220e+08, 3.2163e+08,
        3.9671e+08, 4.5214e+08, 4.2737e+08, 5.1365e+08, 4.3623e+08, 3.7404e+08,
        1.0915e+09, 5.7165e+08, 4.1967e+08, 3.2533e+08, 6.3906e+08, 1.4801e+09,
        6.9158e+08, 7.8286e+08, 3.9033e+08, 1.4734e+08, 4.6593e+08, 3.5888e+08,
        8.9937e+08, 3.0910e+08, 4.5255e+08, 1.2030e+09, 4.8260e+08, 5.4293e+08,
        1.2404e+09, 7.9120e+08, 7.7405e+08, 5.3887e+08, 7.8926e+08, 7.9065e+08,
        7.1899e+08, 4.5237e+08, 8.2575e+08, 4.2302e+08, 8.0005e+08, 4.1788e+08,
        4.2327e+08, 9.3860e+08, 8.7466e+08, 1.2077e+09, 3.2037e+08, 4.3509e+08,
        4.1449e+08, 1.0996e+09, 2.2725e+08, 6.6647e+08])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.6612e+08, 1.0829e+09, 3.0166e+08, 2.2472e+09, 6.7061e+08, 1.1128e+09,
        7.9607e+08, 1.3171e+09, 1.6512e+09, 5.8181e+08, 1.8237e+09, 2.5439e+09,
        2.9671e+09, 2.0245e+09, 1.8212e+09, 5.5422e+08, 2.5533e+09, 1.2758e+09,
        4.8826e+08, 6.5257e+08, 8.5967e+08, 3.6904e+08, 1.2786e+09, 1.2781e+09,
        1.5914e+09, 1.6210e+09, 1.4381e+09, 8.2674e+08, 1.1815e+09, 1.6713e+09,
        2.3874e+09, 1.7032e+09, 1.2382e+09, 9.5604e+08, 2.2880e+09, 9.5547e+08,
        1.2769e+09, 1.4941e+09, 1.0758e+09, 2.9119e+09, 5.5848e+08, 7.7681e+08,
        2.2617e+09, 2.7422e+09, 2.5092e+09, 5.8866e+08, 9.3570e+08, 1.1012e+09,
        9.5086e+08, 1.2872e+09, 1.7459e+09, 9.1551e+08, 1.1725e+09, 9.9059e+08,
        4.9008e+08, 1.2167e+09, 8.3346e+08, 2.7599e+09, 1.1072e+09, 1.6910e+09,
        5.5800e+08, 1.0893e+09, 2.2031e+09, 2.6046e+09, 6.9352e+08, 1.9456e+09,
        1.2000e+09, 8.9532e+08, 7.4743e+08, 1.4134e+09, 2.3186e+09, 1.2100e+09,
        8.9636e+08, 2.8571e+09, 3.1048e+09, 5.6502e+08, 1.4748e+09, 1.1221e+09,
        9.9372e+08, 3.0489e+09, 1.3010e+09, 1.1368e+09, 6.3132e+08, 2.6338e+09,
        1.9769e+09, 1.2130e+09, 6.6244e+08, 2.6513e+09, 1.4518e+09, 1.1067e+09,
        1.0424e+09, 1.4214e+09, 1.7951e+09, 1.3447e+09, 4.9496e+08, 5.8092e+08,
        5.1233e+08, 7.8367e+08, 1.1448e+09, 1.8218e+09, 1.7838e+09, 3.2645e+09,
        3.6540e+08, 1.2778e+09, 2.0455e+09, 8.4561e+08, 1.2228e+09, 1.1913e+09,
        7.0994e+08, 7.9165e+08, 9.0837e+08, 5.2706e+08, 1.5647e+09, 7.3125e+08,
        8.0792e+08, 1.7682e+09, 3.8432e+08, 8.1646e+08, 8.5893e+08, 1.5398e+09,
        2.0364e+09, 1.8802e+09, 2.5180e+09, 5.6051e+08, 2.2296e+09, 2.9352e+09,
        3.6764e+09, 5.4261e+08, 2.0354e+09, 1.0318e+09, 1.9364e+09, 1.4918e+09,
        1.7543e+09, 1.3852e+09, 1.2127e+09, 2.2225e+09, 1.6374e+09, 1.8874e+09,
        1.3645e+09, 1.6641e+09, 7.3769e+08, 3.0064e+09, 2.0251e+09, 1.9917e+09,
        1.4148e+09, 5.8241e+08, 5.7937e+08, 1.0557e+09, 1.0338e+09, 3.2030e+09,
        1.0107e+09, 1.5464e+09, 1.5637e+09, 9.7554e+08, 2.3030e+09, 2.2275e+09,
        1.0150e+09, 1.0049e+09, 2.6031e+09, 1.1545e+09, 1.0739e+09, 7.0411e+08,
        1.3329e+09, 5.4735e+08, 1.4537e+09, 1.7989e+09, 2.7122e+09, 1.7541e+09,
        1.3832e+09, 8.0983e+08, 1.6625e+09, 1.1977e+09, 6.1962e+08, 1.2970e+09,
        1.2802e+09, 1.1007e+09, 1.6071e+09, 1.2348e+09, 1.1758e+09, 2.4629e+09,
        6.8036e+08, 1.4442e+09, 9.8854e+08, 4.7026e+08, 2.2898e+09, 1.1453e+09,
        5.9573e+08, 2.4909e+09, 1.4304e+09, 1.7835e+09, 1.4227e+09, 1.4519e+09,
        8.6956e+08, 6.6561e+08, 5.0401e+08, 1.2699e+09, 1.2203e+09, 1.0892e+09,
        1.3544e+09, 7.9260e+08, 6.4939e+08, 6.3401e+08, 1.5371e+09, 1.8428e+09,
        2.9317e+08, 1.0559e+09, 5.3000e+08, 1.7098e+09, 4.2376e+08, 1.8685e+09,
        6.7869e+08, 8.3059e+08, 1.3810e+09, 3.5075e+09, 1.5577e+09, 1.4642e+09,
        7.2855e+08, 1.1109e+09, 2.5442e+09, 2.4618e+09, 7.5518e+08, 1.7685e+09,
        4.4606e+08, 4.7970e+08, 3.7939e+09, 1.6245e+09, 1.1777e+09, 2.5776e+09,
        1.0279e+09, 5.3100e+08, 2.0737e+09, 2.7529e+09, 1.6927e+09, 5.8897e+08,
        1.7469e+09, 1.6999e+09, 4.0992e+08, 2.6927e+09, 3.9618e+08, 1.1097e+09,
        2.5463e+09, 3.2113e+09, 1.1583e+09, 9.1206e+08, 6.0433e+08, 6.1180e+08,
        1.8175e+09, 1.5261e+09, 6.4460e+08, 1.7386e+09, 1.9921e+09, 9.8175e+08,
        7.5425e+08, 2.4857e+09, 4.0699e+08, 7.0290e+08])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([20386676., 14305923.,  8566303.,  ...,  9628644., 16057611.,
         8363146.])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([26463176., 23182528., 21356420., 18905520., 20453424., 17708894.,
        17832820., 14593205., 25736094., 18249766., 19859100., 20427474.,
        17312812., 21324082., 22483202., 21888752., 16956972., 26663360.,
        20395900., 21697074., 15872441., 19504574., 23787414., 21334064.,
        22755108., 25247712., 25539448., 20569726., 17973364., 17415216.,
        21525692., 19947380., 16981862., 17082540., 16716140., 17770858.,
        16543386., 18916904., 21074660., 19614464., 17117714., 17711172.,
        19204704., 23661642., 17561890., 23866006., 16804918., 21834164.,
        15595367., 16970776., 22419038., 23820752., 26849808., 18623998.,
        24504426., 23917146., 21410344., 17935420., 21682990., 28968906.,
        22058470., 25015324., 17889758., 21114692., 23485722., 22952496.,
        23329016., 18347024., 18159266., 22448430., 21443956., 19178078.,
        20403450., 22373602., 25119224., 26240446., 17104874., 23139020.,
        22469488., 21457608., 22674802., 22471786., 20349364., 20322032.,
        16811768., 21209844., 20265526., 13992470., 24104088., 15862498.,
        14569683., 20078376., 20473320., 15671787., 22632032., 19281730.,
        30714808., 22717946., 17593302., 23947178., 19083070., 22508356.,
        14490122., 23988754., 16936452., 28146312., 25520064., 20173902.,
        16904306., 20684182., 17492360., 17397456., 23051368., 20106450.,
        19386320., 20903224., 25641478., 23988050., 23673074., 23008878.,
        17995846., 27753290., 24363772., 21746002., 17906302., 18812430.,
        15881537., 23396924., 21602766., 19031084., 16956814., 22217536.,
        21167412., 19244020., 15155595., 17557556., 22159598., 18905094.,
        15709887., 20886494., 17899624., 16294958., 20519636., 19588132.,
        18350058., 18546134., 27245664., 23092158., 28084032., 26127352.,
        19665264., 24595186., 24064126., 20223778., 17378092., 21438770.,
        21757336., 18056758., 20181938., 19960740., 19664822., 19566726.,
        22384928., 17753244., 16466628., 20277498., 21460262., 16209862.,
        16540851., 26298114., 21192464., 21674152., 25867756., 21860244.,
        23743774., 19334502., 17188658., 21781012., 17849890., 23903816.,
        19726654., 21781012., 18015040., 21079836., 18838448., 20127100.,
        19386012., 16371510., 24222272., 22063144., 26765352., 22369786.,
        17525584., 15674589., 24437036., 17799462., 15748033., 18992628.,
        29014300., 19029680., 18724700., 21092982., 21272708., 19879798.,
        21526534., 18754898., 22963444., 18331324., 20262410., 19187360.,
        15736116., 17393336., 19554660., 28534110., 20768130., 21504700.,
        18007210., 23671958., 20250206., 19886316., 20474514., 22944914.,
        24684596., 24177346., 22024600., 26970528., 22240754., 20471226.,
        16856972., 21513422., 14326170., 20492296., 21253894., 20556518.,
        16195936., 18663642., 17757872., 18474844., 22931098., 22494140.,
        20733638., 22761018., 21164790., 16362446., 23355150., 16519257.,
        19658062., 19199770., 15822578., 18280992., 21929350., 25322102.,
        15044326., 17547068., 18889684., 24165446.])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([38536028., 44707472., 64398652., 47845116., 41244440., 52413820.,
        57758376., 57418220., 32476652., 52522820., 44683048., 50903440.,
        51686464., 33730856., 40713628., 37223296., 45639108., 49402832.,
        62416520., 59377436., 42447932., 39241836., 39488172., 57194480.,
        49184928., 50067464., 47783176., 70882400., 52505536., 53067884.,
        52035564., 43517520., 56204560., 49265908., 62259000., 51622896.,
        41722460., 49645936., 69088104., 53674016., 46424368., 52290188.,
        44715568., 41804232., 47863668., 48958868., 58008584., 40677248.,
        61092092., 41875876., 57183524., 47610660., 49032524., 42190912.,
        53898304., 44183644., 59493668., 52445268., 61720344., 57243608.,
        55860624., 43901244., 56984948., 59441948., 37359916., 54955048.,
        43643160., 64134760., 61213496., 55496124., 46060096., 44299148.,
        60046576., 59230256., 45254484., 41736308., 59010848., 64159800.,
        44337688., 59291880., 56583884., 48304452., 52075400., 38483304.,
        52504864., 67573048., 49523776., 56858052., 61284684., 49728188.,
        54349444., 47316108., 35794772., 56913108., 55713584., 68808136.,
        38172088., 49609584., 55556212., 57543256., 53598092., 57649548.,
        48923172., 50764628., 52978300., 44213288., 62338672., 56796440.,
        42514344., 52609572., 47424620., 62357804., 44007992., 46676668.,
        41929280., 62222792., 48998412., 57102556., 40953856., 46223028.,
        42454544., 58960956., 38886152., 42397784., 51336416., 51650436.,
        45989288., 52737080., 45332612., 34381016., 49564944., 56620076.,
        60682668., 50274704., 42427608., 40573060., 53038228., 54176544.,
        44197448., 44754364., 53681216., 57041372., 60469276., 53639300.,
        45668532., 58587116., 49429064., 59080580., 49572664., 36805268.,
        51349416., 66402128., 48695808., 49799084., 53656844., 55827416.,
        41294848., 62238372., 62658740., 78316160., 60230256., 48850080.,
        51868220., 57597352., 40256532., 35958800., 57680000., 44665752.,
        48537080., 52149776., 56081244., 60890000., 51180628., 48580268.,
        65874576., 40894460., 40333164., 49776056., 45203400., 45724124.,
        51615796., 58547952., 57939800., 66269400., 56123476., 43509060.,
        37783632., 49486120., 51900512., 49998068., 43661708., 49503536.,
        64657364., 44907996., 53498252., 44369828., 48150384., 52966304.,
        65328200., 48899104., 75495616., 34725896., 42660832., 49051652.,
        51559712., 74300328., 44152148., 55163212., 45741568., 41573480.,
        55020924., 60423716., 49815432., 41114184., 64729568., 63281728.,
        41249056., 61322324., 58490920., 51608132., 58324212., 46713364.,
        59803812., 38843992., 54732724., 40493772., 52454076., 39937036.,
        62448700., 41311024., 39841164., 58404408., 51281980., 53245712.,
        50926196., 52661596., 60084692., 47498904., 46579100., 47409824.,
        46964708., 53929824., 46698280., 45294000., 49904656., 45402500.,
        54480588., 35835392., 45869344., 42354556., 51343136., 62870140.,
        53412232., 47123724., 58352896., 64195164.])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([15268887.0000,  2818270.7500,  7743788.5000,  ...,
          344233.8750,  1583279.7500,   359878.0625])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5372215.5000, 6904215.5000, 4800146.5000, 4138119.2500, 4551276.5000,
        3916420.0000, 6597258.5000, 7219141.5000, 3852348.5000, 4869150.0000,
        6147916.5000, 3859325.5000, 3607041.5000, 3251931.5000, 4204283.5000,
        4453575.5000, 4795265.5000, 5737084.0000, 7703587.0000, 5914269.5000,
        5238954.5000, 6039937.0000, 6032734.0000, 4374479.0000, 3782288.5000,
        4394954.0000, 3762099.7500, 7192380.0000, 3337849.7500, 4108223.5000,
        4600332.0000, 5276321.5000, 5238263.0000, 4888830.5000, 4175611.0000,
        4552843.5000, 4490081.0000, 3513536.2500, 5055159.5000, 4549183.5000,
        8586701.0000, 5631015.5000, 4408424.0000, 3044599.0000, 3839537.7500,
        4638310.0000, 4602454.0000, 4118048.7500, 6683227.0000, 4172538.5000,
        5876576.0000, 3937610.2500, 6419746.0000, 4311848.0000, 4488164.0000,
        5437160.5000, 4356371.5000, 3560900.5000, 5666808.5000, 6216630.5000,
        3632250.7500, 4448332.5000, 4892253.0000, 3987762.7500, 6877699.5000,
        6751061.0000, 4994371.5000, 6437935.5000, 4360501.0000, 3629980.5000,
        4344703.0000, 4096861.0000, 4309627.0000, 4638131.5000, 4504789.0000,
        4842724.0000, 3972771.5000, 4377115.0000, 6699769.5000, 3627616.2500,
        6869222.0000, 5697101.5000, 4368496.5000, 4165084.5000, 4223817.5000,
        4683421.5000, 4647686.0000, 3949562.7500, 5420314.5000, 4303832.0000,
        3296498.0000, 7291782.5000, 4217552.5000, 6211580.0000, 6188659.5000,
        3926806.5000, 5839898.5000, 5662712.0000, 5574376.0000, 5366600.5000,
        4346615.0000, 5296910.0000, 4506702.0000, 4011736.0000, 5484507.5000,
        4772066.0000, 5045114.5000, 7022495.5000, 5391936.5000, 5675556.5000,
        5546751.5000, 5445058.5000, 5029632.0000, 4713098.5000, 3902840.2500,
        3053047.7500, 4411278.0000, 5291283.5000, 5225324.5000, 6163516.5000,
        5252671.0000, 3666162.2500, 5735494.0000, 2952610.0000, 3208889.0000,
        5883586.5000, 6522466.5000, 4999462.0000, 5007513.5000, 5497707.5000,
        5085080.0000, 4771379.0000, 3751613.2500, 4791202.5000, 6104018.5000,
        4951846.5000, 4568540.5000, 3744495.7500, 4901166.5000, 6618437.5000,
        4636568.5000, 5835908.5000, 4434191.0000, 5896932.0000, 4007198.7500,
        4161773.7500, 3498861.2500, 5278003.0000, 5667665.5000, 3017542.5000,
        5233877.5000, 5850406.5000, 4094396.5000, 4936883.0000, 4320005.0000,
        7066918.5000, 4727395.5000, 4309045.0000, 4807341.0000, 4681306.5000,
        4175381.5000, 6215904.5000, 6315326.5000, 5673601.5000, 5978538.5000,
        2850465.0000, 3559034.5000, 4166969.7500, 5705479.5000, 3570324.2500,
        3820044.0000, 4737636.5000, 3949146.2500, 4176584.7500, 5432975.0000,
        5208021.5000, 3403992.7500, 5701575.0000, 4404820.5000, 4586965.5000,
        4480277.5000, 4945187.0000, 6432971.0000, 7058552.5000, 5246210.5000,
        5241788.0000, 4965126.0000, 4025243.7500, 3889971.5000, 5579402.5000,
        5761739.0000, 5849751.5000, 4000198.5000, 3936888.2500, 5095236.0000,
        4802792.5000, 4412840.0000, 5453579.0000, 5738911.0000, 4566267.0000,
        6674485.0000, 5303120.0000, 3744184.2500, 3918950.7500, 4041595.7500,
        8129158.5000, 3271012.0000, 2755165.7500, 3249863.5000, 4957293.5000,
        5897096.5000, 4538269.5000, 3691451.7500, 4162051.0000, 4504535.0000,
        5230862.5000, 3957686.2500, 3810159.7500, 4727216.0000, 5067197.5000,
        5895704.5000, 4597705.0000, 5354453.5000, 4032117.0000, 4465322.5000,
        3851853.7500, 5546732.0000, 6236624.5000, 3853394.2500, 3578271.7500,
        4732505.0000, 4796491.0000, 3943980.2500, 5789565.5000, 3556926.7500,
        4405210.0000, 4283304.5000, 3434263.2500, 4881859.0000, 4141260.2500,
        3274338.7500, 3906299.5000, 5541436.0000, 5821333.5000, 6571939.5000,
        5741466.0000, 4693155.5000, 4601230.5000, 3609920.5000, 3070767.5000,
        4555755.5000, 5798621.5000, 4710060.0000, 4545055.5000, 4309058.0000,
        3696800.5000])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([11472191., 10615842., 16168687., 12989522., 14853206., 18540434.,
        20808148., 18044314., 13785117., 13382342.,  9439485., 16823676.,
        14635164., 12418828., 16052789., 10469373., 15183232., 20033156.,
        20303072., 15943769., 14069289., 16428262., 14063505., 16154100.,
        14164709., 12230811., 14233626., 14212101., 20832480., 14809801.,
        19717394., 14893290., 13523127., 11110289., 14881079., 13743349.,
        15209769., 12182856., 13024544., 13855941., 14986737., 16964174.,
        11516631., 12765567., 15491005., 12688109., 19217690., 13936872.,
        23796496., 13735325., 14691722., 14787242., 19159410., 19368754.,
        17634690., 15608158., 19484434., 14932935., 15912367., 18774696.,
        14796320., 12354856., 21623164., 16400530., 13709742., 13866937.,
        21094046., 13958630., 15379519., 20373194., 14741938., 12391424.,
        16003269., 18937978., 12136630., 12851674., 26529826., 17617474.,
        15806642., 10258204., 16971326., 11607141., 17058502., 14992379.,
        13837489., 13001708., 15228681., 26014076., 11800325., 15064059.,
        14972889., 16226121., 13507225., 15665662., 17377486., 14880530.,
        18069966., 12752483., 24197698., 15553363., 12659205., 16220613.,
        14746471., 14336452., 13077523., 13436716., 16183169., 18690318.,
        19225498., 15707842., 13085826., 13603792., 13084411., 18191236.,
         8900702., 15650915., 12302248., 12184725., 13701783., 12120375.,
        14048033., 18761414., 10541937., 19025144., 16575440., 15645644.,
        10436142., 18834934., 14160208., 13306019., 17553916., 17508542.,
        18665784., 15135698., 17419664., 12608193., 14760778., 12959930.,
        15926249., 18323602., 17360028., 10475303., 13748163., 11131328.,
        15771938., 17571462., 13757578., 15286600., 12895572., 19815778.,
        15986330., 15902881., 14989380., 19882648., 10836351., 14302850.,
        20523026., 15484227., 16254524., 16419847., 16412161., 17213632.,
        10836132., 18437684., 20588702., 15047595., 15066082., 12614187.,
        12818582., 14377248., 15270415., 13392391., 16340398., 12595632.,
        23685606., 22481250., 11321044., 20163168., 15911015., 13160545.,
        12617423., 13616666., 13753905., 16590714., 13582767., 24419322.,
        13839431., 15832827., 21986998., 20296524., 14930258., 17879674.,
        16127978., 15061692., 12241734., 15777027., 16050786., 16850214.,
        19175100., 15850397., 20540234., 10950412., 12443345., 20998846.,
        13630249., 14491950., 15712259., 17915386., 13396318., 15465428.,
        12482376., 14720655., 16930564., 20102158., 16269369., 13849620.,
        17521100., 14428223., 17417516., 12608217., 14758225., 19221812.,
        12754215., 21071316., 13111368., 16121063., 10219820., 10410744.,
        15495783., 11953935., 15824000., 12960594., 15223454., 17045448.,
        24126270., 17082098., 17625160., 17733174., 14754859., 14100548.,
        18211906., 13408839., 11650000., 12941164., 18544654., 17827338.,
        15808601., 13474217., 15489088., 16919724., 25579322., 12100813.,
        16447525., 15385354., 16273047., 17021066.])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 229964.8594, 1621870.0000,  296421.7188,  ...,   53928.2539,
          96972.4219,   24210.0000])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 928635.3750,  960155.0625,  966317.0000, 1137656.3750, 1022285.3125,
        1005073.2500,  959103.7500,  836201.6250, 1316349.5000,  811643.3750,
        1727990.3750,  837794.7500, 1005148.3750, 1083671.8750,  972887.3125,
         862253.0000, 1304450.0000, 1070775.1250, 1061950.7500,  831882.5625,
        1148347.1250, 1053791.7500, 1371990.5000, 1581532.5000, 1311292.1250,
         996864.6875, 1219857.7500, 1258403.0000,  829723.9375, 1415915.2500,
         932369.5625,  916780.1875,  922724.8125, 1091721.7500,  905642.1250,
        1306050.1250,  869226.5625, 1125523.6250,  849534.0000,  852484.6250,
        1681508.8750, 1213278.1250,  764947.0000, 1274263.5000,  937736.8750,
         762501.9375,  944226.5625, 1454998.3750, 1185197.6250,  900404.2500,
         814152.8125, 1479278.8750, 1285839.1250, 1028868.6875, 1253025.1250,
         943923.4375,  821444.1250, 1060895.6250, 1253226.3750, 1309503.6250,
         838521.8125, 1154078.3750, 1053997.7500, 1372270.3750,  836415.8750,
        1000426.3750, 1042032.6250, 1472778.0000, 1064254.5000,  645496.6875,
        1316983.7500,  991095.4375,  919784.8750, 1287514.5000,  871375.2500,
         952322.0000, 1246056.3750, 1151963.7500, 1209217.7500, 1076166.3750,
        1115011.3750,  723005.7500,  651247.2500, 1077545.6250,  984768.0625,
        1612545.6250, 1006663.5625,  673110.1250, 1100894.3750,  863021.5000,
        1162756.3750, 1013013.8125, 1120857.7500,  883442.8750, 1309624.3750,
        1009923.4375, 1042247.3125,  942092.1250,  711446.3750, 1076337.2500,
        1077619.3750,  891982.1875, 1042579.1875,  900643.1250, 1121672.6250,
         848144.2500,  873393.5000, 1350009.5000, 1236036.7500,  729107.5625,
         753462.3125, 1268939.5000, 1249457.2500, 1610484.8750, 1497552.5000,
         860371.3125,  703390.0625, 1192696.7500, 1093776.5000, 1227650.1250,
        1022469.8125,  843127.5000,  905288.5000, 1061055.7500, 1248697.5000,
        1358278.1250, 1158866.2500,  869656.6250,  805933.8750, 1022471.3125,
        1220568.6250,  919380.7500, 1281952.2500, 1127832.1250,  998952.2500,
        1179871.2500,  824857.2500, 1159098.7500,  840680.3750, 1195836.1250,
         870992.6875,  958274.5625,  823892.8750, 1144321.2500,  912777.4375,
        1367207.3750,  830001.1875, 1094426.7500,  945202.6875,  962453.5000,
         993558.1250, 1362599.3750,  817357.5625,  726123.1250, 1009151.3750,
        1085589.5000, 1461342.3750,  911423.1250,  956156.6875, 1009933.6250,
         990971.0000, 1036537.6875, 1151551.7500, 1043782.0625,  889528.8125,
        1127843.1250,  959145.5000, 1256159.7500, 1200292.5000, 1122664.6250,
        1444998.3750,  747767.4375, 1498504.1250, 1486857.0000, 1299476.8750,
        1278961.3750, 1027826.1875,  961226.6250, 1056750.3750, 1114550.1250,
         957627.6875,  937654.5625, 1102998.0000,  852009.6875, 1167139.8750,
        1158106.8750,  850553.3750, 1154459.6250, 1045526.3125, 1741004.2500,
         935638.3750,  965921.8125, 1486322.3750, 1056222.0000,  935474.2500,
         783446.0625,  868979.4375, 1406206.3750, 1213914.3750, 1343755.2500,
         947331.1250,  926052.3750,  781714.8750, 1214788.8750,  752728.3125,
        1245110.1250,  909452.9375, 1308027.7500, 1041987.1250,  864880.3125,
         714730.1875, 1101957.5000,  703043.1250, 1096555.0000, 1315365.8750,
         894067.3750, 1200702.2500, 1224675.8750, 1166317.6250,  934774.1875,
        1211899.0000, 1019432.5625,  992089.5000, 1160978.8750,  921058.7500,
         901718.9375, 1009540.0625, 1306938.7500, 1005092.3125, 1711431.3750,
        1357200.1250, 1236241.6250, 1185148.1250, 1185202.7500, 1074468.7500,
         866303.8125,  923582.2500, 1003440.1875,  950913.4375, 1351752.8750,
         871593.6875, 1037520.0625,  825585.0625,  760270.8750, 1218224.0000,
        1051220.3750, 1060174.6250,  811161.5625, 1058104.3750, 1139834.7500,
        1213156.2500, 1225230.3750, 1135605.5000, 1164040.0000, 1085023.0000,
        1235644.8750])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4247392.5000, 2264191.2500, 2048328.6250, 3551868.2500, 3284363.2500,
        3125829.2500, 3801985.5000, 4844909.0000, 2584390.0000, 2353156.5000,
        2767177.5000, 2710899.0000, 3366765.0000, 3421710.2500, 2769814.7500,
        2283979.0000, 2420347.7500, 2317627.2500, 4675932.5000, 2207134.0000,
        4498760.5000, 3548986.2500, 3186945.0000, 2874737.7500, 3358061.2500,
        3439064.7500, 2415691.7500, 4801095.0000, 2391101.0000, 2558027.5000,
        3490983.7500, 3193879.0000, 3275885.2500, 3581929.0000, 3477824.2500,
        2847249.2500, 3848768.0000, 3065790.7500, 2881395.2500, 3321127.5000,
        3242616.2500, 3178469.0000, 2818120.0000, 2585968.0000, 2207356.5000,
        3042446.0000, 2724263.0000, 2313051.7500, 3646433.2500, 2783674.7500,
        2420784.0000, 3091136.0000, 2582178.2500, 2260228.5000, 3101082.5000,
        2852769.0000, 2916116.2500, 3157099.2500, 2744996.7500, 2862534.5000,
        3257080.5000, 2496763.7500, 2516189.5000, 3014732.0000, 3062339.7500,
        3666743.7500, 2584514.2500, 2678854.5000, 3005549.2500, 4033332.0000,
        2649063.0000, 3310866.5000, 2327616.7500, 3221943.5000, 3157007.5000,
        3047980.0000, 2776900.2500, 3903681.2500, 2504999.0000, 3806128.2500,
        3582807.7500, 3201637.2500, 3574491.5000, 3252404.5000, 3402448.7500,
        2652394.2500, 3596703.5000, 2838021.7500, 2235216.7500, 3608063.5000,
        3090367.5000, 2151771.0000, 1958391.2500, 3312700.5000, 2892119.7500,
        2442877.2500, 2932747.5000, 3186010.0000, 2623770.0000, 2445188.2500,
        2743673.7500, 3779212.5000, 3384247.7500, 2822264.2500, 3481587.5000,
        2139037.7500, 2858379.2500, 2329208.0000, 2221864.5000, 3020193.5000,
        3700767.2500, 3397069.0000, 3118470.2500, 2663351.0000, 2214718.2500,
        2533965.7500, 2398882.0000, 3802271.2500, 3498802.7500, 2744697.5000,
        3387354.5000, 2686526.7500, 3257496.0000, 3551392.5000, 3493973.2500,
        3714008.5000, 2377227.5000, 2867606.2500, 2478885.0000, 2648069.7500,
        2852247.7500, 2819349.0000, 3543176.5000, 5483127.5000, 2675326.0000,
        4549466.5000, 2242874.2500, 4384574.0000, 3542703.7500, 2846143.5000,
        4423817.0000, 2578436.2500, 3516915.2500, 4671849.0000, 2752555.5000,
        2919121.5000, 2683521.0000, 2446109.5000, 3301928.2500, 2931632.2500,
        2603388.7500, 3419108.5000, 3059949.0000, 2548776.2500, 2959966.5000,
        2756022.0000, 3272570.7500, 2570169.5000, 3110375.7500, 3163984.0000,
        2925172.2500, 3894510.0000, 2247135.2500, 3879204.2500, 4150334.2500,
        3113719.2500, 2961000.0000, 2329020.5000, 3588784.0000, 2550121.2500,
        2865174.0000, 3560813.2500, 1977634.2500, 3320673.5000, 3892285.5000,
        3349093.5000, 3065987.5000, 2973493.0000, 2787205.0000, 3027314.2500,
        3544353.5000, 2760662.2500, 3198910.5000, 3737278.5000, 2484331.2500,
        3086838.2500, 3214848.5000, 3332415.5000, 2683220.5000, 2815138.2500,
        2846515.7500, 2679440.7500, 3263226.2500, 2597278.0000, 3254096.5000,
        3813225.0000, 2829753.0000, 3130022.2500, 2853682.0000, 2242903.5000,
        3064522.5000, 3125541.0000, 2669523.0000, 3582197.2500, 3062580.0000,
        3213133.5000, 3470239.7500, 2999015.7500, 2744286.2500, 2786056.5000,
        3224160.7500, 2605239.7500, 3955719.0000, 3133014.7500, 2835848.2500,
        2840550.2500, 3920693.7500, 2804834.7500, 3201364.5000, 4004391.5000,
        3322725.0000, 3727630.5000, 2084194.0000, 2469959.5000, 2567401.0000,
        3279124.5000, 2414149.2500, 2182405.0000, 2425142.0000, 3113011.7500,
        2327125.5000, 2774128.2500, 3434063.2500, 4017708.7500, 3118697.5000,
        2805766.2500, 3286538.0000, 3777907.0000, 3019555.5000, 3971165.5000,
        3908710.2500, 2224121.2500, 2401639.0000, 3704077.2500, 2202698.2500,
        2706299.5000, 3637581.0000, 2936876.5000, 2843944.2500, 2611963.0000,
        3674738.0000, 2118839.0000, 3685395.7500, 3216481.2500, 2452030.2500,
        2618772.5000])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 13801.3008, 813417.2500,  83553.1172,  ...,   6095.4512,
         11478.3047,   9930.1826])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([253076.1875, 213213.8281, 238269.9062, 254175.4531, 207488.0156,
        218288.4688, 228579.1875, 190932.4844, 221366.7500, 167264.2500,
        266956.0938, 251030.6094, 228710.4688, 226227.8438, 219331.2188,
        221675.1250, 293160.9688, 294471.7500, 173022.5625, 239333.4375,
        222376.4375, 223390.6094, 238111.6406, 240236.8906, 208136.9375,
        293103.9375, 217599.9062, 214356.4531, 211655.8281, 224598.8750,
        225400.7969, 276527.1875, 221210.5000, 179746.3906, 230116.2500,
        191873.5938, 242568.8906, 242270.3750, 241541.8594, 263999.0000,
        373724.0312, 202574.9688, 263756.2812, 206441.6875, 247535.7031,
        222756.6875, 185926.4375, 237569.9688, 204822.1562, 221251.8281,
        187352.6250, 284104.5312, 222767.8438, 316333.6250, 215724.9219,
        248986.8125, 255665.8438, 184250.2031, 247745.2031, 236934.6719,
        240994.0625, 249006.1094, 152780.0938, 195109.2656, 220802.1094,
        230924.3438, 298613.6250, 198842.3438, 258020.3906, 293368.5625,
        208933.7500, 229008.2656, 178482.3594, 201184.7656, 198635.0469,
        244643.6719, 223660.6875, 265582.7500, 294819.0938, 177055.0625,
        242520.2812, 201186.1406, 283899.7812, 185060.8438, 244589.4062,
        291362.6250, 236290.5469, 253137.1406, 220411.2656, 215276.8125,
        200104.4219, 180692.2031, 190859.6875, 231342.3281, 226239.7344,
        231588.3750, 272965.2500, 198797.4062, 203025.5938, 330606.4062,
        252617.7656, 257125.5469, 190523.3594, 219626.1406, 213932.9375,
        207403.2656, 182618.6719, 204093.3750, 191123.7188, 170596.9062,
        198718.2969, 189676.2188, 214496.0469, 203395.7031, 255104.6094,
        186147.0938, 268147.5000, 196846.0156, 209478.5781, 183266.5781,
        308591.4688, 192835.9062, 278858.8438, 235288.9219, 212191.7656,
        250568.6094, 202441.4844, 298729.2812, 207198.6562, 198577.4688,
        243166.6250, 226097.2969, 244894.6250, 183789.6094, 332012.5938,
        283605.7188, 277341.1875, 241707.8281, 180859.9844, 222738.2969,
        219649.4219, 195946.1250, 179565.2500, 217690.5156, 270534.0938,
        224052.4531, 257590.0000, 229232.9219, 244188.5312, 206353.3594,
        202290.1250, 297615.3438, 241649.0156, 245258.6406, 199301.6719,
        183528.3750, 198373.6250, 238351.5469, 267207.2500, 171166.9219,
        276223.1875, 247342.5469, 207977.5938, 217886.1719, 224176.7812,
        192134.5938, 217702.6719, 274039.0938, 225252.7656, 197481.8906,
        191394.7500, 223840.7344, 245983.6094, 269983.5625, 215558.8438,
        269371.0625, 226924.1250, 195856.4062, 204594.6562, 199697.5000,
        222178.8125, 231871.4375, 289536.2500, 190809.4688, 276402.5312,
        259646.1562, 269376.1562, 235601.9375, 175166.7656, 251091.2969,
        201757.4062, 290699.9688, 229757.0000, 286060.4688, 294085.9375,
        167897.9375, 240188.0781, 154072.6406, 254278.9844, 265446.7812,
        202126.4688, 258689.1562, 319359.7500, 226798.0625, 262925.8750,
        235227.3281, 244432.1719, 196062.8594, 199966.9531, 234233.3281,
        214710.3906, 177767.5781, 230082.8594, 234760.2344, 347693.8750,
        185873.1719, 251783.3125, 240395.1250, 222118.1250, 197723.5000,
        224046.4062, 300588.0938, 224142.5469, 223641.5938, 244641.3125,
        193604.8281, 269640.7188, 196398.9844, 243690.8125, 248641.3125,
        202353.6875, 218836.3594, 275955.9062, 245821.0469, 259916.0312,
        226935.4062, 168588.4219, 250300.3750, 272128.7188, 198235.0156,
        226358.1875, 262968.0625, 189775.2031, 229301.0781, 214102.0625,
        233049.8438, 241374.8438, 150720.8281, 227801.2188, 213569.3594,
        213568.0781, 248312.0000, 285535.5312, 209163.2031, 225922.3438,
        313425.5000])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 666867.7500,  906111.1875,  815517.5625,  502312.7500,  755175.0000,
         684375.2500,  852744.1875,  925183.0000,  829299.0000,  963881.4375,
         779080.5625,  729197.0000,  723185.0000,  700679.5000, 1052461.3750,
         864122.9375,  721084.0000,  804983.7500,  977130.5625,  876574.4375,
         888128.1250,  638266.0625,  960935.0625,  821566.0000,  701293.4375,
         791347.1875,  773815.0625,  942153.6250,  661697.1875,  471771.5000,
         844201.0000,  735961.0625, 1062773.2500,  731884.3125,  905013.0625,
         729264.9375,  883058.7500,  721795.8750,  597207.1875,  955238.9375,
         945885.5000,  630276.9375,  901370.4375,  554603.0000,  940121.3750,
         835077.6875,  813934.6875,  725343.1875,  802895.5000,  716413.6875,
         739310.6250,  578107.4375, 1053300.7500,  768057.4375,  844982.7500,
         738196.0000,  751622.6250, 1108200.0000,  579608.1875,  714015.5625,
         780220.0625,  733122.3750,  689771.4375,  808007.1875,  800292.1875,
         852106.3750,  760201.6250,  939020.1875, 1082547.2500,  727929.9375,
         872076.9375,  845347.4375,  708985.8125,  883644.5000,  559429.6875,
         809734.6875,  633587.5000,  914978.5625,  578912.0000, 1060728.0000,
         655599.9375,  823218.7500,  656628.5625,  854603.4375,  933355.1250,
         690670.3125,  671413.1250,  749730.1875,  725012.0000,  870137.9375,
        1110709.5000, 1027159.5625,  970840.6250,  620373.1875,  693406.7500,
         978068.9375, 1085654.1250, 1142840.5000,  843066.1250,  913027.3125,
         904721.7500,  674803.7500,  724044.0000, 1056546.3750,  930370.6875,
         651493.5000, 1060840.1250,  800490.8125,  710191.8750,  787564.0000,
         802834.1250,  946328.5625,  948331.5000,  732001.9375,  827106.9375,
         872837.5625,  793496.5625,  665434.8750,  790620.5625,  990279.0625,
         915953.1250,  883570.8750,  642269.6875,  722825.4375,  729553.3750,
         830073.6875,  711509.5000,  796803.7500,  968611.1250,  799235.2500,
         739000.8125,  866881.7500,  683477.0000,  724948.0000,  765163.1875,
        1150073.1250,  614420.5000,  817320.6875,  758977.8125,  799685.1250,
         903672.6875,  836199.1250,  872005.8125,  831582.8750,  785093.2500,
         833605.3125,  669872.0000,  976767.8750,  860701.8750,  584555.2500,
         791658.0625,  914910.8125,  863103.1250,  745885.5000,  841603.0625,
         989600.4375,  801304.8750,  872334.0625, 1153808.6250,  690909.9375,
         718763.8125,  654899.6250,  781380.1250,  916547.5625, 1120258.2500,
         704378.9375,  861655.1250,  946380.3750,  833014.6875,  846640.0000,
         789276.6875,  939048.1250,  610745.5625,  903589.1875,  761546.0625,
         612602.3750,  646505.5000,  643867.1250,  597579.1875,  740001.3125,
         697625.0625,  822438.9375,  762200.3125,  631834.6250,  730102.6875,
         854276.6875,  810130.8125,  918275.5000,  741045.4375,  823102.8125,
         923275.4375,  856948.3750,  996299.2500,  706772.4375,  873145.4375,
         841850.3750,  702768.5625,  734779.0625,  722839.8750,  727746.2500,
         781460.1250,  679952.2500,  667765.8125, 1144649.8750,  661392.5625,
         831348.5625,  661101.6250,  694870.8125,  981778.7500,  797790.8125,
         926542.6250,  808314.5000,  839618.0625,  822246.8750,  784852.7500,
         744799.7500,  787520.4375,  925002.8750, 1054515.7500,  819975.1875,
         756884.7500,  680951.2500,  961051.8750,  719734.0625,  741927.2500,
         789109.9375,  843532.6875,  814692.1875, 1185590.7500,  589827.1875,
         829015.5625,  669184.5000,  719456.1875,  790992.9375,  668610.1250,
         756778.3125,  959103.9375,  934110.5625,  792023.3125,  657260.6875,
         689222.9375, 1020391.8125,  585349.6875,  734729.0000,  775541.6875,
         759699.2500,  755992.0000,  885842.6875,  956830.1875,  676750.2500,
         945990.7500,  759163.5000,  789399.4375,  759971.8750,  866979.4375,
         911869.2500])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 4720.8770, 10746.1338, 56695.8984,  ...,  1234.5120,  1939.4154,
          348.0329])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([13188.2695, 16763.3750, 10161.1191, 11559.8584, 13283.6270, 14827.5703,
        10371.6523,  9923.8779, 16003.1338, 12288.5234, 14203.4385,  9266.2051,
        11884.4541, 15851.7598, 14824.6055, 10600.5410,  8518.4766, 12292.9092,
        13246.6279, 12899.0303, 17907.9434, 14853.1621,  9180.1396, 10079.9814,
        11015.6748,  9070.8359, 14354.0215, 12833.0967, 13933.3682,  8591.0596,
        12996.5420,  9126.9990, 14156.6211, 16783.1543, 15580.4082, 11603.6162,
        15753.5410, 10778.0137, 13912.0371, 11902.7715, 14810.7734, 19189.2520,
        12962.2939, 15646.0596,  8323.7188, 14931.7969, 15073.5215, 14771.0068,
        14245.1553, 16637.0410, 12938.2129,  9685.4531, 10655.2344,  8618.4766,
        16754.9883, 11387.2295, 11686.8340, 13783.3320, 14345.8691, 15131.0869,
        13790.6943,  9917.6172, 11201.5312, 12943.2461, 15399.3311, 11236.2646,
        10255.4434, 13699.1602, 12175.8965, 11466.9424, 13101.2217,  9698.9268,
        14948.2832, 12044.5498, 13024.0439, 16555.5488, 11982.3506, 18430.4922,
        13045.4541, 17157.1523, 10784.3643, 10828.2754, 21241.6562, 17173.8828,
         8097.5317, 11806.6152, 13051.0938, 14996.4043, 11808.6357, 11861.1006,
        10396.1123, 12166.4785, 13982.5225, 11566.7744, 13746.4102,  9177.1035,
        14285.7178, 17754.1543,  9827.3057, 10346.8906, 11201.6201, 19357.0000,
        14913.6709, 10745.8125, 17374.8320, 14413.5088, 12706.8359,  9318.7734,
        17055.0801, 12025.0381, 15383.5635, 10647.6650, 14015.9150, 16011.1074,
        19980.5391, 10625.8389, 19811.8730, 15940.1660, 18370.1562, 14545.9727,
        10928.9678, 10727.9170, 14960.6045, 14533.1807, 16563.4375, 11831.7139,
        15073.5518, 14462.2412, 11952.3408, 14777.4697, 12483.5557, 12875.3975,
        15182.3242, 10842.3721, 14630.3730, 13131.6943, 11293.5322, 12486.5625,
        17583.9102, 12676.3857, 13688.9619, 18058.9805, 13092.4844, 13854.6250,
        13549.3154, 12830.2119,  8431.9248, 11749.8828, 10179.4268, 15172.1191,
        12424.3135,  8838.8281, 10205.7656, 10895.5967, 12963.8545, 16126.0322,
        19518.9883, 14623.7715, 13172.5781, 15590.7969, 14248.3184, 13227.2383,
        12654.9873, 13430.9590, 13150.4473, 13924.5176, 10568.8047, 13571.2197,
        10834.4512, 10253.5850, 17009.4199, 18211.9922, 15465.8330, 14361.2314,
        14314.4873, 13430.6250, 11433.3135, 14896.0449, 12714.1885, 14125.5654,
        10525.6602, 14422.6006, 10182.8096, 15387.0342, 11361.5234, 14403.9502,
        14582.0859, 15852.6797, 12226.0439, 12758.3428, 13762.2656, 11458.4414,
         7981.0312,  9465.0283, 10377.6582, 11806.9023, 17249.2617, 13762.0264,
        14265.1553, 15922.5684, 12681.4404, 12016.5557, 10486.7705, 10927.9424,
        19915.1719, 11165.4600, 18544.8867, 16598.4102,  9410.6924, 13969.5498,
        11588.5762, 14566.5303, 11910.5264,  9801.7002,  9532.9033, 10012.2324,
        11765.8838, 11490.0811, 10254.6699, 11193.5020, 14248.1484, 18848.4590,
        15289.5059,  9342.5938, 12987.5205,  9039.7451,  8965.6689, 17235.0547,
        13334.6074, 13149.7930, 14153.3057, 12917.8398, 18741.6484, 11199.6436,
        16871.6660, 11428.5430, 17077.1230, 14184.6201, 11188.4609, 11603.0469,
        13120.1191, 14489.5889, 12370.0088, 11469.7900, 16660.9160, 12693.6943,
        19053.2285, 18415.1875, 12101.8027, 13180.4248, 18343.8613, 10930.9062,
        11236.1338, 13143.1943, 13697.8428, 16010.0830, 15753.6348, 16588.5449,
        13646.4688, 13990.3174, 11822.8887, 10600.4707, 12862.5723, 11864.9717,
         8587.7314,  8615.0703, 10538.8125,  8103.4092,  9911.2412, 13378.9385,
        16761.2090, 18645.4121, 19468.9258, 15253.3799, 15613.6211, 11243.8340,
        12644.4678, 19875.8379, 13718.2236, 10275.5371, 13270.2988, 12436.4980,
        11134.7607, 15488.3516,  9412.9004,  8782.1934, 13824.2188, 16976.8457,
        13312.2773, 11471.8564, 13748.8125, 14181.9941, 12072.8203, 12204.4570,
        13459.7881, 13945.6797, 16473.8828,  7970.7041, 11040.2061,  8089.7749,
        11367.2754, 13451.0771, 14600.7930, 10925.4854,  9014.9990, 13944.0723,
        12750.7861, 11372.5898, 18746.0762,  9572.6543, 13065.8359,  9936.3057,
         8198.6445, 12270.9658, 16542.9512, 11284.1963,  9344.6602, 17384.8672,
        15886.6387, 14704.2705, 10700.7305,  8251.3330,  9380.9775, 18653.8418,
         8864.5957, 13187.2441, 14025.5811, 10150.6826, 11274.5625, 19175.3398,
         9164.8672, 15801.6533, 10590.6562,  8972.2812, 11067.7266, 12482.5576,
        11507.0674, 12999.8779,  9763.7070, 12576.5176, 10483.0908,  9465.8125,
        14644.2100, 13766.2686, 12091.0537, 17776.8398, 17918.1191, 11953.4062,
        16167.8301, 13467.2266, 13554.8291, 13415.1514, 12191.6016, 12608.9092,
         9525.7256, 12526.4229, 10620.9502, 10196.1924, 15242.6533, 11279.3623,
        12801.4316, 12872.7090, 10660.1719, 17214.4746, 15386.2598, 12052.7588,
        13981.2500, 10507.7998, 13275.8701, 16021.9639, 11265.5078, 14242.4297,
        11601.9678, 13224.0205, 13461.8262, 15803.0566, 13032.7080, 12214.4727,
        17844.6523, 11719.6064, 12771.7539, 12988.9951, 12221.8760,  9559.2500,
         9908.2344,  8796.0635, 11919.4014, 13076.8867, 11860.4463, 11611.8223,
        14526.5566, 15212.4785, 13987.2246, 14007.7939, 14527.6084, 16299.9531,
        16933.8164, 12117.2529, 10838.9834, 17341.5566, 10088.2686, 11917.8418,
        16036.0137, 15075.1602, 20718.7910, 11523.6982, 12469.1543, 15430.9873,
        13349.5742,  9911.2305, 21386.4453, 10325.7627,  9631.1523, 13021.0312,
        18184.0762, 16563.9062, 20953.6016, 14861.1143, 11059.9111, 14175.3955,
        11735.8848, 12603.4043, 12866.4492, 14653.7588, 11384.8818, 11873.1621,
        14550.2900,  9327.1934, 10225.4463,  9784.2852, 11573.9033, 14039.3760,
        11621.1758, 12858.4189, 11145.6846, 15072.4434,  9818.6445, 11689.3350,
        14104.2969, 14868.7393, 11158.1670, 13970.9229, 12681.4785, 11005.8525,
        11103.4902, 16020.8643, 11383.7617, 15159.8340, 14429.6865, 13720.3750,
        11288.9873, 10942.8154, 14339.8916, 13697.8477, 11305.2510, 11236.0957,
        14211.0918,  9952.8047, 11807.2588, 13709.4941, 17493.5742, 15615.4844,
        14649.6719, 11927.9238, 15015.4150, 10840.5869, 19183.4707,  9845.8594,
        11157.5342, 11569.3125, 14122.0088, 13011.0264, 13363.7324, 14065.6748,
        15296.8828, 17527.9590, 10557.2334, 13968.9092, 11771.9521, 12276.7148,
        10120.5498, 13566.3145, 15443.5059, 16922.9746, 13387.4570, 13465.9014,
        12330.3340, 14923.0908, 11363.8975, 10335.4893, 13517.4111,  7811.7710,
        19715.2949, 14812.6064, 13615.4961, 11051.4639, 12334.0801, 13222.7930,
        13709.8369, 17740.5000, 11960.4141, 14031.0771, 15308.8047, 13655.9414,
        13251.5684, 14779.9287, 12520.4834, 19791.8906, 18774.0762, 12369.1807,
         9455.2529, 15471.8301])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 99658.0547,  85543.7656, 107498.3828, 136184.2344, 130930.9531,
        101612.2656,  68801.6250,  76224.6328,  79564.1797, 112527.9922,
         86730.2422, 103440.5703, 102157.8516, 116553.0547, 101464.7656,
        113680.6875,  91225.2969,  79780.0625,  80485.9219,  89958.3281,
         85138.2578, 118153.0156, 125655.2109, 117914.5703, 103152.1641,
        131863.4844,  82078.1484, 117558.2891,  85272.7266,  80432.5156,
        102995.2422, 104113.1484, 109135.9766, 118392.7891,  87677.4375,
        105329.5234,  94610.5859,  88060.2344,  67415.3047, 113008.7969,
        126305.6797, 110762.9219,  97993.0703,  82069.0078,  74416.6250,
         71473.9375,  62301.3477, 104501.8828, 111480.0000, 110499.0000,
         75704.4844, 133915.9375,  84283.2266, 137029.6094, 116638.2422,
         97697.6875,  96971.6406, 105088.0938, 124738.5703,  99011.5938,
        127274.9531, 121591.5312, 102506.3125, 104041.4453,  77945.6641,
         81327.2656, 103028.7109,  69444.9375,  63836.3359, 114712.5391,
        120348.1562, 107927.6797,  68217.6406, 108647.4453, 110767.9844,
        111109.2969,  98490.2734, 128654.8906, 102341.8359,  76630.7188,
         83529.5156,  89623.2891, 118772.8281, 105907.1562,  90092.6094,
         87732.0938, 116556.4609,  88878.6719,  73338.6094,  80981.7188,
         71526.4062,  90779.3672,  90242.1484,  85467.4219,  97243.6875,
        115291.8672, 110684.8359, 101311.6797, 128229.5625, 128464.2344,
         72636.3281,  86429.7500,  96053.3984,  64807.8555,  97298.9609,
        154841.1719,  93861.5469,  87390.5547,  98735.3516,  84370.6562,
        117818.2188,  66154.7891, 127552.9062, 123893.3828, 130701.1406,
        148400.8281,  98946.4531, 140348.1719, 119699.1797, 120542.4375,
        114352.1797, 107368.3828,  87754.3984,  88777.9844,  89635.1484,
         86940.2266,  70468.5781,  87449.3281, 123425.4688,  72462.4297,
         93062.0312,  84256.5625, 122689.9609, 104535.1641,  83083.4609,
         97166.0391,  82167.6016, 107558.4688,  89433.0312, 113347.5547,
         90288.1875, 140911.3438, 105799.4375, 133134.4219,  98920.2734,
         76454.9531,  74269.6562, 121382.9922,  94750.4766,  80014.4609,
         81747.8984, 135381.4531,  90342.9453,  97854.7578,  93838.0547,
        101459.5000, 120059.2500,  93583.0156,  85946.1094,  90177.9531,
        110635.2578,  84817.2344,  93708.5000,  90715.9844, 128346.2969,
         70649.1406, 138884.6875, 115269.6328,  87149.3359,  95951.7812,
        126547.7031,  88987.5703, 115491.6016, 117186.7734,  79782.3125,
        114879.3438,  98726.1719, 113560.6719, 106328.0312, 107046.7656,
         68397.1719, 110509.9297,  97705.1016,  89432.1094,  80436.9375,
         90650.4531, 122277.8203, 105514.7266,  80504.8672, 154104.3906,
         82890.6797,  89253.0156,  96477.7500, 122270.9609, 121139.4531,
        148041.3438,  81304.6172,  97274.7891,  83198.0547,  79101.7734,
        100025.4375,  72918.5781,  91590.9844,  82930.0547, 136517.3281,
        104314.2344,  72816.7656, 103872.6406, 132020.2969,  74280.8828,
        123538.6797,  94198.8516,  80688.4062,  77359.1094,  68965.4844,
        114736.8516, 117547.3984,  91932.5781,  97036.3828,  83340.8125,
        115648.4062,  86938.6250, 130634.3594,  91374.5000,  90521.5938,
         85895.3672,  86936.3750,  88089.9609, 126853.8750,  69974.7734,
        102042.9688,  72801.7031,  76019.7734, 102950.3984, 104730.8750,
         96823.1484, 102592.7422, 118415.0391,  85356.3125, 130419.0078,
        191461.5938, 100582.2031,  74858.8594,  98591.5938,  89202.9297,
        112948.6250,  96619.2188,  96299.9531,  96155.1953, 105481.2656,
        122797.8750, 138715.1875,  98606.8906,  91774.4609, 123711.0625,
         97959.3984,  98046.9766,  84009.9297, 100748.1719,  78472.2812,
         73120.6641, 109229.7578,  86649.4062,  80859.9688,  78440.0156,
        102221.7422, 103393.1328, 126420.9609, 108626.7578, 102441.4688,
         72333.6719, 120881.2656, 107291.6016, 136587.0312,  67927.8828,
        127399.9844,  86103.2578,  79905.3438,  94656.1172, 120224.7422,
         71784.7422,  89390.5469, 105887.7969,  68553.1016, 106987.6641,
         78329.4688, 115199.1953,  85158.7969,  80490.7734,  76514.4453,
         89715.3203,  65338.9453, 123220.1094, 128286.2891, 112320.1094,
         86585.4688,  68015.8906, 101785.8359,  97866.0312,  95243.6641,
         82178.8438,  91867.2734, 123264.8047, 132634.9688,  97840.3828,
         64671.5742,  77106.4141, 112859.7422,  93029.1406,  84941.5703,
        100356.6797,  95117.9531, 134431.2344,  84458.5078,  90761.7812,
         58247.7109, 148403.7188,  71030.7969, 113911.2578, 110976.7031,
         75979.2344, 129504.5469,  86726.6016,  78780.0859,  97866.9844,
        149457.4844,  84161.0156,  87936.2344, 120745.8359,  97591.6641,
        152125.8438,  90337.3359, 120930.1719,  78225.2969,  74181.4219,
         83865.1094,  95747.8203, 116070.5469, 119510.8672, 125386.2266,
        121644.4297, 153190.8594,  94296.9844, 108684.7031, 137474.6719,
        119447.7422, 125684.8906, 159953.0781, 100298.5625,  97887.7656,
        119564.3672,  84746.3281, 110500.5938, 125910.3516,  63880.3906,
        106213.8516,  87304.2188,  92506.5078,  91922.7031,  81629.6016,
        103272.0234,  69634.6484,  91611.5625, 125073.5781, 105844.8203,
         63379.6133,  84679.5938,  87107.0703,  84524.4219, 129313.8906,
        136842.2188, 140154.0938,  69571.4141, 118398.4922, 132469.9375,
         93728.8203,  88546.2656,  70452.4766, 104740.5000,  81694.6797,
         79019.5312,  95729.6328,  73345.6250, 102346.7891,  66971.9609,
        105453.9531, 144953.8750,  76940.9844,  81301.2734, 135618.1875,
         95655.6016,  72044.7969,  84405.2109,  99717.6094, 110833.1250,
         93328.5859,  87150.0625, 105793.3984, 101170.5469, 100439.9141,
         86878.0938,  98596.8828, 100731.7656,  78572.4453,  69704.4844,
        119247.7188,  83277.9531, 101471.5859, 109902.3203,  90060.1406,
         78612.0703, 105285.9062,  92227.5469, 113659.4141, 100690.9844,
         82091.7266, 101904.3516, 110079.4766,  86700.7031, 100697.8516,
         80061.8359,  82017.1797, 135249.1875,  92558.8750, 102462.8438,
        102648.1797, 141449.2031, 104697.5938,  72661.4766,  89121.6328,
        113261.2578, 111074.4609, 110214.1719, 137501.1562,  63324.2148,
         95379.0391, 126913.2266, 107214.8047, 108657.8672,  84683.6562,
        114386.9219, 108991.8594,  98982.4531,  90348.1016, 121686.2188,
        115547.3594, 108593.8906, 104914.9609, 101059.8047, 137575.2656,
        102737.5547,  92515.1328,  99776.8047, 101073.3750, 117164.8281,
        119052.9453, 117343.9219, 129421.3047,  64706.0000,  84777.9609,
         91185.4844, 121706.2188,  99137.1953, 116084.3906,  71196.5234,
         89893.0156, 100878.3359, 108984.6328,  93178.0625, 104712.1953,
         92298.7969, 114127.5156, 140271.8906,  74797.9688,  73180.4297,
         79655.1484,  96073.8516,  67192.6016,  97860.2422,  91873.7656,
        100968.2734, 105591.2188, 107176.6562, 101324.4375, 116924.6016,
         90587.0000,  71192.2812,  80560.4141,  95535.8359, 112063.0938,
        103959.7500, 114035.0234,  69945.2656,  76726.6719,  87460.5781,
        107939.8672,  83465.6406, 103887.2422, 137444.5625, 103183.4141,
         95781.6562,  84740.0000, 145268.4531,  85306.3516, 167569.4375,
         85803.2500,  79691.6328,  95313.2891, 138986.0625, 109438.9922,
        109512.8594, 134638.5312])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.9898e+03, 1.2314e+05, 1.1625e+04,  ..., 1.0664e+04, 7.4060e+00,
        1.4150e+02])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([  6863.1206, 122872.6797,  12241.5850,  ...,  11289.7793,
           582.9120,    778.5390])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2345.0012, 3707.1597, 3794.8745, 3413.4009, 3161.7112, 4128.2671,
        3844.0159, 2779.1343, 6649.2397, 3755.6055, 3337.5161, 3126.4768,
        3238.1191, 4428.2217, 3108.5444, 2757.5801, 4072.6599, 3278.0178,
        2719.5735, 3953.5005, 4053.4539, 2398.9004, 4696.2739, 4498.2910,
        2532.5205, 3589.9192, 3637.9778, 4273.5029, 3135.0247, 4008.2542,
        3081.2612, 4746.4448, 3739.7212, 3715.2209, 4468.5015, 3913.8516,
        3240.4705, 3981.1260, 3391.2563, 5233.6221, 3795.4978, 4825.0020,
        5284.5601, 4033.7317, 3761.2944, 5455.0088, 3705.9312, 3996.9297,
        4161.1094, 4337.8809, 4227.3818, 3396.3052, 5134.3052, 3691.3242,
        4205.5610, 3328.0247, 3708.0164, 4403.1206, 3273.6208, 3293.3149,
        4185.5796, 3725.7568, 7264.6455, 3206.3179, 4463.7319, 4126.7837,
        2939.4939, 3625.0767, 2897.6936, 2898.3564, 4460.2334, 3932.2705,
        3041.0332, 5189.7085, 3487.8477, 4209.4165, 2922.3689, 4098.3398,
        2675.1829, 3146.9905, 3726.7969, 4161.0513, 3370.7817, 3398.3384,
        2690.9622, 3186.6755, 3876.7063, 3264.3477, 3138.8569, 4748.0732,
        4555.7134, 3146.9336, 4805.0327, 3603.8650, 3553.9360, 4649.6782,
        2457.8928, 4847.4854, 3097.5630, 4682.7793, 3949.3093, 4235.3569,
        4860.0918, 3658.5950, 3763.8882, 4677.9966, 3742.5183, 4780.6240,
        3285.3672, 3326.8899, 3614.3552, 3946.8865, 5090.4746, 4850.5747,
        4111.3237, 2935.9260, 2660.1299, 3203.7307, 3214.3831, 3517.5159,
        4552.2817, 4513.7363, 4686.4888, 2716.3914, 2782.1909, 3819.2275,
        3993.5137, 2661.2639, 4473.9502, 3371.5349, 4077.5647, 3408.7346,
        4445.8101, 4203.2427, 4341.6313, 4670.3057, 3965.2737, 2700.4238,
        3538.2869, 3170.2432, 3508.5972, 6522.3203, 3846.4197, 4233.7695,
        3445.9165, 2915.0833, 3202.1060, 2950.9849, 5431.6187, 3491.8589,
        3237.0835, 3205.5415, 4873.6201, 4558.4897, 4568.1616, 4617.1519,
        4006.4458, 3218.3123, 5089.6367, 3221.3750, 3362.2861, 3319.3145,
        2846.5784, 3955.5059, 3255.4746, 4999.8794, 5939.3662, 4325.5103,
        3433.6836, 4975.2578, 4461.7441, 2127.1948, 3970.9824, 4279.7661,
        4885.1396, 6520.0264, 3543.6948, 4704.9219, 4734.6245, 5554.3442,
        3493.9014, 3448.9512, 2674.2490, 3326.8140, 3457.7993, 3606.6343,
        3408.6321, 3756.8242, 3163.9875, 3266.4138, 4144.3335, 5607.0337,
        4016.0647, 4068.1528, 3253.6990, 3492.5874, 4346.9463, 3131.0325,
        3762.9424, 5138.3066, 3986.7632, 4078.4309, 4745.4395, 4245.0654,
        3426.1931, 4780.6641, 4367.3159, 2879.3049, 4052.9436, 4586.6294,
        3952.7239, 4699.4272, 4246.9878, 5299.8633, 3779.7742, 4616.2593,
        3340.0081, 3296.2886, 3554.6882, 3241.1819, 3253.9282, 2983.5273,
        4842.6895, 3065.5122, 3357.8682, 3361.6555, 3413.4702, 3756.6384,
        2914.8564, 5215.4590, 3682.9189, 4473.7661, 3087.7712, 3290.5225,
        4911.0483, 3687.1399, 3888.3870, 3664.1072, 4854.8950, 3725.6682,
        4350.2100, 5606.4287, 4921.4834, 3037.8845, 4027.5059, 5123.2871,
        3356.0381, 4981.5718, 3716.6567, 3753.4246, 2980.0569, 3909.1929,
        3857.9939, 4256.8086, 3192.2688, 5620.5264, 4166.5605, 3090.5837,
        3298.1133, 3380.4570, 7795.6045, 5217.8252, 3590.4927, 4215.6147,
        4052.8762, 3955.3413, 3121.1746, 2897.1526, 3603.4646, 2642.2751,
        3959.7385, 2835.9609, 2971.5947, 3518.7029, 4028.8943, 4553.1665,
        3859.9883, 3960.7295, 5139.1396, 4149.6777, 4051.7437, 2403.7336,
        5023.1611, 3589.3542, 3266.3584, 4313.6147, 6027.5239, 4441.3291,
        3655.4185, 3445.6523, 2904.7297, 2916.1741, 3966.0774, 3593.5527,
        3344.4031, 3156.0667, 3955.9307, 2772.2996, 3226.8838, 3425.3379,
        3609.0715, 3551.9248, 5406.3203, 4414.8564, 3182.2007, 3939.0156,
        3285.6863, 2598.9031, 3823.4797, 2922.8220, 3369.7495, 3981.4814,
        4446.2437, 3587.3250, 4605.8633, 3544.2681, 4049.5361, 3890.5137,
        4629.4775, 3404.7544, 3397.6877, 4376.7959, 3105.8435, 3958.6316,
        3127.3135, 4190.8599, 3338.6816, 4776.8901, 4762.9805, 3468.0627,
        3643.6204, 3356.7678, 4350.2856, 5883.8218, 3592.4465, 3793.2544,
        3043.2710, 2970.7947, 4139.8501, 4719.3374, 3833.2935, 4321.2148,
        3041.5991, 5458.5171, 3513.4775, 4542.3823, 4076.8594, 4652.9150,
        3501.5940, 3830.3542, 3676.8704, 3963.2258, 4845.9482, 6488.2100,
        2900.7617, 3232.2212, 3099.3372, 3219.6575, 3769.5574, 6332.7593,
        4252.4351, 4384.6221, 3656.3958, 3779.4421, 4507.0269, 2583.2961,
        3011.8796, 2478.3782, 3080.7581, 4653.5605, 4552.0522, 2963.0679,
        4107.1362, 4087.4263, 5392.3945, 3541.6421, 2702.6060, 3399.1721,
        3437.1663, 4984.8931, 3050.9150, 3456.8020, 4427.7129, 4311.7139,
        4323.3687, 4458.7793, 3675.2317, 3027.8530, 4860.0352, 3816.6633,
        4286.7939, 3180.3423, 4254.6768, 4827.9126, 3409.3459, 2946.7751,
        3624.1841, 3064.4607, 3693.3369, 4618.7778, 3776.3613, 3966.2522,
        3239.4888, 4166.6167, 2813.2925, 4173.5146, 4665.8481, 4394.0059,
        3508.0676, 5108.3257, 3393.2080, 3927.8884, 4103.6177, 3391.4243,
        2674.0876, 3923.0396, 5758.7578, 3948.0852, 4345.3677, 3728.4460,
        4136.6147, 3558.5266, 2463.8911, 3244.8015, 3107.0061, 3381.9822,
        3296.3848, 4411.3921, 5236.7725, 3167.1055, 4607.2988, 4363.3662,
        3363.9558, 3479.3506, 2778.3337, 3647.3547, 3539.1604, 3613.2612,
        3948.9231, 2927.5645, 3511.5022, 4041.0452, 4079.4678, 3352.9709,
        4116.7046, 3987.7747, 3567.4126, 4087.6304, 3126.5234, 3594.0156,
        2597.2668, 3501.8342, 4141.8857, 4191.4199, 4253.0552, 3711.9573,
        3650.6821, 4227.0938, 3871.5791, 3317.1372, 3727.3845, 2667.1738,
        4196.9395, 3808.6204, 3855.7175, 4042.4250, 4617.2695, 3131.4663,
        3870.7920, 3840.9412, 3726.2751, 3751.1484, 5251.1538, 4093.9678,
        3359.5425, 3370.3245, 4004.0356, 4645.5454, 3119.4543, 5365.4409,
        5164.0156, 3852.9287, 2833.9087, 3719.1355, 3824.8354, 3828.6401,
        3631.6899, 3885.8704, 4158.1919, 4167.8218, 2980.4421, 3392.5862,
        3161.2185, 3804.4512, 3565.7856, 3222.1824, 5160.3862, 3878.0991,
        3362.9285, 4494.0278, 4634.8545, 2609.3738, 4367.5498, 5340.4790,
        3578.0129, 4165.8750, 4783.6836, 3620.8823, 3145.1880, 3909.0127,
        3615.1929, 3880.4363])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([33288.4258, 25784.9023, 28791.9453, 29166.2305, 30677.7832, 15872.1699,
        23659.0918, 27021.9727, 33231.9805, 33495.1484, 45244.3164, 32873.9688,
        32179.3496, 27252.6309, 30919.9980, 40173.4062, 30579.3574, 31241.2949,
        36776.8828, 22603.1406, 25440.4609, 27196.8555, 21705.4980, 27146.9004,
        25377.1523, 20079.1074, 26136.3887, 24555.0957, 23885.6348, 37698.7539,
        26994.9766, 35858.3047, 29975.8965, 27399.9141, 22900.8047, 27927.4395,
        43870.3203, 34209.0547, 30829.9727, 23383.9375, 25867.2832, 37404.1484,
        25598.9902, 25096.9336, 26866.6426, 19203.1875, 27286.0605, 34274.5430,
        21485.8672, 25374.2715, 30817.8105, 32994.7227, 23455.7129, 26683.1094,
        31324.6465, 29167.5449, 18735.2617, 30626.6934, 30446.1875, 21083.9883,
        27340.9609, 34160.4453, 24481.6328, 32499.8789, 31147.3672, 24996.6797,
        26448.4395, 28297.9336, 30222.3906, 25541.0742, 23361.6914, 29616.9141,
        30510.5703, 26027.1777, 28082.0117, 22628.8730, 30455.5508, 35526.6406,
        31945.7422, 28133.8086, 23590.7852, 32813.9727, 37600.0156, 40731.3711,
        33637.8516, 30967.4805, 32398.8750, 30243.4824, 30111.8008, 38028.1289,
        27409.1406, 22996.4180, 31075.9570, 25473.2695, 30145.3496, 23404.5254,
        32084.9336, 24974.1660, 31694.5938, 34492.2344, 23955.7148, 28438.2988,
        32866.0859, 27179.4277, 31087.3359, 25458.0938, 21832.6699, 21367.7109,
        25958.1035, 39898.3750, 49889.3945, 28536.3613, 33057.6719, 23147.0195,
        29015.2891, 33230.8633, 29217.8145, 35350.7773, 37845.8438, 22012.3184,
        20569.4453, 27019.2188, 39394.1094, 32141.9609, 29747.9570, 35775.6523,
        22673.3301, 34592.5664, 27426.3047, 29577.6055, 34912.4570, 25122.0449,
        26277.2090, 35182.0156, 26806.8555, 28514.9434, 32530.2988, 32482.5840,
        26626.7676, 35469.6367, 32189.0020, 24900.6699, 38063.3320, 21909.5957,
        34090.9805, 20070.9727, 32340.8262, 35694.7969, 26883.5098, 26322.8809,
        25801.4160, 36544.6445, 21419.3926, 25968.5293, 23850.3711, 20819.6836,
        28474.2461, 37364.5156, 29829.4062, 23733.8301, 28963.2188, 39384.8398,
        19617.3262, 27726.1797, 33894.1680, 20226.1680, 38284.6406, 29543.8438,
        28863.2793, 26115.8945, 23580.7598, 22571.6816, 26679.5488, 41545.4688,
        35456.5781, 32143.7598, 26874.8457, 28389.5762, 24023.2793, 28571.0195,
        34650.5898, 28399.8359, 24195.4062, 29790.0449, 24137.9668, 32845.3359,
        38492.3047, 29072.8125, 30595.9766, 23590.7305, 27704.9453, 24015.7383,
        27482.9355, 40119.9570, 27061.8848, 30171.9297, 37362.6484, 32599.6719,
        27577.3516, 23188.3262, 27328.6758, 32594.4316, 25233.8770, 31815.6836,
        24282.6582, 24985.0059, 26809.7559, 32438.1602, 28117.5488, 19201.5449,
        33822.7734, 24717.1621, 25475.5117, 27821.2969, 33433.9414, 19217.4043,
        27766.4707, 26161.0430, 24147.3027, 27799.8359, 37412.8398, 36621.7930,
        38232.4531, 27061.0801, 43606.4844, 30074.2168, 26716.5547, 33709.4414,
        25103.1016, 30220.0352, 28755.4336, 25505.2441, 30213.1074, 27982.6797,
        24948.9082, 16658.2559, 26118.0879, 26430.5645, 27581.6133, 24911.7266,
        32660.9258, 29490.0059, 27927.8828, 29861.8945, 27623.4141, 36457.0742,
        32564.5625, 30654.3047, 27323.1055, 20474.9785, 26891.3066, 29145.8730,
        35010.9141, 22282.4180, 30282.4590, 31730.5234, 33305.6719, 33722.5625,
        26058.5898, 24936.8867, 20087.4668, 22019.1191, 34116.9961, 35593.3984,
        28359.8477, 35481.5703, 25625.1641, 29194.2227, 29005.4980, 34580.4570,
        20983.3223, 29181.2207, 26304.6953, 27993.8379, 28073.3750, 27298.0977,
        22543.7520, 32303.4883, 22543.7578, 37552.6484, 23563.7441, 20004.1387,
        29982.1992, 31087.7090, 34524.8672, 26518.2031, 21118.0723, 32961.4023,
        24357.9512, 28153.1191, 29958.1523, 38045.9414, 32227.2051, 30428.2012,
        35068.3203, 30795.2480, 26607.9043, 28345.1797, 29748.6777, 28512.5840,
        30080.7715, 28412.0000, 38071.8438, 23273.7734, 28114.8906, 29814.0898,
        29049.9883, 29474.5371, 27617.7715, 30605.6035, 19338.2676, 28626.9805,
        37702.9258, 31349.5781, 27311.1289, 27015.0312, 24142.2129, 45808.1680,
        35657.3594, 31908.7598, 27152.2363, 19186.5605, 25102.8516, 23885.2988,
        34782.1172, 26621.5332, 24259.3535, 26923.8809, 41621.3047, 26294.1348,
        30321.9004, 27663.5918, 27576.9590, 27202.0352, 32647.3145, 27346.5977,
        26654.8906, 24104.1191, 26616.4043, 28394.8281, 21311.5000, 28031.8633,
        36355.0898, 24531.6895, 31492.8809, 17743.6074, 36853.3398, 33480.9570,
        26252.3691, 28176.4902, 28676.9746, 27688.0293, 37709.7148, 34969.6836,
        23558.3340, 26841.3438, 21328.2793, 26096.4863, 30546.8086, 27707.5684,
        39347.1680, 32934.7227, 24753.4219, 22715.9219, 30355.1504, 29759.5977,
        30897.5430, 26917.2773, 26774.7383, 30861.6758, 29309.0977, 28983.3320,
        18760.3789, 29085.9668, 19703.8301, 24040.5059, 23385.5723, 32568.7227,
        46318.2617, 18775.5156, 27928.8652, 24965.8301, 22105.0312, 30060.0801,
        25263.7871, 33027.3359, 19835.6953, 28649.0273, 28108.0371, 29885.8789,
        36806.8633, 29912.1016, 31979.4609, 35196.0156, 19901.6738, 31816.4336,
        23650.8301, 26226.1680, 25537.8633, 34358.7266, 27825.3457, 29850.5625,
        27155.2363, 28829.2949, 25494.0293, 30712.2480, 29416.9023, 30701.5352,
        37492.0078, 27339.4062, 26698.2109, 39073.8672, 20028.6328, 27212.3809,
        30861.2520, 30297.5664, 23849.4512, 38365.9727, 32739.8340, 29955.9082,
        27502.5234, 29427.7773, 36710.8789, 34693.8516, 25499.9980, 41961.0273,
        24510.8340, 30076.1426, 38397.4062, 20963.3848, 24195.9707, 28596.7207,
        25495.8184, 34938.6367, 25776.3164, 31917.9746, 18222.3301, 50549.5195,
        26310.8672, 30660.3945, 26189.9609, 35330.4023, 31488.6484, 30809.1211,
        27793.0449, 40105.6758, 25178.6504, 31283.4375, 28350.4902, 24752.8848,
        36997.2812, 28204.5371, 25744.0742, 29709.7969, 32131.3789, 28453.4004,
        34828.2891, 21622.4727, 29563.5312, 28036.5605, 28122.7559, 24310.2695,
        33180.7344, 41640.9648, 26526.8691, 27293.2871, 31803.2344, 19121.4414,
        44613.4023, 26072.2012, 26257.3945, 28209.7324, 29013.5918, 26345.7070,
        23914.5898, 26697.2559, 23869.1367, 27677.4551, 34246.3242, 25749.7910,
        28847.3750, 33515.9727, 30371.6719, 31076.7266, 28983.1113, 24385.3750,
        24578.1973, 33307.5781, 26226.8145, 27227.3066, 21481.6953, 32345.7500,
        21787.5781, 35139.8555, 27501.4805, 29702.3145, 35197.9336, 41572.5508,
        27330.2656, 28846.3750, 32998.1250, 32720.4844, 34420.7266, 22901.6035,
        28876.5742, 32971.6250, 36235.9141, 34821.9062, 27642.7227, 21711.1074,
        31767.4941, 24500.2051])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1237.9908,  205.0582,  367.0164,  ...,  648.9700,  257.2404,
         262.0188])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1022.7569, 1062.3546, 1326.5166, 1434.3528, 1498.3915, 1412.9492,
        1258.6722,  685.6587, 1591.9436, 1168.2991, 1323.3201, 1427.0109,
        1106.1173, 1103.1803, 1253.6945, 1224.7893, 1459.6498, 1302.0465,
        1364.4613, 1206.0807,  871.4133, 1175.2682,  970.0175, 1225.7065,
         917.9492, 1401.6206, 1157.5579, 1103.4149, 1230.1411, 1196.8048,
        1262.9852, 1210.4738,  986.4041, 1262.0912, 1562.2958, 1580.3666,
        1576.5093,  993.3828, 1546.3270,  955.2178, 1416.8137, 1233.2424,
        1569.2096, 1527.4816, 1254.5443, 1170.8663, 1183.0323, 1382.2445,
        1233.6805, 1431.3083, 1272.3839, 1029.4375, 1395.6407, 1236.7448,
        1016.5809, 1196.2177, 1214.9252, 1108.8010, 1529.8290, 1514.1472,
        1182.3665, 1135.6788, 1002.9480, 1346.6510, 1031.8975, 1285.1956,
        1087.6470,  934.3832, 1025.7935, 1190.5964, 1367.4169, 1293.2239,
        1025.6152, 1035.5563, 1184.0477, 1162.7323, 1026.0165,  890.2399,
        1261.9948, 1771.1128, 1181.1270, 1142.1094, 1067.8829, 1277.2355,
        1056.4253, 1373.5038, 1003.7043, 1061.5786, 1362.0867,  981.7470,
        1454.2139,  828.5020,  774.2354, 1144.0989, 1310.9714, 1102.6971,
        1318.7751, 1242.6007,  756.2519, 1226.9728, 1193.6300, 1321.5337,
        1532.5983, 1327.8799,  956.7744, 1263.5155, 1564.6321, 1123.9037,
        1192.7863, 1398.5801, 1320.9813, 1084.7275, 1079.7882, 1036.9822,
        1450.0109, 1078.5890, 1482.1019, 1238.2271, 1305.4415, 1468.0004,
        1193.6255, 1202.3612, 1629.0487, 1266.6573, 1688.3002, 1328.8425,
        1022.3351, 1234.5111, 1114.3248, 1094.2520, 1172.3074,  711.7709,
         966.6601, 1220.9379, 1109.3962, 1478.2972, 1214.4587, 1531.3170,
        1399.4961, 1266.7396, 1232.4990, 1651.8442,  927.6999,  819.0856,
        1048.5383,  973.6068, 1066.4531,  978.1001, 1131.9968, 1439.7484,
        1298.1294, 1694.0120, 1457.0112, 1347.4069,  959.6149, 1177.6552,
        1661.6292,  983.1052, 1273.7573, 1153.7800, 1080.7037, 1772.8580,
        1792.7242, 1036.1801,  969.3191, 1077.9800, 1127.6210, 1568.7048,
         871.8293, 1381.1873, 1135.3005,  898.6942, 1183.9244, 1289.9409,
         979.1674, 1058.0540,  974.5714, 1479.9797, 1095.0079, 1677.9822,
         976.4861, 1227.0896, 1335.9022, 1303.4427, 1436.6226, 1409.2137,
        1005.1000, 1305.9132, 1038.8701, 1539.8715, 1243.6277, 1790.8162,
        1078.5770, 1196.4600, 1328.5353, 1336.5077, 1472.4211, 1354.8997,
        1235.0927, 1420.2487, 1327.5155, 1217.6864,  957.4551, 1169.8535,
        1593.1666, 1225.4811,  885.3616, 1008.9305, 1366.2521, 1161.9194,
        1429.7872, 1170.3949, 1212.2559, 1626.0178, 1269.0052, 1573.5763,
        1182.0007, 1396.6672, 1213.7599, 1174.1681, 1334.1359, 1042.3025,
         892.2108, 1241.5801, 1199.9923, 1214.3608, 1261.5369, 1288.3955,
        1163.7289,  927.4079, 1804.9156, 1230.5521, 1270.5989, 1163.6687,
        1632.2264, 1147.2650, 1970.7142,  942.1271, 1455.5256, 1499.1189,
        1424.3789, 1181.3545, 1053.8666, 1365.5682,  960.9783,  775.8193,
        1001.1619, 1193.1709, 1677.8427,  759.2520,  814.5891,  828.4031,
        1262.3195,  926.5740, 1315.5994, 1386.1829,  944.0957, 1385.0791,
        1423.4708, 1068.8744,  825.4818, 1083.2776, 1306.0635, 1237.7700,
        1242.7909, 1333.1335, 1467.6804, 1361.6213, 1074.0522, 1134.5232,
        1167.9530, 1473.2673, 1696.4835, 1072.2430, 1229.1139, 1042.5494,
        1180.3630, 1520.2374, 1471.1472,  899.2190, 1001.0500, 1451.2952,
        1455.4940, 1082.0443,  805.7118, 1546.2758, 1067.7186, 1106.9331,
         845.5047, 1289.3142, 1444.5339,  860.4728, 1451.7146,  942.9360,
        1453.8749, 1654.1042, 1282.7900, 1324.8644, 1239.2006, 1361.8992,
        1126.7867, 1356.6399, 1008.5405, 1443.2004, 1563.8429, 1326.7042,
        1161.5906, 1425.5756, 1315.9398, 1349.2675, 1159.7660, 1441.8519,
         864.9673, 1273.2310, 1354.9402, 1038.7357,  950.8418,  939.2317,
         855.3300, 1386.2433, 1134.0895, 1225.8669,  819.4625, 1541.0529,
        1109.5769,  731.5300, 1247.3315, 1112.8588, 1015.1741, 1359.4838,
        1228.3638, 1186.7800, 1406.6971, 1542.4218, 1348.5619, 1042.6298,
        1261.4331, 1506.3827, 1190.1008, 1220.6235, 1130.5565,  788.8400,
        1098.8190, 1360.7977, 1217.5819, 1438.3661, 1480.4872,  934.4946,
        1570.2250,  926.2692, 1163.7731, 1210.6780, 1227.9987,  940.7311,
        1536.8131,  891.7686, 1486.4720, 1501.2382, 1142.3361, 1760.0952,
        1488.3224, 1284.3345, 1594.0409, 1247.0464, 1297.6453,  968.6674,
         881.3303, 1070.4957, 1004.6504, 1161.5620, 1346.4901, 1039.0403,
        1243.3439, 1340.0842, 1106.5453, 1322.9686, 1148.3553, 1586.1055,
        1093.8610, 1306.3665, 1259.1863, 1515.7325, 1637.3661, 1019.4575,
        1254.1001, 1276.2472, 1249.4608, 1378.5378, 1324.2892,  896.8486,
        1086.3123, 1060.8761, 1199.0253, 1299.7799, 1431.8080,  983.2015,
        1320.4490, 1019.2004, 1003.9299, 1082.8815, 1682.3644, 1526.1451,
        1133.8531,  860.4131, 1002.2352, 1435.4305, 1452.8735, 1432.4938,
        1170.0826, 1314.5396, 1078.2461, 1027.6503, 1169.6396, 1214.5703,
        1390.7045, 1244.2114, 1070.2490, 1213.2866, 1488.2828,  970.7057,
        1413.4764, 1311.8218, 1107.9803, 1168.3270, 1402.5071, 1056.5464,
        1184.5836, 1288.0867, 1504.1858, 1510.6338, 1326.3536,  976.1419,
        1116.5758, 1348.4736, 1095.9607, 1268.8402, 1237.8906,  934.5187,
        1395.8114, 1146.1041, 1191.6278,  974.6891,  807.8442, 1131.6550,
        1071.8983, 1179.9030, 1008.0327, 1406.0968,  978.5762, 1305.5708,
         995.5564, 1047.0135, 1038.0709, 1549.6512,  989.7262, 1244.2505,
        1264.2469, 1268.5974, 1089.8221, 1115.2919, 1124.9783, 1126.7395,
        1407.1150, 1276.6685, 1234.8645, 1348.1351, 1069.6263, 1360.9137,
         915.2034, 1115.3187, 1449.2546, 1191.1934, 1145.6760, 1430.7122,
        1064.3595, 1148.2217, 1176.9559,  971.2924, 1261.6133,  745.7697,
         825.5030, 1039.8297, 1505.9553, 1151.4961, 1185.4705, 1135.2758,
        1820.7400, 1190.6050, 1691.6866, 1191.5731, 1107.8190, 1203.7953,
        1167.4551, 1078.1351, 1379.2891, 1010.7082, 1061.3682, 1342.0750,
        1374.9528, 1409.4933, 1039.1063, 1349.7251,  964.7038, 1295.7911,
        1149.9847, 1237.6874, 1328.9944, 1068.8240, 1362.5800, 1001.9911,
        1130.7649, 1509.7139])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 8156.2417,  8111.8418,  7127.8188,  9830.4697,  9315.3740, 12775.7480,
        10205.5254, 11510.8184, 14279.5049, 10581.1182,  8311.4512, 13844.7412,
         9990.0410, 11757.4570, 10583.3506, 11641.0664, 12803.9111,  8243.4375,
         9855.1357, 10609.0342,  9139.2686,  8419.2705, 13340.4502, 13841.1084,
         9700.9932, 10608.2803, 12479.7510,  6823.6265,  9951.0283, 12624.3672,
        10877.3818,  9888.3408, 12567.5371, 10734.2109, 11659.4727, 11229.1572,
        12240.8486,  8464.7266, 12496.7363,  8506.8779, 10828.1025, 11880.7217,
         8231.7354,  7889.6113, 10699.4258, 12513.5303, 11080.5352,  9791.5117,
         9424.4512, 10902.9531, 13663.7529,  9631.4189, 12718.3809, 12880.7666,
        10748.3408, 12374.8818,  8027.6172, 12130.1016, 13019.0371, 12403.2412,
        11956.4668,  8587.3975, 11490.0400, 11203.1016, 10885.6562, 13053.9971,
         8850.1582, 10364.3926, 10922.5215, 13199.2510, 10136.9316, 10489.5410,
        13053.7861,  8458.4814,  8545.1250, 14036.4043, 10812.8496,  7956.7109,
         9208.0820, 11389.5732, 14934.7705,  8049.2153, 12394.5498, 13449.9990,
        13580.3584,  9836.4443, 15630.0322,  8938.9189,  9682.1377, 10098.5537,
        13231.3242, 13028.5508, 12203.1543, 12924.4971, 10779.0049, 11147.7744,
        10915.5557, 16499.9336, 10589.5410, 10587.3789, 12325.7539,  8158.6455,
        10839.8408, 10827.9375,  8157.5396, 10924.4219, 10332.2871, 13275.5791,
         8803.7305,  8016.4258, 10384.4385,  9027.6201,  9378.7041, 11322.8906,
        10746.5479, 11617.7803, 10197.7666, 13154.3711,  7545.7686, 11150.2002,
         8372.6211, 11394.9883, 11752.2969,  9231.8184,  9414.6406, 11535.9404,
        12667.5156, 12100.9043,  9386.2012, 11814.1172,  7814.1660, 10125.5459,
        10294.1846,  6866.5093, 12974.0439, 12199.1475, 11735.4121, 13040.3623,
        16004.6660,  9595.6299, 10380.1514,  9692.3486, 13008.5977, 10768.1221,
        12807.9873, 10639.4082, 10934.5342,  9396.3057, 11531.3057,  9434.8760,
        14665.3125, 10510.0078,  9263.5791, 10172.9014,  9324.3496,  9753.7529,
         8362.0967,  9760.5400, 12591.5977,  7147.6240, 10013.9717, 11128.6650,
        11889.0127, 10389.1885, 13974.1611, 10980.4092, 13952.5312,  9981.4121,
         8814.9111,  8917.8750,  9402.7598, 11011.0996, 10479.7129,  9663.2510,
        11481.8604, 12350.5342,  9907.7734, 12962.4209,  9843.7295, 11020.6973,
        12167.6279, 13566.2139, 13204.6426, 12799.1064,  9215.3975, 11768.2939,
        14446.3896,  8909.5352,  9002.2227, 11367.6445, 11827.4629, 16551.8184,
        13060.9316, 14355.7061, 12796.8301,  8791.2197, 13279.0195, 15223.3330,
        10274.4180,  8218.4170,  9541.2998,  8742.4688,  8821.5430, 10208.8828,
        12763.3262, 11915.1934, 10193.9238, 11968.1152,  8187.3521, 12518.4092,
        12287.0342, 12962.8584, 12415.6318, 13173.4355, 11954.8018, 14157.5908,
         9875.1064,  8655.2578, 11617.4512, 11919.5508, 12732.7217, 10408.7432,
        10998.7168, 10391.2354, 11623.5303, 12225.0039,  9086.3066,  7852.1055,
        13351.3857, 12498.1553, 14062.6338,  9369.7441, 13134.6270, 11979.5332,
        11355.6455, 10928.8623, 12299.7021,  9900.4893,  8502.5361, 12784.1914,
        10098.3652, 13175.3916,  9459.1377, 12331.5889, 11760.8916, 10639.6582,
        10428.8379, 14785.4404, 12772.0332, 10292.1465,  6680.7109, 14861.4521,
        10473.8574,  9126.5215, 13866.1406,  8252.6689,  9931.4941,  8737.9717,
         9257.4561,  9992.7734,  9045.7617,  8764.6201,  8588.1953, 11443.6592,
        11853.2959, 10696.9893,  7203.3662, 14813.7480, 13404.2686,  9517.4268,
         7963.7139, 10475.3604,  9230.4473, 10628.9707, 13211.4150, 13901.8604,
        11806.0693, 12316.2842, 10930.6426, 11763.2949, 14180.9590, 11634.7646,
         9205.8809, 12348.2920, 14593.4580, 10767.9961, 12133.3428, 11276.5430,
        12349.5068,  7824.2222,  9253.9531, 11369.8838,  8790.5596, 12668.8281,
        13161.9580, 11316.9053, 12450.1318, 10639.0410, 12675.1963,  9264.6934,
         9465.8750, 11887.8271, 10712.7324, 14224.2588, 12114.1768,  7922.9629,
        13464.4561, 12352.1025, 10721.3037, 12475.3984,  9254.2559, 11321.1641,
         9107.6299, 12099.8750, 10343.2178, 13032.1865, 11854.1084, 12364.7373,
         7497.6792,  8550.9619, 12289.6816, 10450.8203, 10385.2754,  8770.2910,
         9978.5244,  7930.7529, 12078.1055, 11489.8838, 15404.1436, 12303.0215,
        10253.8379,  9909.5225, 12151.9600,  8757.2305, 12241.5000, 12770.1377,
        10063.2207, 11681.9023, 13744.6465,  8523.1650, 10608.9648, 11375.3145,
        18318.0137, 10208.5273,  9769.1660, 12186.1885,  8303.9004, 12103.9502,
         8141.2910, 14521.8496, 10130.2051, 13318.0859,  8023.3184, 15591.7148,
        11584.4297, 10760.6104,  9603.6377,  9436.4844, 14451.2207,  9388.4229,
         9057.5654,  9324.6162, 10245.7598,  8704.6084,  9241.6318, 10649.1475,
         9890.2520, 12210.1709, 14732.2920, 10977.2910, 10319.3926, 11156.1768,
        10542.9512,  9541.5918,  7627.0566, 11386.8750, 12150.4229,  7592.4414,
        13202.3193, 11784.2764, 10161.5059, 10562.5664,  8767.8711, 11508.4824,
        10424.4170, 10314.5215,  5696.6987,  8855.6855,  8129.3057, 12342.4717,
        11809.3506, 11613.7090, 11296.6221,  8641.0117, 10592.7246, 11880.4678,
         9059.4023, 10783.9922, 11582.4746, 10608.4297, 10399.2471, 11101.7031,
        13369.0176, 12071.8486, 10746.8418, 11006.2578, 10856.6172,  8437.6191,
         8261.3584,  8387.5703, 13124.8740, 11506.7363, 16232.1650, 17281.1406,
        13785.6387, 15308.5898,  9457.2871, 15233.2705,  9224.2627, 13729.6201,
         9129.6162,  8478.1016,  9061.5068,  9644.1064, 13002.6445, 14418.6904,
         7427.7915,  9867.9043, 14171.1895,  8393.3350,  9665.4082,  8852.5586,
        13320.9053, 10107.2852, 11429.1807,  8152.2598, 11031.8467, 11769.5264,
        11536.1045,  9689.2998, 17753.5898, 13872.6270, 12999.2012, 12113.3438,
        13688.5586,  7415.1108, 12687.2822, 11292.6953,  8322.0771, 14737.6660,
        11444.0137,  8960.9355, 11350.0771,  8342.7959,  9429.6318,  9380.8643,
        12891.2236, 14634.5684, 11071.0283, 11269.5996, 10783.9199,  9238.4150,
        12414.6943,  6544.0029, 13725.5869,  7963.6035,  9303.7402, 10706.8125,
         9466.1270,  8662.2637,  9261.4336,  7798.9004, 11921.0137,  9733.1191,
        10759.5332,  8408.1162, 12249.4639, 13648.9648,  8929.3799, 11936.5322,
        11251.5508, 11631.1094,  6892.6055,  9912.1562,  7368.5742, 10485.8115,
        12178.8809, 15053.7158,  8855.3984, 11124.6924, 12323.9004,  9702.0332,
        13348.6055, 13298.5781, 15955.0850, 10303.9287, 12083.5605, 10132.1123,
         9151.5977, 12113.8770, 13346.5557,  8345.7070,  8237.6904, 11786.0010,
        13590.1309, 11485.6436, 13228.4658, 12183.8467, 10014.9043,  7913.7861,
        10075.6055, 12951.3848])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([339.8187,  48.2184,  43.2532,  ...,   4.5157,   0.0000,  50.5864])
Globale Pruning-Maske generiert: 53 Layer
Layer: conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1.], device='cuda:0')}}
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}}
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0')}}
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1.], device='cuda:0')}}
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1.], device='cuda:0')}}
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 1.], device='cuda:0')}}
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}}
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0.], device='cuda:0')}}
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}}
LRP Pruning applied successfully in Round 1.
=== Runde 2/5 ===
Pruning-Maske anwenden f端r Runde 2...
Layer conv1 not found in model state_dict. Skipping...
Layer encoder.4.0.conv1 not found in model state_dict. Skipping...
Layer encoder.4.0.conv2 not found in model state_dict. Skipping...
Layer encoder.4.0.conv3 not found in model state_dict. Skipping...
Layer encoder.4.0.downsample.0 not found in model state_dict. Skipping...
Layer encoder.4.1.conv1 not found in model state_dict. Skipping...
Layer encoder.4.1.conv2 not found in model state_dict. Skipping...
Layer encoder.4.1.conv3 not found in model state_dict. Skipping...
Layer encoder.4.2.conv1 not found in model state_dict. Skipping...
Layer encoder.4.2.conv2 not found in model state_dict. Skipping...
Layer encoder.4.2.conv3 not found in model state_dict. Skipping...
Layer encoder.5.0.conv1 not found in model state_dict. Skipping...
Layer encoder.5.0.conv2 not found in model state_dict. Skipping...
Layer encoder.5.0.conv3 not found in model state_dict. Skipping...
Layer encoder.5.0.downsample.0 not found in model state_dict. Skipping...
Layer encoder.5.1.conv1 not found in model state_dict. Skipping...
Layer encoder.5.1.conv2 not found in model state_dict. Skipping...
Layer encoder.5.1.conv3 not found in model state_dict. Skipping...
Layer encoder.5.2.conv1 not found in model state_dict. Skipping...
Layer encoder.5.2.conv2 not found in model state_dict. Skipping...
Layer encoder.5.2.conv3 not found in model state_dict. Skipping...
Layer encoder.5.3.conv1 not found in model state_dict. Skipping...
Layer encoder.5.3.conv2 not found in model state_dict. Skipping...
Layer encoder.5.3.conv3 not found in model state_dict. Skipping...
Layer encoder.6.0.conv1 not found in model state_dict. Skipping...
Layer encoder.6.0.conv2 not found in model state_dict. Skipping...
Layer encoder.6.0.conv3 not found in model state_dict. Skipping...
Layer encoder.6.0.downsample.0 not found in model state_dict. Skipping...
Layer encoder.6.1.conv1 not found in model state_dict. Skipping...
Layer encoder.6.1.conv2 not found in model state_dict. Skipping...
Layer encoder.6.1.conv3 not found in model state_dict. Skipping...
Layer encoder.6.2.conv1 not found in model state_dict. Skipping...
Layer encoder.6.2.conv2 not found in model state_dict. Skipping...
Layer encoder.6.2.conv3 not found in model state_dict. Skipping...
Layer encoder.6.3.conv1 not found in model state_dict. Skipping...
Layer encoder.6.3.conv2 not found in model state_dict. Skipping...
Layer encoder.6.3.conv3 not found in model state_dict. Skipping...
Layer encoder.6.4.conv1 not found in model state_dict. Skipping...
Layer encoder.6.4.conv2 not found in model state_dict. Skipping...
Layer encoder.6.4.conv3 not found in model state_dict. Skipping...
Layer encoder.6.5.conv1 not found in model state_dict. Skipping...
Layer encoder.6.5.conv2 not found in model state_dict. Skipping...
Layer encoder.6.5.conv3 not found in model state_dict. Skipping...
Layer encoder.7.0.conv1 not found in model state_dict. Skipping...
Layer encoder.7.0.conv2 not found in model state_dict. Skipping...
Layer encoder.7.0.conv3 not found in model state_dict. Skipping...
Layer encoder.7.0.downsample.0 not found in model state_dict. Skipping...
Layer encoder.7.1.conv1 not found in model state_dict. Skipping...
Layer encoder.7.1.conv2 not found in model state_dict. Skipping...
Layer encoder.7.1.conv3 not found in model state_dict. Skipping...
Layer encoder.7.2.conv1 not found in model state_dict. Skipping...
Layer encoder.7.2.conv2 not found in model state_dict. Skipping...
Layer encoder.7.2.conv3 not found in model state_dict. Skipping...
Reinitializing BatchNorm stats for Layer: encoder.1
Reinitializing BatchNorm stats for Layer: encoder.4.0.bn1
Reinitializing BatchNorm stats for Layer: encoder.4.0.bn2
Reinitializing BatchNorm stats for Layer: encoder.4.0.bn3
Reinitializing BatchNorm stats for Layer: encoder.4.0.downsample.1
Reinitializing BatchNorm stats for Layer: encoder.4.1.bn1
Reinitializing BatchNorm stats for Layer: encoder.4.1.bn2
Reinitializing BatchNorm stats for Layer: encoder.4.1.bn3
Reinitializing BatchNorm stats for Layer: encoder.4.2.bn1
Reinitializing BatchNorm stats for Layer: encoder.4.2.bn2
Reinitializing BatchNorm stats for Layer: encoder.4.2.bn3
Reinitializing BatchNorm stats for Layer: encoder.5.0.bn1
Reinitializing BatchNorm stats for Layer: encoder.5.0.bn2
Reinitializing BatchNorm stats for Layer: encoder.5.0.bn3
Reinitializing BatchNorm stats for Layer: encoder.5.0.downsample.1
Reinitializing BatchNorm stats for Layer: encoder.5.1.bn1
Reinitializing BatchNorm stats for Layer: encoder.5.1.bn2
Reinitializing BatchNorm stats for Layer: encoder.5.1.bn3
Reinitializing BatchNorm stats for Layer: encoder.5.2.bn1
Reinitializing BatchNorm stats for Layer: encoder.5.2.bn2
Reinitializing BatchNorm stats for Layer: encoder.5.2.bn3
Reinitializing BatchNorm stats for Layer: encoder.5.3.bn1
Reinitializing BatchNorm stats for Layer: encoder.5.3.bn2
Reinitializing BatchNorm stats for Layer: encoder.5.3.bn3
Reinitializing BatchNorm stats for Layer: encoder.6.0.bn1
Reinitializing BatchNorm stats for Layer: encoder.6.0.bn2
Reinitializing BatchNorm stats for Layer: encoder.6.0.bn3
Reinitializing BatchNorm stats for Layer: encoder.6.0.downsample.1
Reinitializing BatchNorm stats for Layer: encoder.6.1.bn1
Reinitializing BatchNorm stats for Layer: encoder.6.1.bn2
Reinitializing BatchNorm stats for Layer: encoder.6.1.bn3
Reinitializing BatchNorm stats for Layer: encoder.6.2.bn1
Reinitializing BatchNorm stats for Layer: encoder.6.2.bn2
Reinitializing BatchNorm stats for Layer: encoder.6.2.bn3
Reinitializing BatchNorm stats for Layer: encoder.6.3.bn1
Reinitializing BatchNorm stats for Layer: encoder.6.3.bn2
Reinitializing BatchNorm stats for Layer: encoder.6.3.bn3
Reinitializing BatchNorm stats for Layer: encoder.6.4.bn1
Reinitializing BatchNorm stats for Layer: encoder.6.4.bn2
Reinitializing BatchNorm stats for Layer: encoder.6.4.bn3
Reinitializing BatchNorm stats for Layer: encoder.6.5.bn1
Reinitializing BatchNorm stats for Layer: encoder.6.5.bn2
Reinitializing BatchNorm stats for Layer: encoder.6.5.bn3
Reinitializing BatchNorm stats for Layer: encoder.7.0.bn1
Reinitializing BatchNorm stats for Layer: encoder.7.0.bn2
Reinitializing BatchNorm stats for Layer: encoder.7.0.bn3
Reinitializing BatchNorm stats for Layer: encoder.7.0.downsample.1
Reinitializing BatchNorm stats for Layer: encoder.7.1.bn1
Reinitializing BatchNorm stats for Layer: encoder.7.1.bn2
Reinitializing BatchNorm stats for Layer: encoder.7.1.bn3
Reinitializing BatchNorm stats for Layer: encoder.7.2.bn1
Reinitializing BatchNorm stats for Layer: encoder.7.2.bn2
Reinitializing BatchNorm stats for Layer: encoder.7.2.bn3
Training and communication for Round 2...
Epoch 1/1
----------
Batch 0:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 1:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 2:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 3:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 4:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 5:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 6:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 7:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 8:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 9:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 10:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 11:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 12:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 13:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 14:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 15:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 16:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 17:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 18:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 19:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 20:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 21:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 22:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 23:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 24:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 25:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 26:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 27:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 28:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 29:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 30:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 31:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 32:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 33:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 34:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 35:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 36:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 37:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 38:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 39:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 40:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 41:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 42:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 43:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 44:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 45:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 46:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 47:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 48:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 49:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 50:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 51:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 52:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 53:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 54:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 55:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 56:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 57:
  Data shape: torch.Size([213, 10, 120, 120])
  Labels shape: torch.Size([213, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
Starting validation after Round 2...
NaN detected in predicted probabilities array.
[[nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 ...
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]
 [nan nan nan ... nan nan nan]]
Validation failed due to: Predicted probabilities contain NaN values.
