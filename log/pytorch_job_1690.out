Lines that potentially need to be canonized 309
Using device: cuda:0
Initialized conv1.weight with Xavier uniform.
Layer: conv1.weight | Weight max: 0.04067724943161011, min: -0.040676556527614594, mean: -6.004669558024034e-05
Layer: encoder.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.1.bias with zeros.
Layer: encoder.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv1.weight with Xavier uniform.
Layer: encoder.4.0.conv1.weight | Weight max: 0.21646972000598907, min: -0.2164732813835144, mean: 0.002761438023298979
Layer: encoder.4.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn1.bias with zeros.
Layer: encoder.4.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv2.weight with Xavier uniform.
Layer: encoder.4.0.conv2.weight | Weight max: 0.07216314971446991, min: -0.07216771692037582, mean: -0.00018542866746429354
Layer: encoder.4.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn2.bias with zeros.
Layer: encoder.4.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv3.weight with Xavier uniform.
Layer: encoder.4.0.conv3.weight | Weight max: 0.13692741096019745, min: -0.1369239091873169, mean: 0.0007259170524775982
Layer: encoder.4.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn3.bias with zeros.
Layer: encoder.4.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.downsample.0.weight with Xavier uniform.
Layer: encoder.4.0.downsample.0.weight | Weight max: 0.13693059980869293, min: -0.1369294971227646, mean: 0.0011605871841311455
Layer: encoder.4.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.downsample.1.bias with zeros.
Layer: encoder.4.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv1.weight with Xavier uniform.
Layer: encoder.4.1.conv1.weight | Weight max: 0.13691551983356476, min: -0.13687509298324585, mean: 0.0006812838837504387
Layer: encoder.4.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn1.bias with zeros.
Layer: encoder.4.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv2.weight with Xavier uniform.
Layer: encoder.4.1.conv2.weight | Weight max: 0.07216501981019974, min: -0.07216689735651016, mean: 1.0348584510211367e-05
Layer: encoder.4.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn2.bias with zeros.
Layer: encoder.4.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv3.weight with Xavier uniform.
Layer: encoder.4.1.conv3.weight | Weight max: 0.1369292140007019, min: -0.13692601025104523, mean: -0.0010731382062658668
Layer: encoder.4.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn3.bias with zeros.
Layer: encoder.4.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv1.weight with Xavier uniform.
Layer: encoder.4.2.conv1.weight | Weight max: 0.13687637448310852, min: -0.1369297057390213, mean: -0.00016381590103264898
Layer: encoder.4.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn1.bias with zeros.
Layer: encoder.4.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv2.weight with Xavier uniform.
Layer: encoder.4.2.conv2.weight | Weight max: 0.07216773927211761, min: -0.07216248661279678, mean: -7.553531031589955e-05
Layer: encoder.4.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn2.bias with zeros.
Layer: encoder.4.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv3.weight with Xavier uniform.
Layer: encoder.4.2.conv3.weight | Weight max: 0.13690875470638275, min: -0.1369195133447647, mean: -0.00010216355440206826
Layer: encoder.4.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn3.bias with zeros.
Layer: encoder.4.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv1.weight with Xavier uniform.
Layer: encoder.5.0.conv1.weight | Weight max: 0.12496832013130188, min: -0.12499681860208511, mean: -0.0001610713079571724
Layer: encoder.5.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn1.bias with zeros.
Layer: encoder.5.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv2.weight with Xavier uniform.
Layer: encoder.5.0.conv2.weight | Weight max: 0.05103040114045143, min: -0.05103008449077606, mean: 0.00010282628500135615
Layer: encoder.5.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn2.bias with zeros.
Layer: encoder.5.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv3.weight with Xavier uniform.
Layer: encoder.5.0.conv3.weight | Weight max: 0.09680384397506714, min: -0.09682299941778183, mean: -0.00013683392899110913
Layer: encoder.5.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn3.bias with zeros.
Layer: encoder.5.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.downsample.0.weight with Xavier uniform.
Layer: encoder.5.0.downsample.0.weight | Weight max: 0.08838803321123123, min: -0.08838814496994019, mean: 4.5961554860696197e-05
Layer: encoder.5.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.downsample.1.bias with zeros.
Layer: encoder.5.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv1.weight with Xavier uniform.
Layer: encoder.5.1.conv1.weight | Weight max: 0.0968238040804863, min: -0.09682340919971466, mean: 0.0001376320724375546
Layer: encoder.5.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn1.bias with zeros.
Layer: encoder.5.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv2.weight with Xavier uniform.
Layer: encoder.5.1.conv2.weight | Weight max: 0.05103102698922157, min: -0.051030077040195465, mean: 6.12908334005624e-05
Layer: encoder.5.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn2.bias with zeros.
Layer: encoder.5.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv3.weight with Xavier uniform.
Layer: encoder.5.1.conv3.weight | Weight max: 0.0968230664730072, min: -0.09682440757751465, mean: -3.928405931219459e-06
Layer: encoder.5.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn3.bias with zeros.
Layer: encoder.5.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv1.weight with Xavier uniform.
Layer: encoder.5.2.conv1.weight | Weight max: 0.0968208834528923, min: -0.09682104736566544, mean: -0.00023771461565047503
Layer: encoder.5.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn1.bias with zeros.
Layer: encoder.5.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv2.weight with Xavier uniform.
Layer: encoder.5.2.conv2.weight | Weight max: 0.051029883325099945, min: -0.05103039741516113, mean: -5.108112600282766e-05
Layer: encoder.5.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn2.bias with zeros.
Layer: encoder.5.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv3.weight with Xavier uniform.
Layer: encoder.5.2.conv3.weight | Weight max: 0.09682362526655197, min: -0.09682366251945496, mean: -0.00026535711367614567
Layer: encoder.5.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn3.bias with zeros.
Layer: encoder.5.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv1.weight with Xavier uniform.
Layer: encoder.5.3.conv1.weight | Weight max: 0.09682224690914154, min: -0.09682433307170868, mean: 0.0002519854169804603
Layer: encoder.5.3.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn1.bias with zeros.
Layer: encoder.5.3.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv2.weight with Xavier uniform.
Layer: encoder.5.3.conv2.weight | Weight max: 0.05102936550974846, min: -0.051029060035943985, mean: -6.510120147140697e-05
Layer: encoder.5.3.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn2.bias with zeros.
Layer: encoder.5.3.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv3.weight with Xavier uniform.
Layer: encoder.5.3.conv3.weight | Weight max: 0.09682413935661316, min: -0.09681978076696396, mean: -0.0004380132304504514
Layer: encoder.5.3.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn3.bias with zeros.
Layer: encoder.5.3.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv1.weight with Xavier uniform.
Layer: encoder.6.0.conv1.weight | Weight max: 0.0883883386850357, min: -0.08838707208633423, mean: -4.6534067223547027e-05
Layer: encoder.6.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn1.bias with zeros.
Layer: encoder.6.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv2.weight with Xavier uniform.
Layer: encoder.6.0.conv2.weight | Weight max: 0.036084290593862534, min: -0.0360843800008297, mean: -3.83255974156782e-06
Layer: encoder.6.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn2.bias with zeros.
Layer: encoder.6.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv3.weight with Xavier uniform.
Layer: encoder.6.0.conv3.weight | Weight max: 0.06846526265144348, min: -0.0684647411108017, mean: 8.338077168446034e-05
Layer: encoder.6.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn3.bias with zeros.
Layer: encoder.6.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.downsample.0.weight with Xavier uniform.
Layer: encoder.6.0.downsample.0.weight | Weight max: 0.062499791383743286, min: -0.06249946728348732, mean: -2.5436458599870093e-05
Layer: encoder.6.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.downsample.1.bias with zeros.
Layer: encoder.6.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv1.weight with Xavier uniform.
Layer: encoder.6.1.conv1.weight | Weight max: 0.06846529245376587, min: -0.06846395134925842, mean: -3.848505002679303e-05
Layer: encoder.6.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn1.bias with zeros.
Layer: encoder.6.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv2.weight with Xavier uniform.
Layer: encoder.6.1.conv2.weight | Weight max: 0.03608424961566925, min: -0.03608430549502373, mean: 1.0284024938300718e-05
Layer: encoder.6.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn2.bias with zeros.
Layer: encoder.6.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv3.weight with Xavier uniform.
Layer: encoder.6.1.conv3.weight | Weight max: 0.06846465170383453, min: -0.06846480816602707, mean: -0.0001310463121626526
Layer: encoder.6.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn3.bias with zeros.
Layer: encoder.6.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv1.weight with Xavier uniform.
Layer: encoder.6.2.conv1.weight | Weight max: 0.06846509873867035, min: -0.06846337020397186, mean: -7.293163798749447e-05
Layer: encoder.6.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn1.bias with zeros.
Layer: encoder.6.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv2.weight with Xavier uniform.
Layer: encoder.6.2.conv2.weight | Weight max: 0.03608430549502373, min: -0.036084264516830444, mean: -2.776882865873631e-05
Layer: encoder.6.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn2.bias with zeros.
Layer: encoder.6.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv3.weight with Xavier uniform.
Layer: encoder.6.2.conv3.weight | Weight max: 0.06846494227647781, min: -0.06846532225608826, mean: 0.00012056253763148561
Layer: encoder.6.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn3.bias with zeros.
Layer: encoder.6.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv1.weight with Xavier uniform.
Layer: encoder.6.3.conv1.weight | Weight max: 0.06846419721841812, min: -0.06846524775028229, mean: 0.00011725799413397908
Layer: encoder.6.3.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn1.bias with zeros.
Layer: encoder.6.3.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv2.weight with Xavier uniform.
Layer: encoder.6.3.conv2.weight | Weight max: 0.036084067076444626, min: -0.036084309220314026, mean: 2.5492656277492642e-05
Layer: encoder.6.3.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn2.bias with zeros.
Layer: encoder.6.3.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv3.weight with Xavier uniform.
Layer: encoder.6.3.conv3.weight | Weight max: 0.06846527010202408, min: -0.0684640184044838, mean: 7.014928996795788e-06
Layer: encoder.6.3.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn3.bias with zeros.
Layer: encoder.6.3.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv1.weight with Xavier uniform.
Layer: encoder.6.4.conv1.weight | Weight max: 0.06846483796834946, min: -0.06846508383750916, mean: -8.883020200300962e-05
Layer: encoder.6.4.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn1.bias with zeros.
Layer: encoder.6.4.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv2.weight with Xavier uniform.
Layer: encoder.6.4.conv2.weight | Weight max: 0.03608431667089462, min: -0.03608431667089462, mean: 5.328154657036066e-05
Layer: encoder.6.4.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn2.bias with zeros.
Layer: encoder.6.4.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv3.weight with Xavier uniform.
Layer: encoder.6.4.conv3.weight | Weight max: 0.06846518069505692, min: -0.06846527755260468, mean: -7.06389473634772e-05
Layer: encoder.6.4.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn3.bias with zeros.
Layer: encoder.6.4.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv1.weight with Xavier uniform.
Layer: encoder.6.5.conv1.weight | Weight max: 0.06846494227647781, min: -0.0684652179479599, mean: 0.00014279180322773755
Layer: encoder.6.5.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn1.bias with zeros.
Layer: encoder.6.5.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv2.weight with Xavier uniform.
Layer: encoder.6.5.conv2.weight | Weight max: 0.036084212362766266, min: -0.036084216088056564, mean: 3.2931060559349135e-05
Layer: encoder.6.5.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn2.bias with zeros.
Layer: encoder.6.5.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv3.weight with Xavier uniform.
Layer: encoder.6.5.conv3.weight | Weight max: 0.06846484541893005, min: -0.0684645026922226, mean: -5.314494774211198e-05
Layer: encoder.6.5.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn3.bias with zeros.
Layer: encoder.6.5.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv1.weight with Xavier uniform.
Layer: encoder.7.0.conv1.weight | Weight max: 0.062499746680259705, min: -0.06249968707561493, mean: -4.7113688196986914e-05
Layer: encoder.7.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn1.bias with zeros.
Layer: encoder.7.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv2.weight with Xavier uniform.
Layer: encoder.7.0.conv2.weight | Weight max: 0.02551550604403019, min: -0.025515498593449593, mean: 5.606568265648093e-06
Layer: encoder.7.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn2.bias with zeros.
Layer: encoder.7.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv3.weight with Xavier uniform.
Layer: encoder.7.0.conv3.weight | Weight max: 0.0484122633934021, min: -0.04841214790940285, mean: -6.104654858063441e-06
Layer: encoder.7.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn3.bias with zeros.
Layer: encoder.7.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.downsample.0.weight with Xavier uniform.
Layer: encoder.7.0.downsample.0.weight | Weight max: 0.04419416934251785, min: -0.044194161891937256, mean: 1.5380890090455068e-06
Layer: encoder.7.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.downsample.1.bias with zeros.
Layer: encoder.7.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv1.weight with Xavier uniform.
Layer: encoder.7.1.conv1.weight | Weight max: 0.048412252217531204, min: -0.04841221496462822, mean: -1.565914135426283e-05
Layer: encoder.7.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn1.bias with zeros.
Layer: encoder.7.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv2.weight with Xavier uniform.
Layer: encoder.7.1.conv2.weight | Weight max: 0.025515513494610786, min: -0.025515509769320488, mean: -1.0062530236609746e-05
Layer: encoder.7.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn2.bias with zeros.
Layer: encoder.7.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv3.weight with Xavier uniform.
Layer: encoder.7.1.conv3.weight | Weight max: 0.04841214418411255, min: -0.04841223731637001, mean: -5.687335033144336e-06
Layer: encoder.7.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn3.bias with zeros.
Layer: encoder.7.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv1.weight with Xavier uniform.
Layer: encoder.7.2.conv1.weight | Weight max: 0.04841221123933792, min: -0.04841221123933792, mean: -7.328654464799911e-05
Layer: encoder.7.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn1.bias with zeros.
Layer: encoder.7.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv2.weight with Xavier uniform.
Layer: encoder.7.2.conv2.weight | Weight max: 0.025515513494610786, min: -0.025515489280223846, mean: -6.989866960793734e-06
Layer: encoder.7.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn2.bias with zeros.
Layer: encoder.7.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv3.weight with Xavier uniform.
Layer: encoder.7.2.conv3.weight | Weight max: 0.04841224104166031, min: -0.04841224476695061, mean: 1.8917626221082173e-06
Layer: encoder.7.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn3.bias with zeros.
Layer: encoder.7.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized FC.weight with Xavier uniform.
Layer: FC.weight | Weight max: 0.05387650802731514, min: -0.053874049335718155, mean: 5.725060327677056e-05
Initialized FC.bias with zeros.
Layer: FC.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initialized conv1.weight with Xavier uniform.
Layer: conv1.weight | Weight max: 0.04067443311214447, min: -0.04067574068903923, mean: -8.625841292086989e-05
Layer: encoder.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.1.bias with zeros.
Layer: encoder.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv1.weight with Xavier uniform.
Layer: encoder.4.0.conv1.weight | Weight max: 0.21644310653209686, min: -0.21631990373134613, mean: -0.00022729113698005676
Layer: encoder.4.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn1.bias with zeros.
Layer: encoder.4.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv2.weight with Xavier uniform.
Layer: encoder.4.0.conv2.weight | Weight max: 0.07216863334178925, min: -0.07216640561819077, mean: 1.0057753570436034e-05
Layer: encoder.4.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn2.bias with zeros.
Layer: encoder.4.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv3.weight with Xavier uniform.
Layer: encoder.4.0.conv3.weight | Weight max: 0.1369081288576126, min: -0.13689158856868744, mean: 0.0003038533905055374
Layer: encoder.4.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn3.bias with zeros.
Layer: encoder.4.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.downsample.0.weight with Xavier uniform.
Layer: encoder.4.0.downsample.0.weight | Weight max: 0.13692188262939453, min: -0.13692553341388702, mean: -0.00019917490135412663
Layer: encoder.4.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.downsample.1.bias with zeros.
Layer: encoder.4.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv1.weight with Xavier uniform.
Layer: encoder.4.1.conv1.weight | Weight max: 0.13691674172878265, min: -0.13690952956676483, mean: 0.0006640478386543691
Layer: encoder.4.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn1.bias with zeros.
Layer: encoder.4.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv2.weight with Xavier uniform.
Layer: encoder.4.1.conv2.weight | Weight max: 0.07216829806566238, min: -0.07216733694076538, mean: -0.0001725660840747878
Layer: encoder.4.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn2.bias with zeros.
Layer: encoder.4.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv3.weight with Xavier uniform.
Layer: encoder.4.1.conv3.weight | Weight max: 0.13690121471881866, min: -0.13692238926887512, mean: -1.9641942344605923e-05
Layer: encoder.4.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn3.bias with zeros.
Layer: encoder.4.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv1.weight with Xavier uniform.
Layer: encoder.4.2.conv1.weight | Weight max: 0.13692910969257355, min: -0.13691985607147217, mean: 1.1727745004463941e-05
Layer: encoder.4.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn1.bias with zeros.
Layer: encoder.4.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv2.weight with Xavier uniform.
Layer: encoder.4.2.conv2.weight | Weight max: 0.07216792553663254, min: -0.07216852158308029, mean: 0.0002416285133222118
Layer: encoder.4.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn2.bias with zeros.
Layer: encoder.4.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv3.weight with Xavier uniform.
Layer: encoder.4.2.conv3.weight | Weight max: 0.13692082464694977, min: -0.13689711689949036, mean: 0.0007473821751773357
Layer: encoder.4.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn3.bias with zeros.
Layer: encoder.4.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv1.weight with Xavier uniform.
Layer: encoder.5.0.conv1.weight | Weight max: 0.12499967217445374, min: -0.12499260902404785, mean: 0.0007270096684806049
Layer: encoder.5.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn1.bias with zeros.
Layer: encoder.5.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv2.weight with Xavier uniform.
Layer: encoder.5.0.conv2.weight | Weight max: 0.051030777394771576, min: -0.051031019538640976, mean: 8.319723565364257e-05
Layer: encoder.5.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn2.bias with zeros.
Layer: encoder.5.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv3.weight with Xavier uniform.
Layer: encoder.5.0.conv3.weight | Weight max: 0.09682393819093704, min: -0.09682001173496246, mean: 4.759905641549267e-05
Layer: encoder.5.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn3.bias with zeros.
Layer: encoder.5.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.downsample.0.weight with Xavier uniform.
Layer: encoder.5.0.downsample.0.weight | Weight max: 0.08838748931884766, min: -0.08838830888271332, mean: 3.164959707646631e-05
Layer: encoder.5.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.downsample.1.bias with zeros.
Layer: encoder.5.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv1.weight with Xavier uniform.
Layer: encoder.5.1.conv1.weight | Weight max: 0.09682083874940872, min: -0.09682345390319824, mean: 5.619881994789466e-05
Layer: encoder.5.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn1.bias with zeros.
Layer: encoder.5.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv2.weight with Xavier uniform.
Layer: encoder.5.1.conv2.weight | Weight max: 0.05103030800819397, min: -0.051030151546001434, mean: -3.67478933185339e-05
Layer: encoder.5.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn2.bias with zeros.
Layer: encoder.5.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv3.weight with Xavier uniform.
Layer: encoder.5.1.conv3.weight | Weight max: 0.09682334214448929, min: -0.09682046622037888, mean: 4.6578963520005345e-05
Layer: encoder.5.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn3.bias with zeros.
Layer: encoder.5.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv1.weight with Xavier uniform.
Layer: encoder.5.2.conv1.weight | Weight max: 0.0968208909034729, min: -0.09682445228099823, mean: -0.00014054079656489193
Layer: encoder.5.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn1.bias with zeros.
Layer: encoder.5.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv2.weight with Xavier uniform.
Layer: encoder.5.2.conv2.weight | Weight max: 0.05103055015206337, min: -0.05103069543838501, mean: -0.00022606678248848766
Layer: encoder.5.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn2.bias with zeros.
Layer: encoder.5.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv3.weight with Xavier uniform.
Layer: encoder.5.2.conv3.weight | Weight max: 0.09682442247867584, min: -0.09682152420282364, mean: -0.0002648275694809854
Layer: encoder.5.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn3.bias with zeros.
Layer: encoder.5.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv1.weight with Xavier uniform.
Layer: encoder.5.3.conv1.weight | Weight max: 0.0968242958188057, min: -0.09682434797286987, mean: -0.00043103675125166774
Layer: encoder.5.3.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn1.bias with zeros.
Layer: encoder.5.3.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv2.weight with Xavier uniform.
Layer: encoder.5.3.conv2.weight | Weight max: 0.05103028565645218, min: -0.051029838621616364, mean: -8.361213986063376e-05
Layer: encoder.5.3.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn2.bias with zeros.
Layer: encoder.5.3.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv3.weight with Xavier uniform.
Layer: encoder.5.3.conv3.weight | Weight max: 0.09682223200798035, min: -0.09681341797113419, mean: 0.00023660055012442172
Layer: encoder.5.3.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn3.bias with zeros.
Layer: encoder.5.3.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv1.weight with Xavier uniform.
Layer: encoder.6.0.conv1.weight | Weight max: 0.08838631212711334, min: -0.08838707953691483, mean: -0.00010387962538516149
Layer: encoder.6.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn1.bias with zeros.
Layer: encoder.6.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv2.weight with Xavier uniform.
Layer: encoder.6.0.conv2.weight | Weight max: 0.036084242165088654, min: -0.03608416020870209, mean: -2.9675666155526415e-05
Layer: encoder.6.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn2.bias with zeros.
Layer: encoder.6.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv3.weight with Xavier uniform.
Layer: encoder.6.0.conv3.weight | Weight max: 0.06846529990434647, min: -0.06846486032009125, mean: -9.706433047540486e-06
Layer: encoder.6.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn3.bias with zeros.
Layer: encoder.6.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.downsample.0.weight with Xavier uniform.
Layer: encoder.6.0.downsample.0.weight | Weight max: 0.0624997615814209, min: -0.0624995119869709, mean: -2.6588440960040316e-06
Layer: encoder.6.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.downsample.1.bias with zeros.
Layer: encoder.6.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv1.weight with Xavier uniform.
Layer: encoder.6.1.conv1.weight | Weight max: 0.06846507638692856, min: -0.06846526265144348, mean: -9.137007873505354e-05
Layer: encoder.6.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn1.bias with zeros.
Layer: encoder.6.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv2.weight with Xavier uniform.
Layer: encoder.6.1.conv2.weight | Weight max: 0.03608418628573418, min: -0.03608430176973343, mean: 3.285290586063638e-05
Layer: encoder.6.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn2.bias with zeros.
Layer: encoder.6.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv3.weight with Xavier uniform.
Layer: encoder.6.1.conv3.weight | Weight max: 0.06846403330564499, min: -0.06846524029970169, mean: -0.00011631070083240047
Layer: encoder.6.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn3.bias with zeros.
Layer: encoder.6.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv1.weight with Xavier uniform.
Layer: encoder.6.2.conv1.weight | Weight max: 0.06846525520086288, min: -0.06846407055854797, mean: 0.0001413831632817164
Layer: encoder.6.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn1.bias with zeros.
Layer: encoder.6.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv2.weight with Xavier uniform.
Layer: encoder.6.2.conv2.weight | Weight max: 0.036084145307540894, min: -0.036083757877349854, mean: -3.5651093639899045e-05
Layer: encoder.6.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn2.bias with zeros.
Layer: encoder.6.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv3.weight with Xavier uniform.
Layer: encoder.6.2.conv3.weight | Weight max: 0.0684652328491211, min: -0.06846524029970169, mean: -0.00011516599624883384
Layer: encoder.6.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn3.bias with zeros.
Layer: encoder.6.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv1.weight with Xavier uniform.
Layer: encoder.6.3.conv1.weight | Weight max: 0.06846435368061066, min: -0.06846453994512558, mean: 1.0452002243255265e-05
Layer: encoder.6.3.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn1.bias with zeros.
Layer: encoder.6.3.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv2.weight with Xavier uniform.
Layer: encoder.6.3.conv2.weight | Weight max: 0.0360843650996685, min: -0.03608414903283119, mean: -3.575045775505714e-05
Layer: encoder.6.3.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn2.bias with zeros.
Layer: encoder.6.3.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv3.weight with Xavier uniform.
Layer: encoder.6.3.conv3.weight | Weight max: 0.06846436113119125, min: -0.0684652104973793, mean: -8.108874317258596e-05
Layer: encoder.6.3.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn3.bias with zeros.
Layer: encoder.6.3.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv1.weight with Xavier uniform.
Layer: encoder.6.4.conv1.weight | Weight max: 0.0684649869799614, min: -0.06846495717763901, mean: 4.4112020987086e-05
Layer: encoder.6.4.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn1.bias with zeros.
Layer: encoder.6.4.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv2.weight with Xavier uniform.
Layer: encoder.6.4.conv2.weight | Weight max: 0.03608426824212074, min: -0.03608430176973343, mean: 2.911571391450707e-05
Layer: encoder.6.4.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn2.bias with zeros.
Layer: encoder.6.4.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv3.weight with Xavier uniform.
Layer: encoder.6.4.conv3.weight | Weight max: 0.06846361607313156, min: -0.06846524029970169, mean: -9.374861838296056e-05
Layer: encoder.6.4.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn3.bias with zeros.
Layer: encoder.6.4.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv1.weight with Xavier uniform.
Layer: encoder.6.5.conv1.weight | Weight max: 0.06846332550048828, min: -0.06846430152654648, mean: 8.686840010341257e-05
Layer: encoder.6.5.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn1.bias with zeros.
Layer: encoder.6.5.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv2.weight with Xavier uniform.
Layer: encoder.6.5.conv2.weight | Weight max: 0.0360843725502491, min: -0.03608426824212074, mean: 2.00667891476769e-05
Layer: encoder.6.5.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn2.bias with zeros.
Layer: encoder.6.5.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv3.weight with Xavier uniform.
Layer: encoder.6.5.conv3.weight | Weight max: 0.06846513599157333, min: -0.06846513599157333, mean: -2.9949471354484558e-05
Layer: encoder.6.5.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn3.bias with zeros.
Layer: encoder.6.5.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv1.weight with Xavier uniform.
Layer: encoder.7.0.conv1.weight | Weight max: 0.06249983608722687, min: -0.062499869614839554, mean: 7.983452815096825e-05
Layer: encoder.7.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn1.bias with zeros.
Layer: encoder.7.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv2.weight with Xavier uniform.
Layer: encoder.7.0.conv2.weight | Weight max: 0.02551550418138504, min: -0.02551550418138504, mean: 1.0370516974944621e-05
Layer: encoder.7.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn2.bias with zeros.
Layer: encoder.7.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv3.weight with Xavier uniform.
Layer: encoder.7.0.conv3.weight | Weight max: 0.04841223731637001, min: -0.04841160401701927, mean: -2.7321098968968727e-05
Layer: encoder.7.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn3.bias with zeros.
Layer: encoder.7.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.downsample.0.weight with Xavier uniform.
Layer: encoder.7.0.downsample.0.weight | Weight max: 0.044194161891937256, min: -0.044194113463163376, mean: -2.7125350243295543e-05
Layer: encoder.7.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.downsample.1.bias with zeros.
Layer: encoder.7.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv1.weight with Xavier uniform.
Layer: encoder.7.1.conv1.weight | Weight max: 0.04841228574514389, min: -0.04841221496462822, mean: -1.695438550086692e-05
Layer: encoder.7.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn1.bias with zeros.
Layer: encoder.7.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv2.weight with Xavier uniform.
Layer: encoder.7.1.conv2.weight | Weight max: 0.025515498593449593, min: -0.02551547810435295, mean: -5.611616415990284e-06
Layer: encoder.7.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn2.bias with zeros.
Layer: encoder.7.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv3.weight with Xavier uniform.
Layer: encoder.7.1.conv3.weight | Weight max: 0.0484122708439827, min: -0.04841229319572449, mean: 3.504217602312565e-06
Layer: encoder.7.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn3.bias with zeros.
Layer: encoder.7.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv1.weight with Xavier uniform.
Layer: encoder.7.2.conv1.weight | Weight max: 0.04841209203004837, min: -0.048412151634693146, mean: 2.5998902856372297e-05
Layer: encoder.7.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn1.bias with zeros.
Layer: encoder.7.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv2.weight with Xavier uniform.
Layer: encoder.7.2.conv2.weight | Weight max: 0.02551550418138504, min: -0.02551550790667534, mean: -1.1941109733015765e-05
Layer: encoder.7.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn2.bias with zeros.
Layer: encoder.7.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv3.weight with Xavier uniform.
Layer: encoder.7.2.conv3.weight | Weight max: 0.04841220751404762, min: -0.04841229319572449, mean: 4.925044413539581e-05
Layer: encoder.7.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn3.bias with zeros.
Layer: encoder.7.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized FC.weight with Xavier uniform.
Layer: FC.weight | Weight max: 0.053876351565122604, min: -0.05387625843286514, mean: -1.9380087906029075e-05
Initialized FC.bias with zeros.
Layer: FC.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    7150 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    4263 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initialized conv1.weight with Xavier uniform.
Layer: conv1.weight | Weight max: 0.040677785873413086, min: -0.040677931159734726, mean: -9.46408326853998e-05
Layer: encoder.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.1.bias with zeros.
Layer: encoder.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv1.weight with Xavier uniform.
Layer: encoder.4.0.conv1.weight | Weight max: 0.216496080160141, min: -0.2163010984659195, mean: 0.0017049123998731375
Layer: encoder.4.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn1.bias with zeros.
Layer: encoder.4.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv2.weight with Xavier uniform.
Layer: encoder.4.0.conv2.weight | Weight max: 0.07215647399425507, min: -0.07216817140579224, mean: -6.7628723627422e-05
Layer: encoder.4.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn2.bias with zeros.
Layer: encoder.4.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.conv3.weight with Xavier uniform.
Layer: encoder.4.0.conv3.weight | Weight max: 0.13692033290863037, min: -0.13692161440849304, mean: -0.0007164137205109
Layer: encoder.4.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.bn3.bias with zeros.
Layer: encoder.4.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.0.downsample.0.weight with Xavier uniform.
Layer: encoder.4.0.downsample.0.weight | Weight max: 0.13693003356456757, min: -0.1369297206401825, mean: -0.0003371619386598468
Layer: encoder.4.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.0.downsample.1.bias with zeros.
Layer: encoder.4.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv1.weight with Xavier uniform.
Layer: encoder.4.1.conv1.weight | Weight max: 0.136916846036911, min: -0.13689841330051422, mean: 7.878385076764971e-05
Layer: encoder.4.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn1.bias with zeros.
Layer: encoder.4.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv2.weight with Xavier uniform.
Layer: encoder.4.1.conv2.weight | Weight max: 0.07216735184192657, min: -0.07216639071702957, mean: -0.00030727131525054574
Layer: encoder.4.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn2.bias with zeros.
Layer: encoder.4.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.1.conv3.weight with Xavier uniform.
Layer: encoder.4.1.conv3.weight | Weight max: 0.13692881166934967, min: -0.13691966235637665, mean: 0.0005369760328903794
Layer: encoder.4.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.1.bn3.bias with zeros.
Layer: encoder.4.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv1.weight with Xavier uniform.
Layer: encoder.4.2.conv1.weight | Weight max: 0.13682739436626434, min: -0.13690806925296783, mean: 0.0001359136658720672
Layer: encoder.4.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn1.bias with zeros.
Layer: encoder.4.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv2.weight with Xavier uniform.
Layer: encoder.4.2.conv2.weight | Weight max: 0.07216215878725052, min: -0.07216805219650269, mean: -9.307437721872702e-05
Layer: encoder.4.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn2.bias with zeros.
Layer: encoder.4.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.4.2.conv3.weight with Xavier uniform.
Layer: encoder.4.2.conv3.weight | Weight max: 0.13692143559455872, min: -0.1369139701128006, mean: 0.0008498476236127317
Layer: encoder.4.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.4.2.bn3.bias with zeros.
Layer: encoder.4.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv1.weight with Xavier uniform.
Layer: encoder.5.0.conv1.weight | Weight max: 0.12498944997787476, min: -0.12499094754457474, mean: -0.0007367528742179275
Layer: encoder.5.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn1.bias with zeros.
Layer: encoder.5.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv2.weight with Xavier uniform.
Layer: encoder.5.0.conv2.weight | Weight max: 0.051029741764068604, min: -0.051028694957494736, mean: 6.426055279007414e-06
Layer: encoder.5.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn2.bias with zeros.
Layer: encoder.5.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.conv3.weight with Xavier uniform.
Layer: encoder.5.0.conv3.weight | Weight max: 0.09682173281908035, min: -0.09682398289442062, mean: 2.1142972400411963e-05
Layer: encoder.5.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.bn3.bias with zeros.
Layer: encoder.5.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.0.downsample.0.weight with Xavier uniform.
Layer: encoder.5.0.downsample.0.weight | Weight max: 0.0883878618478775, min: -0.08838794380426407, mean: 0.00017662852769717574
Layer: encoder.5.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.0.downsample.1.bias with zeros.
Layer: encoder.5.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv1.weight with Xavier uniform.
Layer: encoder.5.1.conv1.weight | Weight max: 0.09682390838861465, min: -0.09682263433933258, mean: -0.00026380151393823326
Layer: encoder.5.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn1.bias with zeros.
Layer: encoder.5.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv2.weight with Xavier uniform.
Layer: encoder.5.1.conv2.weight | Weight max: 0.051029905676841736, min: -0.051030341535806656, mean: -2.4596711227786727e-05
Layer: encoder.5.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn2.bias with zeros.
Layer: encoder.5.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.1.conv3.weight with Xavier uniform.
Layer: encoder.5.1.conv3.weight | Weight max: 0.09682188183069229, min: -0.09682369232177734, mean: -0.00022966039250604808
Layer: encoder.5.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.1.bn3.bias with zeros.
Layer: encoder.5.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv1.weight with Xavier uniform.
Layer: encoder.5.2.conv1.weight | Weight max: 0.09682042896747589, min: -0.09682289510965347, mean: 0.00018737484060693532
Layer: encoder.5.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn1.bias with zeros.
Layer: encoder.5.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv2.weight with Xavier uniform.
Layer: encoder.5.2.conv2.weight | Weight max: 0.05103034898638725, min: -0.05102955922484398, mean: -0.00013541757653001696
Layer: encoder.5.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn2.bias with zeros.
Layer: encoder.5.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.2.conv3.weight with Xavier uniform.
Layer: encoder.5.2.conv3.weight | Weight max: 0.09682290256023407, min: -0.09682279080152512, mean: 0.00013306303299032152
Layer: encoder.5.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.2.bn3.bias with zeros.
Layer: encoder.5.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv1.weight with Xavier uniform.
Layer: encoder.5.3.conv1.weight | Weight max: 0.09682061523199081, min: -0.0968187227845192, mean: -0.00014751896378584206
Layer: encoder.5.3.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn1.bias with zeros.
Layer: encoder.5.3.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv2.weight with Xavier uniform.
Layer: encoder.5.3.conv2.weight | Weight max: 0.05103088170289993, min: -0.05102861300110817, mean: -0.00015110794629435986
Layer: encoder.5.3.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn2.bias with zeros.
Layer: encoder.5.3.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.5.3.conv3.weight with Xavier uniform.
Layer: encoder.5.3.conv3.weight | Weight max: 0.09681720286607742, min: -0.09682219475507736, mean: 0.00021245094831101596
Layer: encoder.5.3.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.5.3.bn3.bias with zeros.
Layer: encoder.5.3.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv1.weight with Xavier uniform.
Layer: encoder.6.0.conv1.weight | Weight max: 0.08838653564453125, min: -0.08838754147291183, mean: -0.0001905007811728865
Layer: encoder.6.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn1.bias with zeros.
Layer: encoder.6.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv2.weight with Xavier uniform.
Layer: encoder.6.0.conv2.weight | Weight max: 0.03608431667089462, min: -0.03608433157205582, mean: -1.8698776329983957e-05
Layer: encoder.6.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn2.bias with zeros.
Layer: encoder.6.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.conv3.weight with Xavier uniform.
Layer: encoder.6.0.conv3.weight | Weight max: 0.06846477091312408, min: -0.06846489012241364, mean: 7.90895865065977e-05
Layer: encoder.6.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.bn3.bias with zeros.
Layer: encoder.6.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.0.downsample.0.weight with Xavier uniform.
Layer: encoder.6.0.downsample.0.weight | Weight max: 0.06249960511922836, min: -0.06249967962503433, mean: -3.0854789656586945e-05
Layer: encoder.6.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.0.downsample.1.bias with zeros.
Layer: encoder.6.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv1.weight with Xavier uniform.
Layer: encoder.6.1.conv1.weight | Weight max: 0.06846462935209274, min: -0.06846305727958679, mean: 3.404203016543761e-05
Layer: encoder.6.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn1.bias with zeros.
Layer: encoder.6.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv2.weight with Xavier uniform.
Layer: encoder.6.1.conv2.weight | Weight max: 0.03608408197760582, min: -0.03608414903283119, mean: -1.634050022403244e-05
Layer: encoder.6.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn2.bias with zeros.
Layer: encoder.6.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.1.conv3.weight with Xavier uniform.
Layer: encoder.6.1.conv3.weight | Weight max: 0.068464495241642, min: -0.06846445798873901, mean: -0.00016373040853068233
Layer: encoder.6.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.1.bn3.bias with zeros.
Layer: encoder.6.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv1.weight with Xavier uniform.
Layer: encoder.6.2.conv1.weight | Weight max: 0.06846524029970169, min: -0.06846530735492706, mean: -0.00010736516560427845
Layer: encoder.6.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn1.bias with zeros.
Layer: encoder.6.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv2.weight with Xavier uniform.
Layer: encoder.6.2.conv2.weight | Weight max: 0.036084361374378204, min: -0.036084383726119995, mean: 6.666758167739317e-07
Layer: encoder.6.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn2.bias with zeros.
Layer: encoder.6.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.2.conv3.weight with Xavier uniform.
Layer: encoder.6.2.conv3.weight | Weight max: 0.06846486032009125, min: -0.06846440583467484, mean: -9.916853014146909e-05
Layer: encoder.6.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.2.bn3.bias with zeros.
Layer: encoder.6.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv1.weight with Xavier uniform.
Layer: encoder.6.3.conv1.weight | Weight max: 0.0684652179479599, min: -0.06846486032009125, mean: 9.671739826444536e-05
Layer: encoder.6.3.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn1.bias with zeros.
Layer: encoder.6.3.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv2.weight with Xavier uniform.
Layer: encoder.6.3.conv2.weight | Weight max: 0.036084309220314026, min: -0.03608394414186478, mean: 6.250857495615492e-06
Layer: encoder.6.3.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn2.bias with zeros.
Layer: encoder.6.3.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.3.conv3.weight with Xavier uniform.
Layer: encoder.6.3.conv3.weight | Weight max: 0.0684652179479599, min: -0.06846382468938828, mean: -1.5847104805288836e-05
Layer: encoder.6.3.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.3.bn3.bias with zeros.
Layer: encoder.6.3.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv1.weight with Xavier uniform.
Layer: encoder.6.4.conv1.weight | Weight max: 0.06846524775028229, min: -0.0684652179479599, mean: -5.8604062360245734e-05
Layer: encoder.6.4.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn1.bias with zeros.
Layer: encoder.6.4.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv2.weight with Xavier uniform.
Layer: encoder.6.4.conv2.weight | Weight max: 0.03608432412147522, min: -0.036084309220314026, mean: -1.1973044820479117e-05
Layer: encoder.6.4.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn2.bias with zeros.
Layer: encoder.6.4.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.4.conv3.weight with Xavier uniform.
Layer: encoder.6.4.conv3.weight | Weight max: 0.0684645026922226, min: -0.06846530735492706, mean: 5.6285531172761694e-05
Layer: encoder.6.4.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.4.bn3.bias with zeros.
Layer: encoder.6.4.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv1.weight with Xavier uniform.
Layer: encoder.6.5.conv1.weight | Weight max: 0.06846511363983154, min: -0.0684644803404808, mean: -7.26530997781083e-05
Layer: encoder.6.5.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn1.bias with zeros.
Layer: encoder.6.5.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv2.weight with Xavier uniform.
Layer: encoder.6.5.conv2.weight | Weight max: 0.03608415275812149, min: -0.036084141582250595, mean: -1.8925170763850474e-07
Layer: encoder.6.5.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn2.bias with zeros.
Layer: encoder.6.5.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.6.5.conv3.weight with Xavier uniform.
Layer: encoder.6.5.conv3.weight | Weight max: 0.06846407800912857, min: -0.06846501678228378, mean: 2.966902138723526e-05
Layer: encoder.6.5.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.6.5.bn3.bias with zeros.
Layer: encoder.6.5.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv1.weight with Xavier uniform.
Layer: encoder.7.0.conv1.weight | Weight max: 0.06249970942735672, min: -0.06249966099858284, mean: 1.8767597794067115e-05
Layer: encoder.7.0.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn1.bias with zeros.
Layer: encoder.7.0.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv2.weight with Xavier uniform.
Layer: encoder.7.0.conv2.weight | Weight max: 0.025515452027320862, min: -0.025515511631965637, mean: 2.430725771773723e-06
Layer: encoder.7.0.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn2.bias with zeros.
Layer: encoder.7.0.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.conv3.weight with Xavier uniform.
Layer: encoder.7.0.conv3.weight | Weight max: 0.04841215908527374, min: -0.04841224104166031, mean: -1.1075671864091419e-06
Layer: encoder.7.0.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.bn3.bias with zeros.
Layer: encoder.7.0.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.0.downsample.0.weight with Xavier uniform.
Layer: encoder.7.0.downsample.0.weight | Weight max: 0.044194161891937256, min: -0.04419414699077606, mean: -1.0679711522243451e-05
Layer: encoder.7.0.downsample.1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.0.downsample.1.bias with zeros.
Layer: encoder.7.0.downsample.1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv1.weight with Xavier uniform.
Layer: encoder.7.1.conv1.weight | Weight max: 0.048412125557661057, min: -0.04841221496462822, mean: 1.270947450393578e-06
Layer: encoder.7.1.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn1.bias with zeros.
Layer: encoder.7.1.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv2.weight with Xavier uniform.
Layer: encoder.7.1.conv2.weight | Weight max: 0.025515513494610786, min: -0.02551547810435295, mean: 1.9438402887317352e-05
Layer: encoder.7.1.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn2.bias with zeros.
Layer: encoder.7.1.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.1.conv3.weight with Xavier uniform.
Layer: encoder.7.1.conv3.weight | Weight max: 0.0484122633934021, min: -0.04841228201985359, mean: -3.6745055695064366e-05
Layer: encoder.7.1.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.1.bn3.bias with zeros.
Layer: encoder.7.1.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv1.weight with Xavier uniform.
Layer: encoder.7.2.conv1.weight | Weight max: 0.04841222986578941, min: -0.04841227829456329, mean: 4.181994427199243e-06
Layer: encoder.7.2.bn1.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn1.bias with zeros.
Layer: encoder.7.2.bn1.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv2.weight with Xavier uniform.
Layer: encoder.7.2.conv2.weight | Weight max: 0.025515513494610786, min: -0.02551550790667534, mean: -3.4777829114318592e-06
Layer: encoder.7.2.bn2.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn2.bias with zeros.
Layer: encoder.7.2.bn2.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized encoder.7.2.conv3.weight with Xavier uniform.
Layer: encoder.7.2.conv3.weight | Weight max: 0.04841208457946777, min: -0.04841218516230583, mean: -2.2416208594222553e-05
Layer: encoder.7.2.bn3.weight | Weight max: 1.0, min: 1.0, mean: 1.0
Initialized encoder.7.2.bn3.bias with zeros.
Layer: encoder.7.2.bn3.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Initialized FC.weight with Xavier uniform.
Layer: FC.weight | Weight max: 0.05387689918279648, min: -0.05387478321790695, mean: -6.398450932465494e-05
Initialized FC.bias with zeros.
Layer: FC.bias | Weight max: 0.0, min: 0.0, mean: 0.0
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    7180 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    3248 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initializing LRP Pruning...
LRP initialized successfully.
=== Round 1/3 ===
Training and communication for Round 1...
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Starting validation after Round 1...
Bug fix for empty classification report.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.1979
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.1345

Fhre LRP-Pruning in Runde 1 durch...
Fhre LRP-Pruning in Runde 1 durch...
Berechne LRP-Pruning-Maske fr Land: Belgium
Erstelle DataLoader fr Land: Belgium
Pre-filtered 1939 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    1323 filtered patches indexed
    1323 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([11])
Unexpected input shape for LRP: torch.Size([11]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 11])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 11
Input shape before relevance computation: torch.Size([11, 10, 1, 11])
Model output shape: torch.Size([11, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([11, 19]) to 11 for one-hot encoding.
Targets shape: torch.Size([11])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([11, 19])
Grad output shape: torch.Size([11, 19])
Relevance computation successful. Relevance shape: torch.Size([11, 10, 1, 11])
Relevanzkarten berechnet: ['conv1', 'encoder.4.0.conv1', 'encoder.4.0.conv2', 'encoder.4.0.conv3', 'encoder.4.0.downsample.0', 'encoder.4.1.conv1', 'encoder.4.1.conv2', 'encoder.4.1.conv3', 'encoder.4.2.conv1', 'encoder.4.2.conv2', 'encoder.4.2.conv3', 'encoder.5.0.conv1', 'encoder.5.0.conv2', 'encoder.5.0.conv3', 'encoder.5.0.downsample.0', 'encoder.5.1.conv1', 'encoder.5.1.conv2', 'encoder.5.1.conv3', 'encoder.5.2.conv1', 'encoder.5.2.conv2', 'encoder.5.2.conv3', 'encoder.5.3.conv1', 'encoder.5.3.conv2', 'encoder.5.3.conv3', 'encoder.6.0.conv1', 'encoder.6.0.conv2', 'encoder.6.0.conv3', 'encoder.6.0.downsample.0', 'encoder.6.1.conv1', 'encoder.6.1.conv2', 'encoder.6.1.conv3', 'encoder.6.2.conv1', 'encoder.6.2.conv2', 'encoder.6.2.conv3', 'encoder.6.3.conv1', 'encoder.6.3.conv2', 'encoder.6.3.conv3', 'encoder.6.4.conv1', 'encoder.6.4.conv2', 'encoder.6.4.conv3', 'encoder.6.5.conv1', 'encoder.6.5.conv2', 'encoder.6.5.conv3', 'encoder.7.0.conv1', 'encoder.7.0.conv2', 'encoder.7.0.conv3', 'encoder.7.0.downsample.0', 'encoder.7.1.conv1', 'encoder.7.1.conv2', 'encoder.7.1.conv3', 'encoder.7.2.conv1', 'encoder.7.2.conv2', 'encoder.7.2.conv3']
Calling generate_global_pruning_mask with pruning_rate: 0.3
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.5141e+30, 8.6092e+29, 1.5146e+30, 2.9051e+29, 1.0589e+30, 7.0869e+29,
        1.7355e+30, 1.4337e+30, 1.3772e+30, 1.6567e+30, 1.9531e+30, 1.1066e+30,
        1.5717e+30, 9.3835e+29, 1.6436e+30, 9.7700e+29, 6.2687e+29, 1.9492e+30,
        8.9796e+29, 2.1645e+30, 1.4124e+30, 4.9298e+29, 2.6754e+30, 1.4396e+30,
        1.1972e+30, 2.7266e+30, 1.0827e+30, 1.1088e+30, 2.1892e+30, 5.0663e+29,
        1.2487e+30, 9.8672e+29, 9.3970e+29, 8.9571e+29, 1.2243e+30, 9.1368e+29,
        6.9153e+29, 9.2231e+29, 2.4463e+30, 1.5543e+30, 2.0787e+30, 3.0522e+30,
        1.0601e+30, 1.9929e+30, 3.4405e+29, 1.2592e+30, 7.7677e+29, 5.1069e+29,
        1.0930e+30, 5.9385e+29, 9.8534e+29, 5.4954e+29, 1.5675e+30, 1.4027e+30,
        5.8846e+29, 8.9153e+29, 1.2581e+30, 1.0013e+30, 1.2097e+30, 1.4465e+30,
        2.0662e+30, 9.5424e+29, 1.7731e+30, 1.4249e+30])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.3986e+29, 5.6643e+29, 6.9793e+29, 7.9855e+29, 6.4140e+29, 1.2750e+30,
        7.0673e+29, 2.6744e+29, 5.6819e+29, 3.4768e+29, 7.2099e+29, 9.1026e+29,
        2.0764e+29, 6.3965e+29, 3.4636e+29, 2.8581e+29, 5.6350e+29, 7.3881e+29,
        1.6643e+29, 4.4290e+29, 2.9573e+29, 3.3761e+29, 8.1130e+29, 8.3380e+29,
        8.3281e+29, 8.6357e+29, 3.6217e+29, 3.5536e+29, 1.3302e+30, 4.7333e+29,
        1.6135e+30, 4.0897e+29, 8.6656e+29, 5.0556e+29, 2.3486e+29, 4.3868e+29,
        8.6821e+29, 1.3789e+30, 1.1958e+30, 5.6265e+29, 2.4862e+29, 3.6059e+29,
        1.0880e+30, 7.8656e+29, 4.9694e+29, 3.2309e+29, 6.9880e+29, 1.4754e+30,
        6.6026e+29, 4.0387e+29, 5.4786e+29, 7.2803e+29, 3.8123e+29, 5.4216e+29,
        2.0989e+29, 4.9718e+29, 5.2533e+29, 8.0664e+29, 6.5193e+29, 4.8332e+29,
        3.6283e+29, 5.0172e+29, 1.2117e+30, 3.9161e+29])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0526e+30, 8.9934e+29, 4.6323e+29, 5.1409e+29, 3.7301e+29, 2.5113e+29,
        1.3279e+29, 2.4877e+29, 5.6390e+29, 3.5805e+29, 2.2898e+29, 2.0119e+29,
        3.0853e+29, 6.0759e+29, 5.8817e+29, 2.8547e+29, 5.3233e+29, 6.1644e+29,
        6.4968e+29, 1.9153e+29, 4.1569e+29, 5.9894e+29, 2.6377e+29, 6.8919e+29,
        2.1479e+29, 8.4657e+29, 8.3496e+29, 5.6955e+29, 8.6376e+29, 4.4599e+29,
        4.5180e+29, 1.7442e+29, 4.1493e+29, 3.7807e+29, 3.4305e+29, 4.8473e+29,
        8.4864e+29, 2.7528e+29, 4.2591e+29, 7.3884e+29, 3.5642e+29, 7.6979e+29,
        7.1371e+29, 2.1988e+29, 8.7485e+29, 2.4970e+29, 5.1413e+29, 6.1556e+29,
        8.4518e+29, 5.4215e+29, 3.9084e+29, 7.4079e+29, 6.0975e+29, 7.3456e+29,
        2.7273e+29, 4.0215e+29, 1.9500e+29, 3.5516e+29, 2.2954e+29, 5.0595e+29,
        8.7067e+29, 4.1186e+29, 7.5994e+29, 3.8901e+29])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.2239e+27, 1.6911e+27, 3.4260e+28, 5.0342e+27, 1.4807e+29, 9.8061e+27,
        1.2128e+28, 5.9223e+27, 1.5663e+27, 3.7999e+27, 2.2136e+28, 5.1818e+27,
        6.2404e+27, 1.2033e+28, 2.7067e+28, 1.3989e+28, 7.0828e+28, 6.6505e+27,
        5.7677e+27, 2.4615e+27, 1.1384e+28, 8.3895e+26, 5.9885e+27, 9.4420e+27,
        5.2859e+27, 4.7096e+27, 1.1488e+29, 1.1735e+28, 2.5392e+27, 3.9486e+28,
        5.2468e+27, 5.0297e+28, 4.0336e+28, 1.1098e+28, 3.6142e+27, 3.0037e+28,
        1.1027e+27, 4.8797e+27, 2.4650e+27, 6.0535e+27, 1.0077e+28, 2.8095e+29,
        2.0574e+28, 1.0794e+29, 5.8179e+27, 2.2885e+28, 1.9274e+28, 8.9781e+27,
        2.1065e+28, 3.6437e+28, 7.3244e+27, 8.8597e+27, 9.9356e+27, 1.4496e+28,
        5.6346e+27, 1.0400e+28, 8.4973e+27, 8.1912e+27, 4.4911e+27, 1.4814e+28,
        7.4511e+28, 1.8323e+28, 1.7448e+27, 7.5068e+27, 3.1758e+27, 2.4071e+28,
        1.0937e+28, 6.8442e+28, 6.6773e+27, 1.4010e+28, 1.5699e+28, 7.8707e+28,
        6.9549e+27, 1.3880e+28, 5.3269e+27, 5.7478e+28, 5.8734e+28, 5.6863e+27,
        8.0973e+27, 3.9063e+27, 3.5901e+27, 7.5697e+27, 8.6262e+27, 1.0108e+28,
        1.0890e+28, 6.7099e+27, 5.0421e+27, 2.5194e+28, 5.9754e+27, 6.5846e+27,
        1.4547e+28, 6.2631e+27, 1.1530e+28, 1.7079e+27, 6.4120e+27, 3.7248e+27,
        2.8310e+27, 2.7021e+27, 5.9493e+28, 9.6667e+27, 1.0242e+28, 4.4702e+27,
        6.2852e+27, 5.2594e+27, 1.9725e+28, 2.2020e+28, 1.1354e+28, 9.4480e+27,
        1.0589e+28, 3.4994e+27, 9.3410e+28, 4.7162e+27, 1.0073e+28, 9.2030e+27,
        1.2707e+28, 4.5610e+28, 1.6413e+28, 1.1346e+28, 3.0595e+28, 5.8499e+27,
        5.7840e+27, 3.4133e+28, 6.8847e+27, 5.7901e+27, 8.0615e+27, 1.7918e+27,
        4.0145e+27, 6.8530e+28, 3.5918e+27, 6.3195e+27, 1.2849e+28, 5.2358e+27,
        1.8316e+27, 2.9083e+27, 8.0184e+27, 9.9804e+27, 8.9079e+27, 2.0751e+27,
        7.6851e+27, 1.9509e+28, 1.9874e+28, 5.8051e+27, 1.0234e+28, 7.7558e+27,
        1.1279e+28, 2.9998e+27, 1.4075e+28, 1.4404e+28, 2.2419e+28, 1.5323e+28,
        9.7897e+28, 1.9840e+28, 7.2321e+27, 1.2286e+28, 8.4245e+29, 1.7963e+28,
        7.5690e+28, 9.6755e+27, 1.6262e+29, 4.0647e+27, 1.3923e+27, 1.7893e+28,
        1.2652e+28, 2.3798e+28, 2.5226e+28, 5.2535e+27, 1.9572e+28, 9.3431e+27,
        8.9344e+27, 1.5241e+28, 8.4166e+27, 2.2205e+28, 4.2893e+27, 1.6217e+28,
        1.2514e+28, 8.2537e+27, 6.6676e+27, 1.0218e+28, 1.4419e+28, 1.8949e+27,
        5.5644e+27, 1.5119e+28, 8.4835e+28, 8.6801e+27, 5.3550e+27, 9.9689e+27,
        9.2304e+27, 4.7986e+27, 1.1814e+28, 5.5531e+27, 1.1194e+28, 1.2554e+28,
        8.5543e+27, 1.0689e+28, 8.6862e+27, 7.2028e+28, 1.2634e+28, 1.6015e+28,
        7.3299e+27, 3.2841e+28, 9.1318e+27, 9.0719e+27, 1.7653e+27, 2.0303e+28,
        8.1692e+27, 9.3625e+27, 1.5786e+28, 6.8632e+27, 9.4987e+27, 4.3254e+28,
        7.9698e+27, 3.4152e+28, 2.2216e+28, 6.1654e+28, 6.5585e+28, 1.3750e+28,
        1.1016e+28, 3.8062e+28, 7.5891e+27, 1.9617e+28, 2.7837e+27, 4.2254e+28,
        1.9160e+28, 2.8324e+28, 2.9227e+28, 5.6328e+27, 7.3207e+27, 5.7660e+27,
        5.2248e+28, 5.9556e+27, 1.1212e+28, 9.7765e+27, 2.1022e+28, 2.1023e+27,
        7.7014e+27, 6.2898e+27, 1.9160e+28, 1.0083e+28, 1.0592e+28, 2.1619e+28,
        3.8699e+27, 4.0521e+28, 6.6298e+27, 1.6055e+28, 4.7123e+28, 7.7834e+28,
        6.3015e+27, 1.1047e+28, 4.5308e+28, 1.8577e+28, 7.1003e+27, 3.4535e+28,
        7.0274e+27, 2.3673e+28, 3.8455e+27, 5.0170e+27])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.8666e+27, 1.1103e+28, 2.7973e+28, 4.7991e+27, 1.5337e+29, 3.3095e+26,
        1.6816e+28, 3.2407e+27, 7.1143e+27, 8.3935e+26, 2.0384e+28, 1.1035e+27,
        2.7251e+27, 1.0174e+28, 2.2002e+28, 2.9104e+27, 7.5302e+28, 2.5437e+26,
        1.3416e+27, 1.4200e+28, 1.0044e+27, 5.8830e+27, 1.8643e+27, 1.0603e+27,
        2.9241e+27, 5.0052e+27, 1.1589e+29, 3.3366e+27, 7.3925e+27, 3.7659e+28,
        2.6138e+27, 4.2216e+28, 3.4143e+28, 6.6238e+26, 7.0671e+27, 1.3235e+28,
        3.7291e+27, 3.2983e+27, 5.4270e+27, 1.7845e+27, 8.0792e+26, 2.9474e+29,
        7.9945e+27, 1.0647e+29, 4.4629e+27, 1.6304e+28, 1.5247e+28, 2.2488e+27,
        5.8341e+27, 2.6063e+28, 1.2758e+28, 5.8400e+27, 1.0820e+28, 4.8692e+27,
        5.1711e+27, 1.1843e+27, 4.1739e+27, 2.2786e+27, 6.2177e+27, 2.1414e+28,
        7.5680e+28, 4.9383e+27, 1.4213e+28, 2.0335e+27, 2.1092e+27, 2.1887e+28,
        1.5603e+28, 6.7365e+28, 8.0289e+27, 2.0956e+27, 2.1387e+28, 6.7735e+28,
        4.4857e+26, 5.7441e+27, 3.4658e+27, 6.0484e+28, 6.3369e+28, 2.2653e+27,
        1.3035e+27, 9.3673e+27, 1.1544e+28, 7.1179e+26, 2.6329e+27, 8.7750e+27,
        8.9464e+27, 1.7026e+27, 5.5484e+27, 2.1380e+28, 7.1317e+26, 1.0541e+27,
        6.6284e+27, 9.7486e+26, 3.4758e+27, 4.7038e+27, 2.2549e+27, 1.2887e+26,
        1.1604e+28, 5.8499e+27, 6.0210e+28, 2.7921e+27, 4.0099e+27, 5.2054e+27,
        1.0335e+27, 1.8650e+27, 2.2270e+28, 9.0099e+27, 8.5550e+27, 2.6233e+27,
        1.3783e+27, 5.0276e+27, 9.6696e+28, 2.0453e+27, 2.4217e+27, 1.6034e+27,
        9.3266e+27, 3.4643e+28, 6.9189e+27, 4.8804e+27, 1.3896e+28, 1.5470e+27,
        1.0337e+28, 2.9947e+28, 1.1639e+28, 8.4014e+27, 3.8126e+26, 1.0313e+28,
        8.3157e+27, 6.9582e+28, 7.4577e+27, 2.0871e+27, 2.6372e+27, 1.2130e+28,
        8.0591e+27, 1.8111e+27, 5.0732e+27, 1.4660e+27, 6.9890e+27, 5.7377e+27,
        6.9390e+27, 1.5185e+28, 1.7269e+27, 8.1169e+27, 1.0221e+28, 8.7495e+26,
        5.5366e+26, 6.6836e+26, 7.4476e+27, 8.6694e+26, 2.1112e+27, 1.1325e+28,
        9.2389e+28, 1.5457e+28, 3.3059e+27, 5.4297e+26, 8.4123e+29, 2.2624e+28,
        7.6504e+28, 3.4632e+26, 1.7060e+29, 2.3220e+27, 3.4945e+27, 1.9284e+28,
        1.3087e+28, 2.1619e+28, 8.4734e+27, 1.2548e+28, 1.1551e+28, 4.4196e+26,
        5.5653e+27, 6.4888e+27, 1.2480e+27, 5.8472e+27, 1.6518e+27, 3.3523e+27,
        1.4625e+28, 3.5289e+27, 3.7911e+27, 1.3692e+27, 3.8906e+27, 5.5678e+26,
        7.0622e+27, 1.5502e+28, 7.4714e+28, 2.5003e+27, 2.5445e+27, 1.6241e+28,
        1.1657e+27, 1.7461e+27, 2.9989e+27, 1.3401e+27, 6.1061e+27, 1.4261e+28,
        1.4312e+27, 1.1717e+28, 4.3978e+27, 6.8924e+28, 1.6372e+27, 1.6251e+28,
        3.9210e+27, 2.9664e+28, 1.6890e+28, 5.5429e+27, 3.7511e+27, 1.1713e+28,
        1.6066e+27, 1.6461e+27, 1.3731e+28, 5.1693e+27, 5.0060e+27, 3.8347e+28,
        1.2711e+27, 4.0920e+28, 1.7256e+28, 4.0065e+28, 4.7810e+28, 8.7595e+27,
        5.1804e+27, 2.4903e+28, 5.1985e+27, 2.0318e+28, 1.4890e+28, 4.1704e+28,
        5.8483e+27, 3.1500e+28, 2.6642e+28, 3.0059e+27, 1.0972e+28, 2.7190e+26,
        4.5010e+28, 8.0714e+27, 1.0905e+27, 1.0033e+28, 1.5639e+28, 4.1140e+27,
        4.5248e+27, 1.1280e+28, 1.8756e+28, 6.3278e+27, 8.6654e+26, 1.6738e+28,
        1.0333e+28, 3.8219e+28, 4.2753e+27, 4.4813e+27, 4.0765e+28, 7.8657e+28,
        5.3156e+27, 2.3185e+27, 3.7281e+28, 9.0949e+27, 8.2780e+26, 1.8842e+28,
        1.5501e+27, 2.9053e+28, 5.3414e+26, 9.7623e+26])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1851e+28, 1.9057e+28, 2.5580e+28, 1.5612e+28, 1.8498e+28, 1.0422e+28,
        6.8972e+27, 1.2523e+28, 9.6503e+27, 1.7499e+28, 6.6959e+27, 7.2839e+27,
        6.6020e+27, 2.3079e+28, 1.5351e+28, 1.0850e+28, 1.2234e+28, 1.7909e+28,
        1.1248e+28, 2.3722e+28, 1.4859e+28, 1.9610e+28, 1.2323e+28, 4.3333e+27,
        1.4204e+28, 6.4946e+27, 1.4464e+28, 1.3715e+28, 1.0344e+28, 1.9441e+28,
        1.3836e+28, 1.3174e+28, 1.9945e+28, 1.4533e+28, 1.1957e+28, 1.0230e+28,
        1.2932e+28, 2.3361e+28, 1.5363e+28, 9.0673e+27, 6.4396e+27, 5.5256e+27,
        2.5666e+28, 9.2329e+27, 1.9064e+28, 1.3102e+28, 8.6051e+27, 1.2388e+28,
        1.0840e+28, 5.8665e+27, 8.7437e+27, 2.6652e+28, 1.2079e+28, 1.8222e+28,
        7.9759e+27, 1.4258e+28, 6.4643e+27, 1.5982e+28, 1.8331e+28, 2.2853e+28,
        1.4637e+28, 1.8440e+28, 1.3623e+28, 1.3850e+28])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.6196e+27, 6.8302e+27, 5.8528e+27, 4.0560e+27, 1.6567e+28, 9.1790e+27,
        1.4737e+28, 1.1993e+28, 1.3110e+28, 7.6154e+27, 1.6361e+28, 1.2313e+28,
        7.7243e+27, 6.3113e+27, 1.3540e+28, 1.0263e+28, 8.5384e+27, 6.0198e+27,
        7.0184e+27, 9.5416e+27, 6.0832e+27, 1.8632e+28, 1.5990e+28, 2.0445e+28,
        1.7514e+28, 1.0382e+28, 1.5890e+28, 6.9120e+27, 5.9071e+27, 1.1104e+28,
        1.4382e+28, 8.2170e+27, 1.0657e+28, 2.2003e+28, 1.3747e+28, 1.6186e+28,
        2.2121e+28, 7.2461e+27, 9.7838e+27, 1.2143e+28, 1.1059e+28, 1.1824e+28,
        2.1148e+28, 8.3139e+27, 1.1724e+28, 9.6988e+27, 9.7595e+27, 1.0432e+28,
        7.1675e+27, 8.7009e+27, 1.3214e+28, 6.4956e+27, 1.6908e+28, 1.0805e+28,
        1.2177e+28, 9.9675e+27, 1.0481e+28, 6.9612e+27, 8.1835e+27, 9.0486e+27,
        7.5554e+27, 1.9099e+28, 5.5840e+27, 1.6498e+28])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0902e+26, 1.2980e+26, 1.7394e+26, 1.5235e+26, 8.0370e+25, 5.1420e+25,
        1.5363e+26, 9.2149e+26, 1.8953e+26, 1.8900e+27, 1.4748e+26, 1.6063e+27,
        7.6388e+25, 2.5968e+26, 8.5917e+25, 1.2035e+26, 1.0388e+26, 1.9044e+26,
        1.9537e+26, 2.6763e+26, 2.2787e+26, 7.3526e+25, 1.6601e+26, 1.1891e+26,
        8.0328e+25, 1.3185e+26, 6.0908e+25, 6.6726e+25, 5.3528e+25, 8.4760e+25,
        9.7149e+25, 1.0598e+26, 1.2196e+26, 2.7763e+26, 6.3070e+25, 1.6004e+26,
        3.1782e+26, 3.9095e+26, 3.3768e+25, 1.4152e+26, 2.3718e+25, 7.1581e+25,
        1.4918e+26, 1.0456e+26, 1.8876e+26, 2.5879e+26, 1.9458e+26, 1.8416e+26,
        1.8587e+26, 1.9453e+26, 1.5286e+26, 3.0667e+26, 9.0133e+25, 6.4791e+26,
        4.3299e+26, 2.1458e+26, 8.2548e+26, 1.3469e+26, 1.5413e+26, 3.8419e+25,
        2.2336e+26, 7.4909e+25, 3.2628e+26, 2.1937e+26, 1.4530e+26, 1.7415e+26,
        2.2847e+26, 5.4431e+25, 1.5703e+26, 1.2970e+26, 2.7514e+26, 1.4279e+26,
        3.5217e+27, 6.1243e+25, 1.2282e+26, 1.7670e+26, 1.1302e+26, 1.7628e+26,
        3.5195e+25, 1.2125e+26, 5.2117e+26, 1.7594e+26, 2.0284e+26, 1.4362e+26,
        8.9045e+25, 9.5272e+25, 1.9910e+26, 1.3809e+26, 3.0031e+26, 1.6453e+26,
        9.2804e+25, 2.3266e+26, 1.7133e+27, 5.8762e+25, 4.9622e+25, 1.1850e+26,
        1.3647e+26, 2.7159e+26, 9.1117e+25, 1.2803e+26, 1.3947e+26, 3.8852e+25,
        1.0404e+26, 2.4155e+26, 4.9143e+25, 1.2214e+26, 2.1893e+26, 2.2378e+26,
        1.4404e+26, 6.3870e+25, 1.7969e+26, 9.1092e+25, 1.5624e+26, 8.7704e+25,
        2.6033e+26, 1.1178e+26, 2.3370e+26, 1.2222e+26, 5.1107e+26, 2.4330e+26,
        8.6788e+25, 2.0295e+26, 1.6155e+26, 6.6133e+25, 2.7909e+26, 1.8584e+26,
        1.9952e+26, 1.5694e+26, 1.4877e+26, 2.3861e+26, 2.6074e+26, 1.0718e+27,
        1.3347e+26, 3.0733e+26, 5.8915e+26, 1.8096e+26, 1.4213e+26, 8.1589e+25,
        9.5825e+25, 6.0103e+26, 1.1066e+28, 7.8327e+25, 1.9399e+26, 2.0713e+26,
        3.3286e+26, 3.3000e+26, 9.2005e+25, 5.9495e+25, 3.5987e+25, 4.4500e+26,
        1.6318e+26, 2.1588e+26, 1.9843e+26, 1.2233e+26, 3.0399e+26, 2.0826e+26,
        2.1393e+26, 5.5668e+26, 1.3808e+26, 3.6796e+27, 1.1633e+26, 1.7639e+26,
        1.0522e+28, 1.3971e+26, 2.2329e+26, 1.4977e+26, 1.1787e+26, 5.4602e+25,
        7.4940e+25, 7.1900e+25, 1.1363e+26, 2.6845e+26, 5.4598e+26, 1.3300e+26,
        1.3150e+26, 1.6010e+26, 8.0782e+25, 2.0096e+26, 1.2713e+26, 6.0077e+25,
        1.4904e+26, 5.2622e+25, 1.5955e+26, 1.7728e+26, 3.0095e+27, 7.5204e+25,
        7.5218e+25, 1.6137e+26, 2.3926e+26, 1.1878e+26, 1.9333e+26, 3.1021e+26,
        2.0157e+26, 1.2375e+26, 2.3880e+26, 2.2145e+26, 1.4020e+26, 5.4829e+25,
        1.2297e+26, 1.7768e+26, 1.0083e+26, 2.2874e+26, 2.0476e+26, 2.0436e+26,
        9.5221e+25, 6.0955e+26, 5.4839e+26, 1.3429e+26, 3.8896e+26, 1.1862e+26,
        3.4760e+26, 3.6893e+26, 1.2283e+26, 3.9016e+27, 4.7199e+25, 1.5519e+26,
        8.2151e+25, 1.2965e+26, 3.9793e+26, 2.4915e+26, 3.5924e+25, 1.6860e+26,
        7.7263e+26, 2.2618e+26, 1.7487e+26, 1.7884e+26, 2.1622e+26, 3.9640e+25,
        1.1606e+26, 9.9493e+25, 6.9592e+27, 5.3121e+26, 1.0906e+26, 1.8720e+26,
        1.7984e+26, 1.9351e+26, 2.3903e+26, 2.1617e+26, 5.6658e+25, 1.5496e+26,
        2.7697e+26, 1.1671e+26, 5.8204e+25, 1.1189e+26, 1.7446e+26, 2.5693e+26,
        8.9973e+25, 8.3758e+25, 9.1687e+25, 1.9745e+26, 2.4347e+26, 6.9405e+26,
        1.1140e+26, 1.5918e+26, 6.9539e+25, 7.7864e+25])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.0068e+26, 1.3568e+26, 1.1548e+26, 2.3778e+26, 5.2459e+26, 5.4784e+26,
        5.5745e+26, 2.3985e+26, 8.1843e+25, 1.5957e+26, 2.9238e+26, 1.5771e+26,
        2.3948e+26, 1.1038e+26, 2.1570e+26, 3.7958e+26, 1.8118e+26, 2.8951e+26,
        2.3173e+26, 8.0876e+25, 4.4525e+26, 1.9575e+26, 1.6243e+26, 3.3496e+26,
        2.8220e+26, 2.9196e+26, 4.3412e+26, 4.6830e+26, 8.9991e+25, 3.0200e+26,
        2.6473e+26, 3.4296e+26, 4.0223e+26, 2.0408e+26, 2.2637e+26, 3.6280e+26,
        2.9322e+26, 2.2990e+26, 2.7081e+26, 3.2165e+26, 2.1280e+26, 6.4615e+26,
        2.7104e+26, 1.4903e+26, 1.8215e+26, 3.9779e+26, 1.2234e+26, 1.9859e+26,
        2.4656e+26, 5.3058e+25, 1.8708e+26, 2.5856e+26, 2.8677e+26, 5.3347e+26,
        1.7382e+26, 3.2491e+26, 1.2284e+26, 4.3835e+26, 2.4944e+26, 2.7124e+26,
        4.6817e+26, 1.3106e+26, 4.2451e+26, 3.0408e+26])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.4209e+26, 4.5894e+26, 2.8404e+26, 1.8484e+26, 3.1842e+26, 1.3009e+26,
        1.6989e+26, 1.9917e+26, 2.6747e+26, 1.2029e+26, 2.6824e+26, 3.1462e+26,
        2.9820e+26, 2.4607e+26, 2.6124e+26, 3.2094e+26, 1.5459e+26, 3.7828e+26,
        1.0454e+26, 1.2977e+26, 4.0082e+26, 1.3229e+26, 1.7019e+26, 1.2655e+26,
        4.4440e+26, 2.1611e+26, 2.3318e+26, 1.9689e+26, 1.1645e+26, 1.1970e+26,
        2.4482e+26, 1.2202e+26, 1.3000e+26, 1.6464e+26, 1.9503e+26, 2.1014e+26,
        1.4052e+26, 1.3134e+26, 1.9579e+26, 1.4960e+26, 4.6418e+26, 1.3468e+26,
        9.4600e+25, 3.5191e+26, 1.3278e+26, 6.2950e+25, 2.6449e+26, 4.9304e+26,
        1.9633e+26, 2.5328e+26, 8.8065e+25, 4.2088e+26, 9.7387e+25, 1.4321e+26,
        2.3528e+26, 2.0958e+26, 1.9715e+26, 1.8463e+26, 4.3510e+26, 1.6613e+26,
        2.5570e+26, 1.7658e+26, 1.4997e+26, 3.3295e+26])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.0823e+24, 1.2055e+24, 7.7864e+24, 8.5571e+24, 1.0745e+24, 2.1272e+24,
        4.8723e+24, 3.4705e+24, 2.3827e+23, 1.8947e+24, 3.0832e+24, 3.4995e+24,
        1.1318e+24, 1.7272e+24, 3.5963e+24, 2.2731e+25, 1.2986e+25, 4.7254e+24,
        3.8932e+24, 3.9784e+24, 2.7797e+24, 4.9333e+23, 2.4696e+24, 5.4909e+24,
        1.1683e+24, 3.0299e+24, 9.7471e+23, 3.9980e+24, 6.9775e+24, 1.1353e+24,
        4.0497e+24, 3.3923e+25, 4.7685e+24, 3.4792e+24, 1.6839e+24, 5.9450e+24,
        5.9446e+24, 3.4790e+24, 5.4453e+24, 6.8852e+24, 5.9978e+23, 2.0057e+24,
        7.3403e+23, 2.9813e+24, 3.6008e+24, 1.9064e+24, 2.5143e+24, 2.2144e+24,
        1.2872e+24, 2.7458e+25, 2.6003e+24, 3.2557e+24, 2.4133e+24, 2.0067e+25,
        3.3251e+24, 7.1128e+23, 4.8488e+24, 3.8889e+24, 4.0503e+24, 1.8622e+24,
        5.1460e+24, 5.3717e+24, 8.3081e+24, 3.2873e+24, 2.3057e+24, 3.9985e+24,
        4.2271e+24, 1.7105e+24, 1.4630e+24, 1.5304e+24, 1.1658e+24, 1.0956e+25,
        3.1396e+26, 1.6176e+24, 1.1653e+25, 3.2227e+24, 2.6828e+24, 2.4085e+24,
        3.4832e+25, 1.8322e+24, 4.3504e+24, 2.0995e+24, 4.3202e+24, 2.6614e+24,
        2.0330e+24, 1.6408e+24, 1.9342e+24, 3.4632e+24, 5.9557e+24, 6.3933e+24,
        1.1284e+24, 2.0121e+24, 2.4120e+24, 3.6042e+24, 5.5999e+24, 7.5544e+23,
        3.3360e+24, 6.7926e+24, 2.0744e+24, 8.0377e+24, 6.8737e+24, 5.1785e+24,
        4.7967e+24, 2.8243e+25, 2.6693e+24, 2.2290e+24, 1.4292e+24, 1.2008e+24,
        3.5462e+24, 5.2867e+24, 9.6345e+24, 9.3016e+24, 3.4707e+24, 5.1543e+25,
        3.6782e+24, 1.7844e+24, 3.1032e+24, 2.3318e+24, 4.2760e+24, 5.7742e+25,
        1.1022e+24, 7.9935e+23, 2.2044e+24, 2.2883e+24, 1.4505e+24, 2.3872e+24,
        4.3042e+24, 5.8235e+24, 1.7874e+24, 2.5560e+24, 3.8279e+24, 4.2383e+24,
        3.1567e+24, 2.1506e+24, 2.4779e+24, 1.0962e+24, 9.5797e+24, 6.7263e+24,
        1.6120e+24, 2.1986e+24, 3.6784e+24, 1.1771e+24, 4.0192e+24, 1.8964e+25,
        2.3240e+24, 2.1838e+24, 3.5874e+24, 9.2146e+24, 2.6942e+24, 1.7078e+24,
        4.7989e+24, 2.0784e+24, 1.8832e+24, 2.0981e+24, 3.8768e+24, 1.8458e+25,
        2.8998e+24, 1.7282e+24, 4.9935e+24, 6.6579e+25, 3.7203e+24, 2.2007e+24,
        4.6783e+24, 2.5624e+24, 4.3583e+24, 6.0633e+24, 2.6012e+24, 4.9598e+23,
        2.7789e+25, 7.9580e+24, 9.2274e+24, 5.1877e+24, 4.0388e+24, 1.1525e+24,
        1.4845e+25, 1.5950e+24, 6.9944e+24, 2.0676e+25, 4.1117e+24, 2.1506e+24,
        4.2899e+24, 3.7189e+24, 2.1965e+24, 1.2781e+25, 2.2752e+24, 4.3047e+24,
        9.3998e+23, 5.5737e+24, 3.4686e+24, 3.4194e+24, 3.5075e+25, 1.3340e+24,
        4.3849e+24, 4.6926e+24, 3.6776e+24, 6.3161e+24, 2.1778e+24, 2.4443e+24,
        3.3876e+24, 1.6007e+25, 2.6037e+24, 2.2610e+24, 1.3911e+24, 6.9222e+24,
        1.4016e+24, 3.7470e+24, 1.1984e+24, 2.4052e+24, 2.8202e+25, 5.2088e+24,
        7.8584e+24, 3.5443e+24, 1.6348e+25, 1.3110e+24, 1.5815e+25, 1.9500e+24,
        1.8642e+24, 3.1612e+24, 3.3826e+24, 2.5361e+24, 1.3214e+24, 5.7274e+24,
        5.0336e+24, 3.2229e+24, 1.0451e+26, 1.3572e+24, 6.1113e+23, 6.0812e+24,
        1.7163e+24, 1.4675e+25, 5.2337e+24, 2.2951e+24, 9.0512e+23, 5.0587e+24,
        2.7385e+24, 1.9875e+24, 5.0252e+24, 1.7447e+26, 5.0080e+24, 1.2828e+24,
        1.6605e+24, 3.2617e+24, 2.9332e+24, 5.7903e+24, 3.0350e+24, 4.2229e+24,
        2.8917e+24, 3.4864e+25, 5.9853e+24, 2.1408e+24, 4.4036e+24, 4.5122e+24,
        4.0255e+24, 7.8240e+24, 2.6039e+24, 1.5491e+24])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.1878e+24, 3.5658e+24, 4.2290e+24, 3.8480e+24, 3.6127e+24, 5.1525e+24,
        3.4836e+24, 3.2035e+24, 1.9465e+24, 1.8906e+24, 1.7673e+24, 1.9777e+24,
        3.1092e+24, 4.8379e+24, 5.5753e+24, 2.5076e+24, 4.9308e+24, 2.7457e+24,
        7.6042e+24, 5.2702e+24, 4.7120e+24, 5.5821e+24, 8.4378e+24, 9.0714e+24,
        3.9512e+24, 4.0379e+24, 3.4829e+24, 5.4833e+24, 2.1597e+24, 1.3523e+24,
        5.0767e+24, 1.7565e+24, 4.0807e+24, 3.0422e+24, 9.6595e+24, 3.3586e+24,
        1.3163e+24, 7.8921e+24, 4.1078e+24, 5.6688e+24, 7.2410e+24, 3.7160e+24,
        2.5561e+24, 9.7459e+24, 7.4478e+24, 3.1802e+24, 1.7970e+24, 3.0161e+24,
        3.0443e+24, 7.3365e+24, 6.7372e+24, 5.0489e+24, 2.9983e+24, 4.0946e+24,
        1.7022e+24, 7.5929e+24, 3.7193e+24, 2.4730e+24, 3.2805e+24, 4.5309e+24,
        2.2185e+24, 9.3200e+24, 5.4455e+24, 4.6161e+24, 3.8940e+24, 1.7853e+24,
        2.2482e+24, 4.8554e+24, 3.3596e+24, 1.9009e+24, 6.5006e+24, 7.1231e+24,
        7.2567e+24, 8.1733e+24, 4.7555e+24, 4.2472e+24, 3.2409e+24, 3.0768e+24,
        3.8044e+24, 4.2642e+24, 6.0526e+24, 4.7811e+24, 1.5949e+24, 9.4280e+24,
        4.6037e+24, 8.4459e+24, 3.3435e+24, 2.8652e+24, 5.6765e+24, 3.3273e+24,
        3.8651e+24, 2.8245e+24, 5.1121e+24, 3.9730e+24, 9.5216e+24, 2.9910e+24,
        2.7205e+24, 2.2728e+24, 4.2049e+24, 6.1326e+24, 1.3146e+25, 7.7357e+24,
        3.1648e+24, 7.3940e+24, 2.6223e+24, 5.6434e+24, 4.7982e+24, 3.6550e+24,
        4.4748e+24, 6.9376e+24, 3.6473e+24, 1.4873e+24, 6.4750e+24, 3.7638e+24,
        4.0093e+24, 6.0981e+24, 5.4231e+24, 2.7599e+24, 6.1184e+24, 4.2126e+24,
        2.6361e+24, 6.1683e+24, 3.3505e+24, 9.7367e+24, 4.5058e+24, 5.8794e+24,
        4.0780e+24, 7.0174e+24])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.3815e+24, 4.3738e+24, 4.7317e+24, 2.2848e+24, 1.5534e+24, 4.0048e+24,
        3.3729e+24, 1.3303e+24, 1.9552e+24, 2.8552e+24, 2.7105e+24, 6.0652e+23,
        2.2766e+24, 4.6412e+24, 4.9328e+24, 3.0511e+24, 3.3141e+24, 5.0144e+24,
        2.7372e+24, 5.3578e+24, 2.3358e+24, 4.7478e+24, 3.4930e+24, 2.5527e+24,
        2.7950e+24, 2.0442e+24, 4.4797e+24, 1.7062e+24, 6.1090e+24, 3.2098e+24,
        2.7870e+24, 4.2800e+24, 4.1310e+24, 1.5555e+24, 2.3026e+24, 7.8143e+24,
        6.5216e+24, 3.3107e+24, 3.7803e+24, 3.6008e+24, 2.9182e+24, 4.9418e+24,
        1.0449e+24, 5.0287e+24, 5.0517e+24, 6.7768e+24, 3.6602e+24, 3.4031e+24,
        5.2640e+24, 3.4560e+24, 5.8986e+24, 2.4196e+24, 3.0591e+24, 5.0486e+24,
        3.1112e+24, 8.2967e+24, 3.0705e+24, 4.5670e+24, 5.1415e+24, 6.7482e+24,
        3.3621e+24, 1.5069e+24, 4.7082e+24, 6.4159e+24, 6.3128e+24, 4.5666e+24,
        2.8750e+24, 8.4864e+24, 2.4830e+24, 3.7052e+24, 4.9789e+24, 6.9714e+24,
        4.6959e+24, 5.8638e+24, 2.7150e+24, 7.6773e+24, 6.1861e+24, 5.8585e+24,
        3.3550e+24, 8.1369e+23, 4.7441e+24, 2.9407e+24, 2.6445e+24, 4.4760e+24,
        3.5787e+24, 1.4761e+24, 7.7827e+24, 2.7649e+24, 1.6327e+24, 3.3877e+24,
        2.8224e+24, 5.4778e+24, 4.7532e+24, 2.3715e+24, 3.8041e+24, 4.5760e+24,
        4.4926e+24, 1.4544e+24, 4.1427e+24, 5.4115e+24, 3.5334e+24, 2.4461e+24,
        3.2566e+24, 4.0130e+24, 3.1419e+24, 3.9790e+24, 2.8817e+24, 2.4743e+24,
        6.9224e+24, 2.0702e+24, 9.5957e+23, 3.6488e+24, 2.1682e+24, 2.0508e+24,
        2.5112e+24, 2.0793e+24, 1.7527e+24, 1.9348e+24, 4.4775e+24, 3.5616e+24,
        3.8545e+24, 2.2475e+24, 5.9080e+24, 1.4681e+24, 1.5206e+24, 1.8830e+24,
        2.7829e+24, 7.4097e+24])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.4961e+22, 3.7252e+22, 1.1424e+22, 6.5467e+22, 1.2673e+23, 1.1245e+22,
        9.4601e+21, 6.0716e+21, 1.1100e+22, 8.9207e+22, 2.0776e+22, 3.3710e+21,
        5.1095e+21, 2.9806e+22, 8.3467e+21, 5.4569e+21, 1.1495e+22, 8.3128e+21,
        9.4116e+21, 6.6215e+21, 8.8532e+22, 8.2151e+21, 7.1324e+21, 5.2044e+22,
        6.2584e+22, 4.8772e+21, 7.0946e+21, 9.7085e+21, 2.2825e+22, 6.6801e+21,
        3.0825e+23, 1.4592e+21, 5.6873e+21, 2.0167e+22, 1.4597e+22, 1.1034e+22,
        5.1299e+21, 1.0004e+23, 5.7450e+21, 5.7649e+21, 1.5857e+22, 1.5070e+22,
        1.9351e+21, 7.5478e+21, 2.0703e+22, 1.5885e+22, 4.5893e+21, 5.9889e+21,
        9.1624e+21, 4.1875e+22, 2.1507e+22, 8.2009e+22, 4.4885e+21, 1.6661e+22,
        8.4084e+22, 1.4557e+22, 2.7855e+23, 1.6638e+23, 4.4058e+21, 8.6198e+21,
        8.7984e+22, 3.0121e+22, 4.5489e+21, 1.5919e+22, 6.3758e+21, 1.6413e+22,
        5.5363e+21, 1.0655e+22, 2.0199e+22, 1.9467e+22, 7.4831e+21, 1.5784e+22,
        1.0041e+22, 2.3376e+22, 7.7280e+22, 1.8735e+22, 7.1971e+22, 2.9146e+21,
        1.3680e+22, 8.3218e+20, 5.2310e+21, 2.0791e+22, 1.3294e+22, 7.2018e+21,
        9.7101e+21, 2.9943e+22, 6.9644e+22, 2.3136e+22, 2.8807e+22, 8.4795e+21,
        7.2505e+21, 7.8632e+21, 9.4593e+21, 1.1302e+22, 6.7355e+21, 8.9576e+22,
        4.4842e+21, 7.5523e+21, 6.3492e+21, 7.3943e+21, 7.2076e+21, 5.4366e+22,
        4.5062e+21, 4.9816e+21, 4.0128e+22, 1.5048e+22, 1.3002e+22, 6.2868e+21,
        4.9140e+22, 8.6569e+21, 9.8943e+21, 1.1820e+22, 1.2874e+22, 1.0700e+22,
        1.0621e+21, 9.2195e+21, 7.7262e+21, 3.5638e+21, 5.7278e+22, 4.1711e+21,
        4.2530e+23, 8.0808e+21, 1.0018e+22, 1.2614e+22, 2.1825e+22, 1.2335e+22,
        1.8889e+22, 1.1272e+22, 1.5770e+22, 1.0212e+22, 8.2220e+21, 4.3054e+22,
        6.9534e+21, 1.8005e+22, 1.2915e+22, 7.2438e+21, 1.7606e+22, 2.0226e+22,
        1.0570e+22, 1.3706e+22, 2.0632e+22, 5.0288e+22, 4.5193e+22, 1.8944e+22,
        5.3138e+21, 6.9992e+21, 2.0880e+22, 2.8846e+22, 2.2509e+22, 5.7997e+22,
        2.0793e+23, 3.5439e+21, 1.6281e+22, 1.9772e+23, 6.4693e+21, 8.4104e+21,
        6.2578e+21, 9.5274e+22, 1.2618e+22, 6.9648e+21, 1.3229e+22, 7.2677e+21,
        1.1485e+22, 3.3081e+22, 8.8286e+21, 7.2901e+22, 8.6320e+21, 4.3922e+21,
        2.2151e+22, 1.3544e+22, 1.5334e+22, 7.0470e+22, 9.6007e+21, 2.0650e+21,
        2.5843e+22, 1.4335e+22, 1.6344e+21, 1.7443e+22, 2.3771e+22, 3.9158e+22,
        1.1378e+22, 1.4841e+22, 2.4597e+22, 1.5095e+22, 1.2534e+22, 2.8805e+21,
        2.7053e+22, 1.2036e+22, 3.8199e+21, 1.0392e+22, 2.9460e+22, 5.7724e+21,
        1.3040e+22, 1.7872e+22, 4.6032e+21, 1.2478e+21, 2.6014e+22, 4.8860e+21,
        9.4682e+21, 2.2426e+22, 4.9285e+22, 3.9452e+21, 3.5436e+21, 2.2013e+22,
        8.5562e+22, 8.0475e+21, 6.8096e+21, 1.1733e+22, 5.8172e+21, 1.0353e+22,
        1.5160e+22, 5.6805e+21, 4.0652e+21, 9.3096e+24, 4.8128e+22, 4.1164e+21,
        9.8087e+21, 2.5747e+22, 4.9429e+22, 4.3288e+21, 3.2987e+23, 1.2456e+22,
        2.2430e+21, 3.9783e+21, 1.6586e+22, 3.9643e+21, 5.3747e+21, 1.8568e+22,
        3.3125e+22, 7.8465e+22, 9.7546e+21, 1.5430e+22, 2.4996e+22, 1.2622e+22,
        6.5350e+21, 1.8350e+22, 1.3430e+22, 2.2148e+21, 3.4092e+23, 2.2328e+22,
        1.5247e+22, 3.7493e+22, 3.9911e+21, 2.1007e+22, 5.1380e+21, 4.5717e+21,
        1.1335e+22, 4.6581e+21, 1.6190e+22, 2.9497e+21, 3.3082e+23, 2.2774e+22,
        2.8868e+23, 9.5419e+21, 5.0087e+21, 1.2415e+22, 8.2351e+21, 1.8619e+22,
        1.5724e+22, 4.2723e+22, 1.1533e+22, 7.2959e+21, 9.3074e+21, 7.1063e+20,
        1.1632e+22, 2.6546e+21, 6.6492e+21, 2.3832e+22, 8.6491e+21, 8.1903e+21,
        1.4517e+22, 7.3479e+21, 2.5731e+22, 8.2208e+21, 2.4413e+22, 2.4924e+22,
        6.3132e+21, 1.3760e+22, 9.7316e+21, 5.0903e+22, 7.2816e+21, 6.5313e+22,
        3.7945e+22, 1.0281e+22, 9.5875e+23, 1.7806e+21, 5.6410e+21, 3.7880e+22,
        6.7340e+21, 1.2268e+22, 1.3120e+22, 6.3604e+21, 4.0883e+21, 5.8584e+21,
        6.3772e+21, 5.5967e+22, 9.7692e+21, 1.6913e+22, 4.5776e+21, 2.0665e+22,
        1.4917e+22, 1.1681e+22, 1.4073e+22, 4.4026e+21, 6.7756e+21, 4.1802e+21,
        4.4244e+21, 1.3385e+22, 5.5870e+21, 4.4500e+22, 6.5236e+21, 1.2533e+22,
        8.9162e+21, 7.5850e+21, 8.6058e+21, 1.3068e+22, 1.0665e+22, 4.2278e+21,
        7.2591e+22, 8.7660e+21, 1.5654e+22, 6.6840e+21, 2.7506e+22, 4.7134e+21,
        6.0888e+22, 1.9913e+22, 9.0867e+21, 1.0086e+22, 3.7149e+21, 1.2725e+22,
        1.3805e+22, 1.4208e+22, 2.6634e+22, 1.8723e+23, 1.7484e+22, 4.0214e+23,
        1.3154e+22, 7.4241e+21, 1.3187e+22, 9.1236e+21, 3.2866e+21, 9.6910e+22,
        4.2254e+22, 1.2040e+22, 1.6609e+22, 7.4174e+21, 1.7330e+23, 1.0837e+22,
        5.2942e+22, 1.4248e+22, 2.0637e+22, 3.8046e+23, 4.7712e+22, 2.3173e+22,
        1.4400e+23, 8.0860e+21, 1.8205e+21, 3.9843e+22, 1.1481e+22, 1.2032e+22,
        1.0806e+22, 2.7413e+22, 3.7376e+22, 6.6182e+22, 1.4946e+22, 6.0470e+21,
        1.3059e+22, 2.0523e+22, 2.6308e+21, 8.6282e+22, 1.8589e+22, 7.2420e+22,
        9.7867e+21, 1.1669e+22, 6.7085e+21, 6.8970e+22, 5.6907e+21, 1.1082e+22,
        2.6167e+22, 7.4995e+21, 1.3675e+22, 1.4453e+22, 3.6144e+21, 1.5601e+22,
        8.1158e+21, 2.8876e+22, 4.4221e+22, 1.1319e+22, 1.0859e+22, 9.9475e+21,
        4.0248e+21, 1.0796e+22, 1.2816e+22, 3.4108e+22, 7.7300e+22, 7.7132e+21,
        2.7183e+22, 8.4666e+21, 8.0929e+21, 2.1862e+23, 3.6337e+21, 2.7922e+23,
        5.6933e+21, 3.2783e+21, 6.1491e+21, 8.1081e+21, 5.4645e+21, 8.0593e+21,
        1.1417e+22, 1.1356e+22, 7.0084e+21, 1.4062e+22, 3.0426e+22, 2.1707e+21,
        1.5271e+21, 2.2626e+22, 4.3979e+21, 2.2295e+22, 1.4523e+22, 1.6501e+22,
        2.3023e+22, 8.6212e+21, 1.2572e+22, 1.3863e+22, 1.6136e+22, 7.8669e+21,
        3.3878e+22, 7.3634e+21, 1.1521e+22, 1.8198e+22, 2.1489e+22, 1.7372e+22,
        1.4654e+22, 6.7277e+21, 5.5517e+21, 1.6377e+22, 1.0513e+22, 1.1664e+22,
        6.8899e+21, 8.2525e+21, 1.4899e+22, 1.4884e+23, 9.1697e+21, 2.6573e+24,
        8.5627e+21, 1.3093e+22, 1.2107e+22, 7.2043e+22, 6.4739e+21, 1.6398e+22,
        1.7580e+24, 2.8640e+22, 2.9421e+21, 4.0170e+22, 6.9292e+21, 1.2063e+22,
        6.9363e+21, 3.2305e+22, 2.2997e+22, 1.7763e+22, 4.7516e+21, 8.8863e+21,
        7.5783e+21, 2.2217e+22, 4.0494e+22, 8.5121e+22, 3.5092e+21, 2.4167e+22,
        7.2025e+21, 1.7156e+22, 1.6178e+23, 7.9688e+22, 7.5157e+21, 1.2728e+22,
        9.8738e+22, 1.2181e+22, 1.2553e+22, 1.3239e+22, 2.6927e+22, 8.3734e+21,
        5.0953e+22, 1.0172e+22, 2.6263e+22, 7.1024e+21, 1.2936e+22, 8.3858e+21,
        2.1050e+22, 5.7125e+21, 3.8388e+21, 1.5443e+22, 1.0745e+23, 1.5299e+22,
        1.3462e+22, 1.4595e+22, 1.9716e+22, 6.9871e+21, 8.1988e+21, 9.0189e+22,
        1.6326e+22, 3.6435e+21, 8.0897e+22, 5.9801e+23, 8.2779e+21, 1.3173e+22,
        3.0806e+21, 8.0677e+21, 9.7894e+21, 1.0207e+22, 1.6990e+23, 3.2552e+22,
        1.1522e+22, 9.4759e+21])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.6673e+22, 2.5504e+22, 1.2540e+21, 4.7829e+22, 1.2391e+23, 1.0418e+21,
        3.6228e+21, 3.0047e+21, 1.4425e+22, 9.7610e+22, 2.9248e+22, 9.4425e+21,
        1.4823e+21, 3.2777e+22, 3.0252e+22, 1.1862e+21, 2.4265e+21, 1.2096e+22,
        2.1824e+21, 1.6581e+22, 9.2207e+22, 5.0028e+21, 7.4605e+21, 3.2598e+22,
        5.0065e+22, 1.6031e+22, 2.4549e+21, 3.8614e+21, 1.4566e+22, 6.2502e+21,
        3.1577e+23, 1.5076e+22, 5.8464e+21, 3.0270e+22, 2.3041e+22, 1.5787e+21,
        3.4549e+21, 1.0206e+23, 9.1041e+21, 1.1561e+22, 5.8997e+21, 2.0586e+21,
        7.5047e+21, 2.7042e+21, 1.4042e+22, 3.2142e+22, 6.4960e+21, 4.8036e+21,
        1.0996e+21, 4.3019e+22, 4.7422e+21, 7.6032e+22, 7.7492e+21, 4.2501e+21,
        7.6056e+22, 1.1915e+21, 2.7669e+23, 1.8296e+23, 5.0292e+21, 4.8953e+21,
        7.4249e+22, 1.3311e+22, 5.1692e+21, 3.8322e+21, 3.6598e+21, 1.7135e+21,
        1.4659e+22, 8.1303e+21, 6.1528e+21, 3.8689e+21, 4.6531e+21, 2.1921e+21,
        3.5221e+21, 1.2551e+22, 9.7568e+22, 6.0058e+21, 6.3946e+22, 5.7151e+21,
        8.6452e+21, 1.2875e+22, 4.4068e+21, 1.7755e+21, 3.4756e+21, 2.8801e+21,
        9.2833e+21, 2.5964e+22, 8.0238e+22, 3.8794e+22, 3.3669e+22, 6.4586e+21,
        2.8779e+22, 6.3409e+21, 2.2135e+21, 3.0458e+21, 2.7912e+21, 1.0576e+23,
        1.0431e+22, 4.1170e+21, 4.8998e+20, 1.8076e+22, 8.3822e+21, 4.4611e+22,
        7.4787e+21, 4.1952e+21, 6.2128e+21, 8.1816e+21, 3.9629e+21, 1.4879e+22,
        4.5218e+22, 1.4307e+21, 2.1493e+21, 8.8521e+20, 1.9009e+21, 1.3695e+21,
        5.1546e+21, 2.6631e+22, 1.5391e+21, 1.1615e+21, 4.5583e+22, 1.4549e+22,
        4.1401e+23, 2.8992e+21, 9.2601e+21, 4.0300e+21, 1.0521e+22, 1.8473e+21,
        3.0155e+21, 1.6035e+22, 2.2573e+22, 8.7126e+21, 4.4035e+21, 5.4727e+22,
        1.5708e+22, 1.2668e+21, 1.0495e+22, 1.5156e+22, 1.0414e+22, 1.2482e+22,
        1.2779e+21, 3.6750e+21, 4.4755e+21, 5.7703e+22, 3.7628e+22, 5.2903e+21,
        6.6951e+21, 2.6384e+21, 2.4337e+21, 3.3101e+22, 7.5653e+21, 4.4351e+22,
        2.0263e+23, 1.0661e+22, 1.8108e+21, 2.1360e+23, 1.0074e+22, 1.3386e+22,
        3.9878e+21, 7.7160e+22, 1.1893e+21, 1.5733e+22, 6.8449e+21, 8.4539e+21,
        1.2481e+22, 3.7656e+22, 3.2812e+21, 6.1866e+22, 3.5749e+20, 1.5049e+22,
        2.1296e+22, 1.6086e+21, 1.6755e+21, 5.1805e+22, 2.4808e+21, 1.1644e+22,
        1.1813e+22, 1.3728e+21, 1.8189e+22, 3.1709e+21, 4.6342e+21, 2.8940e+22,
        3.0578e+21, 1.9357e+21, 1.9482e+22, 1.8631e+22, 1.1070e+22, 1.3363e+22,
        9.0056e+21, 3.5042e+21, 9.8793e+21, 4.7949e+21, 3.3529e+22, 4.8104e+21,
        3.4699e+22, 7.2357e+21, 3.8489e+21, 1.1085e+22, 2.6619e+21, 8.1983e+21,
        4.1735e+21, 2.3737e+22, 3.6105e+22, 2.9850e+21, 1.5562e+22, 3.4869e+22,
        8.7354e+22, 1.4888e+22, 1.7171e+21, 3.0227e+20, 9.4564e+20, 5.9366e+21,
        5.0979e+21, 9.7028e+21, 9.5715e+21, 9.3100e+24, 4.4776e+22, 1.2002e+22,
        3.3117e+21, 1.4355e+22, 3.8881e+22, 1.7445e+22, 3.2733e+23, 1.8196e+21,
        5.9265e+21, 1.8336e+22, 5.7303e+21, 1.8858e+22, 1.0305e+22, 2.6220e+22,
        3.5427e+22, 5.5832e+22, 2.4164e+22, 2.2314e+21, 2.0810e+22, 1.0972e+21,
        3.2701e+21, 8.9428e+21, 3.6108e+21, 1.2270e+22, 3.5014e+23, 1.0859e+22,
        6.9072e+21, 3.4140e+22, 8.7793e+21, 4.8796e+21, 1.4458e+21, 4.8962e+21,
        7.3875e+21, 6.7087e+21, 7.8998e+21, 9.3832e+21, 3.3230e+23, 1.6807e+22,
        2.8020e+23, 4.9657e+21, 1.0506e+21, 6.0925e+21, 6.5646e+21, 2.1267e+22,
        1.4983e+22, 5.3552e+22, 2.9854e+21, 1.3749e+22, 3.5567e+21, 9.3402e+21,
        1.7722e+22, 1.7065e+21, 8.5484e+21, 8.0618e+21, 5.1313e+21, 4.4364e+21,
        2.8243e+21, 7.0079e+21, 5.6529e+21, 6.1679e+21, 3.2158e+22, 1.6623e+22,
        1.0828e+22, 2.0743e+21, 5.4096e+21, 3.2799e+22, 2.7137e+21, 6.3743e+22,
        2.7378e+22, 3.1474e+21, 9.6112e+23, 1.3811e+22, 4.4342e+21, 4.2021e+22,
        8.7726e+21, 2.8373e+21, 1.2331e+22, 2.3086e+21, 3.1199e+21, 9.6710e+21,
        4.4371e+21, 4.3519e+22, 6.2436e+21, 1.0671e+22, 1.8538e+22, 1.0718e+22,
        5.5358e+21, 2.0982e+21, 4.5576e+21, 1.7224e+21, 1.0480e+22, 7.5229e+21,
        3.1684e+21, 2.3685e+21, 1.2119e+22, 2.5319e+22, 5.5393e+21, 8.1246e+21,
        2.7444e+21, 1.7367e+21, 7.5628e+20, 5.6355e+21, 7.2964e+21, 3.9889e+21,
        7.8829e+22, 5.4416e+21, 1.4186e+21, 4.5883e+21, 1.4761e+22, 7.3676e+20,
        4.9197e+22, 7.1542e+21, 1.3443e+21, 4.6207e+21, 8.2866e+21, 1.0118e+22,
        6.2282e+21, 7.8691e+21, 1.3246e+22, 1.7632e+23, 3.8990e+21, 3.9445e+23,
        6.7054e+20, 1.9633e+21, 3.8391e+21, 1.1153e+21, 2.8684e+21, 1.0123e+23,
        5.1976e+22, 2.2575e+21, 1.2011e+22, 7.0201e+21, 1.8971e+23, 2.9218e+21,
        5.4355e+22, 4.7599e+21, 6.7658e+21, 3.8268e+23, 3.1775e+22, 7.8480e+21,
        1.5316e+23, 8.7184e+21, 5.8545e+21, 1.7916e+22, 1.4762e+21, 1.8337e+21,
        6.3738e+21, 1.4620e+22, 4.4309e+22, 6.4245e+22, 8.2621e+21, 5.4588e+21,
        3.0952e+21, 8.3668e+21, 1.6464e+22, 9.3190e+22, 2.2381e+22, 7.0705e+22,
        3.9683e+21, 7.6369e+20, 6.1910e+21, 6.4292e+22, 1.1023e+22, 5.9495e+21,
        7.1851e+21, 1.5967e+22, 2.9859e+21, 3.8311e+21, 8.3350e+21, 5.4781e+21,
        1.5488e+21, 1.6160e+22, 6.3248e+22, 2.1882e+22, 1.1098e+21, 2.5923e+21,
        4.4089e+20, 2.2089e+22, 2.5091e+21, 1.2062e+22, 8.2978e+22, 4.3230e+21,
        3.4071e+22, 1.7425e+22, 3.0782e+21, 2.2108e+23, 7.2814e+21, 2.7094e+23,
        7.9470e+21, 5.5771e+21, 1.4217e+22, 1.7186e+22, 2.2393e+22, 1.2929e+22,
        4.4718e+21, 5.4561e+21, 1.4463e+21, 7.1014e+21, 3.0080e+22, 5.3739e+21,
        1.4664e+22, 1.2329e+22, 6.6457e+21, 6.5224e+21, 1.3442e+21, 6.4680e+21,
        1.5094e+22, 3.5022e+21, 1.2141e+21, 3.0092e+21, 1.1918e+22, 6.1900e+21,
        8.7726e+21, 3.0093e+20, 1.2316e+21, 1.2173e+22, 1.6015e+22, 6.9186e+20,
        7.2100e+20, 7.8381e+20, 5.0879e+21, 5.5166e+21, 8.2412e+21, 6.7965e+21,
        9.7853e+21, 4.7547e+21, 2.6466e+22, 1.3908e+23, 9.0844e+21, 2.6500e+24,
        3.2481e+21, 2.2501e+22, 3.5592e+21, 6.5854e+22, 3.6523e+21, 1.0905e+21,
        1.7530e+24, 1.4962e+22, 2.4093e+21, 3.5900e+22, 2.7584e+21, 3.3998e+21,
        2.1284e+21, 3.0993e+21, 2.2106e+22, 4.2459e+21, 2.2602e+21, 2.3469e+21,
        9.3790e+21, 1.1603e+22, 3.7281e+22, 8.3458e+22, 1.3808e+22, 1.1769e+22,
        5.8067e+21, 8.0858e+21, 1.5742e+23, 9.5618e+22, 5.4002e+21, 5.9260e+21,
        9.0242e+22, 6.8746e+21, 4.3328e+21, 4.3185e+20, 2.1728e+22, 1.0941e+21,
        5.1671e+22, 4.0132e+21, 2.0447e+21, 9.2616e+21, 2.0640e+21, 1.1977e+22,
        1.4182e+22, 3.6534e+21, 1.3165e+22, 2.8349e+22, 1.1094e+23, 7.3288e+21,
        1.5403e+21, 7.6798e+21, 1.4803e+22, 6.0934e+21, 2.3659e+21, 9.1741e+22,
        3.1803e+22, 2.0315e+22, 6.9940e+22, 6.0262e+23, 3.7997e+21, 8.4404e+21,
        7.1134e+20, 2.1082e+22, 3.1045e+21, 2.2742e+22, 1.6601e+23, 2.3692e+22,
        6.3002e+21, 3.3086e+21])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.3175e+22, 2.0638e+22, 1.4608e+22, 1.9784e+22, 1.1698e+22, 1.5656e+22,
        1.5515e+22, 3.6519e+22, 3.3452e+22, 3.1601e+22, 2.3332e+22, 2.3584e+22,
        1.6846e+22, 2.6018e+22, 1.2034e+22, 2.4195e+22, 2.2177e+22, 2.3345e+22,
        1.7952e+22, 1.3642e+22, 2.1716e+22, 3.9591e+22, 2.4439e+22, 1.1678e+22,
        1.3387e+22, 1.7697e+22, 1.6059e+22, 1.6407e+22, 2.0254e+22, 1.9875e+22,
        1.9139e+22, 1.1511e+22, 1.0532e+22, 2.7998e+22, 1.2107e+22, 1.6941e+22,
        1.6522e+22, 2.2144e+22, 2.4231e+22, 2.2237e+22, 2.8587e+22, 3.1787e+22,
        1.6765e+22, 1.3279e+22, 1.4701e+22, 8.0541e+21, 1.2900e+22, 2.3498e+22,
        1.9996e+22, 1.3624e+22, 1.2215e+22, 2.1501e+22, 2.3430e+22, 2.4723e+22,
        1.7967e+22, 2.0063e+22, 1.2015e+22, 1.8484e+22, 1.7230e+22, 2.4358e+22,
        2.1888e+22, 2.8654e+22, 3.0656e+22, 1.3753e+22, 1.4013e+22, 3.2181e+22,
        2.4024e+22, 1.4914e+22, 1.9022e+22, 1.2086e+22, 1.0846e+22, 1.2505e+22,
        1.3596e+22, 1.4814e+22, 1.7655e+22, 1.5473e+22, 1.5636e+22, 2.8549e+22,
        2.1417e+22, 2.2595e+22, 3.1242e+22, 3.4134e+22, 2.0439e+22, 2.3198e+22,
        1.4953e+22, 1.8437e+22, 2.6308e+22, 3.6578e+22, 1.2707e+22, 2.0370e+22,
        1.4582e+22, 1.5508e+22, 1.1498e+22, 2.9633e+22, 2.4027e+22, 1.4955e+22,
        3.0000e+22, 1.3823e+22, 2.2536e+22, 1.3091e+22, 1.0838e+22, 1.1598e+22,
        1.8787e+22, 2.3697e+22, 2.0205e+22, 2.4372e+22, 1.2993e+22, 2.0405e+22,
        2.1067e+22, 1.8907e+22, 1.8490e+22, 2.1843e+22, 2.5335e+22, 2.2371e+22,
        1.5409e+22, 2.3965e+22, 2.3904e+22, 2.4526e+22, 1.7324e+22, 1.6531e+22,
        1.6918e+22, 2.3492e+22, 1.3004e+22, 2.4691e+22, 2.6642e+22, 2.0117e+22,
        1.7871e+22, 2.0574e+22])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.9105e+22, 2.0186e+22, 1.3662e+22, 1.4603e+22, 2.3236e+22, 2.2943e+22,
        1.3575e+22, 1.1083e+22, 1.0119e+22, 1.6525e+22, 1.0959e+22, 1.2429e+22,
        1.8371e+22, 2.2568e+22, 1.8640e+22, 9.2813e+21, 3.0208e+22, 2.6458e+22,
        1.6896e+22, 1.2083e+22, 7.2788e+21, 1.9914e+22, 1.7455e+22, 3.2006e+22,
        9.0134e+21, 1.3862e+22, 2.9159e+22, 2.7563e+22, 2.3399e+22, 1.5500e+22,
        1.3013e+22, 2.0730e+22, 1.4873e+22, 2.6351e+22, 1.6082e+22, 3.7437e+22,
        1.4054e+22, 2.2336e+22, 2.2022e+22, 1.5707e+22, 1.4219e+22, 3.8952e+22,
        2.4053e+22, 3.1353e+22, 1.1538e+22, 9.5095e+21, 2.0770e+22, 3.0119e+22,
        1.4480e+22, 1.3585e+22, 1.9942e+22, 3.3644e+22, 2.5641e+22, 1.4872e+22,
        1.9591e+22, 2.0104e+22, 1.2049e+22, 1.3992e+22, 1.7542e+22, 2.1093e+22,
        1.5373e+22, 2.3433e+22, 1.1280e+22, 2.0075e+22, 2.3598e+22, 2.0065e+22,
        2.3366e+22, 1.0835e+22, 1.2551e+22, 1.9707e+22, 1.0357e+22, 1.9643e+22,
        1.6712e+22, 1.5180e+22, 1.9513e+22, 1.5057e+22, 1.2391e+22, 1.3195e+22,
        1.0374e+22, 8.6663e+21, 2.3861e+22, 2.0225e+22, 1.2279e+22, 2.4183e+22,
        1.9927e+22, 1.1677e+22, 1.5460e+22, 2.0535e+22, 1.1622e+22, 7.8447e+21,
        1.4999e+22, 1.1855e+22, 2.0985e+22, 1.9432e+22, 1.9801e+22, 2.3840e+22,
        2.0035e+22, 1.5245e+22, 1.1125e+22, 1.5643e+22, 1.5001e+22, 1.6705e+22,
        1.1557e+22, 1.7426e+22, 2.8133e+22, 1.2536e+22, 1.8946e+22, 1.4064e+22,
        1.7160e+22, 1.5877e+22, 1.1257e+22, 2.9017e+22, 1.0282e+22, 2.7625e+22,
        2.0468e+22, 1.8193e+22, 1.1213e+22, 1.2555e+22, 1.5025e+22, 1.6074e+22,
        1.4331e+22, 1.2969e+22, 2.1684e+22, 1.5885e+22, 1.7280e+22, 3.0985e+22,
        1.7933e+22, 2.4830e+22])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.7704e+19, 9.3435e+19, 7.0198e+19, 1.0900e+20, 7.5968e+19, 2.6347e+19,
        7.8854e+19, 5.6757e+19, 4.0434e+19, 1.8517e+20, 6.7137e+19, 8.1879e+19,
        6.2286e+19, 1.3432e+20, 1.4846e+20, 7.3564e+19, 5.9973e+19, 7.0867e+19,
        4.8976e+19, 7.8172e+19, 1.2526e+20, 7.2357e+19, 5.9566e+19, 7.8505e+19,
        1.5816e+20, 6.3758e+19, 4.8460e+19, 4.0192e+20, 8.8933e+19, 2.8998e+19,
        4.5166e+19, 1.9422e+20, 6.1250e+19, 1.4982e+20, 3.3130e+20, 1.1814e+20,
        1.1564e+20, 4.2278e+19, 3.0611e+19, 1.0419e+20, 9.7528e+19, 5.8394e+19,
        6.0747e+19, 6.7198e+19, 1.6308e+20, 7.2591e+20, 3.4001e+19, 7.0667e+19,
        5.2829e+19, 5.7208e+19, 1.2337e+21, 8.7138e+19, 4.1501e+19, 3.8977e+19,
        4.7470e+19, 5.8100e+19, 1.5040e+20, 4.3137e+20, 1.2463e+20, 1.0734e+19,
        1.6868e+20, 5.7054e+19, 4.6304e+21, 5.5242e+19, 1.1506e+20, 1.2625e+20,
        1.0778e+20, 9.9493e+19, 5.4337e+19, 7.0267e+19, 1.7629e+19, 1.1612e+20,
        1.0813e+20, 1.0266e+20, 9.0817e+19, 3.7806e+20, 6.9690e+19, 4.7913e+19,
        2.8788e+19, 6.0838e+19, 8.2052e+19, 1.2734e+20, 1.0623e+20, 1.1991e+20,
        5.0542e+19, 9.8867e+19, 5.8506e+19, 7.8871e+19, 6.0586e+19, 4.3123e+19,
        3.3045e+22, 4.8167e+19, 2.1211e+20, 2.9825e+19, 7.1018e+19, 9.0005e+19,
        1.1289e+20, 1.1429e+21, 1.0215e+20, 1.1234e+20, 6.4286e+19, 1.2896e+20,
        1.0156e+20, 3.8332e+18, 1.8951e+19, 1.1322e+20, 6.7806e+19, 1.0340e+20,
        7.2709e+19, 6.5392e+19, 9.0865e+19, 1.0671e+20, 6.3910e+19, 1.0929e+20,
        4.9227e+20, 8.1942e+19, 9.9157e+19, 4.9026e+19, 6.5241e+19, 5.4614e+19,
        6.2421e+19, 9.2848e+19, 6.8370e+19, 1.9336e+20, 3.9708e+19, 3.2609e+20,
        1.8880e+20, 6.8344e+19, 6.7657e+19, 5.4136e+19, 1.4098e+20, 1.6386e+20,
        1.0003e+20, 5.8816e+19, 1.3998e+20, 1.0924e+20, 3.5876e+19, 7.3625e+19,
        5.7001e+19, 7.5282e+19, 7.7311e+19, 9.4323e+19, 1.8607e+20, 5.7058e+19,
        3.3357e+19, 1.7452e+20, 5.2310e+19, 5.6840e+19, 6.8786e+19, 2.7531e+19,
        5.4464e+19, 7.5619e+19, 4.5963e+19, 9.3863e+19, 1.1471e+20, 3.2942e+19,
        1.6333e+20, 5.4345e+19, 8.4309e+19, 9.7810e+19, 6.0792e+19, 1.0362e+20,
        4.3921e+19, 8.6143e+19, 1.0041e+20, 2.8544e+19, 7.0409e+19, 5.5350e+19,
        8.6573e+19, 3.9485e+19, 5.0044e+19, 1.2061e+20, 1.6577e+20, 2.3272e+19,
        1.1655e+20, 3.1748e+19, 1.2827e+20, 9.9525e+19, 1.6568e+19, 1.3082e+20,
        2.2952e+20, 7.1591e+19, 7.4453e+19, 1.0594e+20, 1.0493e+20, 1.7725e+20,
        1.0797e+20, 3.3935e+19, 6.2204e+19, 1.2427e+20, 7.7553e+19, 9.6990e+19,
        3.3080e+20, 7.9251e+19, 5.5702e+19, 5.7612e+19, 3.4960e+19, 1.1680e+19,
        8.2264e+19, 3.9290e+19, 4.3753e+19, 1.2804e+20, 5.6014e+19, 1.0186e+20,
        6.8151e+19, 6.6505e+19, 3.6219e+19, 6.4799e+19, 5.0911e+19, 9.8584e+19,
        8.3276e+19, 7.7565e+19, 2.1784e+19, 6.4421e+19, 1.2282e+20, 4.4695e+19,
        1.1726e+20, 1.1301e+20, 1.5405e+20, 4.7971e+19, 1.1874e+20, 2.1356e+20,
        4.3607e+19, 2.1020e+19, 7.1620e+19, 1.1594e+22, 1.8299e+20, 2.8414e+19,
        8.3953e+19, 4.8627e+19, 5.2900e+19, 5.2062e+19, 3.8952e+19, 1.0253e+20,
        7.9242e+19, 8.9871e+19, 1.0145e+20, 3.1865e+21, 3.0684e+19, 8.5466e+19,
        4.7840e+20, 3.9604e+19, 3.0466e+20, 1.2282e+20, 3.0189e+19, 1.3924e+20,
        7.3525e+19, 1.7372e+20, 7.1704e+20, 6.2718e+19, 9.3897e+19, 6.9093e+19,
        6.0769e+19, 5.5547e+19, 2.8229e+19, 1.4830e+20, 8.5870e+19, 1.9369e+20,
        3.0824e+21, 9.9140e+19, 1.9489e+19, 4.1511e+19, 3.3279e+19, 6.0469e+20,
        6.5952e+19, 7.2544e+18, 7.1658e+19, 4.3612e+19, 1.7075e+19, 7.8620e+19,
        1.1973e+20, 8.8087e+19, 8.3242e+19, 8.5886e+19, 3.3339e+20, 4.3823e+19,
        7.6073e+19, 2.2577e+19, 3.1351e+19, 1.1860e+20, 9.1486e+19, 1.0324e+20,
        5.9220e+19, 7.8140e+19, 8.5567e+19, 1.0356e+20, 3.3706e+20, 3.0550e+19,
        2.7851e+19, 6.3488e+19, 8.4769e+19, 6.5402e+19, 7.3733e+19, 2.9997e+19,
        3.0585e+20, 9.4769e+19, 6.2381e+19, 1.0679e+20, 2.6357e+19, 2.2059e+19,
        6.6831e+19, 9.3142e+19, 6.5558e+19, 8.8099e+19, 7.8751e+19, 9.5422e+19,
        3.7165e+19, 5.4800e+18, 1.2449e+20, 5.8717e+19, 2.5433e+21, 1.9108e+20,
        3.4136e+19, 3.2587e+20, 6.3650e+19, 1.0293e+20, 2.7755e+19, 2.2716e+19,
        8.0078e+19, 8.9413e+19, 2.9755e+20, 1.5920e+19, 9.2991e+19, 6.1629e+19,
        3.7691e+19, 6.5457e+19, 7.0596e+19, 4.2093e+20, 7.3602e+19, 9.6448e+19,
        1.8833e+20, 3.3891e+19, 2.8971e+19, 1.3341e+20, 4.5872e+19, 1.1393e+20,
        5.3255e+19, 2.3346e+20, 4.9104e+19, 3.7256e+19, 1.4653e+19, 1.0919e+20,
        9.6671e+19, 4.7097e+19, 4.9473e+19, 7.6124e+19, 5.6974e+19, 9.7373e+20,
        8.2446e+19, 7.7300e+19, 8.9550e+19, 5.3430e+19, 5.9681e+19, 7.9953e+19,
        1.0183e+20, 8.0603e+19, 2.8708e+19, 5.5901e+19, 2.6886e+19, 1.0312e+20,
        8.9819e+19, 3.0153e+19, 4.2314e+19, 1.0343e+20, 3.0715e+21, 8.0693e+19,
        6.8174e+19, 4.7960e+19, 4.9133e+19, 4.4386e+19, 2.5732e+19, 1.0131e+20,
        4.1061e+19, 9.5492e+19, 5.1383e+19, 1.3496e+20, 7.9099e+19, 1.8510e+20,
        5.8360e+19, 1.0515e+20, 5.7907e+19, 5.8403e+19, 5.3120e+19, 8.5773e+19,
        3.6065e+19, 2.1603e+19, 6.7091e+19, 5.5271e+19, 2.3071e+20, 6.9929e+19,
        1.6112e+20, 8.2089e+19, 3.3062e+19, 3.8520e+21, 5.9542e+19, 9.7853e+19,
        1.0058e+20, 7.3664e+19, 7.6550e+19, 4.8335e+19, 5.1261e+19, 4.6101e+19,
        5.0919e+19, 8.5599e+19, 3.1956e+19, 5.4806e+19, 6.4023e+19, 1.4484e+20,
        3.5045e+20, 8.0604e+19, 1.2491e+20, 9.7539e+20, 7.0778e+19, 1.8353e+20,
        6.5941e+20, 6.8794e+19, 1.0692e+20, 4.3762e+19, 1.2747e+20, 4.8493e+19,
        1.1940e+20, 1.5671e+20, 1.5058e+20, 8.1951e+19, 9.8593e+19, 4.6806e+21,
        6.1172e+19, 4.8728e+19, 8.9402e+19, 1.5538e+20, 1.2351e+20, 8.7213e+19,
        3.0013e+19, 6.1293e+19, 2.0881e+19, 1.9065e+20, 6.7408e+19, 9.2890e+19,
        1.2953e+20, 2.5506e+19, 5.2954e+19, 5.6849e+19, 1.1036e+20, 6.8291e+19,
        5.6154e+19, 1.1729e+20, 5.9780e+19, 2.0670e+19, 1.4515e+20, 1.1720e+22,
        4.4055e+19, 9.3059e+19, 6.8080e+19, 5.8990e+19, 4.4046e+19, 5.8596e+19,
        1.1717e+20, 5.8822e+19, 4.6454e+19, 4.3764e+19, 1.3473e+20, 1.0258e+20,
        3.9387e+19, 5.8819e+19, 9.0087e+19, 9.0751e+20, 1.4499e+20, 8.6450e+19,
        1.5259e+20, 5.4859e+19, 2.0913e+20, 2.3821e+20, 8.1389e+19, 3.2434e+19,
        4.4308e+19, 4.7025e+19, 7.2587e+19, 1.0396e+20, 1.1480e+20, 1.3456e+20,
        5.7624e+19, 5.3831e+19, 3.8220e+19, 4.9045e+19, 6.5377e+19, 4.4776e+19,
        8.0417e+19, 5.2240e+19, 4.2557e+19, 4.9012e+19, 4.5660e+19, 9.1636e+19,
        1.6062e+19, 7.7298e+19, 2.5136e+20, 3.8631e+19, 1.1338e+20, 6.2457e+19,
        9.1321e+19, 5.0725e+19, 9.2872e+19, 6.7234e+19, 5.5285e+19, 9.3762e+19,
        6.2109e+19, 8.8655e+19, 8.3769e+19, 1.2373e+20, 5.5301e+19, 8.3143e+19,
        1.5634e+20, 4.4881e+19])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1296e+20, 1.8395e+20, 2.9243e+20, 7.8702e+19, 1.0033e+20, 1.7689e+20,
        2.4821e+20, 9.7641e+19, 2.1046e+20, 1.1434e+20, 1.8167e+20, 1.2463e+20,
        1.3353e+20, 8.8180e+19, 1.3135e+20, 1.0655e+20, 2.3173e+20, 2.7865e+20,
        1.8700e+20, 7.6197e+19, 2.0573e+20, 1.3855e+20, 1.4181e+20, 1.6262e+20,
        7.8474e+19, 1.1650e+20, 1.2280e+20, 1.4250e+20, 1.5799e+20, 8.4250e+19,
        1.6645e+20, 2.1237e+20, 2.4050e+20, 1.5583e+20, 6.7698e+19, 1.8879e+20,
        8.6147e+19, 1.9587e+20, 1.4889e+20, 1.5471e+20, 1.5486e+20, 1.2960e+20,
        1.6007e+20, 1.9458e+20, 2.3747e+20, 1.4245e+20, 1.9652e+20, 9.0539e+19,
        2.0965e+20, 1.8520e+20, 2.3026e+20, 6.1189e+19, 9.4631e+19, 2.3015e+20,
        1.4689e+20, 3.8363e+19, 8.0277e+19, 1.0826e+20, 1.1977e+20, 1.4062e+20,
        1.0154e+20, 1.3311e+20, 1.5681e+20, 7.4783e+19, 1.6383e+20, 1.2289e+20,
        3.8607e+19, 1.0136e+20, 1.6660e+20, 2.9127e+20, 5.2286e+19, 1.8097e+20,
        1.9281e+20, 4.8293e+19, 1.6219e+20, 2.2864e+20, 6.0166e+19, 1.5915e+20,
        2.1283e+20, 1.1491e+20, 2.4157e+20, 1.2037e+20, 1.2265e+20, 1.0530e+20,
        1.3215e+20, 1.7789e+20, 8.8156e+19, 1.0013e+20, 1.6305e+20, 2.1954e+20,
        2.6638e+20, 1.1510e+20, 9.6914e+19, 1.5334e+20, 1.4157e+20, 8.6637e+19,
        1.0294e+20, 8.4564e+19, 5.7160e+19, 1.3345e+20, 8.6610e+19, 1.4846e+20,
        1.4041e+20, 1.2491e+20, 9.2801e+19, 9.6420e+19, 2.3093e+20, 1.0288e+20,
        1.1211e+20, 1.3867e+20, 1.3407e+20, 1.7173e+20, 2.2121e+20, 1.3524e+20,
        1.3997e+20, 1.7599e+20, 9.7588e+19, 1.0842e+20, 2.5620e+20, 2.4164e+20,
        1.7248e+20, 1.7320e+20, 1.0540e+20, 2.0760e+20, 1.3445e+20, 1.9014e+20,
        6.4913e+19, 1.0640e+20])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0749e+20, 9.5500e+19, 9.2472e+19, 9.5648e+19, 9.0474e+19, 2.1124e+20,
        1.2036e+20, 2.1137e+20, 7.3095e+19, 8.4708e+19, 1.4155e+20, 1.4660e+20,
        1.1433e+20, 1.0910e+20, 2.1920e+20, 2.6725e+20, 1.8363e+20, 1.7358e+20,
        9.8776e+19, 1.7924e+20, 1.1753e+20, 1.0599e+20, 1.0662e+20, 6.8236e+19,
        1.2110e+20, 1.0652e+20, 1.1967e+20, 9.6454e+19, 7.1071e+19, 7.7451e+19,
        1.7026e+20, 6.1666e+19, 1.9502e+20, 7.8947e+19, 1.3912e+20, 1.0047e+20,
        9.6358e+19, 1.2748e+20, 2.1266e+20, 2.2018e+20, 1.1536e+20, 8.9947e+19,
        1.3767e+20, 9.2301e+19, 1.1187e+20, 2.3170e+20, 1.3760e+20, 7.8688e+19,
        8.8773e+19, 1.1374e+20, 2.7204e+20, 9.0443e+19, 1.2448e+20, 1.1071e+20,
        1.0435e+20, 1.7004e+20, 2.2359e+20, 1.3878e+20, 8.3787e+19, 1.5593e+20,
        1.9296e+20, 8.4782e+19, 8.1131e+19, 2.6018e+20, 1.0939e+20, 1.2021e+20,
        1.2252e+20, 1.1469e+20, 1.7441e+20, 1.0468e+20, 2.2312e+20, 1.4025e+20,
        1.0157e+20, 5.4554e+19, 1.6942e+20, 8.8295e+19, 8.8769e+19, 7.2900e+19,
        5.7785e+19, 4.7755e+19, 7.2197e+19, 1.2202e+20, 7.5949e+19, 8.0916e+19,
        9.9555e+19, 8.1968e+19, 1.0780e+20, 1.4925e+20, 2.0389e+20, 1.8245e+20,
        2.7280e+20, 9.5835e+19, 1.6072e+20, 9.8267e+19, 1.4533e+20, 2.8028e+20,
        1.4472e+20, 1.9763e+20, 9.1824e+19, 8.5226e+19, 2.4942e+20, 1.5679e+20,
        6.4372e+19, 1.0063e+20, 5.5519e+19, 1.6303e+20, 1.0585e+20, 1.0273e+20,
        2.1031e+20, 1.8787e+20, 1.1628e+20, 1.1584e+20, 1.7014e+20, 1.1207e+20,
        8.3297e+19, 1.2347e+20, 1.9453e+20, 1.5931e+20, 2.2848e+20, 2.0990e+20,
        1.8585e+20, 1.4215e+20, 8.9012e+19, 1.1413e+20, 1.6728e+20, 7.8622e+19,
        1.0275e+20, 9.5523e+19])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.0717e+17, 8.0034e+17, 1.8527e+18, 3.6498e+18, 1.5522e+18, 3.8050e+18,
        1.5196e+18, 8.8750e+17, 3.8872e+18, 1.7414e+20, 1.6651e+18, 1.2841e+18,
        1.5992e+18, 9.1400e+17, 1.7634e+18, 3.6238e+18, 5.5764e+19, 7.7953e+18,
        2.0139e+18, 3.8499e+18, 2.2860e+18, 1.8516e+18, 2.6956e+19, 2.1774e+18,
        8.6395e+17, 9.7235e+17, 5.3777e+18, 9.2125e+17, 3.9549e+18, 3.4453e+17,
        3.2161e+18, 2.1808e+18, 1.9378e+18, 2.7922e+18, 1.6933e+18, 2.3896e+18,
        1.5454e+18, 3.9050e+17, 3.4240e+19, 2.8831e+18, 3.7626e+18, 1.4555e+18,
        2.0830e+18, 7.1242e+17, 3.1615e+18, 3.7700e+18, 3.3537e+18, 3.9301e+19,
        5.9017e+17, 3.3170e+17, 1.6869e+18, 2.6055e+18, 3.4630e+18, 2.5138e+18,
        8.0956e+18, 7.5380e+17, 2.9971e+18, 2.0511e+18, 2.3320e+18, 2.3103e+18,
        1.6327e+18, 7.6884e+17, 1.4624e+18, 5.0824e+18, 1.3050e+18, 1.2415e+18,
        9.3317e+17, 4.5113e+17, 1.4285e+18, 1.8097e+19, 1.6371e+18, 1.6317e+18,
        2.5981e+18, 1.7013e+18, 2.3283e+18, 1.9058e+18, 3.3757e+18, 8.1336e+17,
        5.8641e+18, 5.2154e+18, 9.7103e+17, 2.1709e+18, 1.5028e+18, 2.0033e+18,
        1.1769e+18, 2.0528e+18, 1.4715e+18, 1.8803e+18, 1.8481e+18, 3.9669e+17,
        5.1181e+18, 3.5014e+18, 8.9568e+17, 4.0351e+18, 4.6565e+18, 5.8197e+17,
        3.1370e+18, 6.1143e+18, 1.3874e+18, 3.7905e+18, 5.3019e+18, 1.3686e+18,
        2.2492e+18, 4.2237e+17, 9.5892e+17, 2.8333e+18, 7.9304e+17, 1.2454e+18,
        1.3736e+18, 1.4413e+18, 4.2349e+18, 5.0079e+18, 2.2080e+18, 1.0225e+19,
        2.9036e+18, 6.9461e+17, 7.7628e+17, 3.2155e+18, 2.7768e+18, 1.0099e+19,
        1.6491e+18, 7.7129e+17, 4.3500e+18, 3.1102e+18, 1.4064e+18, 2.1802e+18,
        1.7669e+18, 2.5389e+18, 1.8044e+18, 8.6454e+17, 1.7709e+18, 1.0250e+18,
        1.8873e+18, 1.7851e+18, 8.0820e+17, 1.8587e+18, 2.9795e+18, 3.1342e+18,
        1.0094e+18, 3.2632e+18, 1.6870e+18, 1.2181e+18, 1.7064e+18, 5.2080e+18,
        1.7494e+18, 7.2026e+17, 4.4868e+18, 2.0679e+18, 1.1664e+18, 9.8526e+17,
        4.6416e+18, 2.5142e+18, 1.5942e+18, 1.5890e+18, 1.6308e+18, 1.9725e+18,
        2.5825e+18, 7.6577e+19, 2.1048e+18, 3.2048e+18, 2.5486e+18, 1.0767e+18,
        2.8592e+18, 2.4555e+18, 4.4385e+18, 1.4598e+18, 1.1824e+18, 2.8008e+18,
        7.7729e+17, 2.2722e+18, 7.8627e+18, 2.3516e+18, 1.0667e+18, 2.0109e+18,
        3.4224e+18, 4.7936e+17, 8.2512e+17, 2.8324e+18, 2.9848e+18, 2.1249e+18,
        1.2001e+18, 2.2517e+18, 1.2937e+18, 1.7420e+18, 1.1989e+18, 2.6697e+18,
        1.0704e+18, 1.2355e+18, 3.0250e+18, 1.5098e+18, 3.1874e+18, 3.5284e+18,
        3.2962e+18, 2.7523e+19, 3.1998e+18, 1.0158e+18, 5.3907e+18, 6.3308e+17,
        3.1232e+18, 6.9956e+17, 1.4186e+18, 2.5080e+18, 1.6455e+18, 1.4109e+18,
        1.8892e+18, 2.7644e+18, 2.8065e+18, 6.0277e+19, 4.5341e+17, 1.8801e+18,
        3.8500e+18, 1.1557e+18, 1.4112e+19, 3.4686e+17, 2.8646e+18, 1.9555e+18,
        1.5342e+18, 1.9854e+18, 1.0297e+18, 5.0894e+18, 6.9211e+17, 1.5252e+18,
        1.3872e+18, 8.4391e+17, 2.0456e+18, 1.8572e+18, 5.3404e+18, 2.4268e+18,
        1.6259e+19, 1.6263e+18, 4.3449e+19, 5.0086e+17, 2.5263e+18, 1.0536e+18,
        1.3125e+18, 5.1103e+18, 8.7381e+17, 1.6911e+18, 3.0461e+18, 5.6660e+18,
        8.2251e+17, 1.7425e+18, 2.8801e+18, 2.4679e+18, 2.3061e+18, 9.4837e+17,
        1.1124e+18, 1.2829e+18, 2.2666e+18, 3.0786e+18, 2.0979e+18, 1.9254e+18,
        1.3458e+18, 1.6741e+18, 7.9648e+17, 1.3585e+18, 2.2622e+18, 1.5695e+18,
        1.8663e+18, 2.3044e+18, 1.2079e+19, 5.0154e+18, 2.5754e+18, 3.2942e+18,
        2.8147e+18, 9.9412e+18, 2.4368e+17, 1.3194e+18, 1.5515e+18, 2.7271e+18,
        4.6790e+18, 2.7723e+18, 3.8664e+18, 1.2083e+18, 1.9914e+18, 2.1045e+18,
        2.0619e+18, 5.8396e+18, 1.7233e+18, 3.8783e+17, 3.5048e+18, 1.2392e+19,
        3.5349e+17, 3.3275e+18, 1.1339e+18, 1.1346e+18, 1.7959e+18, 6.4092e+17,
        1.0386e+17, 9.9164e+17, 4.7779e+19, 2.5327e+18, 1.1273e+18, 7.3800e+18,
        1.1761e+18, 2.9820e+18, 1.8053e+18, 2.2553e+18, 4.5304e+18, 3.3031e+18,
        2.1639e+18, 1.5600e+18, 1.1717e+18, 1.6175e+18, 1.9641e+18, 1.2072e+18,
        2.0464e+18, 3.4590e+18, 1.3921e+18, 2.7045e+18, 4.9779e+18, 1.5888e+19,
        1.1433e+18, 3.2010e+18, 4.6359e+17, 1.0678e+19, 4.0124e+17, 2.9845e+18,
        1.7575e+18, 1.5065e+18, 5.2449e+19, 1.0295e+18, 1.1285e+18, 1.6519e+19,
        2.7868e+18, 1.4622e+18, 2.9467e+18, 2.8916e+18, 2.1790e+18, 1.7497e+19,
        2.9454e+18, 1.0505e+18, 6.9583e+17, 2.9458e+18, 8.4489e+17, 1.1822e+18,
        2.0908e+18, 1.6546e+18, 1.3646e+18, 3.6863e+18, 4.0496e+18, 1.6558e+18,
        3.2607e+18, 6.2814e+17, 1.3247e+18, 1.0121e+18, 1.2184e+18, 1.9855e+18,
        3.7562e+18, 1.5752e+18, 2.4219e+18, 2.5536e+18, 5.1372e+18, 1.9669e+18,
        4.7478e+18, 3.1666e+18, 1.6543e+18, 2.3889e+18, 4.2005e+18, 7.0684e+17,
        2.0645e+18, 4.3271e+18, 1.6326e+19, 1.7134e+18, 1.0871e+18, 6.7166e+18,
        1.4890e+18, 2.8831e+18, 1.1628e+18, 4.2371e+18, 9.8685e+17, 6.6140e+17,
        6.3500e+17, 3.1185e+18, 1.4874e+18, 1.5778e+18, 3.4523e+18, 3.3976e+18,
        2.0054e+18, 3.6770e+17, 3.6831e+18, 1.6105e+18, 5.3056e+17, 1.9537e+19,
        3.3673e+18, 1.4978e+18, 2.8702e+18, 4.2917e+18, 2.8667e+18, 1.7014e+18,
        2.1229e+19, 2.0748e+18, 1.9722e+18, 2.2184e+18, 2.0779e+18, 2.0208e+18,
        2.3174e+18, 1.8452e+18, 1.4749e+18, 1.1059e+18, 1.1606e+18, 1.3491e+18,
        1.5898e+18, 1.0974e+18, 9.0477e+17, 2.6680e+18, 8.8764e+17, 1.3222e+18,
        3.4380e+18, 3.6040e+18, 1.5201e+18, 1.3939e+18, 2.4116e+18, 3.2225e+18,
        1.0410e+18, 9.6439e+17, 7.1552e+17, 6.4593e+18, 6.1450e+17, 2.3386e+18,
        4.5725e+19, 1.6951e+18, 1.9691e+18, 1.1799e+19, 2.6112e+19, 1.1843e+19,
        1.9073e+18, 4.2272e+18, 9.1895e+17, 8.9828e+17, 1.7045e+18, 1.1645e+18,
        1.4935e+18, 2.3440e+18, 9.4863e+19, 2.2280e+18, 1.4246e+18, 2.3881e+17,
        2.0151e+18, 7.1673e+17, 1.1727e+18, 1.1226e+18, 2.9704e+18, 2.6594e+18,
        1.8745e+18, 1.0932e+18, 1.7052e+19, 1.4401e+18, 6.5084e+17, 7.6284e+17,
        3.1411e+18, 1.4493e+18, 1.0417e+18, 4.0900e+19, 2.3643e+18, 9.9857e+19,
        1.0435e+18, 2.6276e+18, 3.3695e+18, 2.3818e+18, 1.0171e+18, 2.0977e+18,
        8.7066e+17, 2.1641e+18, 2.9182e+18, 7.9718e+17, 2.6897e+18, 1.1398e+18,
        1.4575e+18, 6.5716e+17, 1.1862e+18, 3.0534e+18, 9.6628e+17, 2.6394e+19,
        1.1889e+18, 1.5822e+19, 1.9736e+19, 4.0665e+18, 4.7759e+18, 2.3776e+19,
        2.7233e+18, 1.2821e+18, 6.2866e+17, 5.9861e+17, 2.1841e+18, 5.0473e+18,
        1.7984e+19, 7.4509e+19, 1.1811e+19, 2.0936e+18, 8.5989e+17, 1.8924e+18,
        4.2217e+17, 1.9883e+18, 2.0676e+18, 1.0701e+18, 3.4716e+18, 1.5554e+19,
        3.0513e+18, 7.8184e+17, 1.2189e+18, 2.1698e+18, 2.1949e+18, 1.4358e+18,
        2.0154e+19, 6.9210e+18, 3.1054e+18, 1.3328e+18, 7.2627e+17, 2.9033e+18,
        6.8170e+18, 2.3890e+18])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.0245e+18, 3.3423e+18, 3.4032e+18, 6.5991e+18, 2.6004e+18, 9.2031e+18,
        3.5192e+18, 2.5391e+18, 3.8399e+18, 2.6497e+18, 4.3379e+18, 3.7140e+18,
        6.7101e+18, 2.5749e+18, 1.3019e+19, 5.2880e+18, 4.4292e+18, 6.2676e+18,
        5.1235e+18, 5.6573e+18, 5.0443e+18, 6.3499e+18, 8.3245e+18, 5.8575e+18,
        3.8126e+18, 2.7305e+18, 1.1209e+19, 3.7309e+18, 5.6835e+18, 3.9743e+18,
        6.5766e+18, 2.9601e+18, 2.7633e+18, 7.6915e+18, 2.8407e+18, 4.0895e+18,
        4.3107e+18, 2.7929e+18, 4.3524e+18, 6.9299e+18, 3.1158e+18, 8.1820e+18,
        4.2832e+18, 5.4820e+18, 3.0989e+18, 2.6385e+18, 6.2903e+18, 2.1827e+18,
        4.8199e+18, 2.9850e+18, 4.9115e+18, 3.0601e+18, 7.0043e+18, 8.6148e+18,
        5.8338e+18, 3.1288e+18, 4.3755e+18, 5.0466e+18, 1.7791e+18, 3.6488e+18,
        4.5899e+18, 1.0031e+19, 3.1801e+18, 3.0129e+18, 4.2491e+18, 6.7156e+18,
        9.4718e+18, 3.5305e+18, 5.6783e+18, 6.4594e+18, 5.4672e+18, 2.5847e+18,
        2.9224e+18, 4.6906e+18, 7.4409e+18, 5.9540e+18, 5.8027e+18, 4.0481e+18,
        4.9670e+18, 4.5462e+18, 9.2871e+17, 2.4932e+18, 5.6765e+18, 7.3095e+18,
        4.7355e+18, 4.3863e+18, 1.1594e+19, 4.2404e+18, 2.2075e+18, 8.1465e+18,
        4.2686e+18, 7.9323e+18, 6.3985e+18, 6.2841e+18, 5.4402e+18, 4.2315e+18,
        1.8743e+18, 8.2978e+18, 5.2162e+18, 4.9144e+18, 2.2638e+18, 7.4427e+18,
        3.7901e+18, 4.2877e+18, 4.3154e+18, 4.5170e+18, 6.9269e+18, 2.0878e+18,
        5.5595e+18, 5.8653e+18, 2.4961e+18, 4.0481e+18, 4.1023e+18, 2.1457e+18,
        3.3972e+18, 2.2286e+18, 9.8228e+18, 6.6546e+18, 4.1143e+18, 8.3341e+18,
        2.2397e+18, 3.6423e+18, 4.2483e+18, 4.2464e+18, 3.9852e+18, 3.3334e+18,
        5.1095e+18, 2.6219e+18])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.5413e+18, 7.7749e+18, 1.9584e+18, 2.9599e+18, 5.6040e+18, 1.5119e+18,
        3.6837e+18, 2.0235e+18, 4.8686e+18, 5.9268e+18, 1.9155e+18, 5.2573e+18,
        2.9968e+18, 4.6680e+18, 3.6731e+18, 4.1510e+18, 3.9140e+18, 3.7670e+18,
        2.9623e+18, 6.2177e+18, 4.9644e+18, 5.4506e+18, 2.6586e+18, 6.2558e+18,
        6.6010e+18, 4.3371e+18, 7.8507e+18, 1.8959e+18, 3.7338e+18, 4.8661e+18,
        6.5807e+18, 6.8092e+18, 3.5956e+18, 3.6637e+18, 5.3676e+18, 3.0940e+18,
        3.9672e+18, 3.5261e+18, 3.5754e+18, 4.7070e+18, 3.6340e+18, 2.5204e+18,
        6.5021e+18, 2.3858e+18, 1.7107e+18, 3.8134e+18, 2.8823e+18, 3.6332e+18,
        6.2635e+18, 6.0951e+18, 4.0895e+18, 4.7933e+18, 6.2530e+18, 7.7073e+18,
        6.2520e+18, 6.0072e+18, 5.0165e+18, 5.4096e+18, 3.0137e+18, 4.9668e+18,
        1.7148e+18, 3.4189e+18, 6.0540e+18, 8.0597e+18, 3.4456e+18, 2.9180e+18,
        2.6031e+18, 4.2814e+18, 7.0036e+18, 5.4217e+18, 6.2803e+18, 3.8627e+18,
        2.3638e+18, 3.2362e+18, 2.3565e+18, 3.5791e+18, 5.4316e+18, 1.4256e+18,
        6.4090e+18, 4.0631e+18, 8.4269e+18, 1.5667e+18, 2.5855e+18, 3.3561e+18,
        3.4804e+18, 5.8481e+18, 5.0794e+18, 5.5322e+18, 7.1767e+18, 6.2469e+18,
        3.9285e+18, 7.7154e+18, 7.3019e+18, 2.0319e+18, 4.8326e+18, 6.5088e+18,
        2.3924e+18, 6.0433e+18, 3.1890e+18, 3.0675e+18, 6.2256e+18, 6.1108e+18,
        7.6308e+18, 5.5839e+18, 1.2412e+19, 4.7553e+18, 3.2096e+18, 3.3367e+18,
        2.0007e+18, 3.7574e+18, 3.0007e+18, 6.1940e+18, 2.0470e+18, 2.8035e+18,
        6.0824e+18, 2.5718e+18, 6.1078e+18, 2.9605e+18, 2.7970e+18, 7.5135e+18,
        5.6288e+18, 6.9490e+18, 7.1793e+18, 2.0657e+18, 3.6342e+18, 5.0914e+18,
        4.5894e+18, 5.2612e+18])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2986e+16, 3.9604e+16, 1.2905e+17, 1.6928e+17, 1.2992e+17, 2.1867e+16,
        1.1052e+17, 8.2190e+16, 1.7685e+16, 3.3827e+17, 1.4655e+17, 1.5943e+17,
        3.7990e+16, 6.7239e+16, 1.6486e+16, 1.5878e+16, 8.7403e+16, 7.4435e+18,
        1.5398e+17, 5.4850e+16, 2.1062e+17, 8.7832e+16, 2.0505e+17, 4.3157e+16,
        9.4952e+16, 3.0384e+16, 4.4302e+16, 1.6566e+17, 2.5970e+17, 1.2008e+16,
        1.4357e+17, 4.0911e+16, 3.7605e+16, 2.1795e+16, 2.4947e+16, 9.6208e+16,
        5.3640e+16, 1.4724e+17, 4.2668e+17, 7.8758e+16, 8.3450e+16, 1.2797e+17,
        2.5040e+16, 1.5758e+17, 1.9947e+17, 1.2410e+18, 9.3190e+16, 4.5187e+16,
        1.7778e+17, 2.6099e+16, 6.1872e+16, 1.4900e+16, 1.5954e+17, 9.7534e+17,
        4.8752e+16, 1.3937e+17, 1.9033e+17, 2.0055e+17, 6.4491e+16, 3.7573e+16,
        3.2955e+16, 1.7256e+16, 5.2667e+16, 3.0292e+16, 1.8473e+17, 5.4591e+17,
        1.2284e+17, 1.8836e+17, 2.4800e+16, 8.0311e+16, 9.8792e+16, 9.2435e+16,
        3.8560e+17, 3.4462e+17, 1.4920e+16, 2.2647e+16, 1.8211e+17, 2.7456e+17,
        8.9996e+16, 7.7358e+16, 6.1701e+17, 2.7543e+17, 7.0866e+15, 2.8862e+16,
        9.0182e+17, 1.7941e+17, 5.4003e+16, 9.3846e+17, 2.3683e+16, 8.9224e+16,
        1.7249e+17, 4.4590e+16, 6.5432e+16, 5.3742e+17, 2.7593e+18, 3.6814e+16,
        1.7743e+17, 6.0293e+16, 1.1962e+17, 1.9742e+17, 1.0732e+17, 1.1153e+17,
        8.3474e+16, 1.5007e+17, 4.7851e+16, 2.6788e+18, 3.4649e+17, 6.0300e+16,
        2.8493e+17, 1.3766e+17, 5.9427e+15, 8.3021e+16, 7.5189e+16, 3.7163e+16,
        2.9777e+16, 1.4544e+17, 3.7772e+17, 4.9658e+16, 1.2229e+17, 1.0004e+17,
        1.9607e+16, 9.1706e+16, 2.2192e+16, 8.3199e+16, 3.7824e+16, 1.5861e+18,
        4.4321e+16, 3.9399e+16, 5.9795e+16, 5.0059e+16, 1.8881e+17, 3.1148e+17,
        5.0671e+16, 8.7880e+15, 6.2755e+17, 1.3005e+16, 1.0665e+18, 3.6838e+16,
        7.4256e+17, 7.1772e+16, 1.6501e+17, 1.9861e+17, 2.4564e+16, 1.0505e+17,
        1.2260e+16, 3.2934e+16, 4.5781e+16, 9.5209e+15, 9.3692e+16, 1.3043e+17,
        1.2706e+16, 8.0749e+16, 4.3122e+16, 1.0948e+16, 4.8026e+17, 3.5519e+16,
        7.0830e+16, 2.7898e+16, 5.1070e+16, 1.3509e+17, 9.5839e+16, 4.0953e+16,
        1.2022e+16, 7.7088e+16, 1.4544e+16, 2.1661e+16, 1.9848e+17, 2.1432e+17,
        1.7088e+17, 1.9105e+16, 7.4545e+16, 1.8961e+17, 6.7932e+16, 1.2191e+16,
        9.3216e+16, 2.1687e+17, 3.3311e+16, 1.3667e+17, 1.2975e+17, 5.8698e+16,
        1.3336e+17, 8.7451e+16, 1.0021e+17, 1.3258e+17, 1.1378e+17, 3.1293e+16,
        4.0912e+17, 3.9475e+16, 1.2749e+18, 1.5259e+17, 2.8271e+17, 9.7970e+16,
        3.1375e+17, 3.2530e+16, 4.1976e+16, 5.0347e+16, 9.0583e+16, 2.5917e+16,
        3.2918e+16, 1.2498e+17, 7.3659e+15, 3.0626e+16, 1.4277e+17, 6.2810e+16,
        5.0569e+16, 1.7879e+18, 2.5193e+18, 1.6429e+17, 1.9631e+17, 1.8040e+17,
        7.1166e+17, 2.8362e+16, 1.3552e+16, 4.8156e+17, 3.6073e+17, 1.0925e+17,
        1.2548e+17, 2.2833e+17, 1.3329e+17, 4.4680e+16, 2.1349e+17, 7.2060e+16,
        1.4040e+17, 1.0428e+16, 1.3350e+17, 8.3733e+15, 8.1743e+16, 6.3904e+16,
        1.0151e+17, 1.5919e+18, 1.7560e+16, 3.2099e+17, 4.2459e+16, 5.0785e+16,
        4.3951e+16, 2.1628e+16, 2.6369e+17, 1.6293e+17, 1.3042e+18, 1.2534e+17,
        7.4313e+16, 2.8127e+16, 8.0635e+17, 3.1409e+18, 2.1004e+16, 1.1563e+17,
        1.5958e+17, 1.6914e+17, 3.2105e+16, 1.0962e+17, 1.6012e+16, 1.1625e+17,
        2.4111e+17, 8.9924e+16, 4.5135e+16, 5.3123e+16, 3.2479e+17, 1.6273e+17,
        1.4809e+17, 5.2535e+16, 4.4108e+16, 6.0099e+16, 3.1883e+16, 1.8697e+16,
        1.0243e+17, 3.0656e+16, 8.7747e+16, 3.2443e+16, 3.4474e+16, 1.3675e+17,
        9.9615e+16, 2.8932e+16, 2.0305e+16, 5.6277e+16, 1.2142e+17, 7.5773e+16,
        4.7272e+16, 8.0544e+17, 6.6850e+16, 4.8216e+16, 2.4916e+16, 4.4617e+16,
        5.2382e+16, 6.4225e+16, 4.0003e+16, 1.0925e+16, 8.6623e+16, 9.2233e+16,
        6.1717e+16, 5.8856e+16, 2.2652e+17, 2.3108e+17, 1.2430e+17, 2.2394e+16,
        2.4331e+17, 3.9086e+17, 1.7343e+17, 2.8526e+17, 7.3330e+16, 3.3573e+17,
        5.0551e+16, 6.1216e+16, 2.1216e+17, 4.1983e+16, 4.2784e+16, 1.1993e+17,
        8.2456e+16, 6.8334e+15, 2.1401e+16, 1.3320e+17, 3.9450e+16, 2.0314e+17,
        6.0243e+17, 1.8261e+17, 2.0558e+17, 3.3378e+16, 3.3953e+16, 8.6355e+16,
        8.1502e+17, 1.3129e+17, 2.5752e+16, 6.2637e+17, 4.9915e+16, 1.0767e+17,
        2.5385e+17, 8.3287e+17, 1.5136e+17, 6.8670e+16, 2.4998e+16, 2.6527e+16,
        5.2188e+16, 1.5699e+16, 1.3322e+16, 9.3462e+16, 2.5737e+16, 6.1566e+16,
        3.1275e+16, 1.5209e+17, 5.1742e+15, 1.7957e+17, 4.6111e+16, 1.3639e+16,
        1.6371e+17, 2.3482e+17, 2.4188e+16, 1.0722e+17, 8.5964e+16, 3.8670e+16,
        1.3629e+17, 1.1955e+17, 5.1844e+16, 1.1658e+17, 1.0747e+17, 1.4874e+16,
        7.3340e+16, 2.3585e+16, 2.7512e+16, 3.5016e+17, 1.5976e+17, 2.6478e+16,
        5.8676e+16, 1.1111e+17, 7.9729e+15, 3.2484e+17, 9.3261e+16, 1.3644e+17,
        9.0177e+16, 4.0475e+16, 1.3237e+17, 1.1765e+17, 1.1709e+17, 2.3395e+17,
        1.5862e+17, 5.2345e+16, 9.6065e+16, 5.6934e+17, 1.8148e+16, 7.8152e+16,
        1.5164e+17, 1.7334e+17, 4.5623e+16, 1.9251e+17, 3.6529e+17, 1.6633e+17,
        5.7100e+16, 1.0542e+17, 2.0192e+17, 3.5919e+16, 9.0111e+17, 9.0348e+15,
        9.6157e+16, 1.4362e+17, 3.2916e+16, 4.9790e+17, 3.9800e+18, 7.7982e+16,
        1.3086e+16, 8.9835e+16, 8.3169e+16, 2.9706e+16, 7.2470e+16, 7.2876e+16,
        8.3639e+16, 6.4343e+16, 4.0175e+16, 3.5639e+16, 1.1805e+17, 1.2819e+17,
        4.6060e+16, 2.3953e+16, 5.4705e+17, 9.4887e+16, 3.0448e+16, 2.1879e+16,
        1.9801e+16, 5.4083e+16, 3.5042e+16, 1.7905e+17, 3.4347e+16, 1.4612e+18,
        5.9288e+16, 5.6240e+16, 6.5306e+16, 2.2962e+17, 1.2156e+17, 6.8298e+16,
        3.0430e+16, 6.3766e+16, 1.9890e+17, 3.1048e+16, 7.6038e+16, 1.6576e+17,
        1.1618e+17, 5.8995e+16, 6.6275e+17, 1.0314e+17, 1.0182e+17, 5.7599e+17,
        1.2644e+17, 7.1472e+16, 7.3599e+16, 1.2321e+17, 1.9913e+17, 1.4031e+17,
        2.8655e+16, 5.3392e+16, 4.2636e+16, 6.6520e+17, 1.9380e+16, 1.1885e+16,
        7.1263e+16, 1.4859e+17, 6.7082e+16, 1.0212e+17, 3.9488e+16, 6.5205e+16,
        5.9190e+16, 1.3437e+17, 1.3065e+17, 1.0880e+17, 8.5920e+16, 1.6674e+17,
        3.2759e+16, 6.8436e+16, 1.8480e+17, 2.1591e+16, 1.7258e+17, 3.2065e+16,
        2.3114e+17, 2.3284e+17, 7.1063e+16, 2.7732e+16, 1.8154e+17, 1.1862e+17,
        1.4614e+16, 3.3618e+16, 3.8799e+17, 4.6202e+16, 1.4653e+17, 9.8984e+16,
        1.0414e+17, 6.9407e+16, 9.1124e+16, 1.5748e+17, 6.6118e+16, 4.2912e+16,
        8.5524e+16, 2.3723e+16, 6.6566e+16, 6.4446e+16, 5.1377e+17, 2.5169e+16,
        5.3839e+17, 1.0574e+17, 2.9759e+16, 9.3320e+16, 7.7764e+17, 1.1126e+17,
        1.9361e+17, 1.0299e+17, 7.1051e+16, 8.8985e+16, 1.8277e+17, 2.2794e+17,
        1.1872e+17, 3.9712e+16, 1.6676e+17, 1.2482e+17, 1.6069e+17, 7.6864e+16,
        9.4143e+17, 5.7265e+17])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.3297e+16, 1.5601e+17, 3.7509e+17, 6.9865e+16, 2.0121e+17, 6.1840e+16,
        6.6590e+16, 1.0101e+17, 5.2091e+16, 7.8876e+16, 1.0795e+17, 2.3102e+17,
        3.8946e+16, 2.6631e+17, 2.0879e+16, 1.7345e+17, 2.6675e+17, 1.5629e+17,
        1.1940e+17, 2.9962e+17, 2.4503e+16, 8.9189e+16, 3.8598e+16, 3.2832e+17,
        1.2532e+17, 2.4470e+16, 3.3111e+16, 5.4240e+16, 1.4323e+17, 1.7980e+17,
        1.6179e+17, 1.3139e+17, 1.3124e+17, 3.5609e+17, 1.7541e+17, 1.3348e+17,
        8.7290e+16, 5.4128e+16, 7.2685e+16, 1.9296e+17, 3.6069e+17, 1.0514e+17,
        7.9079e+16, 1.8264e+17, 7.8823e+16, 7.5990e+16, 7.6985e+16, 5.0322e+16,
        1.0550e+17, 1.3685e+17, 1.9047e+17, 3.4251e+17, 1.3939e+17, 2.4141e+17,
        6.5077e+16, 1.6143e+17, 2.1036e+17, 8.2207e+16, 1.9641e+17, 2.6789e+17,
        2.3128e+17, 2.4653e+16, 2.5506e+17, 1.0983e+17, 8.7072e+16, 5.8027e+16,
        3.4828e+16, 2.6603e+17, 2.1071e+17, 2.7735e+17, 1.2872e+17, 3.2359e+17,
        2.6107e+17, 2.6871e+16, 9.1408e+16, 1.7821e+17, 7.7292e+16, 3.9552e+16,
        3.2673e+16, 2.9318e+17, 1.2464e+17, 2.3855e+17, 8.9338e+16, 5.6788e+17,
        1.0320e+17, 2.2307e+17, 1.2772e+17, 3.1331e+17, 2.4957e+16, 1.5896e+17,
        3.7848e+17, 2.5559e+17, 7.7491e+16, 3.3541e+16, 2.5593e+17, 4.1500e+16,
        6.2724e+16, 3.0291e+17, 1.3239e+17, 2.6598e+17, 2.3577e+16, 3.1508e+17,
        8.8931e+16, 3.8099e+17, 2.0320e+17, 2.8786e+17, 9.2947e+16, 1.2213e+17,
        1.1133e+17, 1.9753e+17, 6.9035e+16, 1.2889e+17, 6.0824e+16, 6.3089e+16,
        9.5484e+16, 1.0380e+17, 9.6986e+16, 4.5120e+16, 6.6895e+16, 5.5819e+16,
        1.4892e+17, 1.5251e+17, 1.5369e+17, 3.0201e+16, 4.6053e+16, 7.5216e+16,
        3.3700e+16, 4.1572e+16, 9.1872e+16, 7.1723e+16, 2.2639e+17, 4.4467e+17,
        2.7224e+17, 2.4562e+17, 1.2731e+17, 4.8745e+16, 8.2914e+16, 4.1885e+16,
        2.9596e+17, 1.1524e+17, 4.6500e+17, 3.4359e+17, 1.3409e+17, 6.9142e+16,
        1.1613e+17, 2.4401e+17, 1.2092e+17, 2.7666e+17, 2.1867e+17, 9.6946e+16,
        3.2354e+17, 2.9555e+17, 1.0961e+17, 1.1734e+17, 7.1213e+16, 1.8646e+17,
        1.0878e+17, 1.4668e+17, 4.0266e+16, 2.6056e+17, 1.0511e+17, 2.0377e+17,
        2.5927e+17, 1.0395e+17, 1.3852e+17, 2.8815e+17, 2.1007e+16, 2.8714e+17,
        2.4987e+17, 7.6707e+16, 9.4335e+16, 4.8524e+16, 9.6658e+16, 5.7077e+16,
        2.3033e+17, 2.6088e+17, 2.1673e+17, 1.0448e+17, 7.7687e+16, 3.3450e+17,
        1.1368e+17, 9.0045e+16, 3.0315e+17, 9.8393e+16, 1.1942e+17, 2.5323e+17,
        1.4867e+17, 1.0838e+17, 5.3629e+16, 1.8295e+17, 1.9340e+17, 1.7945e+17,
        4.0484e+17, 2.2142e+17, 4.4241e+16, 1.0869e+17, 1.3385e+17, 2.4080e+17,
        6.6955e+16, 1.6049e+16, 7.8614e+16, 2.7863e+17, 8.5024e+16, 2.3255e+17,
        7.6859e+16, 1.0551e+17, 8.1380e+16, 1.8928e+17, 1.6730e+17, 6.7799e+16,
        2.6728e+17, 8.3148e+16, 1.3709e+17, 3.8568e+17, 7.8639e+16, 1.9523e+17,
        6.1645e+16, 9.8423e+16, 3.1392e+17, 7.4001e+16, 1.5213e+17, 1.1859e+17,
        1.9397e+17, 4.5446e+16, 3.2949e+16, 1.6338e+17, 4.3694e+17, 4.3823e+16,
        6.1569e+16, 1.2111e+17, 1.8040e+17, 9.0781e+16, 1.5548e+17, 5.5968e+16,
        3.3281e+17, 4.4459e+17, 2.8407e+17, 1.5078e+17, 1.0428e+17, 2.4352e+17,
        4.8471e+17, 1.1448e+16, 2.7615e+17, 1.3419e+17, 2.2057e+17, 1.3267e+17,
        1.3850e+17, 9.1686e+16, 4.4004e+16, 6.9510e+16, 1.0674e+17, 1.7702e+17,
        8.7876e+16, 4.2129e+17, 5.9737e+16, 7.9265e+16])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.7400e+16, 1.9183e+17, 1.3390e+17, 1.1712e+17, 3.1673e+17, 1.4684e+17,
        5.8566e+16, 2.8176e+16, 9.8866e+16, 9.6735e+16, 5.8985e+16, 1.0500e+17,
        7.8703e+16, 4.0131e+16, 4.6266e+17, 3.0487e+17, 2.8202e+17, 2.5014e+16,
        4.0443e+17, 1.0919e+17, 3.0005e+17, 1.1192e+17, 8.3138e+16, 7.9981e+16,
        4.4407e+17, 1.9401e+17, 1.4362e+17, 1.5852e+17, 1.5367e+17, 3.0895e+17,
        4.2842e+16, 3.5432e+17, 6.1348e+16, 1.1824e+17, 8.3339e+16, 7.7371e+16,
        2.1830e+17, 2.3924e+17, 1.6785e+17, 1.5579e+17, 8.6038e+16, 1.3652e+17,
        1.9997e+17, 1.6346e+17, 2.5741e+17, 9.5770e+16, 4.9868e+16, 1.3407e+17,
        9.4354e+16, 1.4103e+17, 2.3323e+17, 1.5332e+17, 4.2586e+16, 2.6791e+17,
        1.4754e+17, 1.0854e+17, 1.4648e+17, 1.2444e+17, 2.1261e+17, 1.2710e+17,
        1.3097e+17, 3.2479e+16, 1.1905e+17, 1.0218e+17, 6.5616e+16, 4.1029e+16,
        3.9914e+16, 1.8080e+17, 5.3892e+16, 4.8707e+16, 1.1321e+17, 7.6767e+16,
        4.0402e+16, 3.7333e+17, 2.3325e+16, 1.3943e+17, 1.4108e+17, 2.5440e+17,
        2.3499e+17, 1.0284e+17, 9.6462e+16, 1.8791e+17, 1.0303e+17, 1.2778e+17,
        2.9647e+17, 1.8126e+17, 2.5708e+17, 4.8895e+16, 3.5429e+16, 1.1842e+17,
        2.5685e+17, 2.6709e+17, 1.2540e+17, 1.0503e+17, 1.7028e+17, 3.2833e+17,
        2.9243e+16, 9.6323e+16, 2.1280e+16, 2.2836e+17, 1.3238e+17, 2.6226e+16,
        1.2658e+17, 2.5320e+17, 8.0865e+16, 5.1438e+16, 3.4165e+16, 1.0985e+17,
        2.2696e+17, 1.0717e+17, 1.9293e+17, 1.7226e+17, 7.6898e+16, 2.0196e+16,
        1.4161e+17, 4.9775e+17, 2.8428e+17, 3.2982e+17, 1.0976e+17, 2.1943e+17,
        1.3826e+16, 2.0501e+17, 3.3088e+16, 1.5727e+17, 3.5346e+16, 6.6783e+15,
        9.8706e+16, 1.7576e+17, 4.1429e+16, 2.6025e+17, 6.7441e+16, 9.0492e+16,
        1.2496e+17, 1.4837e+17, 4.9340e+17, 5.4823e+16, 3.6712e+16, 2.3734e+16,
        5.5218e+16, 6.3960e+16, 1.3989e+17, 1.2745e+17, 9.0650e+16, 9.8549e+16,
        3.7335e+16, 2.1054e+16, 1.8402e+17, 1.1274e+17, 2.9588e+16, 5.5647e+16,
        3.6598e+16, 3.8252e+16, 2.5655e+17, 1.3262e+17, 1.2706e+17, 1.8641e+17,
        1.7461e+17, 2.0538e+16, 1.0844e+17, 6.8015e+16, 1.2003e+17, 8.7736e+16,
        1.3011e+17, 4.5965e+17, 2.6028e+17, 1.5612e+16, 8.0474e+16, 1.2422e+17,
        2.1191e+17, 7.4408e+16, 8.0486e+16, 4.3476e+16, 4.3362e+17, 3.4005e+17,
        4.3701e+16, 7.9580e+16, 3.0048e+17, 5.8341e+16, 5.8738e+16, 2.7576e+17,
        1.3690e+17, 9.7450e+16, 8.4483e+16, 4.4288e+17, 3.1953e+17, 2.2740e+17,
        1.3219e+17, 7.8621e+16, 6.3915e+16, 2.3308e+17, 2.0426e+16, 2.5791e+17,
        1.4910e+17, 1.9515e+17, 2.3284e+17, 3.4368e+17, 4.3965e+16, 2.2659e+17,
        1.4203e+17, 3.0024e+17, 3.5616e+16, 2.1178e+17, 2.7916e+17, 9.3156e+16,
        1.1451e+17, 6.8573e+16, 1.7302e+17, 9.5151e+16, 2.2364e+17, 3.1714e+17,
        1.4381e+17, 1.9925e+17, 1.5890e+17, 3.4952e+16, 2.6284e+17, 1.4136e+17,
        1.5657e+17, 6.3315e+16, 1.6523e+17, 1.6626e+17, 4.6716e+16, 9.5626e+16,
        9.8849e+16, 1.0553e+17, 7.0832e+16, 6.5706e+16, 5.4264e+16, 6.8165e+16,
        3.3041e+17, 1.3435e+17, 2.7320e+17, 2.3269e+17, 1.7098e+17, 3.0514e+17,
        9.5653e+16, 8.5313e+16, 1.8454e+17, 1.4254e+17, 1.6516e+17, 2.3936e+16,
        3.0323e+17, 3.3333e+17, 3.6533e+16, 2.6001e+17, 1.1527e+17, 8.0739e+16,
        8.0001e+16, 9.0927e+16, 9.0226e+16, 7.8694e+16, 9.5973e+16, 5.4832e+16,
        5.4688e+16, 3.3271e+17, 3.4671e+17, 1.6249e+17])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.1944e+15, 1.0459e+14, 1.3493e+15,  ..., 2.4283e+16, 8.0209e+14,
        1.8132e+14])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.3834e+14, 1.9448e+14, 2.7058e+14,  ..., 2.2450e+16, 2.7385e+14,
        2.1678e+15])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.2141e+14, 9.9213e+14, 1.9157e+15, 1.1690e+15, 1.4799e+15, 5.7519e+14,
        8.2065e+14, 6.9710e+14, 7.2376e+14, 3.6354e+14, 5.4049e+14, 1.6514e+15,
        1.0437e+15, 7.0537e+14, 3.3497e+15, 2.1959e+15, 4.1847e+14, 2.9787e+15,
        3.3453e+15, 7.9499e+14, 4.6658e+14, 3.7898e+15, 2.3435e+15, 3.1723e+15,
        4.5982e+15, 3.8662e+15, 2.8628e+15, 1.8358e+15, 2.3933e+15, 1.5041e+15,
        4.1411e+15, 4.8727e+15, 3.0111e+15, 5.2048e+14, 6.5963e+14, 9.2028e+14,
        1.2833e+15, 8.6054e+14, 1.8585e+15, 1.2932e+15, 1.2773e+15, 4.6118e+14,
        9.6476e+14, 1.2375e+15, 2.1614e+15, 8.8951e+14, 3.7868e+15, 6.6088e+14,
        1.5210e+15, 9.8696e+14, 2.0480e+15, 5.5911e+14, 7.7708e+14, 4.8281e+14,
        2.2816e+15, 1.4433e+15, 2.2928e+15, 3.3440e+15, 3.2720e+15, 1.6344e+15,
        1.5912e+15, 2.7708e+15, 2.4395e+15, 3.8870e+15, 2.3870e+15, 1.8033e+15,
        1.7181e+15, 1.0277e+15, 7.7839e+14, 3.1748e+14, 1.5811e+15, 1.5428e+15,
        2.5816e+15, 1.4022e+15, 1.5489e+15, 2.1612e+15, 5.1088e+14, 6.6708e+14,
        2.2245e+15, 1.6674e+15, 1.7448e+15, 1.4809e+15, 8.3251e+14, 1.9777e+15,
        7.9043e+14, 6.0683e+14, 2.5537e+15, 3.8423e+15, 7.3013e+14, 5.6192e+14,
        2.5037e+15, 1.0852e+15, 3.1757e+15, 1.5217e+15, 4.4028e+15, 2.0310e+15,
        1.3265e+15, 4.2612e+15, 2.7196e+15, 2.5827e+15, 8.8206e+14, 3.0938e+15,
        1.1455e+15, 1.1351e+15, 1.0363e+15, 1.2796e+15, 5.7882e+14, 1.1475e+15,
        5.1113e+14, 2.3815e+15, 2.2038e+15, 1.0015e+15, 5.7212e+14, 1.5237e+15,
        6.6993e+14, 1.4077e+15, 3.7604e+15, 6.5497e+14, 5.9765e+14, 6.2104e+14,
        2.5500e+15, 8.4263e+14, 4.2060e+14, 2.1102e+15, 2.2905e+15, 1.2553e+15,
        1.1254e+15, 9.8110e+14, 4.6147e+15, 2.8359e+15, 4.6202e+15, 5.0798e+15,
        7.2366e+14, 1.4774e+15, 8.0770e+14, 2.4097e+15, 1.4171e+15, 2.0981e+15,
        4.2579e+15, 4.3554e+14, 6.2561e+14, 1.4273e+15, 2.6057e+15, 4.2492e+15,
        1.2867e+15, 6.3345e+14, 1.0701e+15, 1.0124e+15, 1.0309e+15, 5.8099e+15,
        1.1139e+15, 1.9150e+15, 7.5060e+14, 3.1399e+15, 5.2642e+15, 2.1623e+15,
        3.1034e+14, 1.1927e+15, 1.4689e+15, 1.8806e+15, 1.9169e+15, 2.4438e+15,
        6.2746e+14, 1.2444e+15, 1.3781e+15, 6.4824e+14, 4.0415e+15, 2.3930e+15,
        2.3477e+15, 1.5533e+15, 1.8660e+15, 2.1711e+15, 2.9956e+15, 1.9192e+15,
        3.4680e+14, 8.2199e+14, 7.7466e+14, 1.4413e+15, 2.6042e+15, 1.9686e+15,
        2.7416e+15, 4.2329e+14, 1.3887e+15, 2.5098e+15, 7.0707e+14, 4.5348e+14,
        4.2986e+15, 1.1893e+15, 1.0285e+15, 5.8898e+14, 2.0525e+15, 1.0704e+15,
        1.9856e+14, 5.8138e+14, 6.7454e+14, 1.8177e+15, 9.9443e+14, 1.0657e+15,
        1.3528e+15, 2.0116e+15, 2.1009e+15, 6.4597e+14, 1.4192e+15, 3.7760e+15,
        5.7350e+14, 8.2100e+14, 4.6870e+14, 9.2273e+14, 1.7041e+15, 1.0051e+15,
        3.1829e+15, 2.4138e+15, 8.7120e+14, 9.3102e+14, 1.4297e+15, 2.3008e+15,
        1.8448e+14, 2.2042e+15, 1.7523e+15, 3.2813e+15, 3.8024e+14, 3.2037e+15,
        1.3395e+15, 5.7973e+14, 3.3491e+14, 1.9217e+15, 3.0638e+15, 9.8906e+14,
        3.1669e+15, 5.1840e+14, 8.9001e+14, 3.3944e+15, 3.3463e+15, 1.8339e+15,
        3.7418e+15, 1.5262e+15, 3.0632e+15, 6.8759e+14, 2.7107e+15, 2.6140e+15,
        2.1248e+15, 1.4959e+15, 3.6080e+15, 1.5230e+15, 1.1110e+15, 5.6279e+14,
        2.9502e+15, 2.4732e+15, 1.7736e+15, 1.8066e+15, 1.0602e+15, 1.7070e+15,
        4.7349e+15, 2.2219e+15, 2.9288e+15, 2.2455e+15])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.0470e+15, 1.8730e+15, 4.3294e+15, 2.3339e+15, 1.8583e+15, 2.4974e+15,
        3.8259e+15, 1.9121e+15, 3.7417e+15, 1.0271e+15, 2.6792e+15, 8.8710e+14,
        2.6527e+15, 2.4787e+15, 1.3597e+15, 2.0769e+15, 4.8509e+14, 2.1097e+15,
        2.9821e+15, 1.7961e+15, 1.1446e+15, 4.3287e+15, 1.2595e+15, 2.4451e+15,
        3.6783e+15, 4.7979e+14, 5.5238e+15, 3.5326e+15, 1.8330e+15, 7.3301e+14,
        2.7767e+15, 1.7619e+15, 2.0214e+14, 6.7617e+15, 8.9824e+14, 1.6815e+15,
        1.4136e+15, 4.1285e+15, 1.7431e+15, 3.1269e+15, 3.1369e+15, 3.6630e+15,
        6.5382e+14, 4.4860e+15, 1.6144e+15, 3.1076e+15, 9.5296e+14, 2.2458e+15,
        3.8559e+15, 3.5113e+15, 1.4215e+15, 2.2393e+15, 1.2696e+15, 1.3178e+15,
        5.9905e+14, 3.5883e+15, 2.7241e+14, 1.0131e+15, 2.3205e+15, 2.5860e+15,
        4.8238e+14, 2.4212e+15, 2.3390e+15, 3.1859e+15, 5.9456e+15, 1.2754e+15,
        2.7120e+15, 1.4249e+15, 6.0407e+14, 2.4808e+15, 4.8960e+15, 4.9887e+14,
        3.7330e+15, 1.3574e+15, 1.4130e+15, 4.1534e+15, 1.2200e+14, 6.9568e+14,
        1.7132e+15, 4.6146e+14, 1.2811e+15, 5.2855e+15, 2.7644e+15, 1.0239e+15,
        3.9620e+15, 1.7454e+15, 2.2968e+15, 4.0435e+15, 2.8534e+15, 2.2636e+15,
        3.6088e+15, 2.9812e+15, 1.1740e+15, 1.8174e+15, 2.8613e+15, 2.0411e+15,
        2.0870e+15, 2.5583e+15, 5.7487e+14, 1.2353e+15, 6.5704e+14, 1.1318e+15,
        1.6696e+15, 1.5167e+15, 5.4939e+15, 1.9695e+15, 3.8852e+15, 6.4973e+14,
        6.9279e+14, 3.1924e+15, 2.6414e+15, 2.9157e+15, 4.6678e+15, 2.2009e+15,
        3.3504e+15, 4.4974e+15, 2.1749e+15, 1.5498e+15, 1.9921e+15, 3.2383e+15,
        1.0160e+15, 1.8622e+15, 2.4139e+15, 2.5071e+15, 9.7042e+14, 4.0010e+15,
        5.0308e+14, 1.1831e+15, 3.5918e+15, 1.4801e+15, 4.7759e+15, 1.2482e+15,
        2.6706e+15, 3.4331e+14, 9.9395e+14, 3.2590e+15, 4.5303e+15, 5.4341e+15,
        1.4369e+15, 2.1398e+15, 7.2124e+15, 4.4315e+15, 4.5684e+15, 2.9714e+15,
        2.3528e+15, 9.6415e+14, 5.5351e+15, 2.1911e+15, 8.4061e+14, 4.3047e+15,
        2.4906e+15, 1.3909e+15, 5.2890e+15, 2.4983e+15, 2.3476e+15, 2.2100e+15,
        2.4209e+15, 2.6029e+15, 4.7887e+15, 8.2760e+14, 3.4625e+15, 3.4135e+15,
        8.5022e+14, 1.5340e+15, 2.6107e+15, 9.8402e+14, 7.9543e+14, 1.7696e+15,
        3.2604e+15, 8.3954e+14, 2.4355e+15, 3.0966e+15, 2.7973e+15, 1.8040e+15,
        3.4647e+15, 4.7192e+15, 2.2532e+15, 3.0282e+15, 4.0141e+15, 2.2476e+15,
        1.4047e+15, 1.8723e+15, 1.0898e+15, 5.1537e+15, 3.0363e+15, 3.6024e+15,
        3.5364e+15, 1.0763e+15, 7.6135e+15, 6.6773e+14, 5.9052e+14, 1.9127e+15,
        3.1703e+15, 4.4913e+14, 7.2805e+14, 3.4788e+15, 1.4508e+15, 4.1142e+15,
        1.7066e+15, 1.0070e+15, 5.6543e+15, 1.5613e+15, 2.9572e+14, 8.0227e+14,
        7.4663e+14, 3.3838e+15, 6.6558e+14, 3.6332e+15, 2.8800e+15, 1.3874e+15,
        3.4108e+15, 2.4948e+15, 1.5334e+15, 3.0969e+15, 4.8265e+15, 8.6719e+14,
        4.2892e+15, 5.9044e+15, 2.4892e+15, 1.5690e+15, 2.0233e+15, 2.4940e+15,
        1.5029e+15, 3.8493e+15, 8.5819e+14, 3.0358e+15, 1.6096e+15, 6.0375e+15,
        1.4413e+15, 2.4512e+15, 3.5848e+15, 1.2785e+15, 1.7988e+15, 1.8072e+15,
        1.5153e+15, 3.9028e+15, 1.9190e+15, 1.2998e+15, 3.4620e+15, 2.4553e+15,
        1.6827e+15, 2.6943e+15, 4.5838e+15, 1.7672e+15, 9.1782e+14, 2.0853e+15,
        1.9137e+15, 1.7898e+15, 1.4333e+15, 3.8495e+15, 3.3899e+15, 1.8638e+15,
        1.5965e+15, 8.4670e+15, 3.2164e+15, 1.5729e+15])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.4940e+11, 8.9835e+11, 1.4064e+12,  ..., 8.8708e+11, 8.3280e+11,
        1.3767e+12])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.5435e+12, 1.8256e+12, 2.1936e+12, 2.9039e+12, 3.7888e+12, 3.6048e+12,
        2.5740e+12, 2.5357e+12, 4.2756e+12, 1.9419e+12, 2.8037e+12, 2.1427e+12,
        1.3976e+12, 2.0834e+12, 2.6325e+12, 2.1621e+12, 2.8510e+12, 1.9513e+12,
        2.5663e+12, 2.6777e+12, 4.4652e+12, 2.2802e+12, 2.8784e+12, 2.0363e+12,
        1.7241e+12, 4.8006e+12, 2.8355e+12, 2.1880e+12, 1.4986e+12, 3.8415e+12,
        1.9953e+12, 4.5246e+12, 2.6210e+12, 1.8851e+12, 1.9846e+12, 1.8139e+12,
        3.6894e+12, 1.6684e+12, 1.5824e+12, 1.8581e+12, 3.1203e+12, 1.2894e+12,
        2.9880e+12, 2.7062e+12, 1.7616e+12, 4.1530e+12, 2.3635e+12, 2.1100e+12,
        2.2391e+12, 2.1922e+12, 2.8197e+12, 3.8158e+12, 3.3632e+12, 2.9271e+12,
        1.9258e+12, 1.9895e+12, 1.9276e+12, 1.7076e+12, 2.1556e+12, 1.3551e+12,
        3.0540e+12, 2.4163e+12, 1.9244e+12, 2.6257e+12, 2.7924e+12, 2.2283e+12,
        3.7499e+12, 3.4462e+12, 2.8558e+12, 3.2608e+12, 2.4156e+12, 1.7310e+12,
        1.5842e+12, 1.4027e+12, 1.9928e+12, 1.6996e+12, 2.7492e+12, 3.1726e+12,
        1.6328e+12, 2.1139e+12, 3.1675e+12, 2.4705e+12, 3.3311e+12, 2.7138e+12,
        2.5321e+12, 2.2990e+12, 2.2920e+12, 2.0667e+12, 2.3574e+12, 2.4989e+12,
        1.5559e+12, 2.6627e+12, 2.2744e+12, 2.4501e+12, 2.8727e+12, 5.8267e+12,
        1.2632e+12, 2.4216e+12, 1.2641e+12, 1.5923e+12, 2.7703e+12, 2.8100e+12,
        2.1887e+12, 2.2613e+12, 3.3958e+12, 3.0617e+12, 1.3150e+12, 1.7418e+12,
        3.2897e+12, 2.3519e+12, 4.2596e+12, 1.6036e+12, 3.4369e+12, 2.7041e+12,
        3.2379e+12, 2.9258e+12, 2.1270e+12, 1.8504e+12, 2.3114e+12, 1.8651e+12,
        3.3594e+12, 2.4295e+12, 1.2573e+12, 1.5039e+12, 2.3164e+12, 2.2476e+12,
        9.1808e+11, 2.7946e+12, 3.5335e+12, 1.9555e+12, 2.3950e+12, 1.9440e+12,
        2.5933e+12, 2.1237e+12, 2.7246e+12, 2.6424e+12, 2.0959e+12, 1.6800e+12,
        1.0848e+12, 2.9040e+12, 2.6252e+12, 2.9546e+12, 1.0465e+12, 1.4554e+12,
        2.5343e+12, 4.0035e+12, 3.9787e+12, 1.5128e+12, 2.4042e+12, 2.6167e+12,
        1.3927e+12, 1.7889e+12, 1.8306e+12, 1.8748e+12, 2.4605e+12, 2.4381e+12,
        3.3029e+12, 3.3940e+12, 3.3027e+12, 1.1237e+12, 2.2890e+12, 2.6919e+12,
        2.8213e+12, 1.7578e+12, 2.6706e+12, 3.2538e+12, 3.4850e+12, 1.7198e+12,
        1.3496e+12, 1.3740e+12, 2.0866e+12, 2.0657e+12, 1.6699e+12, 3.5211e+12,
        2.1507e+12, 3.3154e+12, 1.5675e+12, 3.7556e+12, 2.1370e+12, 2.9640e+12,
        1.5084e+12, 3.3675e+12, 1.7143e+12, 1.2761e+12, 2.5966e+12, 2.1049e+12,
        3.2590e+12, 1.9982e+12, 2.6368e+12, 1.8301e+12, 2.3460e+12, 3.6841e+12,
        2.2679e+12, 1.9717e+12, 3.9361e+12, 2.0438e+12, 1.8628e+12, 1.9382e+12,
        2.7105e+12, 1.6846e+12, 1.3520e+12, 2.5318e+12, 4.6071e+12, 1.2656e+12,
        2.9692e+12, 3.2850e+12, 1.2278e+12, 4.2174e+12, 4.0781e+12, 1.8222e+12,
        2.8129e+12, 2.8457e+12, 1.9024e+12, 2.0399e+12, 2.3833e+12, 3.1978e+12,
        2.3564e+12, 1.1373e+12, 5.1946e+12, 2.1134e+12, 2.1207e+12, 2.9595e+12,
        2.3770e+12, 2.9765e+12, 1.7432e+12, 2.3559e+12, 3.0787e+12, 1.7664e+12,
        2.2752e+12, 2.6492e+12, 2.5156e+12, 1.9393e+12, 2.4851e+12, 2.0595e+12,
        2.5711e+12, 2.4333e+12, 2.1343e+12, 1.4131e+12, 2.5894e+12, 2.8577e+12,
        2.2236e+12, 1.6792e+12, 2.0328e+12, 2.2755e+12, 2.8128e+12, 2.6472e+12,
        1.6560e+12, 2.0299e+12, 2.3966e+12, 9.5334e+11, 1.5591e+12, 1.9650e+12,
        1.1121e+12, 2.9912e+12, 2.9677e+12, 1.5359e+12])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.4026e+12, 1.6365e+12, 3.8421e+12, 2.6279e+12, 2.0716e+12, 2.3691e+12,
        2.9451e+12, 6.2352e+12, 3.1303e+12, 3.1365e+12, 2.7329e+12, 2.0206e+12,
        2.1789e+12, 3.2140e+12, 4.2097e+12, 2.9499e+12, 2.7668e+12, 2.6726e+12,
        2.4260e+12, 4.8684e+12, 3.1100e+12, 2.9425e+12, 3.7486e+12, 3.1521e+12,
        4.6959e+12, 2.3803e+12, 2.8773e+12, 1.9849e+12, 3.5213e+12, 3.0982e+12,
        2.3305e+12, 1.7500e+12, 3.3814e+12, 2.5671e+12, 3.1681e+12, 3.7632e+12,
        4.5289e+12, 2.9852e+12, 2.9575e+12, 5.1547e+12, 3.8150e+12, 3.5814e+12,
        3.8459e+12, 3.6589e+12, 2.2738e+12, 1.7611e+12, 2.5468e+12, 3.7697e+12,
        1.7549e+12, 1.8925e+12, 2.9898e+12, 4.0184e+12, 2.5512e+12, 2.8251e+12,
        4.9944e+12, 3.0518e+12, 3.6316e+12, 2.4563e+12, 3.1853e+12, 1.2448e+12,
        1.9821e+12, 1.9849e+12, 4.1587e+12, 3.8604e+12, 3.4158e+12, 3.7379e+12,
        3.7627e+12, 4.3652e+12, 4.6936e+12, 1.5214e+12, 1.7762e+12, 1.7730e+12,
        4.6873e+12, 4.0563e+12, 3.2911e+12, 5.0095e+12, 4.1613e+12, 2.2011e+12,
        1.7967e+12, 1.8983e+12, 2.2318e+12, 2.0048e+12, 4.8622e+12, 4.3790e+12,
        6.3962e+12, 3.3115e+12, 4.1470e+12, 4.1002e+12, 4.4294e+12, 4.5725e+12,
        2.1171e+12, 3.0768e+12, 3.4448e+12, 4.5756e+12, 2.7764e+12, 3.4352e+12,
        5.4517e+12, 2.6341e+12, 2.1228e+12, 1.9760e+12, 3.2375e+12, 3.0100e+12,
        4.3509e+12, 3.9596e+12, 2.2487e+12, 1.7985e+12, 1.7158e+12, 5.5956e+12,
        1.5929e+12, 3.2097e+12, 4.6931e+12, 3.1065e+12, 7.3649e+12, 4.7913e+12,
        4.1074e+12, 2.1818e+12, 2.5905e+12, 5.7317e+12, 2.1835e+12, 6.1421e+12,
        2.8027e+12, 3.1504e+12, 1.7587e+12, 4.6184e+12, 4.1909e+12, 2.2656e+12,
        1.5681e+12, 2.8508e+12, 2.0727e+12, 2.4513e+12, 4.3550e+12, 3.6389e+12,
        2.1500e+12, 4.0109e+12, 2.2998e+12, 2.7625e+12, 3.4375e+12, 3.2654e+12,
        1.8013e+12, 3.2363e+12, 6.5951e+12, 3.2655e+12, 2.6560e+12, 3.5586e+12,
        2.6211e+12, 2.4529e+12, 2.8006e+12, 1.8243e+12, 2.6316e+12, 2.6201e+12,
        2.2566e+12, 3.2693e+12, 2.6565e+12, 3.1400e+12, 3.3169e+12, 3.4328e+12,
        2.3395e+12, 2.6868e+12, 5.2190e+12, 4.2778e+12, 2.1979e+12, 3.9366e+12,
        2.5710e+12, 2.2799e+12, 3.5580e+12, 3.0725e+12, 3.6543e+12, 3.4545e+12,
        3.1797e+12, 2.2536e+12, 3.2643e+12, 1.8659e+12, 2.3262e+12, 3.3004e+12,
        2.3261e+12, 2.8541e+12, 1.6548e+12, 3.0490e+12, 2.0130e+12, 3.0420e+12,
        2.2001e+12, 5.2048e+12, 4.9888e+12, 4.6408e+12, 1.4339e+12, 2.3493e+12,
        3.2242e+12, 1.6044e+12, 2.1165e+12, 2.6222e+12, 3.7380e+12, 2.0112e+12,
        1.6648e+12, 3.0840e+12, 4.5481e+12, 4.5766e+12, 1.9820e+12, 3.4176e+12,
        3.0671e+12, 2.2677e+12, 2.6935e+12, 4.0263e+12, 1.5147e+12, 2.2420e+12,
        2.7758e+12, 3.5321e+12, 1.7835e+12, 4.6745e+12, 3.1423e+12, 2.3481e+12,
        3.3630e+12, 2.7783e+12, 4.7171e+12, 4.7280e+12, 4.4638e+12, 2.1704e+12,
        2.9741e+12, 2.3990e+12, 5.1280e+12, 1.3580e+12, 3.8750e+12, 3.4798e+12,
        1.9766e+12, 4.9333e+12, 3.4880e+12, 1.5622e+12, 3.1472e+12, 3.0118e+12,
        2.7479e+12, 1.8551e+12, 4.5319e+12, 2.1864e+12, 2.9815e+12, 2.7001e+12,
        3.9731e+12, 1.7909e+12, 2.1958e+12, 3.7169e+12, 2.6069e+12, 3.6473e+12,
        4.1322e+12, 2.9602e+12, 5.0350e+12, 2.3128e+12, 1.1995e+12, 3.5839e+12,
        2.7842e+12, 2.3170e+12, 2.1222e+12, 5.2284e+12, 7.0968e+12, 3.6955e+12,
        2.5620e+12, 3.8617e+12, 3.1136e+12, 3.7045e+12])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.6141e+10, 1.4340e+10, 2.0151e+10,  ..., 3.9705e+10, 1.3027e+10,
        2.1141e+10])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.1179e+10, 4.6203e+10, 4.8403e+10, 5.1488e+10, 3.7068e+10, 5.7891e+10,
        3.0957e+10, 4.7634e+10, 5.6020e+10, 8.3954e+10, 7.2449e+10, 8.9711e+10,
        4.6484e+10, 5.7713e+10, 9.0130e+10, 4.3906e+10, 7.6021e+10, 4.2146e+10,
        8.6203e+10, 4.4491e+10, 6.4199e+10, 7.0302e+10, 6.3606e+10, 7.2021e+10,
        9.0282e+10, 4.8821e+10, 6.8364e+10, 6.4382e+10, 5.4408e+10, 5.6927e+10,
        6.4831e+10, 6.3134e+10, 6.7535e+10, 5.0649e+10, 5.1722e+10, 9.2574e+10,
        5.6755e+10, 6.3647e+10, 4.1231e+10, 1.2087e+11, 8.2901e+10, 4.7564e+10,
        6.6999e+10, 6.7922e+10, 3.2876e+10, 8.2186e+10, 4.9482e+10, 7.5974e+10,
        5.5761e+10, 7.1755e+10, 6.9265e+10, 9.3330e+10, 5.3747e+10, 5.5397e+10,
        9.6627e+10, 5.4678e+10, 6.1941e+10, 5.1286e+10, 1.0288e+11, 8.7400e+10,
        5.4983e+10, 6.5405e+10, 9.8461e+10, 6.0919e+10, 8.8385e+10, 8.0489e+10,
        7.5872e+10, 4.1262e+10, 7.3432e+10, 8.6817e+10, 6.8492e+10, 4.1373e+10,
        8.8958e+10, 5.2342e+10, 5.1343e+10, 6.0361e+10, 5.1418e+10, 5.7341e+10,
        4.9277e+10, 1.1332e+11, 5.2942e+10, 3.5682e+10, 6.8743e+10, 4.3876e+10,
        5.4933e+10, 8.9106e+10, 8.2247e+10, 6.7903e+10, 8.4248e+10, 6.5928e+10,
        6.3641e+10, 5.4329e+10, 6.3959e+10, 5.6314e+10, 4.8908e+10, 6.9392e+10,
        3.1273e+10, 8.7091e+10, 8.9561e+10, 9.1441e+10, 4.6942e+10, 5.5579e+10,
        6.2284e+10, 4.7861e+10, 6.9870e+10, 4.8352e+10, 5.9429e+10, 3.2917e+10,
        8.7531e+10, 7.3705e+10, 6.0703e+10, 6.7500e+10, 7.1263e+10, 4.4297e+10,
        9.4384e+10, 5.0758e+10, 5.6386e+10, 6.4542e+10, 3.3492e+10, 7.9548e+10,
        3.3184e+10, 9.2202e+10, 4.1348e+10, 1.0943e+11, 5.8742e+10, 8.4128e+10,
        7.1292e+10, 1.2969e+11, 5.2464e+10, 8.4361e+10, 4.3327e+10, 5.1558e+10,
        8.1271e+10, 7.3002e+10, 4.4689e+10, 5.3016e+10, 1.0899e+11, 4.6317e+10,
        4.2906e+10, 8.3173e+10, 5.4315e+10, 6.3297e+10, 4.8590e+10, 5.7911e+10,
        5.9407e+10, 9.5996e+10, 8.7868e+10, 7.2251e+10, 4.1104e+10, 7.1505e+10,
        7.0740e+10, 7.8277e+10, 8.6974e+10, 5.9130e+10, 8.8023e+10, 4.4695e+10,
        6.9710e+10, 8.4557e+10, 5.3310e+10, 9.8045e+10, 8.5345e+10, 8.8490e+10,
        8.9334e+10, 4.0885e+10, 5.0982e+10, 8.8594e+10, 7.6055e+10, 7.1188e+10,
        5.2295e+10, 6.3836e+10, 6.0950e+10, 6.7422e+10, 6.1226e+10, 5.2855e+10,
        6.2531e+10, 5.8526e+10, 3.1034e+10, 7.8549e+10, 5.1386e+10, 4.8950e+10,
        8.4526e+10, 8.8731e+10, 4.4256e+10, 4.2995e+10, 4.7077e+10, 4.4844e+10,
        1.0801e+11, 3.7025e+10, 7.3595e+10, 7.5811e+10, 6.3197e+10, 6.1743e+10,
        4.6906e+10, 5.5087e+10, 6.3959e+10, 3.5729e+10, 6.6642e+10, 1.0690e+11,
        6.9622e+10, 4.6225e+10, 6.2184e+10, 7.8681e+10, 7.6305e+10, 6.7907e+10,
        5.8192e+10, 5.5894e+10, 6.0462e+10, 5.5581e+10, 5.6769e+10, 7.8442e+10,
        8.6325e+10, 7.2670e+10, 6.2868e+10, 9.6323e+10, 4.8755e+10, 6.4291e+10,
        6.8528e+10, 9.6471e+10, 3.9071e+10, 9.3920e+10, 4.3408e+10, 5.2373e+10,
        7.3400e+10, 4.7823e+10, 8.1016e+10, 7.3407e+10, 6.9855e+10, 5.3184e+10,
        4.8061e+10, 9.9713e+10, 7.5081e+10, 5.4643e+10, 5.5484e+10, 8.9039e+10,
        8.6200e+10, 4.6139e+10, 5.6857e+10, 8.3985e+10, 7.5168e+10, 8.0946e+10,
        4.7871e+10, 6.5439e+10, 6.4639e+10, 6.9998e+10, 7.0771e+10, 5.7844e+10,
        5.8533e+10, 1.0689e+11, 7.7313e+10, 6.6855e+10, 4.4543e+10, 6.4178e+10,
        4.3799e+10, 7.9089e+10, 7.7823e+10, 7.3361e+10])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.1923e+10, 8.2829e+10, 1.0658e+11, 6.4586e+10, 8.5172e+10, 7.9368e+10,
        9.0826e+10, 7.1736e+10, 7.7194e+10, 1.0363e+11, 8.8365e+10, 6.3279e+10,
        1.2176e+11, 7.4025e+10, 7.1917e+10, 9.7952e+10, 1.0352e+11, 9.9579e+10,
        1.0869e+11, 1.4143e+11, 1.1177e+11, 6.2023e+10, 1.1966e+11, 7.9781e+10,
        1.2264e+11, 9.5339e+10, 1.1202e+11, 1.1076e+11, 1.1040e+11, 1.1881e+11,
        8.4738e+10, 6.9561e+10, 1.0514e+11, 5.8005e+10, 6.3075e+10, 8.2707e+10,
        7.1165e+10, 1.2986e+11, 7.2463e+10, 1.5972e+11, 6.2003e+10, 1.5769e+11,
        9.9161e+10, 1.4854e+11, 1.2858e+11, 9.0308e+10, 1.3599e+11, 7.4298e+10,
        6.6675e+10, 6.5822e+10, 6.2001e+10, 8.7869e+10, 1.2660e+11, 9.7881e+10,
        1.0170e+11, 1.0187e+11, 1.1018e+11, 8.0563e+10, 9.1551e+10, 5.5020e+10,
        9.7139e+10, 8.0223e+10, 6.6120e+10, 1.0777e+11, 1.0085e+11, 9.8181e+10,
        1.0558e+11, 1.3087e+11, 8.4134e+10, 1.0495e+11, 1.1674e+11, 1.0306e+11,
        1.1356e+11, 4.2634e+10, 1.2312e+11, 1.0882e+11, 7.7071e+10, 7.6594e+10,
        1.1627e+11, 9.5148e+10, 9.7579e+10, 1.0414e+11, 1.5034e+11, 8.0944e+10,
        1.2793e+11, 7.7654e+10, 6.0676e+10, 1.3651e+11, 1.0858e+11, 6.7565e+10,
        1.2219e+11, 8.4400e+10, 1.0825e+11, 1.2615e+11, 8.4572e+10, 1.0983e+11,
        1.6590e+11, 1.3442e+11, 1.0001e+11, 6.6943e+10, 7.2317e+10, 9.3748e+10,
        8.5259e+10, 8.0763e+10, 9.7934e+10, 1.1426e+11, 9.7533e+10, 7.3127e+10,
        6.3864e+10, 8.8399e+10, 7.3241e+10, 1.4774e+11, 9.3838e+10, 7.4961e+10,
        7.7133e+10, 7.4971e+10, 7.9745e+10, 1.7895e+11, 9.8394e+10, 7.5397e+10,
        9.3304e+10, 1.4223e+11, 8.5601e+10, 1.0296e+11, 7.2619e+10, 1.0205e+11,
        7.8722e+10, 6.8188e+10, 6.3458e+10, 8.0907e+10, 1.1259e+11, 6.9673e+10,
        8.5707e+10, 1.3571e+11, 7.0731e+10, 8.3945e+10, 1.1072e+11, 1.1943e+11,
        8.7919e+10, 9.7648e+10, 8.4221e+10, 8.5788e+10, 1.3093e+11, 8.4254e+10,
        7.4735e+10, 8.1685e+10, 1.0391e+11, 7.3937e+10, 8.9510e+10, 9.0616e+10,
        1.0638e+11, 1.1123e+11, 7.9674e+10, 9.6131e+10, 7.0716e+10, 1.1166e+11,
        1.2319e+11, 8.3810e+10, 9.8409e+10, 5.9729e+10, 1.1761e+11, 1.1784e+11,
        9.0075e+10, 7.9941e+10, 7.7852e+10, 9.0181e+10, 6.5278e+10, 8.5466e+10,
        8.8542e+10, 1.4787e+11, 7.7931e+10, 1.1174e+11, 6.2505e+10, 8.7553e+10,
        1.2900e+11, 1.3602e+11, 1.0704e+11, 5.9988e+10, 7.7492e+10, 7.9878e+10,
        1.0914e+11, 5.9444e+10, 1.3459e+11, 1.3206e+11, 8.4322e+10, 1.3623e+11,
        1.0178e+11, 1.1504e+11, 8.5293e+10, 1.2443e+11, 8.6984e+10, 8.6007e+10,
        8.4269e+10, 1.1157e+11, 1.0640e+11, 6.7347e+10, 8.1895e+10, 8.1020e+10,
        9.1396e+10, 1.1279e+11, 1.2722e+11, 6.4974e+10, 9.5984e+10, 1.0254e+11,
        9.2924e+10, 8.0184e+10, 1.3212e+11, 9.3432e+10, 7.6811e+10, 6.2294e+10,
        9.2853e+10, 1.0407e+11, 9.4762e+10, 1.0261e+11, 8.1642e+10, 8.2085e+10,
        4.5017e+10, 9.7951e+10, 1.1240e+11, 1.4752e+11, 1.2325e+11, 5.5262e+10,
        7.5913e+10, 3.9966e+10, 8.4930e+10, 1.3494e+11, 1.2698e+11, 7.2338e+10,
        5.1690e+10, 9.6023e+10, 8.6625e+10, 9.0438e+10, 1.3299e+11, 9.7740e+10,
        6.7980e+10, 1.0563e+11, 8.5087e+10, 4.7968e+10, 1.0241e+11, 1.0687e+11,
        5.9217e+10, 9.8374e+10, 9.0859e+10, 4.4043e+10, 9.3667e+10, 7.2222e+10,
        6.7910e+10, 1.0305e+11, 9.8173e+10, 1.5046e+11, 6.7614e+10, 7.3310e+10,
        9.2290e+10, 7.8710e+10, 1.5336e+11, 7.0670e+10])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.4934e+08, 9.3019e+08, 1.9210e+08,  ..., 9.8136e+08, 1.0638e+09,
        1.1057e+09])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.3317e+09, 3.9771e+09, 3.1611e+09, 2.0185e+09, 2.2460e+09, 1.4432e+09,
        1.0073e+09, 2.6517e+09, 2.1119e+09, 1.6183e+09, 2.4796e+09, 2.1670e+09,
        2.4255e+09, 2.0584e+09, 2.5390e+09, 2.2255e+09, 2.6035e+09, 1.4671e+09,
        1.3386e+09, 3.5157e+09, 2.1481e+09, 1.2983e+09, 2.3291e+09, 1.7562e+09,
        1.6277e+09, 3.2877e+09, 2.1921e+09, 2.1331e+09, 2.4145e+09, 2.7806e+09,
        2.1925e+09, 2.7568e+09, 2.5227e+09, 1.5924e+09, 2.4370e+09, 2.2344e+09,
        1.9456e+09, 1.7774e+09, 2.5797e+09, 4.2595e+09, 2.5705e+09, 2.0662e+09,
        3.5866e+09, 2.1412e+09, 1.2101e+09, 1.8312e+09, 2.3463e+09, 1.8106e+09,
        2.1898e+09, 3.1259e+09, 3.2413e+09, 2.9814e+09, 2.8059e+09, 3.9156e+09,
        1.8871e+09, 1.5059e+09, 2.0612e+09, 1.6359e+09, 1.7298e+09, 2.1715e+09,
        2.8622e+09, 3.3463e+09, 1.4333e+09, 2.2095e+09, 2.9698e+09, 1.9275e+09,
        1.3183e+09, 1.4863e+09, 2.6006e+09, 1.7787e+09, 2.7955e+09, 1.6383e+09,
        1.4748e+09, 2.0665e+09, 2.7323e+09, 2.2196e+09, 1.2280e+09, 1.8996e+09,
        2.4543e+09, 4.3844e+09, 3.5088e+09, 2.6236e+09, 4.4616e+09, 1.1202e+09,
        2.2074e+09, 2.2903e+09, 2.5814e+09, 2.0576e+09, 1.8931e+09, 2.2881e+09,
        2.1415e+09, 1.3088e+09, 1.6780e+09, 2.7052e+09, 1.9810e+09, 3.2653e+09,
        1.1728e+09, 2.0288e+09, 2.3692e+09, 2.9898e+09, 1.6478e+09, 3.1131e+09,
        8.5293e+08, 2.0841e+09, 1.5234e+09, 1.3963e+09, 2.1156e+09, 1.0997e+09,
        1.1100e+09, 3.2443e+09, 1.3433e+09, 1.2554e+09, 2.5008e+09, 1.6734e+09,
        2.2368e+09, 3.6072e+09, 3.2285e+09, 3.3933e+09, 1.2029e+09, 2.6031e+09,
        2.5381e+09, 2.2132e+09, 2.8281e+09, 4.0446e+09, 2.6672e+09, 2.4238e+09,
        3.0474e+09, 2.0858e+09, 2.3920e+09, 2.6245e+09, 2.7262e+09, 1.6709e+09,
        2.6436e+09, 2.3869e+09, 1.7965e+09, 8.3451e+08, 2.3174e+09, 1.4876e+09,
        2.1731e+09, 1.6330e+09, 2.5014e+09, 1.5704e+09, 1.2953e+09, 1.0334e+09,
        1.4825e+09, 3.2874e+09, 1.5510e+09, 2.8875e+09, 3.9635e+09, 2.1831e+09,
        2.4265e+09, 1.7138e+09, 1.7784e+09, 2.0208e+09, 2.9615e+09, 1.5170e+09,
        2.4289e+09, 1.5223e+09, 1.4795e+09, 2.4593e+09, 7.5916e+08, 1.7754e+09,
        1.9613e+09, 2.9515e+09, 2.0921e+09, 1.5997e+09, 1.8499e+09, 3.3660e+09,
        2.6667e+09, 1.4959e+09, 1.3583e+09, 1.6234e+09, 2.3828e+09, 1.9403e+09,
        2.2904e+09, 2.3652e+09, 2.5443e+09, 3.0923e+09, 1.1946e+09, 1.6773e+09,
        2.4491e+09, 1.6476e+09, 2.5026e+09, 2.1167e+09, 2.1622e+09, 2.1227e+09,
        1.9195e+09, 2.2140e+09, 1.6635e+09, 3.0791e+09, 1.8553e+09, 1.4861e+09,
        2.4534e+09, 4.8293e+09, 2.6184e+09, 1.1375e+09, 3.1972e+09, 1.7013e+09,
        1.6252e+09, 2.2311e+09, 2.1214e+09, 2.0449e+09, 2.3387e+09, 3.4289e+09,
        2.7529e+09, 1.7675e+09, 2.6583e+09, 1.8506e+09, 9.6971e+08, 1.3194e+09,
        1.7794e+09, 2.1496e+09, 1.9222e+09, 2.0120e+09, 1.0569e+09, 3.9721e+09,
        1.4917e+09, 2.5044e+09, 1.5331e+09, 3.0408e+09, 2.0953e+09, 2.8919e+09,
        1.5516e+09, 3.4668e+09, 2.9397e+09, 1.8550e+09, 1.2406e+09, 3.1017e+09,
        2.6425e+09, 1.7727e+09, 1.2483e+09, 1.2897e+09, 1.3237e+09, 1.2996e+09,
        1.2651e+09, 1.6877e+09, 2.2281e+09, 3.5543e+09, 2.5453e+09, 2.9752e+09,
        2.2421e+09, 2.9630e+09, 2.0613e+09, 3.0045e+09, 1.1779e+09, 2.0372e+09,
        1.9439e+09, 2.7305e+09, 1.8784e+09, 3.6723e+09, 1.1128e+09, 1.4959e+09,
        2.4780e+09, 1.4904e+09, 1.5833e+09, 1.4072e+09])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.9335e+09, 4.0875e+09, 1.8579e+09, 2.4513e+09, 3.2341e+09, 3.9645e+09,
        2.6833e+09, 3.7600e+09, 3.0014e+09, 3.1405e+09, 1.6486e+09, 4.5326e+09,
        2.9973e+09, 2.8980e+09, 1.2600e+09, 2.1035e+09, 3.1823e+09, 3.7904e+09,
        2.6717e+09, 3.2573e+09, 3.5805e+09, 2.7007e+09, 4.8104e+09, 3.0149e+09,
        4.2150e+09, 3.3481e+09, 4.9333e+09, 4.7947e+09, 3.0098e+09, 1.9752e+09,
        2.8513e+09, 3.2332e+09, 3.5028e+09, 3.2687e+09, 2.0105e+09, 6.3672e+09,
        3.3947e+09, 3.9783e+09, 2.5748e+09, 3.7467e+09, 2.4709e+09, 6.8622e+09,
        2.9463e+09, 4.4007e+09, 6.7144e+09, 2.7187e+09, 2.4533e+09, 2.8154e+09,
        4.3060e+09, 1.5384e+09, 4.0017e+09, 3.7516e+09, 2.5775e+09, 3.1510e+09,
        4.4053e+09, 4.3526e+09, 4.1398e+09, 2.7578e+09, 4.5643e+09, 3.0203e+09,
        2.0206e+09, 3.5784e+09, 3.4133e+09, 5.3906e+09, 2.4929e+09, 1.9062e+09,
        3.1781e+09, 2.1261e+09, 3.5330e+09, 2.3350e+09, 2.0931e+09, 3.7121e+09,
        5.0216e+09, 4.9710e+09, 2.7136e+09, 3.1185e+09, 2.6599e+09, 1.6165e+09,
        1.6456e+09, 3.1255e+09, 2.6720e+09, 1.9337e+09, 2.0551e+09, 1.8888e+09,
        3.4644e+09, 3.7542e+09, 3.2085e+09, 4.5013e+09, 3.8131e+09, 4.3386e+09,
        3.2182e+09, 2.1303e+09, 2.3896e+09, 2.7366e+09, 2.8478e+09, 3.9448e+09,
        3.2852e+09, 2.1102e+09, 3.0115e+09, 3.3451e+09, 3.5398e+09, 2.0452e+09,
        1.7264e+09, 3.1982e+09, 2.5101e+09, 2.9151e+09, 4.8326e+09, 2.0194e+09,
        2.0798e+09, 2.0291e+09, 3.3300e+09, 1.7760e+09, 3.0596e+09, 1.6804e+09,
        2.8482e+09, 2.8610e+09, 2.5348e+09, 8.5037e+09, 3.9459e+09, 2.7154e+09,
        4.1156e+09, 1.9958e+09, 2.9769e+09, 3.6921e+09, 3.2750e+09, 3.1309e+09,
        4.1217e+09, 3.1659e+09, 3.1360e+09, 2.4898e+09, 1.7799e+09, 3.3093e+09,
        2.4403e+09, 4.7052e+09, 3.2090e+09, 3.5767e+09, 3.4746e+09, 2.3185e+09,
        2.4610e+09, 2.2920e+09, 3.9989e+09, 1.8903e+09, 2.9902e+09, 4.6083e+09,
        3.0960e+09, 3.4851e+09, 1.5717e+09, 3.6119e+09, 1.8869e+09, 3.9309e+09,
        4.2684e+09, 2.9555e+09, 3.3169e+09, 2.4574e+09, 2.5780e+09, 3.9253e+09,
        2.2203e+09, 2.8764e+09, 3.0448e+09, 4.6786e+09, 2.5828e+09, 2.9290e+09,
        2.6632e+09, 3.5023e+09, 3.2060e+09, 4.8149e+09, 2.0748e+09, 2.6161e+09,
        2.6299e+09, 2.7593e+09, 3.4692e+09, 2.1234e+09, 3.5062e+09, 1.9048e+09,
        3.0723e+09, 4.2257e+09, 4.0048e+09, 2.7106e+09, 4.4320e+09, 3.8684e+09,
        5.6127e+09, 3.1609e+09, 2.2070e+09, 1.7565e+09, 4.0197e+09, 2.4245e+09,
        1.7120e+09, 3.1725e+09, 2.6350e+09, 2.9483e+09, 3.0311e+09, 2.4328e+09,
        2.9220e+09, 2.6058e+09, 6.7603e+09, 2.2760e+09, 2.2890e+09, 2.4061e+09,
        2.4417e+09, 4.6053e+09, 3.3513e+09, 2.3190e+09, 2.7030e+09, 2.7223e+09,
        2.9014e+09, 4.2168e+09, 2.3259e+09, 4.0234e+09, 2.6396e+09, 3.3692e+09,
        5.3280e+09, 1.7339e+09, 2.4687e+09, 3.1577e+09, 2.7060e+09, 3.6321e+09,
        6.7665e+09, 3.0914e+09, 1.7215e+09, 3.5615e+09, 1.7905e+09, 6.0381e+09,
        2.5893e+09, 1.9608e+09, 3.9139e+09, 1.4385e+09, 2.7296e+09, 2.7009e+09,
        4.2064e+09, 2.9959e+09, 3.6151e+09, 5.3216e+09, 3.5475e+09, 2.6517e+09,
        3.7371e+09, 1.3530e+09, 3.3573e+09, 2.8228e+09, 1.9305e+09, 4.4188e+09,
        2.7879e+09, 3.3589e+09, 3.9660e+09, 4.5390e+09, 3.2732e+09, 2.2559e+09,
        4.0437e+09, 2.5220e+09, 2.6322e+09, 2.9920e+09, 3.0846e+09, 1.8656e+09,
        3.8842e+09, 2.7331e+09, 2.1701e+09, 3.1467e+09])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.7729e+07, 2.9972e+07, 9.9633e+08,  ..., 3.1700e+06, 1.6294e+07,
        1.4032e+07])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.2755e+08, 9.5049e+07, 1.1835e+08, 6.6335e+07, 6.3310e+07, 1.0600e+08,
        9.2259e+07, 6.6965e+07, 1.0436e+08, 1.0591e+08, 8.5984e+07, 1.1061e+08,
        1.0800e+08, 8.2905e+07, 1.1853e+08, 9.9902e+07, 9.9696e+07, 8.3094e+07,
        6.1746e+07, 8.0957e+07, 8.4888e+07, 7.1087e+07, 8.2122e+07, 1.0172e+08,
        5.9727e+07, 1.4032e+08, 1.4183e+08, 7.3748e+07, 9.5366e+07, 8.1133e+07,
        1.1284e+08, 6.0532e+07, 1.0071e+08, 9.0528e+07, 4.7970e+07, 5.5671e+07,
        5.6370e+07, 6.4148e+07, 4.5827e+07, 7.7063e+07, 4.1178e+07, 1.1085e+08,
        7.8515e+07, 6.4842e+07, 9.4577e+07, 1.1843e+08, 9.3234e+07, 7.4041e+07,
        3.4951e+07, 4.1130e+07, 5.2958e+07, 1.0352e+08, 1.1699e+08, 1.1953e+08,
        1.1400e+08, 1.1675e+08, 6.5335e+07, 9.4981e+07, 6.0605e+07, 1.3387e+08,
        1.2005e+08, 8.2632e+07, 5.9754e+07, 1.1967e+08, 1.1917e+08, 9.5322e+07,
        5.7261e+07, 8.4700e+07, 7.0180e+07, 6.3923e+07, 1.4500e+08, 6.3927e+07,
        8.5068e+07, 1.0813e+08, 6.7361e+07, 1.1309e+08, 7.1494e+07, 1.1864e+08,
        4.3402e+07, 7.2750e+07, 7.6626e+07, 6.5862e+07, 6.8765e+07, 6.4057e+07,
        6.2924e+07, 7.0590e+07, 9.4630e+07, 5.5826e+07, 9.0288e+07, 8.6694e+07,
        8.1874e+07, 7.3082e+07, 7.9442e+07, 8.4141e+07, 9.2768e+07, 1.5288e+08,
        8.2230e+07, 5.8771e+07, 1.0633e+08, 3.7780e+07, 1.3757e+08, 7.3725e+07,
        1.2620e+08, 8.4721e+07, 6.5359e+07, 6.5128e+07, 6.2444e+07, 8.7688e+07,
        8.0754e+07, 9.9353e+07, 1.1519e+08, 8.5746e+07, 8.1325e+07, 1.2765e+08,
        1.1389e+08, 8.4502e+07, 7.6407e+07, 5.9813e+07, 1.1244e+08, 9.0834e+07,
        5.5286e+07, 9.8315e+07, 6.4634e+07, 7.7147e+07, 5.6899e+07, 5.6913e+07,
        9.0586e+07, 5.3912e+07, 9.5430e+07, 1.0026e+08, 9.3217e+07, 1.1474e+08,
        7.7505e+07, 5.5642e+07, 3.5363e+07, 7.5536e+07, 4.1769e+07, 9.3796e+07,
        7.4748e+07, 6.8406e+07, 9.1237e+07, 1.0102e+08, 8.1241e+07, 4.2628e+07,
        1.5392e+08, 8.5581e+07, 5.6508e+07, 4.0952e+07, 8.8542e+07, 8.7052e+07,
        5.6915e+07, 6.5075e+07, 5.3020e+07, 5.7506e+07, 1.0333e+08, 5.6212e+07,
        1.5251e+08, 1.0716e+08, 9.6930e+07, 4.0551e+07, 6.2794e+07, 1.1303e+08,
        4.6262e+07, 7.3018e+07, 8.8699e+07, 6.5183e+07, 1.0318e+08, 1.2960e+08,
        8.6532e+07, 5.6815e+07, 9.3076e+07, 1.1505e+08, 5.5270e+07, 8.3045e+07,
        9.3583e+07, 6.0484e+07, 5.1125e+07, 7.6172e+07, 7.6762e+07, 1.2277e+08,
        8.7519e+07, 7.0405e+07, 6.6745e+07, 8.7151e+07, 7.9994e+07, 7.0718e+07,
        8.9674e+07, 4.8105e+07, 1.0378e+08, 9.9915e+07, 1.0619e+08, 9.3808e+07,
        1.2678e+08, 9.1467e+07, 7.0179e+07, 1.3640e+08, 1.3398e+08, 8.2750e+07,
        7.5705e+07, 1.3353e+08, 1.3151e+08, 6.8561e+07, 7.2770e+07, 3.6579e+07,
        1.0842e+08, 6.0284e+07, 9.2477e+07, 7.6204e+07, 6.6773e+07, 8.5890e+07,
        6.3726e+07, 6.4930e+07, 7.2825e+07, 8.9534e+07, 1.2049e+08, 8.3608e+07,
        8.9383e+07, 1.0494e+08, 9.9477e+07, 1.5292e+08, 1.0701e+08, 7.8543e+07,
        1.1511e+08, 9.1673e+07, 7.9892e+07, 1.5669e+08, 7.0903e+07, 6.0442e+07,
        5.9368e+07, 1.1283e+08, 8.9693e+07, 9.8935e+07, 1.2108e+08, 5.2425e+07,
        1.0370e+08, 9.2470e+07, 9.0690e+07, 1.0141e+08, 4.2673e+07, 8.5747e+07,
        1.0917e+08, 8.1410e+07, 7.6699e+07, 9.1878e+07, 9.3706e+07, 1.0206e+08,
        1.2110e+08, 8.0508e+07, 7.6267e+07, 9.8656e+07, 1.0611e+08, 7.8371e+07,
        9.7097e+07, 4.7689e+07, 7.6385e+07, 8.2386e+07])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.1303e+08, 1.0049e+08, 1.1808e+08, 1.3656e+08, 1.0286e+08, 6.3788e+07,
        1.5924e+08, 1.3512e+08, 1.2123e+08, 1.4108e+08, 1.0493e+08, 1.4907e+08,
        7.6143e+07, 9.1734e+07, 1.0444e+08, 1.1400e+08, 2.0188e+08, 1.0188e+08,
        8.5820e+07, 1.7013e+08, 1.7672e+08, 1.1265e+08, 1.2749e+08, 1.3361e+08,
        1.2273e+08, 1.3008e+08, 1.0067e+08, 1.4940e+08, 2.0290e+08, 9.9154e+07,
        8.0639e+07, 2.0180e+08, 1.6750e+08, 1.8031e+08, 1.0804e+08, 2.1104e+08,
        1.9920e+08, 9.3602e+07, 1.5574e+08, 1.5763e+08, 9.7174e+07, 1.3535e+08,
        1.1514e+08, 1.2913e+08, 9.0941e+07, 1.4407e+08, 1.2738e+08, 1.1331e+08,
        8.3825e+07, 1.0667e+08, 1.1127e+08, 1.3829e+08, 2.0174e+08, 8.2125e+07,
        7.9770e+07, 1.7204e+08, 1.5227e+08, 6.7073e+07, 1.3215e+08, 1.0329e+08,
        1.3306e+08, 1.5692e+08, 1.1957e+08, 7.4407e+07, 1.0648e+08, 1.7636e+08,
        1.3853e+08, 1.4034e+08, 1.4691e+08, 1.6880e+08, 2.0020e+08, 1.0336e+08,
        1.2508e+08, 1.6978e+08, 2.2583e+08, 1.3436e+08, 1.9345e+08, 8.2127e+07,
        1.6717e+08, 1.3888e+08, 1.5857e+08, 1.2821e+08, 1.3521e+08, 8.7568e+07,
        1.4544e+08, 1.2162e+08, 1.2674e+08, 1.5414e+08, 1.1093e+08, 1.5197e+08,
        1.0836e+08, 9.9772e+07, 1.1691e+08, 1.2518e+08, 1.2193e+08, 1.0640e+08,
        1.8926e+08, 7.3970e+07, 2.2476e+08, 1.5802e+08, 1.3189e+08, 1.3105e+08,
        1.5572e+08, 7.2257e+07, 6.4481e+07, 1.0943e+08, 1.3387e+08, 1.5655e+08,
        8.7337e+07, 1.3336e+08, 1.1394e+08, 8.1357e+07, 2.0859e+08, 1.2506e+08,
        1.9537e+08, 5.2942e+07, 1.0732e+08, 9.5029e+07, 8.9119e+07, 1.1263e+08,
        1.5766e+08, 2.3749e+08, 1.3958e+08, 9.8541e+07, 1.2423e+08, 8.4153e+07,
        8.4775e+07, 1.6946e+08, 1.0956e+08, 2.0096e+08, 1.6243e+08, 1.6855e+08,
        2.5319e+08, 1.3636e+08, 1.7522e+08, 2.2926e+08, 1.0911e+08, 7.7193e+07,
        9.0712e+07, 1.2573e+08, 1.2125e+08, 1.4016e+08, 1.3036e+08, 7.6623e+07,
        1.4429e+08, 1.1811e+08, 7.8608e+07, 1.4017e+08, 1.7401e+08, 1.8493e+08,
        1.1405e+08, 8.8359e+07, 7.6033e+07, 1.3785e+08, 1.6632e+08, 7.6664e+07,
        7.0461e+07, 1.1097e+08, 1.7175e+08, 1.0980e+08, 9.0399e+07, 1.1372e+08,
        2.0041e+08, 1.7121e+08, 1.5545e+08, 1.3650e+08, 1.2579e+08, 7.7310e+07,
        1.1215e+08, 8.4754e+07, 8.9471e+07, 1.6271e+08, 1.2151e+08, 1.9030e+08,
        9.3815e+07, 1.5138e+08, 1.6740e+08, 1.2696e+08, 1.3847e+08, 1.4340e+08,
        1.5017e+08, 1.6104e+08, 1.2223e+08, 8.9799e+07, 1.5520e+08, 1.6645e+08,
        1.0142e+08, 1.1806e+08, 1.3553e+08, 8.1185e+07, 1.0656e+08, 1.3449e+08,
        9.4055e+07, 1.7840e+08, 1.0998e+08, 1.4229e+08, 8.4180e+07, 1.1652e+08,
        1.0421e+08, 1.1217e+08, 1.0504e+08, 1.4294e+08, 1.1518e+08, 1.0797e+08,
        6.4842e+07, 1.5068e+08, 1.6302e+08, 7.7298e+07, 8.9307e+07, 1.0525e+08,
        1.2005e+08, 1.4845e+08, 1.2420e+08, 1.6446e+08, 1.0694e+08, 1.2686e+08,
        1.6282e+08, 1.6094e+08, 9.8849e+07, 1.0939e+08, 9.8732e+07, 1.1375e+08,
        8.6225e+07, 1.4919e+08, 1.2375e+08, 6.8599e+07, 8.5971e+07, 1.1935e+08,
        1.1050e+08, 1.0726e+08, 2.3818e+08, 9.9198e+07, 1.1884e+08, 1.1344e+08,
        9.0935e+07, 8.0123e+07, 1.3633e+08, 1.3190e+08, 1.2267e+08, 1.4826e+08,
        8.5937e+07, 6.8036e+07, 2.0183e+08, 1.1433e+08, 9.5770e+07, 2.0995e+08,
        1.5425e+08, 1.4516e+08, 8.8619e+07, 1.2883e+08, 1.7066e+08, 1.1804e+08,
        8.4727e+07, 1.6995e+08, 1.2616e+08, 1.3790e+08])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1658364.1250, 1796237.6250, 1512307.0000,  ...,  569220.2500,
        2471489.2500, 3514622.5000])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1602215.1250, 3880448.0000, 4785503.0000, 1605151.5000, 3407053.5000,
        4145907.0000, 3404219.0000, 4484203.0000, 4512431.0000, 3816038.7500,
        5235756.0000, 4229534.0000, 2460844.7500, 2677584.2500, 3514908.2500,
        2448010.2500, 2984723.7500, 4471012.0000, 5071930.5000, 4509397.5000,
        4812514.0000, 2764448.0000, 3045314.2500, 3897333.5000, 2039690.1250,
        2819103.2500, 4620018.5000, 4531622.0000, 3067220.0000, 4456416.5000,
        4063017.5000, 2624092.5000, 4700829.5000, 2733357.2500, 4948189.5000,
        3995311.5000, 2247317.5000, 3274323.5000, 3195970.5000, 5374034.0000,
        2774829.2500, 3425357.2500, 2746728.5000, 3365272.7500, 4600524.5000,
        3129179.2500, 2762325.5000, 4911070.5000, 3885423.5000, 1580981.3750,
        2633257.5000, 4336937.5000, 2470086.7500, 3172154.0000, 2653794.5000,
        3212854.2500, 3260162.0000, 2971887.2500, 3229298.7500, 3623761.2500,
        2428589.2500, 3156133.7500, 2658584.5000, 2531672.0000, 2367543.5000,
        2921848.5000, 5119597.0000, 3214882.5000, 4655603.5000, 2503166.2500,
        2355628.2500, 2677838.5000, 3058718.2500, 2541969.7500, 3635122.7500,
        3781236.5000, 3128991.0000, 2613860.7500, 1803752.8750, 1558892.6250,
        3860151.0000, 2895092.5000, 2169322.0000, 5055408.0000, 2207061.2500,
        4035075.0000, 2420628.2500, 4361911.5000, 3399376.5000, 4560103.0000,
        3800115.2500, 2922178.7500, 2506574.0000, 4013628.5000, 2094580.3750,
        2999088.0000, 3592952.7500, 2812798.0000, 3906102.7500, 2262678.0000,
        3474968.2500, 3324956.7500, 3607113.0000, 4316645.5000, 3811218.5000,
        3581627.7500, 2073458.0000, 3157922.7500, 6434379.0000, 3793291.7500,
        5217476.0000, 4681563.0000, 2479508.7500, 2814515.5000, 3372921.5000,
        5363432.5000, 3349177.7500, 2317131.5000, 4383318.5000, 3244414.7500,
        1868030.0000, 4372285.0000, 3421796.7500, 3761723.2500, 5566705.5000,
        2547263.0000, 3244422.7500, 3624731.5000, 2755394.7500, 3138159.2500,
        3097068.5000, 4663351.5000, 2143989.5000, 4294761.0000, 2315761.5000,
        2335550.5000, 1660976.5000, 2647379.0000, 2168066.7500, 5181792.0000,
        4063861.7500, 4475238.5000, 2141204.5000, 2239270.0000, 2402317.2500,
        2339808.7500, 2206303.5000, 3073018.2500, 3396056.5000, 3043255.0000,
        2198006.2500, 4092402.5000, 4610040.0000, 3218479.2500, 2347325.5000,
        3150003.7500, 4275317.0000, 2797518.0000, 4181998.5000, 6981038.5000,
        3672413.5000, 2343053.7500, 3575478.2500, 2890641.0000, 2519855.2500,
        2152845.2500, 5906033.0000, 1803893.2500, 5660495.5000, 4496439.5000,
        2823380.2500, 4446248.5000, 3191928.5000, 4995865.0000, 5305323.0000,
        4682000.5000, 1665369.0000,  914569.5625, 4589072.5000, 3745016.5000,
        3846593.5000, 2019030.0000, 2819600.0000, 4416121.0000, 4079514.0000,
        2477608.0000, 1206465.6250, 2953653.7500, 3093358.0000, 1784146.2500,
        3857652.5000, 3193224.7500, 3489103.7500, 2703382.7500, 2871916.0000,
        3438445.5000, 2337200.5000, 3634399.7500, 2332688.5000, 4996058.5000,
        3610164.7500, 1966857.5000, 2629052.2500, 2215524.0000, 3786137.0000,
        4371527.5000, 2353171.5000, 2634774.5000, 4117159.0000, 1930012.3750,
        3456870.5000, 1437257.7500, 1840404.7500, 2556450.7500, 3485771.5000,
        1738699.3750, 3766189.5000, 3014445.5000, 1952050.8750, 3350156.0000,
        5100377.0000, 4603113.0000, 3083201.0000, 3899444.7500, 3570610.2500,
        3243183.0000, 2840875.7500, 2957671.0000, 6385788.5000, 3207677.5000,
        4299764.0000, 2656206.2500, 3299461.5000, 5191838.5000, 3705511.7500,
        3782720.0000, 3578376.2500, 3190231.2500, 3461901.7500, 2441969.0000,
        3318137.7500, 4076154.0000, 3528516.2500, 2645198.5000, 2628612.5000,
        3393868.7500, 3050228.2500, 4968250.5000, 2681565.5000, 3554385.0000,
        2850932.7500, 6528825.0000, 2283410.0000, 2729480.0000, 4684020.5000,
        2110717.7500, 4336572.5000, 3589142.7500, 3377346.2500, 3728585.0000,
        3855644.5000, 2465387.7500, 3048598.0000, 2465558.0000, 5107642.0000,
        3962643.5000, 4240336.5000, 4043694.5000, 4850366.0000, 4011788.5000,
        4605810.5000, 3638314.5000, 2734865.5000, 4630646.0000, 3442520.5000,
        3492961.5000, 2729681.2500, 3587057.0000, 6448253.0000, 4089275.7500,
        5294995.0000, 2844133.7500, 2399004.2500, 1993679.2500, 2369694.2500,
        4704006.0000, 4696574.5000, 4083264.0000, 1168624.0000, 3807132.7500,
        2747126.2500, 2865970.5000, 2691848.7500, 4339630.0000, 4347703.0000,
        2776229.0000, 5222709.0000, 3471293.7500, 2838510.0000, 3297006.7500,
        2950211.2500, 4615377.5000, 3241815.0000, 2795032.7500, 2721359.5000,
        1951697.1250, 2321610.0000, 2946455.0000, 4852684.5000, 3451480.0000,
        2554802.5000, 1731159.6250, 1807118.1250, 2736682.7500, 5068475.0000,
        1794450.8750, 3014538.7500, 1236048.7500, 2865119.7500, 2367418.5000,
        2307861.2500, 2781293.5000, 3080381.7500, 3386991.7500, 2565570.7500,
        3781216.2500, 3093779.7500, 3015822.7500, 4155401.2500, 2667868.0000,
        3339078.2500, 3135245.0000, 4289244.0000, 1842063.8750, 3318705.5000,
        3500459.5000, 4761263.0000, 4225809.5000, 4218666.0000, 2176493.2500,
        4825227.5000, 3775859.5000, 1283530.2500, 4948265.5000, 5529066.5000,
        4291355.0000, 2950676.2500, 2452238.0000, 4710945.5000, 4238978.5000,
        1685342.7500, 4275509.5000, 5909153.5000, 4006303.7500, 2862639.5000,
        3955314.5000, 3604761.5000, 3516541.0000, 2503102.7500, 3543493.7500,
        2406239.2500, 3736207.2500, 4278128.5000, 3146681.5000, 2136724.2500,
        6064286.0000, 3670138.7500, 4572936.5000, 3179138.2500, 3126303.7500,
        3359246.2500, 2397233.0000, 2225220.5000, 1835299.7500, 4247166.0000,
        2691010.5000, 2096549.3750, 2935118.5000, 4076118.0000, 3157243.5000,
        2530830.2500, 2624325.5000, 5088399.0000, 2204947.7500, 3284202.2500,
        1732056.0000, 3519327.7500, 2605107.7500, 2439262.0000, 2508242.5000,
        2265021.0000, 2797400.0000, 4759738.5000, 2973112.2500, 2979746.2500,
        5632605.5000, 4529313.0000, 4576012.5000, 3941271.7500, 2643925.5000,
        3248631.7500, 4657472.5000, 2281761.0000, 4701719.0000, 1470446.5000,
        4570356.5000, 2643460.7500, 4159271.2500, 1684158.0000, 3882949.2500,
        2990812.0000, 3078465.2500, 4333709.5000, 5538748.5000, 2655509.5000,
        2289645.7500, 2749793.5000, 2728724.0000, 3469063.2500, 3986693.0000,
        4831331.5000, 1865881.1250, 3821982.5000, 4820648.0000, 3411719.0000,
        2410115.0000, 3793748.7500, 4355966.0000, 1942507.2500, 2840361.7500,
        2604109.2500, 2111144.0000, 4256261.5000, 3982797.5000, 3173796.7500,
        2069714.8750, 2195607.5000, 3238202.5000, 5039098.0000, 2602238.2500,
        4941864.5000, 4094984.5000, 2698659.2500, 4071317.7500, 2669292.5000,
        5350892.5000, 3423796.7500, 2605289.7500, 4641384.5000, 2724778.5000,
        2440264.0000, 3109970.7500, 2644505.7500, 2843247.7500, 2935833.0000,
        3149033.0000, 4253423.0000, 3674607.5000, 5263580.5000, 3464309.7500,
        2819736.2500, 1709610.8750, 4441554.5000, 2367799.7500, 3178198.0000,
        3062781.7500, 1605954.8750, 2687933.2500, 3552237.7500, 3685295.5000,
        3869698.2500, 6367780.5000, 3184949.0000, 3222655.7500, 5253657.5000,
        3701645.2500, 2652124.7500, 3594128.7500, 2001606.3750, 2473950.7500,
        5024252.5000, 2450329.7500, 4509706.5000, 5228393.0000, 4391876.5000,
        2308910.2500, 2882666.0000, 4305362.5000, 3108856.7500, 2740547.0000,
        4324285.0000, 2073112.1250, 3318412.2500, 4236352.5000, 4076006.0000,
        2873681.0000, 3019468.0000, 3934412.7500, 1749416.0000, 3163853.7500,
        5033180.5000, 6287573.0000, 3261236.5000, 2541245.0000, 4315911.5000,
        4142250.7500, 5514455.0000, 4453832.0000, 2598489.7500, 2556073.7500,
        2651142.2500, 2102479.7500])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 5114628.0000, 13067298.0000,  6334016.0000,  5382093.5000,
        10841686.0000,  8668214.0000,  3969751.7500, 10275019.0000,
         6479682.0000,  6229760.5000, 12660861.0000,  8930964.0000,
         7386475.5000,  3636721.0000,  6860331.5000,  7759064.0000,
        10919947.0000,  8398499.0000,  3655059.2500,  4374563.5000,
         9921808.0000,  4929517.0000,  6802172.5000,  6760450.5000,
         7497754.0000,  4975908.0000,  9354666.0000, 13393171.0000,
         8086162.5000,  5135389.0000,  9701328.0000,  6195694.0000,
         6027530.0000,  6532382.5000,  5074976.5000, 10257772.0000,
         4636717.0000,  9268350.0000,  8820299.0000,  4044068.2500,
         6434164.0000, 10604969.0000,  4740942.0000, 12255380.0000,
        10902241.0000,  7337513.0000,  8679003.0000, 10295317.0000,
         4255697.5000,  5625878.5000,  7797538.0000,  5940026.0000,
        13366191.0000,  7091008.0000,  9622245.0000,  7096531.0000,
         9732752.0000,  9849086.0000,  8330218.0000, 12876670.0000,
         7157361.5000,  6992603.5000,  6506366.5000,  3282093.5000,
         5696390.5000,  7185138.0000,  9767097.0000,  9067856.0000,
         7080266.0000,  9374233.0000,  3315760.7500, 13967364.0000,
         5634528.5000,  5845163.5000,  4890572.0000, 14251361.0000,
         3043585.7500,  8895673.0000,  6403653.5000,  6011922.5000,
         7771716.5000,  7895095.0000,  7159372.5000,  4042392.2500,
         8348756.5000,  6924369.5000,  4763354.0000,  9295219.0000,
         6249549.5000,  5966683.5000,  5507588.0000,  7579088.0000,
         8424771.0000,  7012398.0000,  8115542.0000,  4695369.0000,
         5741346.5000,  5423969.0000,  4092575.2500,  8272077.5000,
         8455333.0000,  6313574.5000,  8102999.5000, 11196107.0000,
         9860515.0000,  4487190.5000,  7211812.0000,  6324118.5000,
         2643410.0000, 10416413.0000,  6340550.5000, 11228685.0000,
         6040927.0000, 11259543.0000,  6532729.5000, 10242962.0000,
         5812172.5000,  4262319.5000,  9549323.0000,  9246204.0000,
         7300790.5000,  4318059.0000,  8922474.0000,  4011877.0000,
         7318849.5000,  7963273.5000,  6659827.0000,  7866177.5000,
         7663402.0000,  8083625.5000,  4510669.0000,  6129374.5000,
         8451776.0000,  8344937.5000,  5325777.0000, 13165274.0000,
         5081939.0000, 13114214.0000,  7686722.0000,  7396231.0000,
         6262200.0000,  6802013.5000,  7511746.5000,  8570987.0000,
         5110083.0000,  6993988.5000, 10110363.0000,  5857643.5000,
         5141436.0000,  5406697.0000,  7958465.5000,  9420606.0000,
         7886118.0000, 11441485.0000,  6231400.5000,  9177352.0000,
         4652620.0000,  4294382.5000,  8110900.0000,  5905504.5000,
         6723985.0000,  7292162.5000,  4372287.5000,  5893117.5000,
         5757714.0000, 14700181.0000,  3800405.2500,  6990582.5000,
         7686629.5000,  5478843.0000, 10402617.0000,  7416179.0000,
         3812560.0000,  7655987.5000,  9509538.0000,  7660653.0000,
         9072388.0000,  5546669.5000,  7886364.5000,  7794153.0000,
         8914252.0000,  5433867.5000, 11551571.0000,  9252137.0000,
         8182813.5000,  9068454.0000,  8036020.0000,  4716847.5000,
         6265583.0000,  4207128.0000,  6120954.5000, 10315215.0000,
         7278861.5000, 10209188.0000,  8005271.0000,  4800507.0000,
        11470279.0000,  4476239.0000,  9977190.0000,  2212257.7500,
         6353379.0000,  8164483.0000,  6179249.0000,  4228052.0000,
         8631122.0000, 10419688.0000,  4868815.0000,  7865021.0000,
         7560653.5000,  4922939.5000,  6256308.0000,  6984598.0000,
        11945898.0000,  6538782.5000,  5499408.5000,  4960265.0000,
         8768059.0000,  6034850.5000,  8465903.0000,  8443989.0000,
        14081367.0000,  6137306.0000,  7977713.5000,  7269195.0000,
         6111776.0000,  7654499.5000, 12018718.0000,  8661036.0000,
         6986710.5000,  7018823.0000,  3705606.5000,  7289585.0000,
         6549052.5000,  6991093.0000,  7528082.5000,  7822689.0000,
        10290549.0000,  7337369.0000,  6408846.5000,  8599307.0000,
         4105603.7500,  7192043.0000,  4340154.0000,  8009185.5000,
         8963029.0000,  4905909.5000,  8727973.0000, 12446025.0000,
         6107516.5000, 10213278.0000,  7901691.5000, 11805168.0000,
         6683316.0000,  8996049.0000,  3962234.0000,  8999315.0000,
         5235897.5000,  6536156.0000,  8380988.0000,  4688758.0000,
         6456830.0000,  9850783.0000,  4802061.0000, 12614665.0000,
         7144037.5000,  6288044.5000,  8019047.0000,  7793759.0000,
         7422657.5000,  6831208.5000,  9537486.0000,  8061539.0000,
         8170349.0000,  4865228.5000,  4818992.5000,  8201098.5000,
        11207973.0000,  4974430.5000,  5569336.0000,  7183170.5000,
         6975326.5000,  6219830.5000,  7666597.0000, 10200717.0000,
         8162616.5000,  8706750.0000,  7046271.0000,  9956996.0000,
         3326679.7500, 12235381.0000,  4615637.0000,  8429873.0000,
         8510983.0000,  4315386.5000,  4778086.5000,  5316414.5000,
        10909975.0000,  4139184.7500,  5677394.5000,  9197487.0000,
         6461203.5000,  7683796.5000,  3825874.2500,  5229736.5000,
         6370826.0000,  6706370.0000,  1808594.5000,  4734196.0000,
         4965689.0000,  6886774.0000,  3790457.2500,  5336906.5000,
         6350086.0000,  4108141.0000,  8090231.5000,  3528848.0000,
         7913654.5000,  3981895.5000,  8852147.0000, 11909735.0000,
         5976723.5000,  5732411.0000,  4601730.5000,  4389180.5000,
         5710073.5000,  8510616.0000,  9567620.0000, 14612050.0000,
         9345201.0000,  3942821.7500,  8083278.5000,  7267735.0000,
         9836384.0000,  3117918.2500,  5113820.0000, 10248912.0000,
         3591925.2500,  4544468.0000,  8308646.5000,  7638628.5000,
         8344505.0000, 11408870.0000,  3993758.7500,  8698034.0000,
         7007517.5000,  7843838.0000,  7050712.5000,  5595355.5000,
         9393933.0000,  7731264.0000,  9139629.0000,  5540328.5000,
         6597637.5000,  7831419.5000, 11259202.0000, 12054957.0000,
         6785468.0000,  4656744.0000,  7320994.5000,  5607631.0000,
         5087209.5000,  8036326.5000,  6262407.5000,  6422086.0000,
        10988941.0000,  7865419.0000,  6589864.5000,  3966721.0000,
         3672705.5000,  6205475.0000,  6441788.0000,  4578475.0000,
         3754429.0000,  3702249.2500, 12116642.0000, 10978416.0000,
         4911827.0000,  6938973.5000,  8143002.5000,  7611319.5000,
         6798615.5000,  5409365.5000,  6990197.5000,  6503299.0000,
         6545510.5000, 11345228.0000, 10252035.0000,  7493388.5000,
         5080455.5000,  5333888.5000, 11367792.0000,  5584683.5000,
        11845897.0000,  7653896.5000,  6115060.5000,  4360166.0000,
         7969221.5000,  5796007.0000,  9048403.0000,  3874626.5000,
         5159742.5000,  5369175.0000,  7327299.0000,  7869722.5000,
         8209960.0000,  4587183.0000,  8256526.0000,  6287242.5000,
         5271090.5000, 14557344.0000,  3956870.0000,  7971901.0000,
         8482396.0000,  7000018.5000,  4281951.5000,  7868803.0000,
         7585455.5000,  7097423.5000,  5378804.0000,  5561852.5000,
         4662913.0000,  4652530.0000,  6084345.0000,  9010407.0000,
        12505987.0000,  5967939.0000,  9805830.0000,  8330408.0000,
         5414405.5000,  8166427.5000,  6407074.5000,  9213656.0000,
         7212688.5000,  5283993.5000,  7407522.0000,  7385814.0000,
         7600792.0000,  8577056.0000,  4914873.5000, 11456537.0000,
         8991702.0000,  2875831.7500,  9657358.0000,  8908761.0000,
         8995941.0000,  9298577.0000,  7696258.5000,  8586570.0000,
         3431621.2500,  7649322.5000,  9464962.0000,  5217018.0000,
         3975898.0000,  7764851.0000,  6614220.0000,  6172465.0000,
        11271913.0000,  6373857.0000,  7908960.5000,  6258180.0000,
         6468765.5000,  6754560.5000,  5029233.5000,  9335315.0000,
         8626341.0000, 11393836.0000,  8901507.0000,  6445763.5000,
         4536010.5000, 10043331.0000,  7276159.0000,  4324069.5000,
         3826439.5000,  5716040.0000, 10338133.0000, 10131326.0000,
         7026797.0000,  6517553.0000,  9380446.0000,  7251826.5000,
         7496023.0000,  5851317.5000,  4530994.5000,  3622312.0000,
         6415221.5000,  6251164.0000,  4906799.5000,  5835909.5000,
         4184610.5000,  5021030.5000, 10246116.0000,  4394895.0000,
         6440918.5000,  7049241.0000,  6215053.5000,  9219631.0000,
        10208059.0000,  7403114.0000,  6913871.5000,  5749337.0000,
         7582585.5000,  6660935.5000,  5861022.0000,  7927236.0000,
         9383993.0000,  9456911.0000,  8703778.0000,  7330825.5000,
         4888742.0000,  8978438.0000,  8656390.0000,  7610149.5000])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([188307.6406,   4967.3086,   3814.5908,  ...,  25488.2461,
          4196.8521,   3716.1257])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([163494.5625,   6885.6719,  12849.6426,  ...,  56303.6094,
         27484.6836,  15457.3350])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([38263.1953, 42031.5195, 30716.9590, 25542.9824, 50341.0430, 41305.5195,
        26295.9355, 45570.0547, 21636.0645, 43517.8438, 35017.5195, 62884.8516,
        30611.1113, 34026.3828, 36743.3125, 38120.1836, 40615.9219, 39780.5234,
        39483.9570, 25433.4023, 34077.7852, 26258.0293, 36527.2852, 26221.0293,
        32525.2383, 47353.0625, 26990.3066, 47290.2773, 50354.1562, 50707.4180,
        49260.7344, 45841.3672, 23678.1426, 45676.1680, 36755.8281, 55715.7695,
        23544.5898, 48517.3438, 26042.8438, 33644.8203, 23826.4785, 49835.1523,
        32746.1660, 26534.1953, 39322.3516, 50845.1641, 22874.7676, 36656.0391,
        26300.6660, 17617.4375, 23107.4746, 41201.4805, 48575.9648, 36165.8945,
        46213.7031, 35414.1094, 39001.4844, 43122.6992, 33889.6992, 40507.6484,
        30970.2559, 22925.3379, 38339.5117, 21193.8574, 29641.2441, 57469.0508,
        25112.1289, 36633.0117, 49732.0586, 36249.5664, 46089.4531, 29986.3516,
        22776.7520, 38431.5000, 33853.3477, 25496.2832, 28063.5840, 19559.3398,
        30395.6914, 50552.6055, 52371.9883, 40270.1562, 33078.3438, 44443.9805,
        30093.2754, 20730.5293, 23863.4277, 45477.9609, 38972.6172, 35413.6133,
        20563.5723, 33722.8672, 31998.1602, 55389.7812, 32073.6367, 38845.6641,
        27387.0059, 35952.9062, 46549.6055, 33486.3047, 37667.2578, 25582.6895,
        23242.2812, 50086.0039, 15659.1201, 23783.4473, 36697.9180, 27581.0195,
        29102.7266, 21851.9785, 34547.1484, 42146.9688, 73957.1172, 37551.1211,
        41213.3750, 36781.3672, 31772.2363, 39408.7383, 29578.0215, 20035.3867,
        38819.5977, 22274.4121, 34401.3984, 25781.9336, 45094.6328, 48124.3594,
        38949.4062, 46183.1562, 29478.7090, 34037.9297, 46694.3945, 33232.1797,
        40463.1016, 57326.4648, 31969.9609, 44251.6914, 44548.8711, 38520.1836,
        31247.8613, 42239.8906, 29265.8633, 32699.5625, 46212.1133, 19580.3125,
        24655.4453, 35929.8203, 46954.8711, 30692.0996, 24807.0410, 35318.9375,
        53230.7812, 32551.6562, 48490.5859, 45606.3203, 36443.0938, 33148.8047,
        39859.6719, 30231.2773, 25760.1797, 61974.5781, 28875.5371, 27130.8105,
        27288.8301, 37971.3633, 23475.7617, 25899.4785, 27868.4160, 21157.4219,
        27261.4609, 43212.2930, 27874.0332, 67082.6562, 22801.9434, 25703.2012,
        22524.3262, 30598.4062, 25738.1035, 51517.0703, 30325.4375, 38334.9570,
        34232.4258, 52986.2500, 31722.6055, 41002.9219, 36590.8320, 42720.1797,
        45839.4180, 28494.5488, 36689.6641, 23813.6152, 22925.1699, 25168.3203,
        26208.8848, 18479.9199, 32453.8613, 24491.3633, 25620.1348, 21581.7305,
        35381.6992, 26037.1016, 36101.1094, 31927.0391, 76248.3906, 34394.4609,
        45265.0547, 30355.5840, 60293.9727, 42852.0664, 25523.9180, 29080.0234,
        35558.8555, 23722.1934, 37353.2891, 18838.6465, 27373.6875, 47064.6367,
        44132.1953, 35954.9648, 38327.1797, 22625.4414, 35993.6406, 55519.3828,
        48712.6328, 23276.7754, 33055.1836, 44198.3906, 34776.1602, 28376.3496,
        33378.4219, 30047.3848, 27368.0254, 26966.7188, 17130.2676, 25208.1660,
        55185.2891, 31095.5605, 53948.1406, 43463.1484, 49763.0625, 46068.9766,
        48653.3008, 33104.1445, 46358.8164, 54703.9375, 38534.2852, 23484.9941,
        37574.2148, 32871.9023, 24662.0781, 42837.6758, 59699.9180, 44493.3477,
        23851.8691, 39408.1016, 23810.2090, 34451.0469, 21349.9414, 21583.5977,
        36282.0352, 44847.5156, 33490.5586, 43867.6914, 28789.2480, 37656.1055,
        20836.0293, 17281.0742, 42956.4844, 42080.2617, 23805.7441, 36445.6055,
        28011.4492, 43059.6953, 37430.4727, 19947.3281, 44203.4336, 32156.9844,
        28701.3242, 46639.8477, 48547.8516, 56786.0273, 30567.4961, 38816.8281,
        24249.2910, 66262.9922, 45955.0703, 50146.2930, 41676.6953, 26175.2090,
        32911.9688, 26222.3906, 42494.6602, 32474.2578, 40871.5625, 27249.0293,
        17346.3008, 22969.8203, 19181.7617, 17827.1953, 23652.8086, 54947.2500,
        27666.2305, 34348.8672, 33151.3594, 31927.1543, 29906.0898, 32906.3086,
        24092.0352, 28359.0801, 43031.9102, 34127.3008, 29129.0996, 41514.3516,
        47869.6562, 21204.2871, 38636.0938, 51241.5664, 37718.6992, 22342.5332,
        62857.5664, 33490.0039, 28204.5547, 30817.7734, 23584.5410, 32048.3945,
        39022.5039, 33108.6680, 21474.4727, 32886.8516, 30008.4863, 56627.9102,
        16974.4746, 39520.8203, 35071.4570, 43926.7617, 25008.5586, 43044.8516,
        25169.5762, 30245.6504, 25226.9062, 26905.9277, 28498.5781, 41913.4023,
        21235.9297, 34716.8906, 48115.4414, 27558.0723, 27466.1328, 48986.7305,
        56368.5938, 44918.5625, 48375.4180, 31692.2656, 24679.7520, 26885.6016,
        30225.0293, 27224.2715, 37807.9727, 41787.9258, 22361.2695, 63491.0273,
        40384.2734, 41049.1094, 47253.4375, 48299.6328, 53707.8672, 26033.5352,
        42111.9648, 61043.1445, 30953.2383, 22895.8984, 30529.3965, 20735.5371,
        36918.1680, 42527.5430, 53193.3477, 43479.0156, 31748.9062, 40356.7539,
        17413.7246, 25784.8926, 35487.0586, 19308.3809, 42280.0312, 18141.3750,
        47080.7031, 44446.6289, 36346.0703, 32251.1465, 36725.0000, 40740.6992,
        53751.6523, 21909.9707, 59682.6289, 31972.1758, 70842.1250, 45390.7969,
        34250.6719, 47800.3281, 43388.8281, 48244.8789, 23173.5918, 53467.2578,
        35676.8789, 33040.4727, 41151.2148, 66984.2500, 40309.1211, 23755.3555,
        32504.4336, 33103.8398, 22895.0645, 29316.4980, 46699.4102, 41226.5938,
        41930.5664, 30671.5684, 39530.9375, 41791.6367, 24684.9688, 19786.5801,
        21752.1641, 50011.4219, 43800.8945, 38955.3359, 36770.7617, 59471.3750,
        39545.3828, 29338.8203, 33617.6172, 28781.0996, 42194.6602, 44222.0508,
        15464.2793, 36742.9844, 22412.9492, 43051.4844, 23072.1953, 48225.1992,
        48154.8555, 35349.1641, 48286.8633, 30589.9238, 31164.8340, 40947.1133,
        61484.5312, 27434.3555, 34736.5703, 30620.3281, 36324.7148, 50407.6367,
        34005.7812, 39454.6992, 41419.7305, 35824.5625, 48146.9062, 54043.5586,
        33524.6719, 43787.7812, 42281.4062, 34012.3594, 31960.6543, 45800.8789,
        31208.2969, 22807.1895, 40435.5234, 41612.1367, 28627.6328, 47098.3359,
        29251.3672, 29735.7793, 20120.3418, 38840.1992, 20558.4414, 29303.2207,
        72419.5859, 39540.5742, 32889.7266, 29603.3438, 26110.6113, 33373.7383,
        35080.3750, 60124.0547, 37168.9531, 25053.1738, 40862.9336, 54885.7305,
        55896.1875, 36850.9492, 54485.0078, 31320.1309, 69215.5000, 39536.0156,
        38153.6289, 53244.6406, 44421.0781, 38566.4453, 21574.8145, 36976.2031,
        44253.6914, 62925.8320, 33298.4922, 44222.5898, 26573.4902, 34767.2930,
        32989.2617, 30103.1152, 40553.9766, 17399.4395, 30918.9648, 28854.5137,
        40687.2812, 31522.7305])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 99397.3125,  78080.6172,  44376.9961,  64177.3047,  38283.9258,
         68248.0547, 113425.1094, 100302.2109,  63838.8945,  51654.0703,
         80757.6172,  41973.6484, 121877.4766,  52782.3867,  76677.3906,
         75610.4766, 137570.7969,  60174.6953,  78136.8047,  70227.8438,
         50897.6641,  76586.2656,  95701.9297,  61539.4062,  41638.8711,
         87086.9219,  98964.6953,  86715.3125,  88816.6797,  62487.8398,
         81993.3281,  80206.1172, 109884.0938,  44737.1914,  45315.0273,
        107702.8047,  39185.1406,  81426.4375,  40955.4961, 106741.1875,
         56406.0469,  60984.9062,  48449.3047,  77288.5156,  43231.8516,
         78159.7422,  70271.5391,  41978.7891, 120958.8438,  90428.0156,
         68788.1719,  46323.3672, 103373.2812,  75368.8359,  69746.7188,
         56180.2773,  63170.8594,  97399.1328,  76006.1484,  75566.6875,
         45043.3672,  31127.2188,  42147.3047,  54598.4453, 111230.0078,
         81017.4688, 105156.7656,  64284.4883,  76286.8984, 105761.0312,
         44179.0352,  70731.4062,  57885.6445,  30091.1016,  64294.3477,
         83039.7578,  59671.1445,  56982.0586, 132379.9531,  61740.3125,
         51638.6211,  97554.3203,  79343.4141,  92321.6172,  88854.7109,
         50446.1719,  71895.0938, 157202.8750,  65414.7305,  66899.4609,
         67079.0859,  60740.6758,  83431.5312, 115889.8984,  57730.2539,
        117711.1094,  69372.7109,  33182.4141,  67303.9688,  51335.1914,
         73915.9531,  74029.2188,  68114.4062,  51497.6406,  95342.2422,
         98902.1797,  57753.1836,  68381.2812,  58271.7812,  54110.2656,
         55457.1680,  95932.9297, 139914.8906,  62891.4688,  74038.8828,
        126424.5469,  69293.8750, 103553.2266,  50771.5898,  55615.0391,
         78722.3047,  85373.3359,  92268.5781,  98500.7969, 111175.5625,
         65060.1211,  80256.6797, 113554.2109,  56948.2031,  61989.9219,
         82520.4219,  59761.1602, 108989.7031,  58518.0391,  74430.8281,
         47703.6719,  40821.1172,  78879.3594,  29111.9746,  80156.9062,
        123364.8516, 125551.8984,  42869.7812, 130365.6484,  37193.3438,
         90749.5312, 107883.6406,  81292.2188,  79156.3125,  52074.6445,
         71479.5938,  68791.0234,  79930.1328,  99221.8203,  30108.2168,
         54131.6406,  62650.1953,  71753.1719,  82233.5547,  52745.8086,
         64044.6484, 130935.8750,  80809.0625,  85666.1641,  92489.9844,
         52826.2461,  67346.2891, 111973.2500,  66212.3438,  72203.4844,
         66154.3906,  58261.7188,  64467.0234,  54351.9414,  48787.5391,
         24722.8633,  56794.9609,  85234.6641,  63873.2383,  62319.7734,
         37755.4727,  84998.1797,  61543.8359, 112426.3203, 113631.1484,
         71566.0938,  86291.9297,  87790.2188,  56710.8750,  69350.4766,
         51773.5117,  58485.9219,  45687.8281,  49983.1133,  90139.7188,
         69191.1406,  74870.2656,  45078.3086,  41086.9141,  63673.2969,
         65942.7266,  95299.1797,  66423.6172, 111303.7969,  74309.4141,
         80436.1406,  89213.1797,  71764.6406,  70754.8047,  81349.6953,
         63828.8281,  82326.9453,  47915.5156,  62047.4609,  84743.9062,
         59806.0859,  72084.7109,  68281.6406,  58250.2422,  95369.6406,
         63518.2969,  57575.6445,  56788.7578,  57141.8516,  80276.1875,
         54415.7695,  78717.2578,  93403.1328,  71245.8438, 135791.5469,
         52931.1328,  57150.2305,  83687.8281,  52321.8633,  75781.1094,
         52646.5117,  49872.9688,  60388.6406,  76616.7031,  47369.6953,
         63242.9766,  73558.2500,  65399.9219,  83477.7734,  83636.6797,
         41740.1055,  74957.5312,  70092.8125,  66355.3359,  53982.2891,
         56734.9062,  44958.3359,  55992.9102,  40056.4570,  93109.0391,
         53533.4219,  69817.5156,  62583.5625,  73062.3672,  50783.7148,
         83323.9219,  46330.5625,  73625.9062,  69571.9062,  94486.8984,
         47909.5352,  79112.3828,  86425.4922,  54416.5352,  95474.3125,
        106926.9531,  72834.2656,  72666.2578,  55317.9531,  86692.4453,
         53780.6914,  40036.8281,  54481.9766,  65773.1016,  83105.0000,
         60608.0430,  60529.9570, 105088.3828,  72734.9688,  53807.2305,
         81721.3984,  60064.3789,  95735.8750,  88663.3828,  58610.0586,
         60308.7969,  75753.0312,  75763.7812,  50051.9570,  95761.6250,
         72775.8828,  55678.6719,  78468.2812,  96049.4922,  91470.5000,
         89191.9219,  27510.7617,  76092.8672,  74443.7812, 117541.9062,
         47969.2578,  63034.0859,  42932.0508,  43506.7344,  98356.3906,
         88898.5312,  55244.2305,  81021.1875,  53870.9648,  78310.2344,
         56567.4375,  52677.9023,  63639.0469,  90080.6875,  62257.7227,
         69421.8516,  70430.1875, 110592.9453,  91571.8750,  61048.2383,
         73573.4844,  55513.8750,  31673.7637,  72113.6250,  57669.3281,
         69628.7812,  66679.5938,  39605.6953,  67558.7109,  83971.7422,
         54734.4141,  87415.7344,  61722.2422,  61547.9688,  65096.4961,
         54285.8359,  72519.6328, 119315.1250,  53637.0117,  81385.8906,
         58914.3359,  62190.5312,  67720.0703,  68993.9141,  47574.1562,
         48201.9492,  69569.4609,  75172.3750, 100765.2812,  45612.6484,
         81941.7266,  49409.2344,  70740.8125, 102610.4375,  58454.0312,
        102820.5000,  33551.1484,  89599.3672,  76211.5625,  88283.3516,
         64248.5664,  65942.3750,  50576.0820,  39580.0156,  29855.7891,
        102480.5234,  64483.1953,  60600.9805,  56942.7031,  55899.3164,
         54725.1992,  94666.0391,  95609.5938,  77071.2891,  70785.9141,
         64573.5117,  60899.4258,  76574.2656,  80738.6719, 121576.3906,
         63860.8633,  73926.8281, 110438.7422,  58739.4727,  75634.8516,
         53611.2539,  81495.0156,  79333.0000,  72992.3047,  59842.3125,
         72250.4297,  43193.8008,  81957.3750,  43806.5117,  61065.7305,
         53993.6719,  77401.3281,  64654.7812,  94940.6094,  77051.5547,
         57510.0312, 116133.4688,  65536.2422,  82552.3750,  77070.7500,
         67738.6641,  50493.2656,  78453.7109,  77585.7500,  41535.8477,
         40718.2812,  89219.8047,  53419.2266,  46489.2188,  53183.8281,
         73806.8828,  53351.8125,  92320.2109,  75354.4141,  70661.6406,
         64647.6367,  58793.0273,  39032.9883,  81665.2656,  41191.1172,
         52038.6016,  94130.5703,  99695.7500,  54502.7891,  84827.0234,
         57485.5000, 147324.4688,  84392.7266,  86529.9062,  64245.8047,
         55137.1367,  72983.4062,  50060.5820, 104295.1328, 107059.0938,
         63752.8750,  92559.8438,  57718.0977,  98976.6328,  48366.1328,
         70077.9922, 112648.2656,  84358.9141,  77864.0469,  72845.9766,
         69075.5781,  62287.3672,  47097.9648,  40972.8945,  72880.8516,
         76314.1875,  44395.3398,  56929.6602,  50646.4375,  52302.2422,
         72843.3516, 105738.6484,  92512.1406,  78665.3125,  64927.0625,
         45463.7617,  53483.7148,  38434.7852,  78704.6406,  52104.6875,
         80281.1484,  71031.1797,  62010.0391,  65636.3750,  57576.9648,
         59711.0938,  49100.9844,  57924.1875,  38631.2891,  63863.0117,
         60101.5234,  56370.7383,  47680.1133,  53816.7461,  74938.8125,
         64475.8320,  75448.8672,  63518.5000,  92628.7109,  58363.9922,
         88990.8125,  83499.4141,  76913.6484,  48258.3711,  39377.0664,
         88406.1328,  66487.4219,  50920.0391,  78175.5625,  63313.1484,
        118663.3672,  54968.7812,  56829.7305,  54824.6719,  46195.1016,
         55080.9805,  48571.3281])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 350.8782,  417.2469,  941.4415,  ...,  618.1906,  441.4433,
        1274.5996])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1418.5370, 2242.5334, 1551.5265, 2199.2231, 2237.8027, 4493.1421,
        2603.5701, 3344.8047, 1562.7018, 1547.5707, 2243.6130, 2312.8521,
        1498.8898, 2313.2439, 1967.2290, 1780.3179, 2939.4299, 2280.5254,
        5061.8462, 2555.1357, 4464.6899, 2881.5107, 1598.6868, 3806.0039,
        2849.0957, 3466.4146, 2311.7595, 2376.4805, 2256.5339, 2169.6257,
        2533.2888, 2303.4460, 1930.4396, 3689.2126, 3063.1487, 1335.1420,
        4065.0649, 4203.2793, 2906.8411, 1731.1427, 2061.4592, 1132.8175,
        3002.9795, 1848.2666, 3788.2007, 1936.9482, 1401.1918, 3053.3625,
        3210.9622, 2010.4337, 2955.1912, 2185.7527, 2006.0529, 4409.2109,
        1844.1434, 2701.1787, 1804.6978, 1929.8857, 2271.1072, 1775.0715,
        2391.6589, 2195.7363, 4988.8994, 2550.9456, 2946.0396, 2176.4644,
        1876.4554, 1268.6740, 4690.1274, 4287.7568, 2099.5095, 2407.2764,
        3931.6553, 3697.1704, 5454.4707, 2441.7461,  735.3401, 2017.9980,
        1722.6151, 2851.9917, 1864.1703, 1522.7944, 1524.5425, 4205.8843,
        1711.2927, 3812.5684, 2668.9277, 2258.9802, 2735.3948, 1424.9390,
        3570.8264, 2791.3589, 1726.6036, 3083.4590, 1560.9603, 2300.4856,
        1616.2324, 4062.6328, 3565.7168, 2516.2354, 3729.1948, 2681.7793,
        5208.8916, 1963.1735, 3812.4397, 1548.0883, 2312.6772, 1206.5078,
        2894.3813, 1958.0541, 3331.8767, 2468.1152, 2448.0698, 2751.9712,
        3688.7727, 3304.2527, 2255.4954, 4146.6006, 2293.3179, 2145.5720,
        2461.0833, 2011.9414, 3733.8909, 1652.9521, 4162.2910, 2411.7485,
        1340.4645, 3520.3879, 1938.4905, 1904.2678, 1428.6819, 1648.0122,
        2721.8391, 1120.5101, 2160.9272, 3781.5286, 1500.9528, 2409.0933,
        2129.1880, 3326.3945,  809.1604, 1957.3115, 1900.9830, 3976.9077,
        2921.3914, 2814.2993, 4956.1230, 2734.2910, 3592.0049, 2639.8962,
        4384.6377, 3320.5549, 2332.0623, 2368.3838, 3011.4905, 1523.7750,
        3495.6201, 2890.0703, 2789.5291, 1629.4050, 1918.1792, 1768.0416,
        2959.4231, 1389.1410, 1953.7205, 1469.5878, 1998.5015, 1612.4849,
        5151.5757, 3912.5730, 1306.0103, 3708.8450, 3301.3357, 2526.5369,
        4246.8740, 3001.5537, 3504.2346, 2590.6555, 1456.3994, 2646.6194,
        1763.6971, 2349.5713, 2511.4009, 3320.6733, 2921.5081, 2050.2898,
        1267.8101, 1638.9973, 3555.1125, 2302.3689, 1798.2662, 2705.7820,
        1330.2904, 2704.4006, 3531.8955, 2282.1360, 1331.8234, 2219.4297,
        3290.9961, 2324.4346, 2971.2856, 1658.3361, 1635.7803, 1713.6857,
        3796.1096, 3329.0894, 1418.6819, 3348.1052, 5576.1079, 1996.6691,
        3183.0112, 1564.2681, 3293.2966, 2392.4065, 3149.8608, 2590.9985,
        3557.4678, 2207.4128, 1815.5310, 3595.0469, 3411.1924, 1879.8612,
        1638.5314, 2327.6548, 2883.7109, 4987.6812, 2760.6357, 3064.1370,
        3156.9900, 2631.5850, 2641.8904, 2377.0911, 2434.0706, 2459.0662,
        1039.0187, 2550.3198, 2543.7578, 5128.4043, 3337.5205, 6217.6357,
        3034.7822, 2350.7397, 1885.4243, 3352.6375, 1384.3585, 1173.0114,
        2165.3750, 1619.5334, 2664.9873, 2081.1763, 1888.7513, 2289.8831,
        2812.0530, 3230.5500, 3997.2617, 3017.2769, 3651.2783, 3742.1399,
        1864.0161, 2575.6833, 1427.0615, 2384.3479, 1487.0533, 2257.3406,
        3647.0681, 2966.2849, 2845.9878, 2382.2937, 1843.9937, 1609.1387,
        1316.9446, 1694.1190, 2591.3293, 3103.1917, 2356.3989, 3103.8765,
        3750.4712, 3074.2346, 2772.7302, 4454.6846, 3471.2627, 2071.9275,
        3320.6982, 5371.4619, 1851.3639, 1278.5898, 3091.9580, 1807.6975,
        2953.6990, 1962.0361, 1442.1506, 2041.2169, 2542.5867, 2953.6741,
        1681.5928, 2273.8730, 1302.1418, 3498.3967, 4245.8560, 1848.4464,
        1996.5347, 4001.2603, 2124.0400, 2408.3784, 1562.9690, 3252.1113,
        1648.0677, 2438.4993, 2612.4548, 3601.0950, 1481.3424, 1730.9954,
        2143.6890, 2699.9426, 2967.7993, 2867.0513, 2976.8350, 1246.2039,
        5026.8481, 3172.7271, 3138.9961, 1847.4296, 1903.1630, 2195.4473,
        4837.3496, 3069.2695, 2928.2114, 2688.4436, 3035.1470, 1852.8142,
        2296.3020, 3035.2263, 2016.4929, 1326.1824, 1817.4762, 5109.0669,
        3251.2019, 1353.6989, 1750.2131, 2634.7935, 1540.3562, 2814.6475,
        3640.4231, 2973.3716, 1410.3926, 1706.6875, 2552.2539,  999.6223,
        2386.7581, 1917.7374, 2983.5312, 2397.2588, 2675.7864, 1819.6978,
        1824.6528, 3962.5754, 5428.0317, 2974.2510, 2956.8811, 2462.7700,
        3237.1458, 2133.5427, 3998.9021, 2432.4177, 2370.3601, 2345.1821,
        2990.2686, 3015.5513, 2818.5906, 2665.2922, 4354.8374, 1901.2623,
        3546.0188, 2649.0396, 3003.8650, 1082.9347, 1782.8124, 2843.8210,
        3317.7925, 2705.5093, 2004.7874, 1968.9558, 3014.0583, 2243.3560,
        2628.3181, 3051.5737, 4687.0298, 2936.8745, 4362.1870, 3863.4739,
        2953.5764, 3214.4641, 1945.4395, 2200.5869, 1544.2145, 1866.8795,
        2351.0815, 2948.1482, 1980.2131, 4251.3091, 1630.1307, 2044.8774,
        3211.7729, 1689.1813, 1165.1027, 2994.5117, 1719.6724, 2128.3755,
        1670.8512, 2953.6482, 1691.6542, 2908.2019, 3243.1990, 2454.1553,
        1280.6875, 2364.3376, 2035.7775, 4673.8916, 3626.0557, 1856.1644,
        1461.9286, 4715.9136, 3394.3845, 2725.5085, 1793.1522, 1463.0043,
        2471.9451, 1653.1141, 3668.7056, 1272.3341, 1628.9340, 2074.9773,
        2518.7190, 1724.1975, 1855.7140, 1977.3159, 1990.2186, 2935.7412,
        2937.7166, 3011.4993, 3375.8828, 2086.8091, 2019.6227, 3823.6714,
        1621.5223, 2735.7939, 3965.3245, 1945.5601, 2531.0981, 4947.9614,
        1654.7955, 4692.0083, 1817.4365, 5708.4058, 1688.1774, 2980.3767,
        1282.4460, 2441.8945, 4415.2495, 2707.2139, 1346.0100, 1722.8342,
        4356.8237, 1583.0922, 1614.9662, 1955.5679, 2668.7336, 3613.1338,
        3127.7283, 3098.4775, 2033.7881, 5065.3818, 2992.7637, 1500.0718,
        2462.2607, 2076.3179, 2511.7827, 1504.2552, 6797.1763, 2206.4417,
        1711.7073, 1819.3396, 3665.5759, 2235.6335, 1230.9369, 1419.3776,
        4791.6260, 1871.3270, 1898.9635, 2067.0498, 2285.2080, 2241.6738,
        4235.5591, 3996.6560, 3096.5225, 2755.0017, 2623.5378, 3095.5488,
        2056.5798, 1819.1289, 1843.3196, 1243.0289, 2159.8259, 2938.5105,
        2317.0164, 1905.2106, 5458.6084, 3422.3845, 2382.3513, 2501.9583,
        3434.1445, 2044.8827])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 7897.8613,  4661.1733,  2980.7725,  2856.7319,  3679.0974,  6763.5083,
         4939.3809, 10631.6514,  7167.3208,  6778.7998,  7793.4019,  4506.6855,
         5204.2905,  3889.2239,  8440.7197,  6256.0039,  9210.3604,  4931.0625,
         2561.5308,  6257.9302,  6348.1704,  4510.1353,  3701.3494,  9342.1406,
         3301.9070,  3594.9019,  5563.6440,  5464.2925,  5497.7593,  8606.0537,
         4843.3384,  3082.8857,  4807.6611,  4049.7776,  3911.9214,  3244.0664,
         5813.7271,  7457.5410,  7327.3354,  7916.9111,  5520.5532,  5853.7251,
         3438.3818,  5721.9961,  4582.4106,  3532.7661, 12655.6543,  6876.0625,
         4913.0732,  6484.5361,  7560.2183,  5372.0659,  7244.8472, 11972.1182,
         3367.9653,  2988.0210,  4295.7798,  4702.9497, 10480.9092,  6626.7974,
         5382.4512,  5527.1763,  5008.6768,  6731.7852,  8293.2490,  2724.2378,
         3562.5452,  5019.9590, 12631.9980,  5445.3491,  5865.6831,  7980.2637,
         8978.6064,  2658.1912,  4182.1055,  8624.4238,  4940.8174,  5714.7959,
         7840.9263, 11628.1055,  5681.6807,  8117.1069,  5794.6377,  5327.2485,
         6946.4160,  8861.6523,  4491.1665,  5405.6592,  5867.4194,  3771.7920,
         3856.4346,  2588.1509,  5374.9482,  6865.3882,  3781.2280,  3857.8896,
         4221.6997,  5838.5479,  7682.9604,  7729.2549,  7351.1885, 11809.2871,
        14078.7402, 14074.1221,  3057.1770,  9203.0752,  7833.0518,  8743.9795,
         3296.5430,  6547.1304,  5200.8618,  5507.8516,  8262.4043, 11369.4219,
         4684.2891,  8009.3545,  5484.4858,  3604.7957, 10069.5605,  3753.4033,
         7921.2578,  6525.6909,  6153.8516,  6702.3452,  5915.7847,  4154.8662,
         3881.2942,  7229.1172,  5542.3994,  4103.4023,  4992.1426,  7376.5298,
         5512.8740,  5951.3208,  8730.1787,  5186.5859,  3519.9727,  9423.2881,
         5208.6226,  3520.6426,  7279.2603,  5779.3115,  4188.4858,  7487.9189,
         3513.1348,  9197.7568,  8793.8223,  3632.1255,  4713.2642,  4659.4854,
         4095.8450,  7550.9849,  7575.4058,  4443.5908,  4398.5498,  5992.7192,
         6623.5693,  6545.4844,  8985.9980,  8970.6328,  7514.6025,  5862.0845,
         6247.4497,  4800.5059,  5021.8643,  6134.7363,  4365.4629,  9007.3594,
         9393.1572,  6043.8975,  6969.1929, 10236.7969,  7665.6392,  7276.1611,
         5545.8052, 12680.3438,  3550.8337,  6978.3828,  6495.0376,  9564.9209,
         5785.0303,  5660.6211,  5667.9116,  3594.9009,  7688.9185,  4544.1479,
         7233.2891,  5220.8574,  5400.3843,  6299.4858,  9502.1562,  4982.5562,
         6869.9629,  4714.0273,  3307.6208,  7713.6323, 10795.6699,  7668.0337,
         6287.2212,  4727.3794,  9654.4951,  3254.3860,  8253.2979,  4996.4038,
         4459.7490,  6311.9473,  4591.9805,  7787.3120, 10373.3711,  2595.1760,
        10900.8857,  5691.9004, 10343.6230,  7918.1436,  8659.5908,  3890.6367,
         3891.1533,  6092.4834,  3065.1411, 10782.7949, 11259.1445,  2732.2458,
         4952.3540,  8149.2646, 11559.4639,  3092.7625,  3995.2458, 13944.9092,
         5477.0596,  4104.5903,  2575.8171,  8253.6855,  3527.3792,  4859.2847,
         8366.9072,  2699.7375,  4161.3398,  4986.7310,  3925.5583,  5717.6011,
         4066.5051,  4289.8159,  8658.7646, 11087.0020,  3966.4653,  8261.2227,
         5072.1372,  6269.8477,  6019.7778,  7095.1562,  4915.3477,  6312.6187,
         3191.7749,  5396.7334,  5401.0864,  4867.0010,  4856.3350,  6471.7026,
         6162.0942,  9542.4434,  5614.0308,  7785.2017,  5823.0342,  6041.3652,
         8967.3232,  4410.3198,  4901.2197,  4742.2075,  4924.1797,  4312.0137,
         6293.0635,  3400.2825,  5581.1533,  5993.1533,  7977.0029,  7091.7446,
         8378.8105,  4330.8477,  2850.0928,  4810.2144,  6107.1187,  9045.8809,
         5116.4736,  7813.8589,  5508.7402,  6981.8965,  4849.9331,  9533.3867,
         4458.6519,  5961.0200,  7872.6079,  4445.6484,  3362.0696,  9499.6514,
         2813.9104,  5931.5034,  6943.0332,  9679.6719,  4703.4419,  5757.4541,
         6458.2383,  4282.9146,  5269.0674,  9989.4648,  4354.7729,  5245.4883,
         4010.8760,  7198.1313,  4775.8672,  5189.6875,  3352.5325,  5129.8340,
         4357.0107,  1749.2894,  3796.6641,  4465.9409,  3173.3711,  3464.4351,
         6848.3726,  7790.9658,  5700.5898,  8031.2759,  8052.6704,  3152.8223,
         7906.9526,  8599.7266,  2789.7783,  4831.2617,  4071.8503,  3477.8335,
         7927.9141,  6024.9062,  8090.3130,  7434.8779,  3803.1084,  9038.7764,
         3490.1401,  5694.5322,  4883.8613,  5369.6025,  3387.7034,  3761.1311,
         5190.5176, 11900.0430,  4367.8955,  7739.7871, 10493.2070,  4169.6201,
         4897.9199,  9882.7119,  6576.0142,  5378.7368,  4328.4443,  7061.0464,
        10580.1377,  5323.0835,  3786.6824,  4126.5869,  6339.8706,  5856.5776,
         4560.0918,  4835.7539,  6601.8657,  4437.1704,  4710.6777,  6853.6113,
         3687.7397,  9229.3486,  3718.8298,  6044.3472,  6415.1279, 12636.0059,
         4649.4253, 10516.7998,  5119.8945, 12294.5098,  7863.5049,  5484.9531,
         3976.5381,  3504.0784,  6106.3779,  7185.7422,  4871.6211,  4305.3442,
         4427.3130,  5030.2563,  3890.9119, 12238.9160,  7105.2812,  6489.0518,
         7471.6650,  7626.7744,  5903.8174,  3535.5610,  9341.9570,  3047.5120,
         3742.4802,  8105.2515,  6939.1235,  8020.9697, 10682.0703,  6454.3516,
         4598.9268, 11527.3545,  2533.1985,  5922.8672,  5079.9326,  5591.1802,
         5430.8823,  5055.4985,  4139.6924,  5969.4141,  5123.5503,  7240.1611,
         5939.4351,  4070.8682,  6038.8955,  4566.5317,  4349.9512,  8648.7832,
         7668.3086,  5173.3091,  6456.8076, 10816.8018,  5514.5786,  5241.2476,
         8990.3291,  4969.5210,  3360.7322,  6162.5166, 11497.2744, 15069.3037,
        12806.1250,  3368.5410,  6306.5337,  6365.4766,  7805.5308,  8005.2466,
         3255.5559,  4675.1289,  4152.7812,  8579.1797,  4540.7339,  3644.5903,
         4008.0085,  5943.1245,  5921.0405,  4542.1484,  6609.6582,  6725.0259,
         2212.8855,  6437.1865,  6235.1089,  6888.9819,  4408.7632,  4644.1846,
         7570.3716,  9263.4561,  6303.5659, 13136.6660,  7778.4756, 11899.8945,
         7536.9844,  4068.5186,  4373.2378,  4872.3428,  3318.3335, 10960.4375,
         8346.7637,  4446.5420,  7696.4824,  3085.8533,  6907.1470, 10049.6836,
         4650.9917,  3191.5986,  2549.7886,  4541.6724,  5712.9463,  9766.5264,
         2849.0652,  4950.0996,  4385.0405,  4260.8501,  4747.3906, 10287.2119,
         7374.7461,  6771.3125,  5168.4575,  6914.8228,  2522.8955, 10319.0879,
        10418.6006,  3305.8081,  9548.7383,  4576.7397,  7136.8398, 10447.9678,
        10109.1699,  2358.6633,  6267.2173,  5568.8662,  9640.2930,  9429.7549,
         4822.4326,  7914.3340, 10431.6787,  4983.3105,  5080.0430,  3182.5288,
         8829.8115,  4087.0845])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 0.0000,  9.2538, 60.4807,  ...,  6.8053, 28.5200,  0.4838])
Pruning mask statistics:
Failed to compute pruning mask: 'weight'
