Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    29135 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Loading BEN data for train...
    237871 patches indexed
    237871 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
==================================================
ROUND 1/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1468
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0641

==================================================
ROUND 2/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0959
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0934

==================================================
ROUND 3/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1171
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1217

==================================================
ROUND 4/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0787
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0734

==================================================
ROUND 5/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0042 | recall: 0.0062 | f1-score: 0.0050 | support: 1282 | mAP: 0.0579
macro     precision: 0.0526 | recall: 0.0026 | f1-score: 0.0050 | support: 1282 | mAP: 0.1045

==================================================
ROUND 6/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0010 | recall: 0.0008 | f1-score: 0.0009 | support: 1282 | mAP: 0.0614
macro     precision: 0.0526 | recall: 0.0003 | f1-score: 0.0007 | support: 1282 | mAP: 0.0709

==================================================
ROUND 7/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0501
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0842

==================================================
ROUND 8/20
==================================================
Finale Anzahl der Pruning-Patches: 10
Anzahl eindeutiger Klassen: 11
Klassenverteilung in den Pruning-Patches (Häufigkeiten): {'Marine waters': 7, 'Transitional woodland, shrub': 6, 'Mixed forest': 5, 'Coniferous forest': 4, 'Land principally occupied by agriculture, with significant areas of natural vegetation': 2, 'Inland waters': 1, 'Broad-leaved forest': 1, 'Arable land': 1, 'Urban fabric': 1, 'Industrial or commercial units': 1, 'Coastal wetlands': 1}
[INFO] Erstelle Prune Loader...
[INFO] 10 Patches nach Filterung übrig.
[SUCCESS] Prune Loader erfolgreich erstellt.
Starting relevance computation and pruning mask generation.
Type of components_relevances: <class 'collections.OrderedDict'>
Layer: conv1
Total layer relevance: 1.0673523622878296e+25
--------------------------------------------------
Layer: encoder.4.0.conv1
Total layer relevance: 6.487640339139831e+24
--------------------------------------------------
Layer: encoder.4.0.conv2
Total layer relevance: 4.2920368018928827e+24
--------------------------------------------------
Layer: encoder.4.0.conv3
Total layer relevance: 9.353258732411902e+23
--------------------------------------------------
Layer: encoder.4.0.downsample.0
Total layer relevance: 9.791525989717507e+23
--------------------------------------------------
Layer: encoder.4.1.conv1
Total layer relevance: 1.035246409511328e+23
--------------------------------------------------
Layer: encoder.4.1.conv2
Total layer relevance: 7.062479183087852e+22
--------------------------------------------------
Layer: encoder.4.1.conv3
Total layer relevance: 8.566872186073817e+21
--------------------------------------------------
Layer: encoder.4.2.conv1
Total layer relevance: 3.2057610828478887e+20
--------------------------------------------------
Layer: encoder.4.2.conv2
Total layer relevance: 2.1190775012136164e+20
--------------------------------------------------
Layer: encoder.4.2.conv3
Total layer relevance: 8.131013391924435e+19
--------------------------------------------------
Layer: encoder.5.0.conv1
Total layer relevance: 3.5270512227798483e+19
--------------------------------------------------
Layer: encoder.5.0.conv2
Total layer relevance: 2.638223175267123e+19
--------------------------------------------------
Layer: encoder.5.0.conv3
Total layer relevance: 4.477814430431707e+18
--------------------------------------------------
Layer: encoder.5.0.downsample.0
Total layer relevance: 4.5248059081354117e+18
--------------------------------------------------
Layer: encoder.5.1.conv1
Total layer relevance: 4.144404984055726e+17
--------------------------------------------------
Layer: encoder.5.1.conv2
Total layer relevance: 3.271213393767301e+17
--------------------------------------------------
Layer: encoder.5.1.conv3
Total layer relevance: 3.3304050439028736e+16
--------------------------------------------------
Layer: encoder.5.2.conv1
Total layer relevance: 723686420119552.0
--------------------------------------------------
Layer: encoder.5.2.conv2
Total layer relevance: 592322966323200.0
--------------------------------------------------
Layer: encoder.5.2.conv3
Total layer relevance: 146398037147648.0
--------------------------------------------------
Layer: encoder.5.3.conv1
Total layer relevance: 16926056120320.0
--------------------------------------------------
Layer: encoder.5.3.conv2
Total layer relevance: 13548831899648.0
--------------------------------------------------
Layer: encoder.5.3.conv3
Total layer relevance: 2526057594880.0
--------------------------------------------------
Layer: encoder.6.0.conv1
Total layer relevance: 274057854976.0
--------------------------------------------------
Layer: encoder.6.0.conv2
Total layer relevance: 272042442752.0
--------------------------------------------------
Layer: encoder.6.0.conv3
Total layer relevance: 46444855296.0
--------------------------------------------------
Layer: encoder.6.0.downsample.0
Total layer relevance: 47927496704.0
--------------------------------------------------
Layer: encoder.6.1.conv1
Total layer relevance: 3693463552.0
--------------------------------------------------
Layer: encoder.6.1.conv2
Total layer relevance: 4457421312.0
--------------------------------------------------
Layer: encoder.6.1.conv3
Total layer relevance: 2644727040.0
--------------------------------------------------
Layer: encoder.6.2.conv1
Total layer relevance: 1678328832.0
--------------------------------------------------
Layer: encoder.6.2.conv2
Total layer relevance: 2185787648.0
--------------------------------------------------
Layer: encoder.6.2.conv3
Total layer relevance: 301168448.0
--------------------------------------------------
Layer: encoder.6.3.conv1
Total layer relevance: 56795196.0
--------------------------------------------------
Layer: encoder.6.3.conv2
Total layer relevance: 81071168.0
--------------------------------------------------
Layer: encoder.6.3.conv3
Total layer relevance: 8151120.0
--------------------------------------------------
Layer: encoder.6.4.conv1
Total layer relevance: 1828736.0
--------------------------------------------------
Layer: encoder.6.4.conv2
Total layer relevance: 2564945.5
--------------------------------------------------
Layer: encoder.6.4.conv3
Total layer relevance: 505838.0
--------------------------------------------------
Layer: encoder.6.5.conv1
Total layer relevance: 141845.6875
--------------------------------------------------
Layer: encoder.6.5.conv2
Total layer relevance: 240186.65625
--------------------------------------------------
Layer: encoder.6.5.conv3
Total layer relevance: 58356.59375
--------------------------------------------------
Layer: encoder.7.0.conv1
Total layer relevance: 32184.91015625
--------------------------------------------------
Layer: encoder.7.0.conv2
Total layer relevance: 92950.359375
--------------------------------------------------
Layer: encoder.7.0.conv3
Total layer relevance: 26014.6640625
--------------------------------------------------
Layer: encoder.7.0.downsample.0
Total layer relevance: 27786.203125
--------------------------------------------------
Layer: encoder.7.1.conv1
Total layer relevance: 2639.884765625
--------------------------------------------------
Layer: encoder.7.1.conv2
Total layer relevance: 11044.0185546875
--------------------------------------------------
Layer: encoder.7.1.conv3
Total layer relevance: 4759.65673828125
--------------------------------------------------
Layer: encoder.7.2.conv1
Total layer relevance: 2171.720947265625
--------------------------------------------------
Layer: encoder.7.2.conv2
Total layer relevance: 3958.90576171875
--------------------------------------------------
Layer: encoder.7.2.conv3
Total layer relevance: 1698.530029296875
--------------------------------------------------
--------------------------------------------------
Global Pruning Mask
Pruning-rate: 0.5
Layer: conv1		% of pruned neurons: 0.00%
Layer: encoder.4.0.conv1		% of pruned neurons: 0.00%
Layer: encoder.4.0.conv2		% of pruned neurons: 0.00%
Layer: encoder.4.0.conv3		% of pruned neurons: 0.00%
Layer: encoder.4.0.downsample.0		% of pruned neurons: 0.00%
Layer: encoder.4.1.conv1		% of pruned neurons: 0.00%
Layer: encoder.4.1.conv2		% of pruned neurons: 0.00%
Layer: encoder.4.1.conv3		% of pruned neurons: 0.00%
Layer: encoder.4.2.conv1		% of pruned neurons: 0.00%
Layer: encoder.4.2.conv2		% of pruned neurons: 0.00%
Layer: encoder.4.2.conv3		% of pruned neurons: 0.00%
Layer: encoder.5.0.conv1		% of pruned neurons: 0.00%
Layer: encoder.5.0.conv2		% of pruned neurons: 0.00%
Layer: encoder.5.0.conv3		% of pruned neurons: 0.00%
Layer: encoder.5.0.downsample.0		% of pruned neurons: 0.00%
Layer: encoder.5.1.conv1		% of pruned neurons: 0.00%
Layer: encoder.5.1.conv2		% of pruned neurons: 0.00%
Layer: encoder.5.1.conv3		% of pruned neurons: 0.00%
Layer: encoder.5.2.conv1		% of pruned neurons: 0.00%
Layer: encoder.5.2.conv2		% of pruned neurons: 0.00%
Layer: encoder.5.2.conv3		% of pruned neurons: 0.00%
Layer: encoder.5.3.conv1		% of pruned neurons: 0.00%
Layer: encoder.5.3.conv2		% of pruned neurons: 0.00%
Layer: encoder.5.3.conv3		% of pruned neurons: 0.00%
Layer: encoder.6.0.conv1		% of pruned neurons: 0.00%
Layer: encoder.6.0.conv2		% of pruned neurons: 0.00%
Layer: encoder.6.0.conv3		% of pruned neurons: 0.00%
Layer: encoder.6.0.downsample.0		% of pruned neurons: 0.00%
Layer: encoder.6.1.conv1		% of pruned neurons: 0.00%
Layer: encoder.6.1.conv2		% of pruned neurons: 0.00%
Layer: encoder.6.1.conv3		% of pruned neurons: 0.00%
Layer: encoder.6.2.conv1		% of pruned neurons: 0.00%
Layer: encoder.6.2.conv2		% of pruned neurons: 0.00%
Layer: encoder.6.2.conv3		% of pruned neurons: 0.00%
Layer: encoder.6.3.conv1		% of pruned neurons: 0.00%
Layer: encoder.6.3.conv2		% of pruned neurons: 0.00%
Layer: encoder.6.3.conv3		% of pruned neurons: 16.60%
Layer: encoder.6.4.conv1		% of pruned neurons: 0.00%
Layer: encoder.6.4.conv2		% of pruned neurons: 0.00%
Layer: encoder.6.4.conv3		% of pruned neurons: 81.93%
Layer: encoder.6.5.conv1		% of pruned neurons: 14.45%
Layer: encoder.6.5.conv2		% of pruned neurons: 0.78%
Layer: encoder.6.5.conv3		% of pruned neurons: 97.56%
Layer: encoder.7.0.conv1		% of pruned neurons: 100.00%
Layer: encoder.7.0.conv2		% of pruned neurons: 98.63%
Layer: encoder.7.0.conv3		% of pruned neurons: 99.41%
Layer: encoder.7.0.downsample.0		% of pruned neurons: 99.41%
Layer: encoder.7.1.conv1		% of pruned neurons: 100.00%
Layer: encoder.7.1.conv2		% of pruned neurons: 100.00%
Layer: encoder.7.1.conv3		% of pruned neurons: 100.00%
Layer: encoder.7.2.conv1		% of pruned neurons: 100.00%
Layer: encoder.7.2.conv2		% of pruned neurons: 100.00%
Layer: encoder.7.2.conv3		% of pruned neurons: 100.00%
Sendeing pruning mask to clients...
[INFO] Pruner and pruning mask received and stored.
[INFO] Pruner and pruning mask received and stored.
[INFO] Pruner and pruning mask received and stored.
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.2062
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0860

==================================================
ROUND 9/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.2141
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1601

==================================================
ROUND 10/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1690
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1494

==================================================
ROUND 11/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1405
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1708

==================================================
ROUND 12/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0806
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1221

==================================================
ROUND 13/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0735
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1304

==================================================
ROUND 14/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0745
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1504

==================================================
ROUND 15/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0771
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1415

==================================================
ROUND 16/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0770
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1320

==================================================
ROUND 17/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0762
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1411

==================================================
ROUND 18/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0784
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1477

==================================================
ROUND 19/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0785
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1657

==================================================
ROUND 20/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0762
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1254

[{'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}]
