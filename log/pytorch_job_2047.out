Lines that potentially need to be canonized 309
Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    7150 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    4263 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    7180 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    3248 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initializing LRP Pruning...
LRP initialized successfully.
=== Round 1/5 ===
Training and communication for Round 1...
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
[INFO] Performing LRP Pruning in Round 1...
[INFO] Computing LRP pruning mask...
Erstelle DataLoader f√ºr Land: Finland
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([5])
Input shape before relevance computation: torch.Size([5, 10, 1, 5])
Model output shape: torch.Size([5, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([5, 19]) to 5 for one-hot encoding.
Targets shape: torch.Size([5])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([5, 19])
Relevance computation successful. Relevance shape: torch.Size([5, 10, 1, 5])
[INFO] Relevance maps computed for 53 layers.
[DEBUG] Layer: conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.0.downsample.0
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.1.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.1.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.1.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.2.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.2.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.2.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.5.0.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.0.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.0.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.0.downsample.0
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.1.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.1.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.1.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.2.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.2.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.2.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.3.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.3.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.3.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.6.0.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.0.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.0.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.0.downsample.0
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.1.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.1.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.1.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.2.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.2.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.2.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.3.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.3.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.3.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.4.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.4.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.4.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.5.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.5.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.5.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.7.0.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.0.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.0.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.0.downsample.0
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.1.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.1.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.1.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.2.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.2.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.2.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 1808
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.6808e+17, 6.9466e+17, 7.6118e+17, 3.4264e+17, 1.2910e+18, 5.1377e+17,
        5.6931e+17, 5.5448e+17, 1.0907e+18, 8.3338e+17, 1.0300e+17, 9.3989e+16,
        6.8987e+16, 7.4231e+17, 4.5091e+17, 1.3684e+18, 7.5298e+16, 4.9801e+17,
        6.3238e+17, 1.0076e+17, 3.8831e+17, 1.9640e+17, 9.7839e+17, 5.4606e+17,
        1.9132e+17, 4.6855e+17, 9.1471e+17, 2.3876e+17, 9.0557e+17, 4.7275e+17,
        1.2120e+17, 3.4370e+17, 5.5922e+17, 4.8488e+17, 5.6478e+17, 4.9122e+17,
        1.2428e+17, 1.0303e+18, 2.5357e+17, 1.0851e+18, 3.2415e+17, 1.4568e+18,
        1.0898e+18, 1.0548e+18, 4.6501e+17, 3.0846e+17, 6.1165e+17, 1.1448e+18,
        9.2328e+17, 6.7921e+17, 6.1539e+17, 9.0704e+17, 3.3049e+17, 3.7545e+17,
        3.7861e+17, 6.3539e+17, 1.4607e+17, 6.7636e+17, 3.3634e+17, 2.3055e+17,
        2.9005e+17, 1.0148e+18, 8.8755e+17, 1.0309e+18])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.7719e+17, 2.0064e+17, 2.3550e+17, 2.3870e+17, 2.5035e+17, 3.4833e+17,
        2.5301e+17, 2.1996e+17, 2.5950e+17, 2.0754e+17, 1.8382e+17, 2.4592e+17,
        1.3392e+17, 5.4625e+17, 1.8054e+17, 2.3054e+17, 1.6273e+17, 5.3751e+17,
        2.4520e+17, 8.6225e+16, 1.7266e+17, 2.4374e+17, 2.4760e+17, 7.7345e+16,
        6.5148e+17, 9.8339e+16, 6.0504e+17, 2.8675e+17, 9.2517e+16, 2.1412e+17,
        3.6774e+17, 1.3582e+17, 1.9315e+17, 1.4992e+17, 3.2430e+17, 3.7253e+17,
        4.0809e+17, 3.2395e+16, 6.4496e+16, 6.4454e+17, 8.4097e+17, 2.4316e+17,
        7.7846e+16, 2.6349e+17, 2.1562e+17, 3.8158e+17, 2.7508e+17, 2.5505e+17,
        3.2338e+17, 2.4021e+17, 3.5599e+17, 2.1865e+17, 1.3082e+17, 4.1060e+17,
        2.4749e+17, 2.4149e+17, 4.4066e+17, 5.7967e+16, 2.4693e+17, 7.7880e+16,
        4.5999e+17, 4.6662e+16, 3.6781e+17, 4.1112e+17])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.4278e+17, 3.5991e+17, 2.9663e+17, 6.9566e+16, 1.7579e+17, 2.2195e+17,
        3.9637e+17, 7.8630e+16, 1.6951e+17, 1.6078e+17, 6.0403e+17, 3.5427e+17,
        3.4930e+17, 7.5871e+17, 4.0705e+17, 2.6156e+17, 9.0095e+17, 2.0339e+17,
        3.7228e+17, 8.8948e+16, 3.5285e+17, 2.0851e+17, 6.4060e+17, 2.6567e+17,
        1.6591e+17, 6.7532e+17, 7.9319e+16, 2.5693e+17, 6.6049e+17, 1.7886e+17,
        4.1834e+17, 1.9557e+17, 1.5223e+17, 8.4461e+16, 2.0121e+17, 4.4710e+17,
        1.2888e+17, 2.9439e+17, 2.6275e+17, 5.9470e+16, 2.7482e+17, 2.8130e+17,
        2.5440e+17, 1.2616e+17, 1.9391e+17, 5.1906e+17, 3.5460e+17, 6.1607e+17,
        4.3966e+17, 2.5711e+17, 4.8953e+17, 7.2838e+17, 2.7917e+17, 1.2250e+18,
        3.1493e+17, 9.2527e+17, 6.3099e+16, 3.5733e+17, 9.1385e+16, 9.7359e+16,
        2.4295e+17, 8.1483e+17, 9.6870e+16, 6.8241e+16])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.9061e+14, 5.4310e+15, 8.6888e+14, 1.5285e+14, 3.3623e+14, 8.5604e+14,
        2.9207e+15, 2.0605e+15, 1.4747e+15, 1.2890e+14, 4.4920e+15, 1.9154e+15,
        2.6579e+15, 1.4320e+14, 3.0100e+14, 1.1969e+14, 1.2307e+14, 3.3236e+14,
        8.4589e+14, 6.3810e+14, 3.1500e+14, 6.3480e+14, 5.0441e+14, 2.5353e+14,
        1.4020e+14, 3.3957e+15, 2.5278e+14, 1.4631e+15, 6.8496e+14, 5.4702e+15,
        4.9730e+14, 8.6606e+14, 3.6103e+13, 2.5608e+14, 4.8019e+14, 5.5613e+15,
        4.4624e+14, 2.4229e+14, 9.5469e+15, 4.6378e+14, 6.5445e+14, 4.6432e+16,
        2.4928e+14, 1.5168e+15, 1.8425e+14, 2.6938e+14, 1.1699e+15, 6.4260e+14,
        7.8012e+14, 2.3172e+15, 5.5808e+16, 2.5510e+15, 7.9821e+14, 6.7869e+15,
        2.7510e+14, 6.9370e+14, 2.6310e+16, 9.5811e+14, 1.0396e+14, 1.2044e+15,
        5.0395e+14, 8.7561e+13, 1.0963e+15, 4.6864e+14, 6.0092e+14, 6.0183e+14,
        1.0956e+15, 2.3084e+14, 1.3035e+15, 3.7221e+14, 1.7966e+14, 1.9046e+15,
        1.0822e+14, 1.2433e+15, 7.1280e+14, 1.4176e+14, 8.4670e+14, 1.1757e+14,
        2.7219e+13, 8.7691e+13, 1.8670e+15, 1.7191e+14, 1.8748e+14, 9.5524e+14,
        4.2937e+14, 1.6625e+14, 1.8241e+14, 1.3867e+14, 1.4312e+14, 1.8739e+14,
        6.6233e+14, 2.4311e+14, 2.2720e+15, 2.8621e+15, 9.5570e+13, 1.7868e+14,
        2.5048e+14, 2.0392e+15, 1.0775e+14, 1.5767e+14, 1.5098e+14, 1.1071e+15,
        4.4892e+14, 5.0005e+14, 2.6441e+14, 2.9633e+14, 2.7147e+15, 2.4446e+14,
        7.6952e+14, 2.8823e+14, 7.1096e+14, 4.4710e+15, 8.8293e+14, 1.3459e+15,
        2.5754e+15, 1.4478e+15, 1.6081e+15, 2.9728e+14, 1.9574e+15, 2.4905e+14,
        5.5354e+14, 1.2236e+15, 2.1482e+15, 3.3204e+14, 1.3695e+14, 4.2665e+13,
        5.9286e+15, 6.6887e+14, 1.1578e+14, 2.2870e+15, 9.2660e+14, 4.8591e+14,
        7.2138e+14, 1.0952e+15, 6.3555e+15, 5.5567e+14, 1.0793e+15, 3.3592e+14,
        4.9253e+14, 1.6144e+15, 2.7908e+13, 8.0835e+15, 1.1437e+15, 1.8758e+14,
        4.5588e+15, 1.6076e+15, 1.3156e+14, 2.6045e+16, 1.2826e+16, 1.3090e+15,
        6.8978e+15, 1.2019e+14, 1.8974e+15, 8.9160e+14, 7.1195e+14, 3.8615e+15,
        9.6670e+14, 6.6530e+14, 1.8645e+18, 1.6158e+15, 2.8551e+15, 3.3798e+14,
        3.9730e+14, 2.8299e+15, 5.0503e+15, 1.2097e+15, 7.2768e+14, 7.3791e+13,
        2.9202e+15, 1.1317e+15, 2.5190e+14, 4.3759e+14, 1.2789e+16, 3.4875e+16,
        6.6366e+13, 1.2367e+14, 1.0504e+15, 2.0547e+15, 3.0915e+14, 1.0933e+16,
        1.0397e+15, 4.0885e+14, 4.6462e+14, 6.8001e+14, 2.0948e+15, 9.4547e+13,
        2.7137e+14, 2.6818e+15, 1.0314e+16, 4.3951e+14, 1.6943e+14, 8.4137e+13,
        1.2108e+14, 7.6518e+15, 1.0764e+14, 2.6651e+15, 1.5932e+15, 4.0557e+14,
        7.8092e+13, 1.2942e+14, 1.4536e+15, 1.1206e+14, 6.0399e+14, 1.8783e+17,
        4.4445e+14, 9.0153e+14, 4.9004e+14, 5.3631e+13, 2.1085e+15, 2.9421e+14,
        7.8598e+14, 8.6529e+14, 5.5573e+13, 5.6163e+14, 7.4364e+13, 1.7132e+14,
        2.9822e+13, 2.6002e+14, 7.2087e+14, 4.6306e+15, 2.4550e+14, 8.0714e+14,
        7.9042e+14, 4.0704e+13, 1.7072e+16, 1.2268e+15, 8.9920e+13, 1.0051e+15,
        4.5793e+13, 1.6915e+15, 2.4968e+14, 6.7293e+15, 1.5658e+14, 8.1247e+14,
        1.3449e+14, 1.4051e+14, 1.6579e+14, 9.8562e+14, 7.8061e+14, 4.1879e+15,
        3.3976e+14, 1.6450e+15, 9.5784e+14, 1.4084e+15, 9.6173e+14, 1.2374e+15,
        9.1741e+14, 3.7404e+15, 3.8037e+14, 7.9254e+15, 5.2141e+14, 4.6478e+15,
        3.3683e+14, 5.1203e+14, 1.6071e+15, 1.7431e+14])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.1856e+14, 5.4086e+15, 1.3800e+15, 3.6822e+14, 5.6766e+14, 1.6261e+15,
        2.7276e+15, 1.8483e+15, 1.9896e+15, 2.1303e+14, 4.1903e+15, 2.3804e+15,
        4.1039e+15, 7.6161e+14, 1.2259e+15, 7.5726e+14, 9.2490e+14, 3.6514e+14,
        9.9567e+14, 9.5200e+14, 1.4429e+14, 2.8624e+14, 1.1127e+15, 5.5501e+14,
        5.3651e+14, 4.7379e+15, 2.5981e+14, 1.7761e+15, 8.2537e+14, 5.4656e+15,
        1.4078e+15, 9.4521e+14, 4.4141e+14, 7.5318e+14, 7.1430e+14, 5.2357e+15,
        1.2337e+15, 7.0184e+14, 1.0819e+16, 7.4062e+14, 7.3054e+14, 4.5644e+16,
        6.8953e+14, 1.7510e+15, 9.5466e+14, 9.2574e+14, 1.3507e+15, 1.2502e+15,
        3.9244e+14, 1.3896e+15, 5.5780e+16, 2.5482e+15, 5.9881e+14, 6.9625e+15,
        4.1991e+14, 6.1464e+14, 2.6647e+16, 5.7257e+14, 9.9743e+13, 2.3771e+15,
        4.0752e+14, 2.4898e+14, 1.4453e+15, 1.0998e+15, 7.9300e+14, 6.6218e+14,
        9.6413e+14, 1.0432e+15, 1.8168e+15, 4.7356e+14, 7.3874e+14, 2.8529e+15,
        2.4307e+14, 2.2448e+15, 1.2127e+15, 3.8424e+14, 1.2952e+15, 6.5034e+14,
        9.9885e+13, 8.5176e+14, 1.6480e+15, 5.4938e+14, 6.4532e+14, 8.3262e+14,
        7.9949e+14, 2.5126e+14, 5.9632e+14, 3.8832e+14, 4.6658e+14, 4.7926e+14,
        7.1153e+14, 8.3971e+14, 1.4596e+15, 3.3697e+15, 9.0248e+14, 3.6648e+14,
        1.1441e+15, 2.5044e+15, 6.6216e+14, 2.6415e+14, 8.2377e+14, 4.3320e+14,
        5.7510e+14, 8.9035e+14, 6.1522e+14, 4.3060e+14, 2.0050e+15, 8.2154e+14,
        7.6740e+14, 1.0261e+15, 4.0071e+14, 4.6234e+15, 1.2829e+15, 1.9386e+15,
        2.2265e+15, 1.3350e+15, 2.1180e+15, 3.5005e+14, 1.1615e+15, 3.5066e+14,
        1.9108e+14, 1.8032e+15, 2.0393e+15, 5.3051e+14, 9.5589e+14, 4.2787e+14,
        3.1319e+15, 8.7721e+14, 3.5595e+14, 1.9086e+15, 9.9864e+14, 1.0650e+15,
        7.9234e+14, 7.6421e+14, 5.8013e+15, 8.1331e+14, 1.1619e+15, 2.5113e+14,
        1.0174e+15, 1.0391e+15, 2.6795e+14, 7.6803e+15, 1.6010e+15, 8.1971e+14,
        4.9625e+15, 1.6683e+15, 3.3842e+14, 2.6172e+16, 1.4201e+16, 1.2335e+15,
        7.2171e+15, 3.2001e+14, 1.5172e+15, 1.6340e+15, 7.9859e+14, 4.6172e+15,
        9.9517e+14, 4.2488e+14, 1.8636e+18, 1.2662e+15, 2.9592e+15, 9.9745e+14,
        1.6547e+15, 3.9421e+15, 5.7081e+15, 1.1716e+15, 1.0295e+15, 2.5405e+14,
        2.8811e+15, 1.5434e+15, 9.1006e+14, 5.2048e+14, 1.3933e+16, 3.4345e+16,
        3.3672e+14, 4.2040e+14, 2.5219e+14, 2.2770e+15, 4.7144e+14, 1.1387e+16,
        1.7493e+15, 2.9043e+14, 5.8741e+14, 2.6831e+14, 2.6954e+15, 4.8757e+14,
        9.5277e+14, 2.6834e+15, 1.0210e+16, 1.1228e+15, 4.7082e+14, 6.3307e+14,
        3.7035e+14, 7.6747e+15, 5.1532e+14, 2.5271e+15, 1.8945e+15, 4.5065e+14,
        3.2994e+14, 4.9332e+14, 1.0433e+15, 1.9414e+14, 1.3252e+15, 1.8836e+17,
        5.6410e+14, 6.5632e+14, 3.4343e+14, 9.1947e+14, 2.0520e+15, 1.2273e+15,
        9.3094e+14, 9.5750e+14, 7.5859e+14, 4.8091e+14, 3.0911e+14, 5.3101e+14,
        5.7467e+14, 5.2974e+14, 2.7346e+14, 4.4394e+15, 2.8084e+14, 1.2121e+15,
        6.3546e+14, 4.7807e+14, 1.7552e+16, 1.5951e+15, 5.3538e+14, 7.4544e+14,
        4.7251e+14, 1.8324e+15, 7.0369e+14, 7.0714e+15, 3.5925e+14, 1.5418e+15,
        4.8010e+14, 6.8805e+14, 9.3537e+14, 1.3688e+15, 1.7129e+15, 4.7188e+15,
        1.1209e+15, 1.1363e+15, 1.1625e+15, 8.8559e+14, 3.5244e+14, 1.2147e+15,
        8.3422e+14, 3.7819e+15, 9.7576e+14, 7.9449e+15, 1.0385e+15, 4.3188e+15,
        5.3689e+14, 8.9363e+14, 9.2781e+14, 1.0614e+15])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6047e+15, 1.0553e+15, 1.9334e+15, 1.0245e+15, 1.0125e+15, 1.3237e+15,
        1.7811e+15, 6.7111e+14, 1.7724e+15, 1.0578e+15, 6.6661e+14, 1.1812e+15,
        5.6198e+14, 1.5968e+15, 1.7161e+15, 1.6142e+15, 5.0928e+14, 7.7951e+14,
        1.2940e+15, 1.6890e+15, 5.6909e+14, 8.5096e+14, 1.0804e+15, 1.3303e+15,
        1.1225e+15, 4.7822e+14, 8.4953e+14, 1.3408e+15, 1.3096e+15, 1.2217e+15,
        7.5241e+14, 1.0755e+15, 1.5579e+15, 1.3720e+15, 1.0534e+15, 1.2799e+15,
        8.0826e+14, 9.6862e+14, 8.5511e+14, 8.8491e+14, 1.1304e+15, 1.5586e+15,
        6.1450e+14, 8.1219e+14, 1.3499e+15, 1.5989e+15, 1.1960e+15, 1.2266e+15,
        6.3320e+14, 1.2087e+15, 9.7150e+14, 1.3502e+15, 1.4255e+15, 1.9702e+15,
        5.1065e+14, 6.3456e+14, 6.3425e+14, 2.2122e+15, 1.0677e+15, 2.5236e+15,
        1.5370e+15, 1.5803e+15, 2.0310e+15, 8.4635e+14])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.5015e+15, 1.9614e+15, 1.3393e+15, 1.2922e+15, 1.4573e+15, 2.3439e+15,
        2.1278e+15, 1.8841e+15, 9.6016e+14, 1.7683e+15, 7.6797e+14, 2.1413e+15,
        1.5927e+15, 2.0478e+15, 7.9031e+14, 1.1886e+15, 2.0090e+15, 1.1027e+15,
        1.2596e+15, 5.7625e+14, 6.1218e+14, 1.7755e+15, 1.3755e+15, 1.3776e+15,
        2.2123e+15, 1.0382e+15, 2.0707e+15, 1.6071e+15, 7.3767e+14, 9.7783e+14,
        1.3756e+15, 1.1151e+15, 1.9734e+15, 4.7458e+14, 1.2640e+15, 2.2177e+15,
        9.2371e+14, 1.2847e+15, 1.2910e+15, 7.5517e+14, 1.4735e+15, 1.3127e+15,
        5.9457e+14, 1.1575e+15, 1.6054e+15, 2.3442e+15, 1.5884e+15, 5.5831e+14,
        8.7295e+14, 1.8711e+15, 2.9565e+15, 1.3676e+15, 2.0310e+15, 1.0579e+15,
        1.4461e+15, 1.9293e+15, 2.1141e+15, 2.1881e+15, 1.1350e+15, 1.7149e+15,
        1.1217e+15, 2.8125e+15, 1.1193e+15, 9.2330e+14])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.7922e+13, 2.0691e+13, 2.9284e+13, 2.1431e+13, 2.0776e+13, 6.0135e+13,
        3.4672e+13, 2.3731e+13, 3.9874e+13, 2.2633e+12, 2.6158e+13, 3.5069e+13,
        1.2627e+15, 2.0971e+13, 3.3258e+13, 2.2555e+13, 1.9301e+13, 2.3448e+13,
        2.0937e+13, 2.3160e+13, 1.2746e+13, 1.1629e+13, 6.8085e+12, 8.0050e+12,
        1.0033e+13, 3.1587e+13, 3.0604e+13, 2.5210e+13, 2.4875e+13, 9.1304e+12,
        6.3797e+12, 8.8503e+13, 1.4119e+13, 2.1638e+13, 9.7914e+12, 2.1849e+13,
        2.1388e+13, 2.5223e+13, 1.1905e+14, 1.8030e+13, 2.6055e+13, 2.7577e+13,
        1.8819e+13, 1.0935e+13, 2.6742e+13, 1.3794e+13, 2.8684e+13, 1.6332e+14,
        1.6791e+13, 1.1108e+13, 2.9402e+13, 2.9480e+13, 2.0743e+13, 1.6213e+13,
        4.1860e+13, 1.9055e+13, 2.6897e+13, 2.2748e+13, 4.2872e+13, 5.4027e+12,
        3.2162e+13, 1.3542e+13, 3.0406e+13, 6.6222e+12, 2.0025e+13, 2.7016e+13,
        2.1189e+13, 4.5552e+12, 9.7053e+13, 1.8219e+13, 5.9844e+12, 2.6282e+13,
        2.1942e+13, 2.4603e+15, 2.5930e+13, 1.8160e+13, 7.3475e+13, 3.2555e+13,
        2.3609e+13, 2.0220e+13, 1.4905e+13, 2.6908e+13, 4.5366e+13, 1.6775e+13,
        7.0192e+12, 1.9559e+13, 1.3638e+13, 1.5610e+13, 2.6308e+12, 3.2842e+13,
        4.1347e+13, 1.9305e+13, 3.0292e+13, 1.5321e+13, 1.2493e+13, 1.7680e+13,
        2.8612e+13, 2.6000e+13, 8.4463e+12, 1.7161e+12, 2.4591e+13, 4.2609e+13,
        1.6005e+13, 2.7772e+13, 4.2672e+13, 2.9105e+13, 5.4631e+13, 1.9597e+13,
        2.3674e+13, 1.4369e+13, 3.2357e+13, 2.1600e+13, 2.3924e+13, 1.2430e+13,
        4.0843e+13, 2.4404e+13, 1.3110e+13, 1.1461e+13, 4.1443e+13, 1.8617e+13,
        2.3977e+13, 2.1644e+13, 1.2565e+13, 2.3278e+13, 2.0586e+12, 3.1181e+12,
        3.0889e+15, 2.0711e+13, 3.2698e+13, 7.0068e+13, 2.6848e+13, 5.4735e+12,
        1.5352e+13, 9.5836e+12, 1.0224e+13, 1.5356e+14, 1.2495e+14, 1.2303e+14,
        2.0002e+14, 2.1840e+13, 5.3613e+12, 9.4239e+12, 2.3667e+13, 1.8673e+13,
        3.5053e+13, 2.8256e+13, 1.6054e+13, 1.1300e+13, 1.1154e+15, 1.8392e+14,
        1.3729e+13, 3.4632e+13, 5.0448e+13, 5.4079e+12, 9.7607e+13, 2.4592e+13,
        9.7043e+12, 1.7682e+13, 3.0912e+13, 2.0335e+14, 2.2772e+13, 6.3316e+13,
        5.8321e+13, 1.6278e+13, 1.5347e+13, 5.7336e+13, 6.8141e+13, 9.0803e+12,
        9.6494e+13, 3.2647e+13, 1.1562e+14, 7.2514e+12, 8.6808e+12, 3.9722e+14,
        1.5156e+13, 2.0814e+12, 1.3066e+13, 2.2495e+13, 1.9651e+13, 1.4077e+13,
        8.5153e+12, 2.9300e+13, 2.0705e+13, 1.8971e+13, 1.3523e+13, 2.4588e+13,
        1.2369e+13, 1.1355e+13, 1.0023e+13, 2.6960e+13, 2.2325e+13, 1.3624e+13,
        5.3469e+12, 6.0770e+13, 3.6537e+12, 4.3149e+13, 1.1633e+13, 4.4813e+13,
        4.3606e+12, 1.9842e+13, 2.6606e+13, 8.5846e+12, 1.6801e+14, 1.4125e+13,
        1.4118e+13, 1.9930e+13, 1.4330e+13, 1.5062e+13, 3.1200e+13, 2.0741e+13,
        2.7483e+13, 1.6552e+13, 4.7932e+12, 5.9459e+13, 7.3328e+12, 1.8513e+13,
        1.8342e+13, 1.6445e+13, 6.2775e+13, 3.4606e+13, 2.4257e+13, 1.0825e+13,
        2.4341e+13, 2.7115e+13, 1.2958e+13, 3.2615e+13, 3.4862e+13, 7.4368e+12,
        4.6418e+12, 7.6880e+12, 2.7697e+13, 1.5494e+13, 2.7055e+12, 2.5928e+13,
        1.1586e+13, 4.0481e+12, 9.6606e+12, 1.9077e+13, 2.1508e+13, 6.7280e+12,
        3.3409e+12, 7.5440e+14, 6.0724e+13, 3.4757e+13, 3.0301e+13, 4.6163e+13,
        1.5851e+13, 1.3755e+13, 2.3837e+14, 3.0340e+13, 1.8736e+13, 1.5086e+13,
        1.7960e+13, 1.2160e+13, 5.4199e+13, 1.6437e+13])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5536e+13, 3.2724e+13, 6.0963e+13, 5.0207e+13, 4.5762e+13, 4.8805e+13,
        3.8145e+13, 3.0999e+13, 6.3711e+13, 3.8315e+13, 5.5892e+13, 4.0731e+13,
        3.5513e+13, 2.7292e+13, 7.5720e+13, 3.4733e+13, 2.9471e+13, 4.0610e+13,
        4.2683e+13, 6.3582e+13, 4.6477e+13, 2.8201e+13, 2.0048e+13, 3.9847e+13,
        5.0144e+13, 5.7228e+13, 4.1683e+13, 4.5296e+13, 3.4629e+13, 6.1065e+13,
        6.3453e+13, 4.4352e+13, 3.7430e+13, 4.0814e+13, 7.7526e+13, 4.0112e+13,
        4.1164e+13, 4.6965e+13, 3.6216e+13, 4.7829e+13, 3.3642e+13, 3.4719e+13,
        2.5829e+13, 4.0687e+13, 6.1756e+13, 3.3879e+13, 4.9910e+13, 3.6193e+13,
        6.8114e+13, 8.5898e+13, 2.7346e+13, 5.2519e+13, 3.5549e+13, 6.4894e+13,
        7.2217e+13, 3.0572e+13, 4.9521e+13, 2.5319e+13, 4.3873e+13, 2.6194e+13,
        3.0240e+13, 5.4069e+13, 2.8177e+13, 4.7649e+13])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.7713e+13, 9.1515e+13, 3.1406e+13, 9.0704e+13, 3.3498e+13, 2.8696e+13,
        6.0031e+13, 4.2602e+13, 4.2946e+13, 8.0545e+13, 5.5692e+13, 6.5318e+13,
        4.2224e+13, 3.9172e+13, 7.2220e+13, 4.6892e+13, 6.7438e+13, 4.3192e+13,
        2.3098e+13, 4.6788e+13, 3.4548e+13, 9.5834e+13, 5.8644e+13, 8.5789e+13,
        5.5545e+13, 7.8201e+13, 9.8777e+13, 3.1291e+13, 4.3322e+13, 5.5053e+13,
        8.0316e+13, 5.1322e+13, 5.2287e+13, 6.5341e+13, 6.6425e+13, 8.4741e+13,
        4.0633e+13, 3.7242e+13, 5.3818e+13, 6.8256e+13, 4.6663e+13, 4.5408e+13,
        7.7567e+13, 2.8678e+13, 9.1673e+13, 4.3326e+13, 4.6470e+13, 4.6142e+13,
        3.4263e+13, 2.9380e+13, 4.3346e+13, 3.4556e+13, 4.2785e+13, 3.8604e+13,
        5.8122e+13, 7.8431e+13, 4.4317e+13, 8.3600e+13, 3.6375e+13, 4.1815e+13,
        6.0740e+13, 3.8769e+13, 4.2121e+13, 5.5818e+13])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.2435e+11, 8.0631e+11, 1.0663e+12, 6.6990e+12, 2.1308e+12, 5.6794e+11,
        2.9551e+12, 1.3005e+12, 6.8860e+12, 2.5513e+11, 3.2954e+12, 1.4116e+12,
        1.8696e+12, 1.5491e+12, 1.7015e+12, 1.2958e+13, 3.4563e+12, 1.1027e+12,
        8.1999e+11, 6.4202e+11, 2.4054e+12, 5.9672e+12, 5.6238e+11, 2.8995e+12,
        1.9914e+11, 1.7646e+12, 1.1106e+12, 3.9428e+12, 1.7154e+12, 3.4212e+12,
        4.6951e+11, 6.5624e+12, 2.2887e+12, 3.0553e+13, 2.8570e+12, 1.2967e+12,
        1.3297e+12, 2.5867e+12, 2.1430e+12, 1.8107e+12, 8.4263e+12, 2.5211e+12,
        5.4099e+11, 8.7342e+11, 2.3576e+12, 1.8109e+12, 2.4985e+13, 4.8954e+12,
        3.3860e+12, 5.5397e+12, 3.8076e+12, 9.9961e+11, 1.6954e+12, 2.4557e+12,
        3.2880e+12, 2.1917e+12, 4.7720e+12, 3.2575e+12, 2.1741e+12, 2.2544e+11,
        3.2871e+12, 2.7302e+12, 8.1689e+11, 3.1117e+11, 8.2350e+12, 2.6703e+12,
        2.5806e+12, 2.8873e+11, 3.0885e+12, 5.2018e+12, 1.1953e+12, 1.8893e+12,
        1.0892e+12, 3.5565e+12, 1.5725e+12, 1.6945e+12, 3.7775e+12, 2.2767e+12,
        1.1037e+12, 1.6308e+12, 2.3940e+12, 1.4712e+12, 2.8144e+13, 9.3029e+11,
        2.1004e+11, 1.9324e+12, 2.2764e+12, 3.4280e+12, 1.7769e+11, 6.0842e+11,
        2.1967e+12, 1.0116e+12, 6.3550e+11, 4.0614e+11, 2.7731e+11, 1.5643e+12,
        1.4930e+12, 1.3132e+12, 7.6495e+12, 1.3692e+11, 5.2278e+11, 4.3169e+11,
        3.1479e+12, 1.0986e+12, 4.2795e+12, 1.9931e+12, 1.0227e+12, 7.4836e+12,
        9.4566e+11, 2.6492e+12, 2.2958e+12, 9.3929e+12, 2.1022e+12, 2.9381e+12,
        1.6944e+12, 1.3104e+12, 5.0586e+11, 1.4731e+12, 2.9172e+13, 1.6885e+12,
        1.6358e+12, 3.9490e+12, 2.5816e+12, 1.2885e+12, 5.5761e+11, 5.5310e+11,
        1.9197e+12, 8.8094e+12, 1.9571e+12, 2.0117e+13, 3.2680e+12, 2.7309e+11,
        7.9676e+11, 5.9011e+11, 6.5531e+12, 2.1532e+13, 1.4466e+12, 5.5744e+12,
        2.1454e+12, 1.3664e+12, 1.9604e+11, 3.2929e+11, 1.8285e+12, 1.2208e+12,
        2.9191e+12, 1.0230e+12, 5.6328e+11, 1.1044e+12, 1.4411e+12, 2.8537e+12,
        3.3482e+11, 1.3257e+13, 1.0918e+12, 1.0071e+12, 4.2839e+12, 1.2455e+12,
        7.6870e+11, 1.7373e+12, 1.0788e+13, 6.6995e+13, 1.7979e+12, 1.8987e+12,
        5.6363e+12, 4.6841e+12, 1.4705e+12, 1.3880e+12, 6.6773e+12, 2.0785e+12,
        2.3052e+12, 3.1520e+13, 1.0089e+12, 6.5654e+11, 1.2751e+12, 1.2410e+12,
        3.3786e+12, 6.9521e+11, 5.6039e+12, 1.6477e+13, 1.2794e+12, 2.8657e+12,
        4.7488e+11, 1.6033e+12, 1.2406e+12, 2.5287e+12, 2.3382e+12, 1.8476e+12,
        1.2446e+14, 1.4004e+12, 1.1012e+12, 8.4940e+12, 1.3198e+12, 4.0783e+11,
        6.2180e+11, 2.2003e+12, 2.5776e+11, 3.1967e+12, 1.2657e+12, 9.7060e+11,
        1.3710e+11, 2.8703e+12, 1.4512e+12, 2.4205e+11, 1.2061e+12, 1.2792e+12,
        2.2723e+12, 9.6754e+11, 8.3063e+11, 3.5170e+12, 6.5797e+11, 1.5385e+13,
        1.9424e+13, 5.7211e+12, 5.3965e+11, 5.2417e+11, 1.9517e+11, 1.5417e+12,
        1.7504e+12, 2.4720e+12, 2.1497e+12, 4.5795e+12, 2.9353e+12, 4.5633e+11,
        3.5513e+11, 1.1758e+12, 9.7536e+11, 1.5485e+12, 8.4729e+11, 1.0308e+12,
        5.9801e+11, 4.2346e+11, 2.2574e+12, 1.2768e+12, 1.1409e+12, 2.2625e+12,
        1.7329e+12, 9.7423e+10, 5.6437e+11, 4.9340e+12, 3.4940e+12, 1.3559e+12,
        4.5164e+11, 2.7385e+12, 1.2751e+12, 1.1065e+12, 1.3754e+12, 1.5964e+12,
        2.1540e+13, 2.9647e+12, 1.0064e+12, 1.6207e+12, 1.5036e+12, 1.8560e+12,
        1.5690e+12, 5.9828e+11, 1.5625e+12, 1.6708e+12])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.5438e+12, 1.3087e+12, 2.8596e+12, 2.2259e+12, 1.6535e+12, 1.8511e+12,
        2.2945e+12, 2.7407e+12, 2.4116e+12, 3.1199e+12, 2.1822e+12, 1.7484e+12,
        2.0791e+12, 1.7789e+12, 2.1074e+12, 1.8889e+12, 3.1627e+12, 1.5797e+12,
        1.7919e+12, 2.9344e+12, 2.6188e+12, 1.1603e+12, 3.0599e+12, 2.1501e+12,
        1.8438e+12, 2.3079e+12, 2.7863e+12, 2.9014e+12, 1.6183e+12, 2.0217e+12,
        2.5301e+12, 2.8560e+12, 2.1821e+12, 3.0371e+12, 1.2896e+12, 2.4290e+12,
        3.5780e+12, 1.6360e+12, 2.2948e+12, 2.3175e+12, 2.4950e+12, 1.4689e+12,
        3.0537e+12, 2.4429e+12, 3.4854e+12, 1.4639e+12, 3.3430e+12, 2.6244e+12,
        1.9890e+12, 3.3445e+12, 2.0520e+12, 1.7903e+12, 1.9107e+12, 2.4270e+12,
        2.7711e+12, 2.5517e+12, 1.7234e+12, 2.7766e+12, 2.3104e+12, 2.0328e+12,
        1.2881e+12, 1.8499e+12, 3.3430e+12, 2.3493e+12, 1.8313e+12, 2.6078e+12,
        3.4882e+12, 2.5651e+12, 2.1265e+12, 1.7812e+12, 1.8291e+12, 1.0798e+12,
        2.2151e+12, 3.9085e+12, 2.1329e+12, 2.2034e+12, 2.5083e+12, 1.7657e+12,
        2.4484e+12, 3.1856e+12, 2.6532e+12, 1.6400e+12, 2.4419e+12, 3.4758e+12,
        1.6370e+12, 1.7446e+12, 1.9819e+12, 1.9952e+12, 2.0003e+12, 2.5252e+12,
        2.3728e+12, 2.2700e+12, 2.6207e+12, 2.1591e+12, 1.4755e+12, 1.5135e+12,
        3.0040e+12, 4.0039e+12, 3.9445e+12, 2.3338e+12, 2.2112e+12, 3.0009e+12,
        2.2687e+12, 1.9454e+12, 1.7914e+12, 1.8355e+12, 2.4937e+12, 1.8819e+12,
        2.3353e+12, 3.1908e+12, 4.7198e+12, 2.4962e+12, 2.3895e+12, 3.1372e+12,
        2.7781e+12, 2.1451e+12, 1.7967e+12, 3.7514e+12, 3.2993e+12, 2.0282e+12,
        2.2093e+12, 1.9850e+12, 1.9154e+12, 4.3239e+12, 1.1972e+12, 2.7031e+12,
        1.3628e+12, 2.7315e+12])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.7505e+12, 2.6917e+12, 4.5053e+12, 2.4839e+12, 3.8909e+12, 3.2651e+12,
        4.1055e+12, 3.1694e+12, 2.9919e+12, 4.6180e+12, 5.0132e+12, 2.4288e+12,
        3.1001e+12, 3.1771e+12, 2.6219e+12, 3.8277e+12, 5.5021e+12, 4.2409e+12,
        3.6352e+12, 4.3183e+12, 4.6110e+12, 2.6366e+12, 5.4258e+12, 3.2821e+12,
        2.6890e+12, 2.2986e+12, 2.9126e+12, 6.9399e+12, 3.7915e+12, 3.8059e+12,
        3.5821e+12, 2.8563e+12, 4.9524e+12, 4.6383e+12, 2.8778e+12, 4.2618e+12,
        2.2081e+12, 4.0303e+12, 2.7240e+12, 4.1485e+12, 3.4066e+12, 2.8429e+12,
        2.9798e+12, 3.1903e+12, 2.9252e+12, 2.8626e+12, 4.3615e+12, 4.5032e+12,
        2.5605e+12, 3.2318e+12, 6.4272e+12, 1.9912e+12, 3.0879e+12, 3.2006e+12,
        3.9420e+12, 2.9004e+12, 4.9275e+12, 2.6911e+12, 3.8843e+12, 4.4547e+12,
        3.2551e+12, 3.4100e+12, 3.5305e+12, 3.5671e+12, 2.8249e+12, 4.6612e+12,
        2.2566e+12, 3.6060e+12, 2.8555e+12, 3.0109e+12, 2.8519e+12, 2.1571e+12,
        2.0830e+12, 2.4603e+12, 4.6816e+12, 2.6056e+12, 3.8398e+12, 2.5830e+12,
        2.4815e+12, 3.2767e+12, 3.7051e+12, 3.8576e+12, 3.1540e+12, 5.0638e+12,
        1.0449e+12, 3.7546e+12, 4.4385e+12, 3.4866e+12, 5.2549e+12, 3.1521e+12,
        5.2259e+12, 2.9891e+12, 3.4153e+12, 3.5846e+12, 3.1570e+12, 3.0783e+12,
        2.1817e+12, 5.0968e+12, 3.6063e+12, 3.2281e+12, 4.4496e+12, 2.3388e+12,
        3.9271e+12, 4.5349e+12, 3.3736e+12, 3.5176e+12, 3.3205e+12, 3.9798e+12,
        2.8981e+12, 3.3535e+12, 2.1470e+12, 3.9603e+12, 4.8773e+12, 2.8725e+12,
        3.1607e+12, 3.3354e+12, 4.1446e+12, 2.4407e+12, 2.2608e+12, 4.2064e+12,
        4.6367e+12, 4.3145e+12, 4.4549e+12, 2.5836e+12, 4.9563e+12, 4.0248e+12,
        4.0769e+12, 4.1218e+12])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.0425e+11, 4.9826e+10, 1.1372e+11, 4.8364e+10, 2.4810e+10, 1.5796e+10,
        2.6008e+10, 6.0760e+10, 7.0488e+11, 9.5286e+10, 1.1520e+10, 9.2395e+11,
        3.5469e+11, 2.2940e+11, 2.0631e+10, 1.3850e+11, 2.9294e+10, 2.5752e+12,
        3.8368e+10, 3.3758e+10, 2.6433e+11, 5.5608e+10, 8.0057e+10, 6.7603e+11,
        1.9702e+10, 2.3685e+11, 2.4480e+10, 8.5279e+10, 4.5319e+10, 3.0141e+10,
        1.2588e+11, 6.3300e+10, 6.9912e+10, 1.0389e+10, 1.9877e+11, 3.4298e+10,
        1.5073e+11, 1.1604e+11, 3.6148e+10, 1.7231e+11, 2.4829e+11, 1.3066e+12,
        4.9078e+10, 1.6158e+10, 1.0941e+11, 1.4618e+10, 7.8686e+11, 6.7186e+10,
        1.1029e+11, 1.0052e+11, 2.2245e+11, 1.7817e+11, 2.1209e+11, 9.4728e+10,
        8.6461e+10, 3.3011e+10, 1.3286e+10, 2.1138e+10, 6.4560e+10, 7.7618e+09,
        1.4528e+11, 2.1153e+11, 1.6102e+11, 8.0278e+10, 2.2528e+10, 2.3947e+11,
        3.1008e+10, 1.8475e+10, 3.1293e+11, 4.4280e+10, 3.6924e+10, 2.7557e+10,
        5.1116e+10, 1.4578e+11, 1.2203e+11, 3.8358e+10, 2.0273e+10, 3.4472e+10,
        6.4805e+10, 2.5524e+10, 7.9772e+10, 2.7832e+10, 1.0774e+11, 2.5863e+11,
        6.2918e+10, 1.7614e+11, 2.0818e+11, 1.2960e+11, 7.4873e+10, 1.2266e+10,
        4.9945e+10, 1.5096e+10, 1.1113e+11, 7.8135e+10, 2.9986e+10, 1.5514e+11,
        6.6428e+10, 1.7616e+10, 4.2866e+11, 2.8541e+10, 7.3433e+10, 9.6286e+10,
        3.7722e+10, 3.3083e+10, 4.1040e+10, 3.8961e+10, 9.7629e+09, 6.2449e+10,
        7.1150e+10, 1.4327e+10, 2.0864e+11, 6.3923e+10, 1.1727e+10, 4.3822e+11,
        2.9635e+11, 5.2481e+10, 2.9141e+10, 3.5483e+10, 1.4660e+10, 4.2902e+10,
        7.2727e+10, 1.8957e+11, 1.6331e+10, 9.4674e+09, 8.0829e+10, 9.6574e+10,
        2.6091e+10, 1.0728e+11, 2.7207e+11, 8.6843e+10, 6.6694e+10, 8.0655e+11,
        1.3836e+11, 1.8205e+11, 1.8007e+11, 8.5700e+10, 5.0011e+11, 9.6058e+10,
        1.0116e+10, 1.4016e+11, 2.2706e+10, 1.9836e+10, 1.6642e+11, 8.4810e+10,
        1.3546e+10, 5.7975e+11, 5.1515e+10, 3.9489e+11, 1.1379e+11, 4.7346e+11,
        1.0257e+11, 1.0370e+12, 1.9819e+11, 7.0365e+11, 1.6021e+11, 2.9521e+10,
        5.4917e+11, 8.5811e+10, 1.3127e+10, 1.1568e+11, 6.8192e+10, 2.3548e+10,
        1.8132e+10, 2.1120e+11, 7.8838e+10, 2.4632e+10, 3.6364e+11, 1.1598e+11,
        1.1946e+10, 2.1913e+11, 7.8490e+11, 5.5381e+11, 1.2564e+11, 1.9892e+10,
        2.4731e+11, 1.7191e+10, 1.7522e+10, 3.5073e+10, 1.8833e+10, 1.1921e+11,
        2.8672e+11, 1.0166e+11, 8.3920e+10, 2.5845e+12, 1.0475e+11, 2.1379e+10,
        1.3095e+11, 1.2346e+10, 1.5327e+11, 4.6842e+10, 3.1720e+10, 3.9487e+11,
        2.0061e+10, 1.0950e+11, 8.9244e+11, 6.5357e+10, 7.8509e+10, 1.4917e+12,
        4.7959e+10, 1.0699e+11, 1.1095e+11, 1.2971e+11, 1.2659e+11, 1.2281e+11,
        1.4333e+11, 2.3761e+11, 1.2184e+11, 2.8875e+11, 5.2408e+10, 1.5386e+10,
        5.9762e+10, 1.8475e+10, 3.3097e+10, 1.1361e+11, 2.7757e+12, 2.1475e+11,
        1.5198e+11, 3.5961e+11, 3.8451e+11, 1.8183e+11, 2.6927e+10, 6.9163e+09,
        4.1015e+10, 5.6800e+10, 9.0830e+10, 8.8996e+10, 1.0519e+11, 6.2554e+10,
        4.7931e+10, 2.9300e+10, 2.2221e+10, 3.4870e+10, 3.4276e+10, 1.0460e+10,
        5.7218e+12, 3.4414e+10, 2.0520e+11, 1.2658e+10, 5.0668e+11, 7.6770e+10,
        1.1326e+11, 9.2603e+10, 4.6945e+10, 1.6530e+10, 3.7292e+10, 1.7633e+10,
        3.5352e+10, 1.5877e+10, 2.8971e+11, 3.1801e+10, 1.6319e+11, 1.2477e+11,
        1.0553e+11, 2.3823e+10, 6.7133e+10, 1.5613e+11, 1.2979e+11, 1.0038e+11,
        5.8109e+10, 2.4707e+11, 1.5509e+10, 1.9320e+10, 3.6504e+10, 4.9951e+10,
        2.9139e+10, 5.1733e+10, 6.7774e+10, 1.2055e+11, 4.7907e+10, 6.1604e+10,
        8.4748e+10, 1.7195e+11, 1.6362e+11, 2.2087e+11, 1.7558e+11, 6.9212e+10,
        4.1525e+11, 2.6286e+10, 3.7067e+10, 1.2501e+10, 1.3779e+11, 1.6599e+11,
        3.0761e+11, 6.6906e+10, 4.9553e+10, 1.4160e+10, 1.1961e+11, 6.0991e+10,
        6.1169e+10, 2.7991e+10, 2.1770e+11, 1.3952e+10, 2.7808e+11, 5.0561e+10,
        2.5335e+11, 3.5763e+10, 5.3596e+10, 1.3664e+11, 1.6395e+11, 5.5056e+10,
        1.8288e+11, 2.1903e+10, 3.0581e+10, 1.6138e+11, 6.0640e+10, 4.1640e+10,
        8.2446e+09, 2.8217e+11, 2.2439e+11, 2.8013e+11, 1.2141e+11, 9.6375e+10,
        9.5119e+10, 1.0059e+11, 1.4376e+11, 2.9386e+10, 8.3426e+09, 2.7225e+10,
        9.9137e+11, 1.0783e+11, 1.0380e+11, 1.0853e+11, 3.7513e+11, 2.4076e+10,
        1.0546e+10, 4.9987e+10, 5.9365e+10, 1.9573e+12, 8.9568e+11, 4.4448e+10,
        4.5579e+11, 3.7777e+10, 1.2059e+11, 9.3010e+10, 4.8242e+10, 3.5864e+10,
        6.1321e+10, 1.1241e+11, 7.1470e+11, 1.9771e+12, 2.7944e+10, 1.5147e+11,
        1.7482e+11, 2.3091e+11, 1.3101e+10, 6.9316e+11, 3.7028e+10, 3.8791e+10,
        6.8902e+10, 2.4658e+11, 2.9172e+10, 2.7846e+10, 1.7012e+10, 1.0088e+11,
        6.5681e+10, 6.1182e+10, 8.5446e+11, 2.3133e+10, 7.0591e+10, 5.3223e+10,
        9.6457e+10, 5.7756e+10, 3.0158e+11, 4.8565e+10, 1.4295e+11, 1.7094e+11,
        2.4888e+10, 9.5815e+10, 7.3293e+10, 1.5236e+11, 2.5022e+11, 2.9331e+11,
        4.5306e+11, 2.8932e+11, 3.6375e+10, 3.3700e+11, 3.2449e+10, 1.4693e+10,
        1.1418e+10, 1.1781e+11, 1.2040e+11, 1.8346e+10, 2.7670e+11, 6.9008e+10,
        8.8479e+10, 3.1075e+10, 1.4820e+10, 7.4279e+10, 5.7182e+10, 1.4987e+11,
        1.7237e+11, 7.3135e+10, 1.1625e+11, 2.7831e+10, 2.3118e+10, 2.5281e+11,
        2.3944e+12, 2.4254e+10, 4.1505e+10, 2.2006e+10, 4.7506e+10, 1.0658e+11,
        8.8729e+10, 1.3155e+11, 8.3088e+11, 1.1324e+10, 3.2764e+10, 1.2675e+11,
        3.8516e+10, 1.5696e+11, 3.3194e+10, 8.8769e+11, 4.5658e+10, 7.2114e+10,
        6.0688e+10, 5.8720e+10, 1.2165e+11, 1.3560e+11, 2.1945e+11, 1.9351e+11,
        2.7142e+11, 3.8240e+10, 5.0686e+10, 2.2316e+10, 2.0978e+11, 1.7336e+11,
        2.5696e+10, 1.5906e+10, 3.4475e+10, 3.0982e+10, 1.1689e+10, 1.7810e+11,
        6.7567e+10, 8.9929e+10, 1.1213e+11, 9.2466e+10, 3.0856e+11, 2.7401e+10,
        4.8059e+10, 1.5425e+10, 1.0660e+10, 1.1153e+11, 4.3063e+11, 4.4024e+11,
        6.4658e+10, 4.3029e+10, 2.4264e+11, 1.9695e+10, 2.5248e+12, 7.8956e+10,
        2.8173e+10, 3.1372e+11, 5.5868e+10, 4.9541e+10, 1.4287e+11, 7.2304e+12,
        6.4866e+09, 4.6264e+10, 1.6763e+11, 3.7224e+10, 3.7619e+11, 1.7525e+10,
        6.7772e+11, 3.3493e+11, 1.6644e+11, 4.9627e+11, 3.0510e+10, 6.9894e+10,
        8.0270e+10, 5.3393e+10, 1.3508e+11, 1.3083e+10, 9.8501e+10, 2.6546e+11,
        1.8756e+11, 8.3120e+10, 3.0322e+10, 9.4505e+10, 1.9242e+11, 7.6621e+10,
        2.6927e+10, 7.1397e+10, 5.3634e+10, 2.1722e+11, 7.7284e+10, 1.0478e+11,
        2.5400e+10, 1.9254e+10, 8.8661e+10, 3.2119e+10, 4.3761e+10, 6.8631e+09,
        1.3723e+11, 1.7221e+10, 8.6747e+09, 1.2837e+11, 2.6727e+10, 9.5787e+10,
        4.4163e+10, 1.5032e+11, 3.0828e+10, 5.2032e+11, 1.7654e+11, 7.0980e+10,
        2.2952e+11, 5.1389e+10, 5.8838e+10, 7.3032e+10, 5.5855e+10, 5.2764e+10,
        3.9470e+10, 1.1633e+11])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.1057e+11, 5.2390e+10, 1.3327e+11, 7.8173e+10, 4.2843e+10, 3.3544e+10,
        3.6766e+10, 1.0160e+11, 6.8512e+11, 1.6750e+11, 4.4987e+10, 9.0256e+11,
        3.5051e+11, 2.1745e+11, 1.1641e+11, 1.3093e+11, 1.1624e+11, 2.6311e+12,
        6.5912e+10, 6.6357e+10, 2.2417e+11, 9.5375e+10, 9.6608e+10, 6.6015e+11,
        4.2326e+10, 2.4595e+11, 8.0507e+10, 2.5165e+10, 9.0392e+10, 1.0088e+11,
        1.0862e+11, 1.0032e+11, 1.6513e+11, 4.5632e+10, 2.2537e+11, 7.9402e+10,
        1.7536e+11, 1.7216e+11, 6.8078e+10, 1.1948e+11, 2.6126e+11, 1.2369e+12,
        1.2291e+11, 1.9036e+10, 1.2663e+11, 9.0761e+10, 7.4950e+11, 8.8328e+10,
        1.1872e+11, 8.3446e+10, 2.3059e+11, 1.5241e+11, 2.2565e+11, 1.2529e+11,
        9.0275e+10, 2.3490e+10, 5.2859e+10, 3.5644e+10, 6.2176e+10, 6.6106e+10,
        1.7361e+11, 3.0238e+11, 9.2845e+10, 1.7920e+11, 8.0085e+10, 1.9665e+11,
        1.5254e+11, 5.9849e+10, 2.7097e+11, 8.0682e+10, 5.0764e+10, 6.2410e+10,
        5.3463e+10, 1.0606e+11, 1.4729e+11, 8.6303e+10, 1.0153e+11, 1.0813e+11,
        9.0963e+10, 6.6634e+10, 1.2924e+11, 4.7412e+10, 9.0320e+10, 2.6589e+11,
        5.6885e+10, 1.6590e+11, 2.0275e+11, 1.4913e+11, 5.6446e+10, 7.4116e+10,
        1.2954e+11, 1.0066e+11, 6.8806e+10, 9.4867e+10, 6.3109e+10, 2.2056e+11,
        8.0433e+10, 6.1947e+10, 4.4040e+11, 3.4668e+10, 7.0873e+10, 1.0736e+11,
        4.6994e+10, 7.8019e+10, 3.1875e+10, 8.4164e+10, 8.8614e+10, 2.1905e+11,
        6.1104e+10, 9.3959e+10, 2.4463e+11, 1.7159e+11, 5.4564e+10, 4.8777e+11,
        2.5648e+11, 3.9670e+10, 3.7801e+10, 1.1616e+11, 7.3003e+10, 1.2634e+11,
        4.9467e+10, 2.1160e+11, 4.8711e+10, 4.2646e+10, 5.3684e+10, 3.9998e+10,
        5.7927e+10, 7.9746e+10, 2.5872e+11, 1.3850e+11, 9.4301e+10, 1.6778e+12,
        9.6484e+10, 1.6083e+11, 1.6611e+11, 1.0768e+11, 5.6216e+11, 1.4022e+11,
        9.4589e+10, 1.7261e+11, 5.9894e+10, 7.4722e+10, 1.6402e+11, 1.0907e+11,
        2.8045e+10, 6.6824e+11, 6.3946e+10, 4.3834e+11, 8.6566e+10, 5.3805e+11,
        8.3474e+10, 1.0466e+12, 1.8769e+11, 7.5928e+11, 1.7475e+11, 7.1931e+10,
        5.9967e+11, 6.0998e+10, 8.0431e+10, 1.6817e+11, 1.1353e+11, 1.0608e+11,
        6.2896e+10, 2.0999e+11, 6.0002e+10, 1.4159e+11, 3.1175e+11, 1.3948e+11,
        4.6768e+10, 2.8785e+11, 7.6163e+11, 6.0967e+11, 1.4743e+11, 1.0334e+11,
        2.3304e+11, 6.7211e+10, 6.6803e+10, 8.0404e+10, 5.6420e+10, 1.1423e+11,
        3.1658e+11, 7.4711e+10, 8.4449e+10, 2.6030e+12, 1.8482e+11, 8.1425e+10,
        1.3125e+11, 6.0283e+10, 1.5148e+11, 5.4960e+10, 6.8237e+10, 3.0721e+11,
        6.2555e+10, 1.0710e+11, 9.2788e+11, 7.9740e+10, 4.1997e+10, 1.4755e+12,
        2.3070e+10, 1.0584e+11, 1.0345e+11, 1.2036e+11, 1.4184e+11, 1.3916e+11,
        8.7257e+10, 3.1067e+11, 1.8311e+11, 3.1969e+11, 6.7148e+10, 4.6488e+10,
        8.4533e+10, 2.9042e+10, 6.1260e+10, 1.5258e+11, 2.7952e+12, 2.8234e+11,
        2.0583e+11, 3.1267e+11, 3.9277e+11, 2.5766e+11, 4.5637e+10, 4.1006e+10,
        8.6618e+10, 9.3306e+10, 7.6600e+10, 5.2847e+10, 7.5267e+10, 7.5515e+10,
        1.3185e+11, 7.5889e+10, 8.4376e+10, 5.0875e+10, 2.9935e+10, 1.1596e+11,
        5.7122e+12, 4.9546e+10, 2.2273e+11, 6.5305e+10, 4.8604e+11, 9.5477e+10,
        2.5070e+11, 6.6583e+10, 7.5494e+10, 6.0503e+10, 9.4913e+10, 6.0223e+10,
        7.6205e+10, 8.2110e+10, 3.2720e+11, 5.5022e+10, 1.7271e+11, 1.0640e+11,
        1.6711e+11, 4.6801e+10, 6.8490e+10, 2.0625e+11, 9.2819e+10, 5.0354e+10,
        6.6199e+10, 2.2935e+11, 2.5993e+10, 1.2386e+11, 3.2667e+10, 9.8857e+10,
        8.9990e+10, 4.7098e+10, 1.0515e+11, 1.3760e+11, 6.2204e+10, 6.4657e+10,
        1.3293e+11, 1.8319e+11, 1.7321e+11, 1.9250e+11, 2.0708e+11, 8.8344e+10,
        3.9464e+11, 1.2416e+11, 3.2841e+10, 8.3055e+10, 1.7364e+11, 1.8173e+11,
        3.8172e+11, 4.6195e+10, 7.7099e+10, 3.5882e+10, 7.9397e+10, 1.0141e+11,
        6.0966e+10, 7.1334e+10, 1.8716e+11, 7.6534e+10, 3.2722e+11, 1.5642e+11,
        2.8373e+11, 5.9263e+10, 6.6366e+10, 1.8678e+11, 2.1527e+11, 1.2083e+11,
        1.9709e+11, 1.0439e+11, 3.8178e+10, 1.8422e+11, 8.4131e+10, 8.1616e+10,
        5.8868e+10, 2.5037e+11, 2.2461e+11, 3.1622e+11, 1.5032e+11, 1.4275e+11,
        1.5562e+11, 1.0153e+11, 1.2157e+11, 8.7115e+10, 4.1054e+10, 3.0830e+10,
        9.8974e+11, 1.2582e+11, 1.4609e+11, 1.2199e+11, 3.8228e+11, 6.3302e+10,
        8.2585e+10, 9.9737e+10, 5.8215e+10, 1.9773e+12, 8.7202e+11, 5.3380e+10,
        4.7776e+11, 4.6520e+10, 9.0758e+10, 1.1890e+11, 8.0027e+10, 6.3934e+10,
        7.0069e+10, 1.2529e+11, 7.1538e+11, 2.0270e+12, 2.8021e+10, 1.7760e+11,
        1.7151e+11, 2.5259e+11, 6.7380e+10, 7.0965e+11, 7.6296e+10, 6.7872e+10,
        5.4311e+10, 2.7703e+11, 9.1032e+10, 7.8067e+10, 5.5754e+10, 1.1452e+11,
        1.4400e+11, 4.6961e+10, 8.9325e+11, 5.8965e+10, 3.7778e+10, 1.3001e+11,
        1.6494e+11, 1.0297e+11, 3.3592e+11, 8.8741e+10, 2.4131e+11, 4.7828e+11,
        7.6223e+10, 1.1401e+11, 3.9459e+10, 7.8824e+10, 3.0516e+11, 2.1100e+11,
        5.0369e+11, 3.4443e+11, 4.6519e+10, 3.0491e+11, 5.5008e+10, 3.8601e+10,
        7.3953e+10, 8.5918e+10, 1.8401e+11, 6.1351e+10, 2.3835e+11, 7.2127e+10,
        1.2343e+11, 5.4710e+10, 6.7393e+10, 3.5840e+10, 1.4329e+11, 1.7077e+11,
        2.0660e+11, 7.4606e+10, 1.1247e+11, 7.2626e+10, 6.4718e+10, 2.7275e+11,
        2.4148e+12, 5.5302e+10, 3.3711e+10, 5.0528e+10, 3.1370e+10, 7.3938e+10,
        6.4906e+10, 2.0953e+11, 8.3381e+11, 5.3421e+10, 5.7083e+10, 9.9252e+10,
        8.0942e+10, 2.3410e+11, 8.9091e+10, 8.2520e+11, 4.4640e+10, 7.5179e+10,
        5.9101e+10, 2.2966e+10, 1.5538e+11, 2.0259e+11, 2.0751e+11, 2.5219e+11,
        3.6944e+11, 4.3608e+10, 5.7870e+10, 3.7022e+10, 4.9337e+11, 2.0830e+11,
        8.8753e+10, 7.8377e+10, 1.2348e+11, 3.3718e+10, 1.1100e+11, 1.6204e+11,
        1.2504e+11, 1.5698e+11, 1.4875e+11, 1.2288e+11, 3.4853e+11, 7.7424e+10,
        1.0976e+11, 7.6203e+10, 5.6263e+10, 1.2113e+11, 4.2660e+11, 4.0716e+11,
        9.4519e+10, 1.2050e+11, 1.9632e+11, 6.3572e+10, 2.5818e+12, 8.7422e+10,
        9.7251e+10, 3.4202e+11, 2.5276e+10, 6.0547e+10, 1.7610e+11, 7.2006e+12,
        3.9665e+10, 3.3414e+10, 1.8215e+11, 8.2686e+10, 3.4965e+11, 4.3161e+10,
        7.1889e+11, 3.4821e+11, 1.6402e+11, 3.9587e+11, 6.6572e+10, 1.2176e+11,
        1.7416e+11, 1.0663e+11, 1.6842e+11, 5.2047e+10, 9.6897e+10, 4.1405e+11,
        2.2331e+11, 1.3386e+11, 6.3549e+10, 1.5789e+11, 2.3901e+11, 2.0627e+11,
        5.4807e+10, 4.0576e+10, 4.3223e+10, 2.3963e+11, 1.2482e+11, 1.2522e+11,
        4.9371e+10, 8.9439e+10, 8.7258e+10, 7.4757e+10, 5.7771e+10, 4.4510e+10,
        1.5616e+11, 3.7893e+10, 6.3187e+10, 1.1241e+11, 9.4770e+10, 5.2598e+10,
        1.1805e+11, 1.3735e+11, 1.1086e+11, 5.0844e+11, 2.2406e+11, 1.0675e+11,
        2.7317e+11, 1.2692e+11, 4.9097e+10, 9.1056e+10, 4.1389e+10, 6.1888e+10,
        6.4307e+10, 1.8699e+11])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.7051e+11, 8.2334e+10, 1.2263e+11, 1.1736e+11, 1.2669e+11, 1.7409e+11,
        1.7558e+11, 1.5954e+11, 2.1671e+11, 2.5956e+11, 1.0825e+11, 9.7105e+10,
        1.4485e+11, 1.4543e+11, 1.4621e+11, 9.4208e+10, 1.9230e+11, 1.4371e+11,
        1.7416e+11, 1.7194e+11, 1.7407e+11, 2.8606e+11, 1.4152e+11, 1.0716e+11,
        2.2482e+11, 1.3968e+11, 1.7529e+11, 1.2008e+11, 1.0893e+11, 1.1136e+11,
        1.6270e+11, 1.9251e+11, 1.5518e+11, 1.2503e+11, 1.9857e+11, 1.6461e+11,
        1.4883e+11, 2.2354e+11, 1.6310e+11, 1.1470e+11, 1.2584e+11, 1.9069e+11,
        1.6098e+11, 2.4303e+11, 1.0499e+11, 7.5400e+10, 1.6910e+11, 1.2364e+11,
        1.8820e+11, 1.2933e+11, 1.9071e+11, 1.6653e+11, 1.1620e+11, 1.8539e+11,
        1.2242e+11, 1.0898e+11, 1.3000e+11, 1.7204e+11, 1.6559e+11, 1.3572e+11,
        1.6799e+11, 1.7534e+11, 1.6433e+11, 1.9064e+11, 1.2146e+11, 8.0312e+10,
        1.8244e+11, 1.7275e+11, 1.7059e+11, 1.8308e+11, 6.7531e+10, 1.5001e+11,
        1.6575e+11, 1.0587e+11, 1.3564e+11, 8.1954e+10, 1.9401e+11, 1.5375e+11,
        1.2585e+11, 1.7690e+11, 7.0777e+10, 1.0067e+11, 1.0420e+11, 1.2282e+11,
        9.7933e+10, 8.1013e+10, 1.3743e+11, 1.9337e+11, 1.0230e+11, 1.3543e+11,
        1.1719e+11, 1.0514e+11, 8.2626e+10, 1.6036e+11, 1.3500e+11, 1.8501e+11,
        1.5251e+11, 1.7672e+11, 1.6463e+11, 3.0536e+11, 1.9624e+11, 1.4966e+11,
        1.6228e+11, 2.1707e+11, 2.5419e+11, 1.3668e+11, 1.5742e+11, 1.0943e+11,
        1.4432e+11, 1.9688e+11, 1.0834e+11, 2.0988e+11, 1.2852e+11, 6.6600e+10,
        2.2461e+11, 1.6065e+11, 1.4401e+11, 1.8879e+11, 1.1685e+11, 1.6670e+11,
        1.6219e+11, 1.5500e+11, 1.6185e+11, 2.6208e+11, 1.8326e+11, 1.1977e+11,
        1.3814e+11, 1.5325e+11])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.1208e+11, 2.4629e+11, 2.6894e+11, 2.9433e+11, 2.5313e+11, 3.7763e+11,
        2.9895e+11, 2.0424e+11, 3.8010e+11, 1.4878e+11, 3.0364e+11, 2.0843e+11,
        2.6640e+11, 2.4803e+11, 2.5655e+11, 2.1478e+11, 3.4092e+11, 4.1046e+11,
        1.5953e+11, 1.5958e+11, 3.2800e+11, 1.3102e+11, 3.2068e+11, 2.2230e+11,
        1.6553e+11, 3.0574e+11, 2.6242e+11, 2.2024e+11, 1.4740e+11, 2.0991e+11,
        4.3836e+11, 3.7570e+11, 2.2636e+11, 3.8445e+11, 3.0537e+11, 2.2829e+11,
        2.6071e+11, 3.6173e+11, 2.1530e+11, 2.2167e+11, 1.9541e+11, 3.1887e+11,
        2.8795e+11, 2.7261e+11, 3.4052e+11, 3.0305e+11, 2.0794e+11, 2.2133e+11,
        3.5852e+11, 1.6084e+11, 3.2815e+11, 2.5669e+11, 1.3404e+11, 1.9288e+11,
        3.1443e+11, 2.5332e+11, 3.4406e+11, 2.3779e+11, 3.6856e+11, 4.1005e+11,
        2.4775e+11, 4.0057e+11, 4.0640e+11, 4.1823e+11, 2.4650e+11, 2.6600e+11,
        2.6208e+11, 2.4413e+11, 1.9532e+11, 3.2845e+11, 2.3379e+11, 2.3752e+11,
        2.8863e+11, 3.8082e+11, 2.5688e+11, 4.5117e+11, 2.3564e+11, 2.0893e+11,
        1.8678e+11, 1.5127e+11, 1.1350e+11, 1.6654e+11, 2.7145e+11, 4.0038e+11,
        2.4899e+11, 1.1642e+11, 4.0724e+11, 3.2089e+11, 2.9868e+11, 1.9021e+11,
        3.2368e+11, 1.8329e+11, 1.8264e+11, 2.4457e+11, 2.8159e+11, 2.6264e+11,
        2.4946e+11, 2.8482e+11, 2.3833e+11, 1.9178e+11, 4.1792e+11, 1.7108e+11,
        1.9274e+11, 2.6496e+11, 3.2363e+11, 3.4669e+11, 1.0093e+11, 1.7671e+11,
        3.7804e+11, 2.2688e+11, 3.2973e+11, 2.7734e+11, 2.9493e+11, 1.8210e+11,
        2.9299e+11, 3.0080e+11, 2.9487e+11, 3.5358e+11, 3.8869e+11, 2.2280e+11,
        1.7100e+11, 2.0262e+11, 1.9989e+11, 3.2583e+11, 1.7474e+11, 3.2675e+11,
        4.4819e+11, 3.8937e+11])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.2822e+10, 3.1938e+10, 5.0207e+09, 3.8426e+09, 6.2111e+09, 1.0653e+10,
        2.5512e+09, 7.9191e+09, 3.0130e+10, 1.2080e+10, 9.7419e+09, 9.6136e+09,
        9.7561e+09, 1.0815e+10, 6.8235e+08, 7.7309e+09, 3.5935e+09, 1.3010e+10,
        2.2806e+10, 8.1941e+09, 2.4033e+10, 1.0528e+10, 2.3991e+10, 1.1230e+10,
        3.5627e+09, 2.7153e+10, 9.2540e+09, 7.5806e+09, 7.6572e+10, 9.7700e+09,
        5.6377e+09, 6.5734e+09, 6.3517e+09, 2.2001e+09, 5.2363e+09, 2.9369e+09,
        1.3131e+10, 6.9839e+09, 2.6473e+09, 3.9827e+09, 1.0927e+10, 6.2598e+10,
        3.4410e+09, 1.3892e+10, 1.6932e+10, 4.8396e+09, 7.0165e+09, 4.9228e+09,
        1.0356e+10, 1.5082e+10, 1.0204e+10, 1.9023e+10, 6.9334e+09, 2.2147e+10,
        7.5300e+09, 1.6868e+10, 8.9839e+09, 1.2456e+10, 1.0125e+10, 1.5742e+09,
        5.4055e+09, 2.2883e+10, 3.3137e+10, 2.1293e+09, 2.9817e+09, 4.5430e+09,
        1.0268e+10, 2.2367e+10, 5.3455e+10, 1.1349e+10, 1.2705e+10, 7.1023e+09,
        8.7885e+09, 1.1454e+10, 9.1806e+09, 1.8643e+10, 6.0474e+09, 2.7366e+09,
        1.1521e+10, 2.0896e+09, 3.9286e+09, 9.0101e+09, 8.0131e+09, 3.5189e+10,
        2.4471e+10, 7.3966e+09, 4.4841e+09, 2.1540e+09, 9.3645e+09, 1.5057e+10,
        9.3589e+09, 1.1324e+10, 4.2032e+09, 9.5066e+09, 4.2463e+09, 8.4687e+09,
        9.4269e+09, 8.0518e+09, 9.1825e+09, 1.1927e+10, 3.8460e+09, 9.4307e+09,
        2.7881e+10, 8.1016e+09, 1.0155e+10, 2.3356e+10, 6.6481e+09, 1.3347e+11,
        6.8280e+09, 1.1363e+10, 7.7056e+09, 1.3812e+10, 1.9314e+09, 4.7097e+09,
        1.3864e+10, 1.6690e+10, 1.0393e+09, 7.4947e+09, 1.7462e+09, 1.4615e+10,
        7.5510e+09, 3.8556e+09, 4.4538e+09, 8.1640e+09, 6.4782e+09, 5.2742e+09,
        1.1639e+10, 6.3145e+09, 1.3659e+10, 2.2576e+09, 6.9477e+09, 8.4224e+11,
        6.3994e+09, 1.4068e+10, 6.7855e+09, 1.2791e+10, 1.5141e+10, 1.0496e+10,
        1.3917e+10, 9.4497e+09, 1.2538e+10, 2.0499e+09, 1.0873e+10, 5.6658e+09,
        1.1409e+10, 1.1605e+10, 8.6408e+09, 8.0843e+09, 1.0006e+10, 3.4932e+09,
        4.8837e+09, 6.7200e+09, 1.2952e+10, 1.2161e+10, 1.0497e+10, 5.3725e+09,
        1.2892e+10, 8.4590e+09, 1.7135e+10, 1.0068e+10, 1.0926e+10, 2.7261e+09,
        1.1596e+09, 1.4795e+10, 3.1235e+09, 1.7709e+10, 2.9530e+10, 1.2058e+10,
        1.0036e+10, 6.2445e+09, 6.7516e+09, 8.7434e+09, 2.4664e+10, 4.5610e+09,
        7.6976e+09, 7.5605e+08, 7.6888e+09, 6.8695e+09, 3.5349e+09, 5.3270e+09,
        2.4802e+09, 9.1762e+09, 1.3551e+10, 2.8344e+09, 1.5621e+10, 8.8425e+09,
        7.6421e+09, 1.0395e+10, 1.0998e+10, 3.6335e+09, 1.2017e+10, 7.1152e+09,
        4.0516e+08, 1.6738e+09, 7.9903e+09, 8.6307e+09, 3.7175e+09, 1.0604e+10,
        8.4589e+09, 5.1242e+09, 1.4489e+10, 8.6601e+09, 2.3267e+09, 1.8971e+10,
        5.9152e+10, 1.1162e+10, 7.9970e+09, 1.9532e+10, 1.2537e+10, 1.0593e+10,
        6.1100e+09, 7.9049e+09, 5.8273e+09, 1.0460e+10, 3.1539e+09, 2.1894e+10,
        1.1331e+10, 1.2646e+10, 7.9541e+09, 1.0540e+10, 8.1004e+08, 2.0258e+10,
        2.7176e+09, 1.8524e+10, 1.3363e+10, 7.7441e+09, 9.7754e+09, 9.2253e+09,
        6.8719e+09, 2.3483e+09, 5.3855e+09, 1.2218e+10, 9.2999e+09, 2.6920e+09,
        9.6042e+09, 1.9317e+10, 9.2222e+09, 7.2371e+09, 7.8006e+09, 5.4272e+09,
        3.4351e+10, 7.0504e+09, 5.3627e+09, 1.3238e+10, 7.0698e+09, 8.2583e+09,
        3.3244e+10, 1.4476e+09, 1.5182e+10, 1.0276e+10, 9.7369e+09, 2.2402e+10,
        8.5415e+09, 1.3345e+10, 2.8546e+10, 8.5283e+09, 1.5369e+10, 5.6566e+09,
        9.2718e+09, 8.4263e+09, 2.2033e+09, 9.5601e+09, 8.2283e+09, 9.3760e+09,
        1.1260e+10, 1.0380e+10, 1.4989e+10, 1.1237e+10, 8.2821e+09, 7.8763e+09,
        8.1284e+09, 1.4500e+10, 5.1426e+09, 9.7853e+09, 2.5537e+10, 1.3846e+10,
        1.3418e+10, 1.8958e+10, 1.2824e+10, 6.5245e+08, 4.1088e+09, 1.4089e+10,
        1.0912e+11, 7.4186e+09, 5.0350e+09, 9.6812e+09, 9.2789e+09, 1.1095e+10,
        1.4662e+10, 7.3911e+09, 3.1064e+11, 7.7837e+09, 5.7321e+09, 1.6454e+10,
        6.8936e+09, 3.9644e+09, 7.1660e+09, 3.8120e+09, 1.5448e+10, 7.9959e+09,
        2.0392e+10, 2.7949e+09, 6.8168e+09, 5.6887e+10, 1.1971e+10, 1.1650e+10,
        5.1680e+08, 8.5564e+09, 1.7431e+10, 7.5926e+09, 5.7251e+09, 6.9008e+09,
        1.9301e+10, 8.9461e+09, 2.7887e+09, 9.9861e+09, 1.6910e+10, 1.1331e+10,
        6.9750e+09, 1.2170e+10, 8.1413e+09, 1.3910e+10, 2.2912e+10, 3.4870e+09,
        5.8535e+09, 5.0800e+09, 8.6947e+09, 7.7481e+09, 9.0689e+09, 1.0409e+10,
        1.6702e+10, 7.2840e+09, 1.3511e+10, 1.7712e+10, 5.3901e+10, 6.4704e+09,
        7.9271e+09, 6.0588e+09, 1.2855e+11, 1.7082e+10, 3.9994e+09, 9.4841e+09,
        9.3944e+09, 1.2523e+10, 6.6522e+09, 8.1715e+09, 7.0157e+09, 1.3230e+10,
        6.8842e+09, 1.9939e+10, 2.0499e+10, 8.1561e+09, 1.3322e+10, 6.0817e+10,
        2.9476e+09, 7.0454e+09, 3.8182e+09, 7.2736e+09, 5.3787e+09, 1.9962e+10,
        1.4130e+10, 1.1299e+10, 2.9848e+09, 4.5804e+09, 1.9727e+11, 1.8711e+11,
        9.5262e+09, 3.4275e+10, 1.0038e+10, 7.0181e+09, 5.2969e+09, 6.5620e+09,
        1.9978e+10, 7.8444e+09, 6.1835e+09, 7.5700e+09, 3.8757e+10, 7.7543e+09,
        7.5435e+08, 8.8325e+09, 3.6352e+09, 7.4812e+09, 5.9272e+09, 9.7821e+09,
        7.4221e+09, 9.8618e+09, 9.3366e+09, 1.0333e+10, 2.2151e+09, 5.7694e+09,
        1.3839e+10, 1.4174e+10, 4.4833e+09, 1.2224e+10, 6.4037e+09, 1.1174e+10,
        5.8003e+09, 5.5049e+09, 5.4792e+09, 7.4776e+10, 6.9853e+09, 1.6866e+10,
        3.3699e+09, 1.3989e+10, 1.0399e+10, 3.0738e+10, 2.8713e+10, 1.1795e+10,
        4.1548e+09, 5.4331e+10, 3.4441e+09, 8.9428e+09, 1.7242e+09, 1.1906e+10,
        1.1505e+10, 6.4456e+08, 8.6871e+09, 6.1829e+09, 1.1287e+10, 1.3316e+10,
        5.6338e+10, 1.2649e+10, 3.7640e+10, 7.4320e+09, 2.9624e+11, 2.5761e+09,
        5.1611e+09, 2.7156e+09, 4.3430e+09, 2.7608e+09, 6.0851e+08, 1.3398e+10,
        1.1337e+10, 1.5213e+10, 5.2293e+09, 9.6116e+09, 2.9979e+09, 6.7262e+09,
        7.1734e+09, 7.6701e+09, 1.2557e+09, 8.1656e+09, 7.3297e+09, 5.7431e+10,
        1.1431e+10, 6.9005e+09, 1.1610e+10, 8.8937e+09, 8.2712e+09, 2.3517e+10,
        1.9359e+10, 8.0660e+09, 5.8375e+09, 2.7446e+10, 1.9148e+10, 8.9562e+09,
        2.1723e+10, 8.3856e+09, 7.7149e+09, 5.1638e+09, 5.7954e+09, 5.0753e+09,
        2.7858e+09, 9.9442e+09, 2.6446e+09, 1.0341e+10, 1.6206e+10, 8.5883e+09,
        1.4666e+11, 3.3034e+09, 9.4830e+10, 7.5205e+09, 3.3548e+09, 7.6737e+10,
        1.4965e+10, 1.0767e+10, 4.1146e+09, 7.1914e+09, 2.6705e+10, 8.5659e+09,
        1.3298e+10, 3.8932e+09, 2.1205e+09, 7.1227e+09, 1.6432e+10, 8.0113e+09,
        8.7955e+09, 8.0768e+09, 9.8663e+09, 2.0521e+10, 4.9524e+09, 4.1763e+08,
        1.2932e+10, 7.2625e+09, 1.2075e+10, 6.5589e+09, 1.2194e+10, 1.1195e+10,
        1.8120e+10, 3.0902e+09, 8.8904e+09, 7.8728e+09, 6.9842e+09, 1.2521e+10,
        9.2799e+09, 6.4596e+09, 5.8786e+09, 4.9199e+09, 6.6505e+09, 7.1482e+09,
        5.6276e+09, 3.2874e+09])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.4922e+10, 2.6763e+10, 2.6188e+10, 3.0079e+10, 2.5896e+10, 1.4715e+10,
        4.2975e+10, 2.9714e+10, 1.8482e+10, 2.6236e+10, 1.7311e+10, 1.9928e+10,
        2.4721e+10, 2.5933e+10, 2.7678e+10, 2.0371e+10, 2.4965e+10, 1.7642e+10,
        2.8466e+10, 2.0299e+10, 1.9240e+10, 2.0395e+10, 4.5888e+10, 3.3232e+10,
        2.0305e+10, 1.4758e+10, 1.7927e+10, 3.1256e+10, 2.0590e+10, 2.8146e+10,
        3.1836e+10, 1.8521e+10, 3.4261e+10, 1.9386e+10, 3.1413e+10, 4.5388e+10,
        2.3589e+10, 1.9129e+10, 2.1605e+10, 3.9150e+10, 1.6714e+10, 1.7748e+10,
        2.3322e+10, 2.7938e+10, 2.3094e+10, 1.7611e+10, 1.4598e+10, 1.5807e+10,
        1.8104e+10, 3.2017e+10, 1.9873e+10, 3.6883e+10, 2.6566e+10, 1.7870e+10,
        2.5131e+10, 1.3164e+10, 2.3488e+10, 1.7072e+10, 2.8055e+10, 2.2096e+10,
        2.1720e+10, 2.2095e+10, 2.5801e+10, 2.1114e+10, 3.4553e+10, 2.4844e+10,
        2.3859e+10, 1.8075e+10, 3.9872e+10, 3.2947e+10, 2.4876e+10, 1.9565e+10,
        2.2506e+10, 2.4988e+10, 1.0254e+10, 3.4290e+10, 3.1500e+10, 4.2212e+10,
        3.0000e+10, 1.2213e+10, 1.6529e+10, 2.0353e+10, 2.0622e+10, 2.7661e+10,
        2.5932e+10, 2.8915e+10, 2.2254e+10, 3.0269e+10, 3.3065e+10, 3.1257e+10,
        1.8026e+10, 3.4670e+10, 3.0840e+10, 2.0541e+10, 1.8110e+10, 4.6578e+10,
        1.9983e+10, 3.1676e+10, 1.5578e+10, 2.1571e+10, 2.4841e+10, 1.7619e+10,
        1.7030e+10, 1.0740e+10, 1.7210e+10, 2.5137e+10, 1.3615e+10, 1.9404e+10,
        1.2026e+10, 1.6174e+10, 1.8974e+10, 2.1781e+10, 1.5553e+10, 2.2093e+10,
        2.1903e+10, 3.9586e+10, 3.0371e+10, 2.7947e+10, 3.3868e+10, 3.5694e+10,
        2.4125e+10, 2.4637e+10, 2.0887e+10, 4.3591e+10, 1.7696e+10, 2.7498e+10,
        1.9171e+10, 3.1830e+10])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.8206e+10, 4.2199e+10, 6.7610e+10, 6.7499e+10, 4.4345e+10, 5.6218e+10,
        2.7837e+10, 2.5634e+10, 5.1480e+10, 5.0958e+10, 3.6594e+10, 4.7107e+10,
        5.6267e+10, 3.9623e+10, 3.4171e+10, 7.7053e+10, 2.8136e+10, 5.8606e+10,
        5.8967e+10, 2.5403e+10, 4.5154e+10, 3.8100e+10, 6.4418e+10, 4.0321e+10,
        5.0296e+10, 8.2668e+10, 7.8988e+10, 6.8727e+10, 4.2471e+10, 4.2849e+10,
        4.2695e+10, 2.3499e+10, 2.6896e+10, 7.1055e+10, 3.4490e+10, 3.3006e+10,
        4.1592e+10, 4.7782e+10, 6.4768e+10, 3.9751e+10, 6.2268e+10, 6.7744e+10,
        3.6174e+10, 5.4060e+10, 6.9701e+10, 3.5614e+10, 7.8087e+10, 4.9384e+10,
        4.5060e+10, 4.1883e+10, 3.6683e+10, 5.5333e+10, 3.9313e+10, 5.7203e+10,
        4.6250e+10, 4.9462e+10, 3.4116e+10, 2.4181e+10, 4.4159e+10, 2.9725e+10,
        7.4056e+10, 7.1876e+10, 4.1217e+10, 6.9317e+10, 4.9471e+10, 2.6849e+10,
        6.7410e+10, 5.7822e+10, 3.5707e+10, 2.9376e+10, 7.6469e+10, 5.9759e+10,
        2.9318e+10, 3.4293e+10, 5.0084e+10, 6.1277e+10, 2.8636e+10, 3.7454e+10,
        3.3218e+10, 6.8013e+10, 3.0300e+10, 4.2109e+10, 3.1465e+10, 3.8785e+10,
        4.9315e+10, 5.3364e+10, 3.0747e+10, 4.6577e+10, 3.4479e+10, 5.7850e+10,
        4.4723e+10, 6.4432e+10, 5.4477e+10, 5.9359e+10, 4.3757e+10, 4.9127e+10,
        4.3669e+10, 4.5050e+10, 6.8096e+10, 3.5654e+10, 5.8794e+10, 5.2158e+10,
        4.5378e+10, 4.2375e+10, 2.7327e+10, 3.6987e+10, 4.0042e+10, 6.7597e+10,
        4.5285e+10, 7.5889e+10, 3.4435e+10, 4.2009e+10, 2.8665e+10, 5.1817e+10,
        2.9023e+10, 3.9440e+10, 5.2432e+10, 3.4446e+10, 4.4334e+10, 4.4355e+10,
        1.5940e+10, 6.5295e+10, 2.1565e+10, 5.5569e+10, 3.3323e+10, 6.9815e+10,
        4.1588e+10, 4.3417e+10])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.3595e+09, 8.5990e+08, 2.4124e+09, 3.2537e+08, 2.5839e+09, 2.1767e+09,
        6.4989e+08, 3.0248e+09, 1.0084e+09, 7.0669e+09, 4.9264e+09, 1.6736e+09,
        1.8961e+09, 7.3096e+09, 4.9549e+08, 1.2033e+09, 2.5080e+09, 8.6663e+08,
        1.7688e+10, 3.6842e+09, 3.6892e+09, 1.9020e+09, 7.5177e+09, 1.0123e+09,
        6.0639e+08, 3.3010e+09, 1.3393e+09, 2.0745e+09, 1.0538e+09, 1.5219e+09,
        3.1293e+08, 2.3292e+09, 2.6601e+09, 5.2962e+08, 8.8369e+08, 2.6667e+09,
        2.5440e+09, 2.3871e+09, 2.3145e+08, 1.6484e+09, 1.7806e+09, 3.2600e+09,
        2.2778e+09, 1.7722e+09, 2.0571e+09, 2.7922e+09, 1.6126e+09, 9.1960e+08,
        1.4002e+09, 8.1815e+09, 6.8514e+08, 2.9276e+09, 8.3929e+08, 1.2759e+09,
        8.7391e+08, 2.7647e+09, 1.7543e+09, 2.2308e+09, 1.5275e+09, 3.6903e+08,
        4.6900e+09, 4.8806e+09, 2.6285e+09, 2.0148e+09, 3.3090e+08, 4.6747e+09,
        6.1087e+08, 1.7232e+10, 4.2189e+09, 9.4359e+08, 1.4922e+09, 7.2336e+09,
        3.3119e+09, 1.7264e+09, 3.7290e+08, 1.0754e+09, 2.3327e+09, 8.3721e+08,
        4.2013e+09, 6.1654e+08, 2.6365e+09, 1.9255e+08, 3.7193e+09, 1.0366e+09,
        1.7228e+09, 1.6763e+09, 6.9591e+08, 4.0731e+08, 1.4851e+09, 2.2240e+09,
        3.0337e+09, 1.6511e+09, 8.2135e+08, 1.3204e+09, 4.1255e+08, 4.5064e+09,
        2.7074e+09, 1.8194e+09, 3.2047e+08, 4.1053e+09, 1.3501e+09, 6.3120e+08,
        1.1450e+09, 8.2071e+08, 3.9856e+08, 3.5403e+09, 1.9129e+09, 1.8711e+09,
        1.1909e+09, 3.7815e+09, 1.5649e+09, 3.1312e+09, 4.0212e+08, 6.4920e+08,
        2.1724e+09, 5.3339e+09, 7.6768e+09, 7.5516e+08, 3.4441e+08, 8.0066e+08,
        2.5290e+09, 1.9676e+08, 1.3606e+09, 6.3497e+09, 3.5890e+09, 1.2744e+09,
        3.8579e+08, 4.9173e+08, 1.0557e+09, 1.0282e+09, 8.4252e+08, 9.0298e+09,
        2.8914e+09, 5.5150e+09, 1.1183e+09, 1.3768e+09, 4.1091e+09, 1.8189e+09,
        7.9466e+08, 5.1259e+09, 5.1165e+09, 9.5567e+08, 6.3893e+08, 4.0358e+08,
        5.5722e+08, 1.2203e+09, 2.8487e+09, 2.3335e+09, 1.6398e+09, 4.5071e+08,
        2.3157e+08, 1.2909e+09, 2.0888e+09, 2.8716e+09, 6.7232e+08, 8.4392e+09,
        7.4871e+08, 3.1235e+09, 2.0730e+10, 1.5115e+09, 2.9803e+09, 6.6527e+08,
        8.4911e+08, 1.0486e+10, 2.1886e+09, 7.5219e+09, 2.5609e+09, 1.0591e+09,
        1.6905e+09, 2.2856e+08, 2.1327e+09, 5.5722e+09, 1.0490e+09, 2.8668e+09,
        3.1219e+09, 1.8428e+08, 1.3121e+09, 3.1803e+08, 1.3807e+08, 5.7208e+08,
        9.5570e+08, 4.3871e+09, 1.4263e+09, 7.0514e+08, 1.8574e+09, 2.4390e+09,
        1.3754e+09, 2.1790e+09, 5.0104e+09, 8.2292e+08, 1.7942e+10, 7.5909e+09,
        5.2514e+08, 3.2576e+09, 1.7466e+09, 2.8633e+09, 3.7902e+08, 7.5168e+08,
        2.4136e+09, 5.3007e+08, 5.9719e+09, 2.2888e+09, 8.0834e+08, 2.6621e+09,
        1.5161e+09, 2.1746e+09, 2.8401e+09, 3.8835e+09, 2.2051e+09, 1.0179e+09,
        6.6368e+08, 4.7393e+08, 1.4357e+09, 1.1793e+09, 1.4498e+09, 5.2055e+10,
        2.4265e+09, 5.9090e+09, 1.5498e+09, 2.4259e+09, 1.3468e+09, 1.0615e+10,
        2.2926e+08, 1.5504e+09, 1.9823e+09, 6.2887e+09, 4.2585e+09, 3.5575e+09,
        1.0868e+09, 9.0405e+09, 4.1798e+08, 2.0753e+09, 3.6401e+08, 9.3466e+07,
        1.4935e+09, 9.7269e+09, 1.3129e+09, 2.2324e+09, 1.2479e+09, 1.3919e+09,
        2.5337e+09, 1.9943e+09, 4.5478e+08, 1.0361e+09, 2.2283e+09, 1.4328e+09,
        2.3446e+09, 1.0588e+09, 4.9453e+09, 4.2362e+08, 2.0714e+09, 9.7026e+09,
        3.4133e+09, 2.3838e+09, 4.2437e+09, 3.0405e+09, 2.0998e+09, 1.7339e+09,
        1.5076e+09, 7.4571e+08, 2.4948e+08, 3.9719e+09, 9.7602e+09, 9.6424e+08,
        1.9479e+09, 3.1319e+08, 9.6375e+09, 1.3410e+09, 2.0342e+08, 8.5582e+08,
        2.0921e+09, 3.0669e+08, 7.9743e+08, 2.1218e+09, 3.5121e+09, 1.0666e+09,
        1.6090e+09, 4.8117e+09, 4.0446e+08, 2.1057e+08, 5.5837e+08, 7.0769e+09,
        1.5242e+09, 2.0978e+09, 5.3817e+08, 3.6173e+09, 3.7807e+09, 7.8294e+08,
        1.6934e+09, 5.2314e+08, 1.5899e+09, 6.1992e+09, 1.4651e+09, 1.6095e+09,
        3.3808e+09, 5.0248e+08, 5.9177e+08, 5.2555e+08, 2.2979e+09, 2.7501e+09,
        1.7119e+10, 2.9325e+09, 5.5214e+09, 1.2554e+09, 2.7224e+09, 4.4920e+08,
        3.5105e+08, 8.8637e+08, 2.1903e+09, 7.5153e+08, 4.4394e+08, 1.2435e+09,
        2.5823e+09, 3.6289e+09, 2.9871e+08, 2.7730e+09, 1.1677e+10, 9.5782e+08,
        2.0878e+09, 2.3444e+09, 5.7256e+09, 5.3957e+09, 1.6200e+09, 5.3122e+08,
        1.3474e+09, 5.1283e+08, 1.6181e+09, 1.6980e+09, 4.6961e+09, 3.9298e+09,
        6.5134e+09, 4.5358e+08, 1.5676e+09, 7.7731e+09, 2.3300e+09, 2.2332e+09,
        1.3232e+09, 3.0251e+09, 3.0643e+10, 1.0124e+09, 4.4599e+08, 1.3696e+09,
        9.8704e+08, 7.8078e+09, 1.6658e+09, 5.1920e+08, 1.8112e+09, 1.0577e+09,
        1.9188e+08, 2.2732e+09, 5.7659e+10, 1.0791e+09, 5.0759e+09, 3.7012e+09,
        5.9969e+07, 5.6106e+08, 1.6757e+09, 2.9259e+09, 9.7476e+08, 9.9707e+08,
        1.0028e+10, 1.3071e+09, 5.0455e+08, 1.2178e+09, 1.5243e+09, 8.8600e+10,
        2.7633e+09, 1.1454e+09, 2.4350e+08, 3.1742e+09, 3.8866e+08, 1.2424e+09,
        1.6423e+09, 9.2803e+08, 1.2520e+09, 1.7662e+09, 2.1623e+09, 1.3096e+09,
        3.1182e+08, 8.6595e+08, 2.2162e+09, 3.4636e+09, 1.3765e+09, 1.8698e+09,
        3.5339e+09, 1.0438e+09, 2.7793e+09, 5.6624e+09, 3.8736e+09, 1.4364e+09,
        3.0547e+09, 2.2878e+09, 2.0635e+09, 5.7331e+09, 2.5285e+09, 1.0482e+09,
        4.6210e+08, 3.0313e+09, 3.6114e+08, 5.1376e+10, 1.5078e+09, 7.6834e+08,
        5.2226e+08, 3.5683e+09, 3.5471e+09, 2.5038e+10, 1.8969e+10, 1.6157e+09,
        6.6122e+08, 9.1036e+08, 1.7302e+09, 1.3034e+09, 6.5431e+08, 1.0414e+09,
        1.1549e+09, 2.1978e+08, 1.4271e+08, 8.7389e+08, 6.9300e+09, 4.8907e+09,
        2.5130e+09, 2.4539e+09, 2.9426e+09, 9.2989e+08, 2.2595e+09, 5.4327e+08,
        3.2478e+09, 4.0417e+08, 4.9743e+09, 5.6516e+08, 4.7148e+08, 7.4963e+08,
        1.4372e+09, 4.6619e+09, 3.4393e+09, 4.9215e+09, 3.7986e+08, 1.5661e+09,
        3.5802e+08, 1.8322e+09, 1.7510e+08, 1.1175e+09, 6.2517e+08, 5.3655e+10,
        3.8610e+09, 7.1433e+08, 2.0231e+09, 6.2310e+08, 2.6240e+09, 6.9903e+09,
        1.6150e+10, 1.1908e+09, 1.0069e+10, 2.5046e+09, 1.4583e+09, 5.5412e+09,
        6.9029e+09, 2.1676e+09, 6.7748e+09, 9.9745e+09, 5.5266e+08, 3.2849e+08,
        1.0998e+09, 2.1442e+09, 4.5581e+08, 3.5869e+09, 1.2635e+09, 5.1949e+08,
        6.1901e+08, 4.3913e+08, 8.9298e+10, 1.3890e+09, 1.1368e+09, 2.7250e+09,
        1.4757e+09, 2.8720e+09, 4.2565e+08, 2.6507e+09, 4.8416e+09, 2.9614e+09,
        1.1816e+09, 4.3437e+08, 2.1759e+08, 1.2831e+09, 1.6005e+09, 1.1318e+09,
        1.1310e+09, 4.7043e+08, 1.3661e+09, 2.6617e+09, 7.6204e+08, 5.3675e+08,
        2.2522e+09, 5.6409e+08, 2.2187e+09, 9.0181e+08, 3.7196e+09, 2.1689e+09,
        5.8883e+09, 4.3390e+09, 8.6281e+08, 1.2400e+09, 4.2700e+08, 2.8097e+10,
        1.8821e+09, 2.9295e+09, 1.8870e+09, 2.4262e+08, 2.5393e+09, 1.4714e+09,
        5.2287e+08, 3.7096e+08])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1363e+10, 6.8062e+09, 5.1741e+09, 8.2516e+09, 5.2863e+09, 6.1140e+09,
        4.4970e+09, 8.9243e+09, 4.4472e+09, 8.4379e+09, 6.8964e+09, 9.1845e+09,
        4.4002e+09, 3.3093e+09, 6.8654e+09, 1.0140e+10, 5.9926e+09, 7.2769e+09,
        5.3782e+09, 8.9411e+09, 5.4096e+09, 9.0404e+09, 3.6300e+09, 6.6084e+09,
        3.5561e+09, 4.5752e+09, 7.5766e+09, 6.2028e+09, 2.4475e+09, 3.9656e+09,
        4.6330e+09, 6.5254e+09, 2.8102e+09, 7.0794e+09, 2.6444e+09, 4.0994e+09,
        1.0208e+10, 3.7880e+09, 6.0998e+09, 2.6551e+09, 5.2207e+09, 7.6004e+09,
        6.1718e+09, 1.0120e+10, 8.5586e+09, 5.4400e+09, 4.3881e+09, 4.5291e+09,
        9.3287e+09, 5.1088e+09, 7.0376e+09, 4.6594e+09, 4.4044e+09, 7.0574e+09,
        8.8527e+09, 5.7459e+09, 5.6040e+09, 2.4548e+09, 9.4146e+09, 5.4264e+09,
        8.1113e+09, 4.7587e+09, 1.0454e+10, 6.7253e+09, 6.2966e+09, 7.7886e+09,
        4.6463e+09, 3.6867e+09, 5.4591e+09, 3.6273e+09, 5.3600e+09, 2.5401e+09,
        6.4209e+09, 5.2679e+09, 2.7326e+09, 4.9715e+09, 5.6432e+09, 7.7201e+09,
        1.0239e+10, 4.9523e+09, 4.5642e+09, 3.3778e+09, 4.1630e+09, 6.8614e+09,
        3.6512e+09, 9.6333e+09, 3.6239e+09, 6.6090e+09, 3.9266e+09, 4.4368e+09,
        2.8925e+09, 9.1773e+09, 7.9662e+09, 2.1941e+09, 6.6574e+09, 5.2372e+09,
        6.2618e+09, 7.9468e+09, 5.0548e+09, 9.6244e+09, 7.5709e+09, 6.9253e+09,
        3.0331e+09, 4.2572e+09, 4.2887e+09, 8.8394e+09, 7.2976e+09, 4.8009e+09,
        4.9071e+09, 2.8819e+09, 3.8980e+09, 5.0898e+09, 5.4642e+09, 6.9050e+09,
        5.9669e+09, 6.3295e+09, 5.3403e+09, 4.3797e+09, 6.0400e+09, 5.0225e+09,
        5.7171e+09, 6.4828e+09, 2.6239e+09, 4.9578e+09, 4.3348e+09, 6.6212e+09,
        8.5111e+09, 8.2764e+09])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.2705e+10, 6.7357e+09, 8.8491e+09, 1.0831e+10, 5.6735e+09, 1.6543e+10,
        1.2916e+10, 4.9189e+09, 6.4837e+09, 5.7424e+09, 1.4204e+10, 1.5905e+10,
        3.1355e+09, 1.5541e+10, 1.1407e+10, 8.2824e+09, 1.2165e+10, 1.3528e+10,
        1.3469e+10, 6.1046e+09, 1.4497e+10, 7.4382e+09, 1.5880e+10, 1.1900e+10,
        1.0612e+10, 1.2870e+10, 1.2024e+10, 6.6077e+09, 1.0109e+10, 6.6450e+09,
        1.2491e+10, 1.5180e+10, 7.4811e+09, 1.3789e+10, 1.2910e+10, 1.1896e+10,
        1.6631e+10, 6.6493e+09, 7.3686e+09, 5.4954e+09, 2.0863e+10, 8.0699e+09,
        7.2368e+09, 1.1314e+10, 3.5140e+09, 1.2819e+10, 1.2938e+10, 7.9352e+09,
        1.0323e+10, 1.4457e+10, 9.5929e+09, 1.0814e+10, 1.1449e+10, 1.2327e+10,
        1.2356e+10, 1.1920e+10, 9.5665e+09, 8.9885e+09, 1.5164e+10, 8.7774e+09,
        1.5327e+10, 7.3823e+09, 1.4103e+10, 1.0543e+10, 1.0004e+10, 1.2625e+10,
        1.3346e+10, 1.4007e+10, 9.0548e+09, 1.1024e+10, 8.1105e+09, 1.0678e+10,
        9.3382e+09, 1.1456e+10, 1.6124e+10, 1.4985e+10, 4.3289e+09, 2.3797e+10,
        1.3936e+10, 1.5911e+10, 1.0436e+10, 8.5762e+09, 8.5492e+09, 6.5529e+09,
        1.5228e+10, 6.5270e+09, 1.4009e+10, 1.4436e+10, 7.9878e+09, 1.1323e+10,
        4.2558e+09, 1.1378e+10, 9.7914e+09, 1.0461e+10, 1.6799e+10, 3.0514e+09,
        1.4195e+10, 7.2258e+09, 7.8591e+09, 8.2174e+09, 1.0162e+10, 1.2842e+10,
        1.0227e+10, 1.5947e+10, 5.7366e+09, 4.5595e+09, 4.5595e+09, 4.1958e+09,
        8.6031e+09, 1.2627e+10, 1.1501e+10, 9.3654e+09, 1.4267e+10, 7.7296e+09,
        1.0775e+10, 9.9168e+09, 8.3706e+09, 4.2615e+09, 1.1823e+10, 7.5130e+09,
        5.9374e+09, 1.2533e+10, 1.3385e+10, 1.3359e+10, 2.3983e+10, 1.2210e+10,
        1.0978e+10, 8.0465e+09])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.3974e+08, 3.5767e+08, 5.8389e+08, 8.4215e+07, 7.9181e+08, 7.3844e+07,
        1.2496e+08, 4.0871e+08, 6.3435e+08, 7.2810e+08, 8.5599e+08, 3.8087e+08,
        5.5367e+08, 6.3100e+07, 1.4528e+07, 6.9325e+07, 9.0611e+07, 5.1356e+07,
        2.0331e+08, 5.1802e+08, 1.3859e+09, 1.9738e+08, 1.5956e+10, 1.0242e+09,
        9.4223e+08, 6.5321e+08, 2.7318e+08, 2.3519e+08, 8.3162e+07, 6.6404e+07,
        5.9539e+06, 8.6119e+08, 7.0606e+08, 2.6677e+08, 2.4258e+08, 1.6810e+08,
        3.5607e+08, 1.6085e+09, 3.8792e+07, 2.3282e+08, 1.2573e+08, 2.8698e+09,
        3.9300e+08, 6.5083e+08, 1.2907e+09, 3.0000e+07, 3.1561e+08, 3.7677e+07,
        9.6062e+07, 2.5873e+08, 1.3552e+08, 4.2404e+07, 1.1015e+08, 2.2566e+08,
        4.1426e+08, 2.2086e+08, 4.8958e+08, 1.9334e+08, 5.0928e+08, 1.3386e+07,
        8.9228e+08, 1.5158e+09, 3.1600e+08, 1.4809e+09, 3.7704e+07, 3.8758e+08,
        1.6288e+08, 7.9721e+07, 3.4107e+08, 1.4998e+08, 2.5617e+08, 1.9954e+09,
        3.1472e+08, 5.3434e+08, 7.0837e+07, 8.1180e+07, 7.1836e+08, 3.1183e+07,
        9.4092e+08, 2.2878e+08, 5.0669e+08, 1.3862e+08, 1.7590e+08, 6.6889e+07,
        1.0263e+08, 8.9039e+08, 1.0449e+08, 7.9629e+07, 2.4233e+09, 2.3065e+09,
        2.6228e+08, 5.1399e+08, 5.9887e+07, 5.2607e+07, 1.3366e+08, 1.2358e+09,
        1.5303e+09, 7.5320e+08, 4.7976e+07, 2.1903e+08, 3.5107e+08, 1.0709e+08,
        9.9458e+09, 1.1268e+09, 3.7350e+07, 1.0957e+09, 1.4374e+08, 4.9509e+07,
        2.2321e+08, 3.0430e+09, 1.2672e+09, 3.7037e+08, 2.6912e+07, 6.7332e+07,
        9.3191e+08, 5.6084e+08, 1.9126e+08, 1.3793e+08, 2.3136e+08, 1.7423e+08,
        9.9922e+08, 1.4971e+07, 1.4130e+08, 5.5632e+08, 2.5291e+08, 2.1869e+08,
        9.4063e+07, 1.9243e+08, 6.8522e+07, 6.5838e+08, 2.0512e+09, 4.4758e+08,
        2.2943e+09, 7.4469e+07, 5.3959e+08, 3.4506e+08, 4.1456e+08, 5.0147e+07,
        6.8254e+08, 1.5225e+09, 7.5223e+08, 1.4047e+08, 5.3940e+07, 8.5427e+07,
        2.4318e+09, 2.7934e+08, 6.3265e+08, 9.4357e+07, 1.5241e+08, 7.2940e+07,
        6.5485e+07, 1.2848e+08, 3.1802e+08, 1.1956e+08, 1.3890e+08, 9.5541e+09,
        1.1057e+08, 1.4446e+08, 1.9825e+10, 2.3362e+08, 4.3270e+08, 1.9783e+08,
        3.7532e+07, 5.1238e+08, 1.3725e+09, 5.8573e+08, 3.2838e+07, 4.2944e+08,
        3.5035e+08, 3.4765e+08, 2.1202e+09, 6.3993e+08, 6.6585e+07, 2.8239e+09,
        1.9850e+08, 4.1157e+07, 2.4854e+08, 1.4163e+08, 1.0383e+08, 1.4080e+08,
        2.7023e+08, 6.3725e+08, 4.1050e+08, 5.1608e+07, 5.2044e+08, 1.8327e+08,
        5.3106e+08, 3.2681e+08, 4.8868e+08, 1.6482e+07, 2.8470e+08, 9.2014e+08,
        1.7933e+08, 6.6216e+08, 2.1838e+08, 3.9130e+08, 3.4648e+07, 6.7129e+07,
        2.4120e+08, 2.4793e+08, 1.1875e+10, 7.8131e+07, 3.3086e+08, 1.6195e+08,
        1.1532e+09, 1.3075e+09, 1.2631e+08, 8.0665e+08, 3.8401e+08, 9.5997e+08,
        1.3217e+08, 2.8664e+08, 5.6879e+07, 1.2699e+08, 1.0676e+08, 3.6337e+08,
        7.7903e+08, 1.9672e+09, 1.9588e+08, 3.9621e+09, 1.0932e+08, 5.8448e+08,
        6.1677e+07, 2.4120e+07, 8.3774e+08, 7.9477e+08, 3.9052e+08, 1.3300e+08,
        2.2762e+07, 8.6859e+09, 1.2542e+08, 2.4067e+08, 3.2598e+08, 3.3938e+07,
        6.2994e+07, 4.2033e+09, 8.7679e+08, 7.5366e+08, 6.8347e+08, 1.1219e+09,
        4.4373e+07, 1.8196e+08, 8.7912e+07, 3.9290e+08, 1.8628e+08, 2.1618e+08,
        5.6498e+08, 1.1218e+09, 1.2341e+08, 2.5725e+08, 1.5114e+08, 8.7285e+07,
        4.2171e+09, 8.0914e+08, 1.1659e+09, 1.4972e+09, 3.5119e+08, 7.8377e+08,
        1.5678e+08, 3.2632e+07, 6.2659e+07, 3.7684e+09, 9.2450e+09, 2.2150e+08,
        4.2562e+09, 6.9858e+07, 1.6366e+08, 3.0792e+08, 1.3717e+07, 2.4103e+08,
        1.3388e+08, 3.3664e+08, 1.4364e+08, 3.3025e+08, 4.5725e+07, 1.0971e+08,
        1.1303e+08, 1.3376e+09, 1.6203e+08, 6.0803e+07, 4.5712e+07, 4.7529e+08,
        5.7962e+08, 1.7790e+09, 1.6008e+08, 2.1642e+08, 1.5635e+08, 5.8044e+08,
        5.7427e+08, 3.2529e+07, 4.2742e+07, 2.4183e+08, 1.7588e+08, 5.6625e+08,
        5.6054e+08, 4.5540e+07, 5.3840e+07, 2.7699e+07, 4.8765e+08, 8.0533e+08,
        8.9558e+07, 4.3176e+08, 1.2880e+08, 5.3233e+08, 1.1293e+08, 4.1228e+08,
        1.0655e+08, 1.7091e+08, 2.1530e+08, 2.5611e+07, 1.0522e+08, 1.0778e+08,
        2.2358e+09, 9.2108e+09, 9.3366e+07, 4.4543e+08, 2.1664e+08, 1.0018e+08,
        1.6050e+09, 1.4112e+08, 7.0751e+08, 3.6730e+08, 4.8946e+08, 1.1392e+08,
        7.1299e+08, 1.3704e+08, 4.7701e+07, 3.0167e+08, 6.7041e+08, 1.7719e+09,
        8.8184e+08, 2.7004e+07, 1.4795e+09, 4.0593e+08, 2.0781e+08, 1.4274e+09,
        7.8597e+08, 9.1610e+07, 9.3782e+08, 5.2711e+08, 5.4736e+07, 4.3553e+08,
        2.8173e+08, 3.3500e+08, 2.0037e+08, 2.0613e+08, 4.1645e+08, 4.8296e+07,
        9.7831e+08, 2.9377e+09, 2.7819e+09, 7.1748e+08, 4.4432e+09, 4.4177e+08,
        3.2916e+08, 1.6439e+08, 2.7961e+08, 1.9300e+09, 7.2409e+07, 2.8228e+08,
        7.2139e+09, 9.7142e+08, 6.5701e+07, 6.3926e+07, 2.4171e+08, 3.8334e+08,
        1.6398e+07, 6.9929e+08, 1.4732e+08, 1.7230e+08, 1.4206e+07, 1.8764e+08,
        6.6251e+08, 6.1190e+07, 1.1283e+08, 1.1478e+09, 2.5159e+08, 3.8100e+08,
        6.9689e+07, 1.1790e+08, 9.6956e+08, 1.8722e+09, 3.7365e+08, 9.4247e+07,
        1.8071e+08, 1.0237e+08, 2.2880e+08, 2.4651e+09, 8.1236e+08, 2.1233e+08,
        1.2139e+09, 2.6176e+08, 2.9040e+08, 1.0820e+08, 6.8020e+08, 1.8602e+08,
        1.1402e+08, 1.3947e+09, 6.9209e+07, 2.5017e+10, 4.3243e+07, 8.6513e+07,
        7.8332e+07, 3.1198e+08, 8.6975e+08, 1.1227e+09, 3.8159e+08, 5.6497e+08,
        6.6661e+07, 2.5389e+08, 1.4767e+08, 1.9962e+08, 1.1302e+08, 3.8448e+08,
        1.0654e+08, 2.5486e+08, 5.9156e+08, 1.4436e+08, 5.2475e+09, 6.7722e+09,
        2.4755e+08, 3.4742e+08, 4.3116e+08, 2.6396e+08, 4.4722e+08, 1.3030e+08,
        2.4330e+08, 2.6377e+07, 8.7196e+08, 5.1984e+08, 3.3723e+07, 1.7937e+08,
        5.6707e+08, 2.2726e+09, 4.6451e+09, 1.3222e+09, 7.8912e+07, 3.1908e+08,
        8.5970e+08, 4.9964e+08, 5.2158e+07, 5.2906e+08, 2.1005e+07, 4.9031e+08,
        4.3379e+08, 5.0182e+07, 1.9345e+08, 2.9385e+08, 4.8561e+08, 6.3093e+09,
        6.1497e+08, 4.2837e+08, 2.8091e+08, 1.6690e+08, 6.1075e+08, 4.8553e+08,
        8.5860e+08, 6.0573e+08, 5.0121e+08, 2.0435e+08, 1.0260e+09, 3.6233e+08,
        8.6279e+07, 1.2312e+08, 9.3752e+07, 7.7062e+07, 1.9734e+08, 3.6571e+08,
        2.2402e+08, 1.3859e+08, 1.0868e+08, 6.3894e+08, 2.2371e+08, 1.9732e+08,
        5.8230e+08, 1.2736e+09, 8.6668e+07, 6.3815e+07, 1.0492e+09, 2.5385e+08,
        1.2757e+08, 2.2624e+08, 3.6205e+07, 4.0291e+08, 4.0911e+08, 2.4964e+08,
        4.8436e+08, 1.7824e+08, 8.9071e+07, 4.3143e+08, 4.5920e+07, 1.5913e+08,
        1.1159e+09, 2.2645e+07, 4.2533e+08, 9.2869e+07, 3.8784e+09, 4.6218e+08,
        1.2737e+08, 2.9148e+08, 7.3208e+08, 4.5309e+08, 1.6874e+08, 6.5029e+08,
        4.8220e+08, 1.6694e+08, 8.6531e+08, 9.3201e+07, 2.2274e+08, 1.8756e+09,
        3.1535e+07, 7.7985e+07])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.0299e+08, 9.3166e+08, 7.4425e+08, 3.9138e+08, 2.2186e+08, 3.3732e+08,
        6.4100e+08, 6.4829e+08, 3.6105e+08, 3.5558e+08, 7.6893e+08, 9.4464e+08,
        5.5867e+08, 7.4478e+08, 7.3623e+08, 7.0650e+08, 8.2082e+08, 1.9789e+08,
        4.8629e+08, 7.9495e+08, 1.1500e+09, 8.7522e+08, 4.6968e+08, 2.9972e+08,
        1.2873e+08, 1.4754e+08, 3.4232e+08, 1.6259e+09, 7.3530e+08, 3.6814e+08,
        1.0206e+09, 4.1888e+08, 8.2329e+08, 1.0482e+08, 3.1681e+08, 3.6646e+08,
        4.1148e+08, 4.1714e+08, 2.0881e+08, 1.6515e+09, 8.4114e+08, 9.6223e+08,
        4.2693e+08, 1.3796e+08, 1.4984e+09, 2.1686e+08, 1.4886e+08, 1.9014e+09,
        9.2899e+08, 5.8059e+08, 8.4791e+08, 2.0008e+08, 2.0084e+09, 5.3722e+08,
        2.2905e+08, 7.5803e+08, 3.5603e+08, 8.0059e+08, 1.1049e+09, 3.0484e+08,
        5.6306e+08, 9.0108e+08, 3.4707e+08, 1.9578e+08, 5.4094e+08, 1.1910e+09,
        8.3150e+08, 5.4129e+08, 8.8900e+08, 1.1037e+09, 6.3258e+08, 3.7364e+08,
        1.0334e+09, 1.3412e+08, 9.4854e+08, 1.6817e+09, 4.9534e+08, 1.0131e+09,
        9.4938e+08, 7.5779e+08, 5.4840e+08, 5.8200e+08, 7.0127e+08, 3.3185e+08,
        4.1981e+08, 3.6572e+08, 3.5139e+08, 7.5736e+08, 1.2124e+09, 1.3628e+08,
        1.3521e+09, 7.8908e+08, 8.8085e+08, 4.8720e+08, 1.3243e+09, 6.2238e+08,
        3.6350e+08, 9.3953e+08, 2.5141e+08, 7.2680e+08, 1.0500e+09, 6.0117e+08,
        2.5637e+08, 3.1182e+08, 3.9992e+08, 1.3203e+09, 7.3196e+08, 7.9704e+08,
        7.5080e+08, 5.6136e+08, 8.0287e+08, 1.2299e+08, 1.1650e+09, 1.4473e+08,
        1.0759e+09, 1.4922e+08, 8.6307e+08, 3.6210e+08, 5.5518e+08, 1.1250e+08,
        1.3926e+08, 5.6643e+08, 2.4369e+08, 2.8992e+08, 1.2370e+09, 5.2895e+08,
        1.1280e+08, 1.2582e+09, 2.8956e+08, 7.3445e+08, 3.6120e+08, 8.5288e+08,
        1.1558e+09, 3.5930e+08, 1.9949e+08, 3.9463e+08, 1.2205e+09, 4.8077e+08,
        9.2475e+07, 1.1751e+09, 2.6198e+08, 2.0247e+08, 5.8828e+08, 1.1430e+09,
        2.5139e+08, 3.4831e+08, 1.2351e+09, 4.4211e+08, 3.3469e+08, 3.9250e+08,
        1.0966e+09, 3.4734e+08, 2.2746e+08, 2.5133e+08, 5.8222e+08, 1.3041e+09,
        8.0297e+08, 1.2658e+08, 9.7519e+08, 9.2187e+08, 3.9354e+08, 3.8523e+08,
        6.9430e+08, 5.1332e+08, 1.6565e+09, 5.4791e+08, 3.0633e+08, 4.7063e+08,
        6.9863e+08, 2.5057e+08, 4.0464e+08, 1.2183e+09, 7.9798e+08, 9.5966e+08,
        7.5976e+08, 8.5582e+08, 5.5603e+08, 1.8430e+08, 2.2275e+08, 1.8449e+08,
        9.6309e+08, 5.3156e+08, 2.2847e+08, 5.0016e+08, 7.0176e+08, 5.9536e+08,
        2.4455e+08, 6.2348e+08, 3.7960e+08, 6.6841e+08, 4.4225e+08, 4.1215e+08,
        7.5032e+08, 6.0566e+08, 6.6592e+08, 6.7423e+08, 3.5144e+08, 1.1536e+09,
        2.5514e+08, 1.2435e+09, 1.5301e+08, 1.6937e+09, 6.5830e+08, 1.0748e+08,
        5.2332e+08, 4.0850e+08, 8.9836e+08, 4.0094e+08, 6.5769e+08, 5.8044e+08,
        1.2418e+08, 1.1271e+09, 1.1869e+09, 2.5925e+08, 8.2573e+08, 4.8999e+08,
        8.7794e+08, 1.3087e+09, 7.8975e+08, 8.3544e+08, 2.0925e+08, 9.0562e+08,
        1.0905e+09, 9.3240e+08, 1.9783e+08, 6.8340e+08, 6.9636e+08, 7.8413e+08,
        1.4986e+09, 1.2089e+09, 5.1844e+08, 1.7293e+09, 2.0785e+08, 8.8007e+08,
        8.2691e+08, 8.9130e+08, 6.8443e+08, 6.5750e+08, 1.1986e+09, 5.2010e+08,
        4.0125e+08, 2.0288e+08, 1.4653e+09, 8.3883e+08, 1.2057e+09, 4.1523e+08,
        1.2607e+09, 2.6277e+08, 5.2754e+08, 7.5453e+08, 1.4769e+09, 1.1386e+09,
        2.8319e+08, 4.4943e+08, 1.3637e+09, 1.1140e+09])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.8991e+08, 5.0938e+08, 1.0209e+09, 1.2094e+09, 1.1366e+09, 1.3604e+09,
        6.5658e+08, 5.0772e+08, 4.7244e+09, 6.2918e+08, 1.1332e+09, 5.1874e+08,
        7.4899e+08, 6.8775e+08, 3.2610e+08, 3.5743e+08, 5.3589e+08, 1.3204e+09,
        3.8185e+08, 1.8564e+09, 1.2812e+09, 1.3341e+09, 1.0321e+09, 1.8877e+09,
        1.1119e+09, 9.2268e+08, 1.0438e+09, 6.7056e+08, 5.4034e+08, 2.8345e+09,
        3.1584e+09, 1.7568e+09, 1.9980e+09, 3.2451e+08, 1.3214e+09, 5.2504e+08,
        2.1873e+09, 7.4704e+08, 1.2844e+09, 1.4749e+09, 8.9023e+08, 7.7482e+08,
        3.0996e+09, 2.0989e+09, 8.4714e+08, 1.4428e+09, 2.5642e+09, 1.8899e+09,
        4.4284e+08, 1.5592e+09, 4.0338e+09, 1.8563e+09, 5.9764e+08, 1.0913e+09,
        1.3140e+09, 1.6341e+09, 1.0689e+09, 1.9502e+08, 1.7618e+09, 1.5227e+09,
        8.4146e+08, 8.7534e+08, 1.2995e+09, 3.4257e+08, 5.7069e+08, 2.7068e+09,
        1.5965e+09, 3.7116e+09, 1.2781e+09, 2.3094e+09, 9.1338e+08, 5.4346e+08,
        2.0509e+09, 4.7238e+08, 8.5011e+08, 7.2703e+08, 2.3376e+08, 1.2536e+09,
        2.5722e+09, 7.2115e+08, 1.9961e+09, 2.7583e+09, 4.6483e+08, 4.8044e+09,
        6.1394e+08, 3.1638e+09, 2.4659e+09, 2.2354e+09, 1.9544e+09, 1.9896e+09,
        1.9042e+09, 1.9595e+09, 1.0014e+09, 4.1902e+08, 1.1730e+09, 1.4434e+09,
        2.3526e+09, 1.1662e+09, 8.4614e+08, 2.9941e+09, 5.2385e+08, 1.2122e+09,
        1.8184e+09, 7.2536e+08, 2.2314e+09, 2.7381e+09, 1.4729e+09, 7.1575e+08,
        6.3255e+08, 1.0700e+09, 1.2959e+09, 6.6487e+08, 1.0030e+09, 3.4698e+08,
        9.5981e+08, 1.2525e+09, 3.7811e+09, 1.1770e+09, 1.5126e+09, 4.1675e+09,
        8.6136e+08, 1.2261e+09, 1.5080e+09, 4.1945e+08, 2.9794e+09, 1.7442e+09,
        1.0966e+09, 4.2114e+08, 2.4509e+09, 1.6763e+09, 3.5772e+09, 6.4252e+08,
        5.5207e+08, 6.4577e+08, 9.5868e+08, 9.9856e+08, 1.3457e+09, 3.2621e+09,
        6.5536e+08, 9.5984e+08, 2.4909e+09, 1.4487e+09, 1.6182e+09, 1.6720e+09,
        8.2254e+08, 6.9292e+08, 1.2990e+09, 2.7661e+09, 5.3414e+08, 2.0230e+09,
        2.8179e+08, 2.6267e+09, 1.9767e+09, 2.3376e+09, 1.5272e+09, 2.9843e+09,
        1.7056e+09, 1.4783e+09, 7.4682e+08, 4.1179e+08, 1.0709e+09, 1.2306e+09,
        8.4619e+08, 5.4500e+08, 3.2729e+09, 2.0925e+09, 1.7889e+09, 8.3737e+08,
        2.0443e+09, 4.5346e+08, 2.7588e+09, 3.6445e+09, 5.2104e+08, 2.2333e+09,
        1.3503e+09, 2.9227e+09, 7.5011e+08, 4.4890e+08, 7.8339e+08, 9.7312e+08,
        1.6846e+09, 3.3056e+09, 1.1812e+09, 1.5384e+09, 9.7102e+08, 2.2165e+09,
        2.1915e+09, 1.8874e+09, 2.5729e+08, 1.9996e+09, 1.8186e+09, 3.1170e+09,
        2.1605e+09, 6.8624e+08, 3.9169e+08, 1.2930e+09, 3.3906e+08, 1.3076e+09,
        1.8986e+09, 1.8186e+08, 8.8677e+08, 2.1815e+09, 6.7587e+08, 6.3977e+08,
        6.7994e+08, 4.3770e+09, 2.4247e+09, 3.6354e+08, 2.4591e+09, 9.8907e+08,
        4.5912e+08, 1.4399e+09, 2.3106e+09, 1.0289e+09, 2.5734e+09, 3.3716e+09,
        7.6299e+08, 3.7366e+08, 3.6636e+09, 1.0505e+09, 4.6551e+09, 1.7371e+09,
        1.6736e+09, 8.5388e+08, 2.1581e+09, 4.1746e+08, 3.8876e+09, 4.5944e+09,
        8.2828e+08, 1.9234e+09, 2.9223e+09, 6.8712e+08, 1.1407e+09, 5.8525e+08,
        1.0574e+09, 1.7492e+09, 2.1804e+09, 2.3069e+09, 1.7219e+09, 1.3753e+09,
        5.8784e+08, 1.8659e+09, 7.5175e+08, 2.6331e+09, 3.8521e+09, 2.2015e+09,
        1.0608e+09, 6.1170e+08, 2.1282e+09, 5.3353e+08, 1.8624e+09, 2.0994e+09,
        1.3100e+09, 2.6544e+09, 2.8989e+09, 1.9693e+09])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([24002710.0000, 17920570.0000, 13667175.0000,  ...,
         8369098.5000,  4285503.5000, 14327925.0000])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3894334.7500, 4993660.0000, 7778059.5000,  ..., 9511701.0000,
        7756703.5000, 7790793.5000])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.4844e+07, 9.2867e+07, 4.1660e+07, 2.6128e+07, 8.0860e+07, 1.0771e+07,
        1.7917e+07, 1.1890e+07, 3.8416e+07, 1.0495e+07, 4.4847e+07, 8.1367e+07,
        1.5066e+07, 3.7861e+07, 3.2335e+07, 1.0697e+07, 5.5766e+07, 5.0894e+07,
        8.6627e+07, 3.5278e+07, 3.1999e+07, 1.0940e+07, 6.6512e+07, 3.0293e+07,
        3.2807e+07, 2.0602e+07, 5.7693e+06, 8.5256e+07, 3.6037e+07, 4.0352e+07,
        2.6763e+07, 7.1436e+07, 2.3570e+07, 1.9880e+07, 3.2779e+07, 1.1796e+07,
        3.3831e+07, 2.1044e+07, 1.8789e+07, 2.9460e+07, 6.8654e+07, 6.5193e+07,
        7.5832e+07, 9.0986e+07, 1.4563e+07, 1.2994e+07, 1.1485e+07, 7.8879e+07,
        1.4135e+07, 2.9758e+07, 9.0871e+06, 7.9300e+07, 6.2098e+07, 3.4132e+07,
        8.6751e+07, 1.0519e+08, 2.8384e+07, 8.4703e+07, 1.2923e+07, 3.0324e+07,
        6.4029e+06, 4.4076e+07, 1.8946e+07, 8.4885e+07, 1.0257e+08, 8.3672e+06,
        4.7877e+07, 9.3986e+07, 3.6359e+07, 1.1640e+07, 9.5447e+06, 3.5455e+07,
        3.2736e+07, 7.2714e+07, 1.0025e+08, 6.2945e+07, 7.0366e+06, 4.8823e+07,
        5.1863e+07, 5.9802e+07, 2.2158e+07, 2.8422e+07, 1.9600e+07, 1.7222e+07,
        2.1488e+07, 1.8243e+07, 3.6703e+07, 5.5050e+07, 5.4874e+07, 9.8598e+07,
        8.0431e+06, 6.1789e+07, 2.0129e+07, 3.2339e+07, 1.4134e+07, 4.0292e+07,
        4.4816e+07, 4.6425e+07, 6.8137e+07, 4.7948e+07, 1.2065e+07, 1.3989e+07,
        6.1001e+07, 1.0611e+07, 2.6780e+07, 8.0637e+07, 7.2221e+07, 2.1443e+07,
        2.4533e+07, 1.5762e+07, 4.7444e+07, 4.7520e+07, 1.5744e+07, 3.5295e+07,
        4.4892e+07, 1.3900e+08, 2.9184e+07, 7.4095e+07, 4.0821e+07, 6.2931e+07,
        3.9210e+07, 2.7426e+07, 8.5060e+07, 7.0845e+07, 3.2087e+07, 2.0537e+07,
        5.5809e+07, 3.8645e+07, 4.2264e+07, 2.1067e+07, 1.4053e+07, 6.6317e+07,
        2.5686e+07, 2.0346e+07, 9.0269e+07, 1.6050e+07, 5.1826e+07, 2.0700e+07,
        3.0570e+07, 4.3240e+07, 3.0704e+07, 3.1311e+07, 5.1991e+07, 7.9941e+06,
        5.0214e+06, 9.0119e+06, 6.3781e+07, 1.6616e+07, 3.5495e+07, 3.4807e+07,
        8.0243e+07, 3.0454e+07, 8.2961e+07, 9.4563e+07, 3.1373e+07, 1.8928e+07,
        4.0627e+07, 4.7013e+07, 1.2091e+07, 5.5714e+07, 3.4016e+07, 6.5091e+07,
        1.1733e+07, 5.0550e+07, 2.4625e+07, 8.4219e+07, 2.3477e+07, 4.3733e+07,
        3.1148e+07, 2.9279e+07, 5.7797e+07, 6.5241e+07, 3.5537e+07, 8.2557e+07,
        6.3183e+07, 8.2033e+07, 7.8278e+07, 1.4858e+08, 2.1240e+07, 6.5880e+06,
        6.4381e+07, 3.1034e+07, 2.0861e+07, 7.4343e+07, 6.1513e+07, 3.6366e+07,
        1.8935e+07, 4.0650e+07, 1.0554e+07, 5.5378e+06, 8.7966e+07, 8.4903e+06,
        5.1102e+07, 6.4274e+07, 4.8768e+06, 1.2687e+07, 2.2945e+07, 6.7061e+07,
        2.4206e+07, 7.2737e+07, 1.8800e+07, 1.5294e+07, 3.8201e+07, 1.9276e+07,
        5.8579e+07, 7.1136e+06, 3.0526e+07, 3.7078e+07, 1.1056e+07, 2.7774e+07,
        2.0377e+07, 3.0879e+07, 3.2177e+07, 2.0106e+07, 4.0145e+06, 5.0524e+07,
        2.1770e+07, 7.3188e+07, 9.7299e+07, 3.0795e+07, 1.5127e+07, 6.0195e+07,
        1.1222e+08, 3.0553e+07, 5.1178e+06, 1.8965e+07, 9.1620e+07, 9.2484e+06,
        1.5677e+07, 7.0518e+07, 3.6828e+07, 8.5280e+07, 2.2086e+07, 5.2730e+07,
        1.1830e+08, 8.7088e+06, 1.0389e+08, 1.5170e+07, 5.6810e+06, 6.6016e+07,
        7.0836e+07, 6.5899e+07, 1.1958e+08, 8.3685e+07, 1.9498e+07, 3.0985e+07,
        2.9260e+07, 6.0072e+07, 1.5642e+07, 4.9817e+07, 7.5967e+07, 1.7793e+07,
        2.4373e+07, 2.9284e+07, 7.9342e+07, 1.9727e+07])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.9812e+07, 5.5717e+07, 1.3577e+08, 1.2216e+08, 4.5192e+07, 1.7275e+08,
        1.3448e+08, 5.6095e+07, 1.1937e+08, 8.1390e+07, 2.0388e+08, 1.3474e+08,
        1.2909e+08, 5.1704e+07, 1.3379e+08, 2.3093e+08, 9.4801e+07, 1.5732e+08,
        4.8342e+07, 8.9273e+07, 4.9243e+07, 1.4979e+08, 8.6978e+07, 5.8947e+07,
        5.3299e+07, 3.9510e+07, 4.8184e+07, 1.6577e+08, 2.3077e+08, 3.2487e+07,
        1.2680e+08, 7.6509e+07, 1.2680e+08, 1.7165e+08, 2.2969e+08, 7.8683e+07,
        7.3838e+07, 1.1434e+08, 6.9596e+07, 1.4544e+08, 1.3995e+08, 5.1269e+07,
        7.0865e+07, 1.1981e+08, 5.6930e+07, 4.3869e+08, 2.5720e+08, 4.9632e+07,
        2.2819e+08, 1.4937e+08, 6.4749e+07, 1.6385e+08, 1.8120e+08, 2.1445e+08,
        1.6584e+08, 5.3508e+07, 1.1973e+08, 3.8246e+07, 5.8889e+07, 2.5843e+08,
        1.4243e+08, 3.4382e+07, 1.1154e+08, 1.7364e+08, 3.5573e+07, 2.1247e+08,
        1.2944e+08, 1.3824e+08, 1.4810e+08, 4.1276e+07, 8.3606e+07, 1.6985e+08,
        2.4611e+08, 8.9864e+07, 8.3542e+07, 2.6186e+08, 2.5785e+08, 3.9094e+07,
        1.0745e+08, 3.2574e+08, 7.5758e+07, 3.4606e+08, 1.8207e+08, 3.5728e+07,
        1.6388e+08, 1.5801e+08, 4.5421e+07, 7.1457e+07, 2.7280e+08, 9.9114e+07,
        7.5858e+07, 3.6993e+07, 1.7905e+08, 1.1856e+08, 1.1450e+08, 1.3553e+08,
        4.1850e+08, 1.7844e+08, 1.2892e+08, 1.0713e+08, 2.4014e+08, 6.2776e+07,
        2.8280e+08, 2.3472e+08, 1.1232e+08, 4.9715e+07, 2.8878e+08, 2.6942e+08,
        1.1100e+08, 2.6442e+07, 1.2642e+08, 4.6242e+07, 1.8854e+07, 1.7655e+08,
        7.0290e+07, 1.0015e+08, 7.9435e+07, 1.2362e+08, 5.5995e+07, 1.3960e+07,
        1.7262e+08, 1.6541e+08, 2.0270e+07, 1.2932e+08, 1.9946e+08, 2.7634e+08,
        8.3999e+07, 2.9730e+08, 2.1876e+08, 6.9235e+07, 9.7174e+07, 2.0776e+08,
        3.4505e+08, 3.6387e+07, 2.0882e+07, 1.5501e+08, 1.0586e+08, 1.0992e+08,
        4.8612e+07, 3.7219e+07, 1.7487e+08, 2.3193e+08, 2.8944e+08, 8.5282e+07,
        1.1344e+08, 1.0542e+08, 8.9301e+07, 1.8042e+08, 4.9053e+07, 1.0410e+08,
        2.6296e+07, 1.5119e+08, 1.8251e+08, 6.0383e+07, 1.7495e+08, 4.9114e+07,
        3.3120e+08, 2.7556e+08, 8.7718e+07, 3.8169e+07, 5.1364e+07, 8.3784e+07,
        1.3805e+08, 1.4383e+08, 1.4247e+08, 1.0788e+08, 8.2645e+07, 1.3430e+08,
        2.8741e+08, 2.6190e+08, 1.1175e+08, 7.4961e+07, 1.0455e+08, 1.1188e+08,
        6.7603e+07, 1.2333e+08, 1.4387e+08, 1.6290e+08, 4.3372e+07, 7.9465e+07,
        2.4515e+07, 3.5923e+07, 1.2377e+08, 3.1113e+07, 6.1574e+07, 1.0094e+08,
        1.2313e+08, 1.4113e+08, 1.1206e+08, 1.9567e+08, 5.9788e+07, 2.8853e+07,
        1.1632e+08, 1.0966e+08, 5.3418e+07, 5.6816e+07, 2.6271e+08, 7.7838e+07,
        1.7938e+08, 8.7065e+07, 3.7624e+07, 1.4422e+08, 2.9337e+08, 1.1121e+08,
        1.2946e+07, 3.1700e+08, 3.6570e+07, 7.5015e+07, 6.8126e+07, 2.3607e+08,
        3.3263e+07, 3.0480e+07, 8.9594e+07, 1.5248e+08, 1.0911e+08, 1.8116e+08,
        3.0258e+07, 1.7813e+08, 7.6098e+07, 1.4327e+08, 2.1180e+08, 3.6314e+08,
        9.9132e+07, 3.6441e+08, 3.1154e+08, 2.3805e+07, 8.8615e+07, 1.1445e+08,
        1.8713e+08, 8.1182e+07, 1.9791e+07, 2.5078e+08, 9.1467e+07, 1.1572e+07,
        8.6797e+07, 9.9913e+07, 1.3837e+08, 2.2214e+08, 8.1303e+07, 5.6195e+07,
        1.2132e+08, 1.1410e+08, 2.1283e+08, 2.7004e+08, 2.0282e+08, 1.0471e+08,
        7.6506e+07, 9.2341e+07, 2.1154e+08, 3.0564e+07, 7.4448e+07, 4.4546e+07,
        1.7660e+08, 1.1772e+08, 6.1674e+07, 3.5767e+07])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([225215.7500, 108511.5312, 705967.0625,  ..., 788324.1250,
        127833.5000, 261890.4219])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1420821.8750, 1745469.1250, 1009693.8125,  974457.8750,  908520.4375,
        1353567.1250,  927302.0000, 1076863.0000, 1269470.0000, 1685199.6250,
        1519506.7500, 1096956.5000,  935315.5625, 1435163.3750,  581842.9375,
         741272.0000, 1873622.2500, 1343670.8750, 1137878.1250, 1494972.1250,
        1031727.6875, 1598577.5000, 1037887.6875,  985294.5625, 1040121.8125,
        1152667.2500, 1857098.5000,  693265.3125,  690444.5625, 1423713.7500,
        1532581.8750,  725252.3750,  955765.6250,  823060.4375, 1264621.7500,
        2430446.2500,  923787.7500,  981752.8750, 1109456.1250, 1713366.1250,
        2090353.6250,  933207.0000,  811418.9375, 1201597.2500,  971563.0000,
        1357055.0000, 1195635.8750,  791159.5625, 1462261.7500, 1297902.7500,
        2026404.1250,  762040.1250, 1574998.3750,  971774.8750, 1973056.7500,
        1144113.5000, 1118486.5000, 1838740.5000, 1044609.3125, 1237320.7500,
        1989119.5000,  764516.1250, 1246653.6250, 2103582.7500,  995093.1875,
         963252.8125, 2070990.1250, 1776365.1250, 1490978.1250, 1416225.6250,
        1350523.7500,  853759.3125, 1309299.3750,  646551.5000, 1981580.0000,
        1267840.2500, 1200042.8750, 1126618.0000, 1520010.5000,  982712.3750,
        1194252.8750, 1453040.6250,  925926.9375, 1364675.5000, 1223211.7500,
         951433.8125, 1321500.3750, 1485594.6250, 1668260.1250, 1075860.3750,
         655718.4375,  969069.5000, 1188504.5000, 1263888.6250, 1292620.5000,
        1658734.7500, 1332969.3750,  932984.1875,  955230.0625,  983591.0625,
        1129969.8750,  975126.2500, 1149387.5000,  848483.1875,  842252.0000,
        1185470.3750,  884774.9375, 1031142.1250, 1411474.8750, 2008617.0000,
         824846.6875, 1469281.7500, 1792957.6250, 2326636.5000, 1265479.2500,
        1021712.1875, 1106671.3750, 1456744.2500,  490326.9375, 1104054.8750,
        1498006.0000, 1794521.5000,  763974.1250,  922361.5000,  814579.7500,
        1015498.8750, 1774076.5000,  915546.3750,  781203.8750, 1063999.2500,
        1604387.7500, 1524692.3750, 1461160.8750, 1378095.5000,  727380.8125,
         666725.8750, 1231193.3750,  792564.0000, 1510127.0000, 1313891.3750,
         813221.1250, 1309304.5000, 1406426.7500, 1163171.8750, 1508984.0000,
        1092865.7500, 1147067.1250,  854957.1875, 1918175.2500, 1456467.3750,
        1265549.1250, 1046438.3750, 1634575.5000, 1026222.8125, 1468861.7500,
        1253345.5000, 1232832.5000, 1046063.6250,  726852.8750, 1829773.3750,
        2107905.2500, 1932236.8750, 1149164.5000, 1942318.6250, 1002760.5625,
        1342857.8750, 1325582.2500, 1947781.1250, 1292181.3750,  966043.2500,
         873767.0625, 1660225.5000, 1846547.2500, 1074615.6250, 1440738.8750,
        1248647.6250, 1105286.5000, 3062112.7500, 1645654.1250, 1674866.1250,
        1382037.6250, 1581293.0000, 1024912.1250, 1041537.4375,  685993.8750,
         987452.8125, 1419479.1250, 1390014.8750,  969353.1250, 1142595.7500,
        1066550.3750, 1032965.4375,  889121.1875,  739150.5625,  704139.7500,
        1317678.1250,  838152.1250, 1087721.0000, 1096786.7500, 1059948.7500,
        1673362.7500, 1363824.1250, 1251089.7500, 1837471.1250,  997320.9375,
        2179612.0000, 1630614.8750, 1766984.8750, 1108839.6250,  799983.2500,
        1465939.1250, 1511049.3750, 1047508.5625, 1021195.1250, 1167827.6250,
        1659504.3750, 1524109.5000,  794320.4375, 1336956.5000,  989272.8125,
        2001157.7500, 1829317.5000, 1832212.7500, 1012371.4375,  990591.0000,
        1127452.5000, 1420197.2500, 1296176.6250,  917656.0000, 1164213.2500,
        1056965.2500, 1820429.7500, 2010635.1250, 1401702.6250, 1485370.3750,
         878492.2500,  763245.6250, 1139306.7500, 1737158.7500, 1493708.6250,
        1907707.2500,  805782.7500, 1289702.5000,  758497.1250, 1044707.3125,
        1050311.2500, 1518137.3750, 1664213.3750, 2083055.0000, 1617696.8750,
         723647.5625, 1848877.5000, 1257621.6250, 1397991.0000, 1695620.2500,
        1337002.3750])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3546376.0000, 2724884.2500, 2867101.0000, 2339713.0000, 4019278.0000,
        3542767.7500, 2993779.2500, 4616029.5000, 4955275.5000, 4934799.0000,
        4700254.0000, 3334546.5000, 4941639.0000, 3213265.5000, 6756989.0000,
        6708926.0000, 2823012.7500, 5400615.5000, 4444488.0000, 4211206.5000,
        3657960.2500, 3954515.0000, 3217198.7500, 5236969.5000, 6987800.5000,
        3594185.2500, 3780245.0000, 2917743.2500, 4326465.5000, 3903640.7500,
        2997507.5000, 2661247.5000, 4771490.0000, 3902076.0000, 5699529.0000,
        2601819.7500, 3922839.0000, 5668785.0000, 3023253.5000, 6833450.0000,
        4948550.0000, 3503959.5000, 4775080.0000, 3834986.2500, 4213385.5000,
        4930068.0000, 3012067.5000, 4401446.0000, 3087577.0000, 3070363.0000,
        4190762.2500, 4686310.0000, 4093305.2500, 7845744.0000, 4913366.0000,
        5111295.0000, 4373551.5000, 4476045.5000, 4456424.5000, 3876466.2500,
        5802301.5000, 7209342.0000, 5885037.0000, 4475131.0000, 3930317.2500,
        3333613.5000, 4397823.5000, 3968432.2500, 4083627.7500, 3523351.0000,
        3825774.5000, 4780385.0000, 4219431.0000, 5477947.5000, 5253419.0000,
        5527920.5000, 5628176.0000, 5329684.5000, 2806661.5000, 4623476.0000,
        5173225.0000, 7077143.0000, 3133557.7500, 5511470.0000, 3536667.7500,
        5522402.5000, 3355775.5000, 6829041.0000, 4567438.0000, 4302824.0000,
        2917724.5000, 4831899.0000, 5211149.0000, 5021934.0000, 2593864.0000,
        2368475.2500, 4529849.0000, 2327998.5000, 6830292.5000, 4617678.0000,
        6485705.0000, 4467125.5000, 3382754.7500, 4564710.0000, 3323349.2500,
        2785371.2500, 4000867.5000, 3994558.2500, 7068677.5000, 2881789.5000,
        4341975.0000, 3880931.5000, 7626804.5000, 4301197.5000, 5627209.5000,
        3346661.2500, 5494283.0000, 5406225.5000, 3008193.5000, 5752258.0000,
        2503961.0000, 4795341.5000, 3401810.5000, 2854569.5000, 3805721.7500,
        3297950.2500, 7031248.5000, 7977016.5000, 5953980.0000, 7268167.5000,
        4454539.0000, 4916432.0000, 4258359.5000, 2450756.7500, 3422844.7500,
        2414031.2500, 5437587.5000, 4246570.5000, 3511646.2500, 5666280.0000,
        6800299.5000, 4013617.2500, 4639266.0000, 4522308.5000, 4903035.0000,
        3505752.2500, 5230515.0000, 2998336.0000, 3882647.5000, 4068175.2500,
        4259383.0000, 3750296.2500, 4269757.5000, 3647746.0000, 3621480.5000,
        4045194.7500, 2973891.2500, 4797907.0000, 4346067.0000, 7679114.0000,
        4463327.0000, 3071302.5000, 2415321.5000, 2288803.5000, 7677500.0000,
        6587764.0000, 5220952.0000, 5996801.0000, 4348400.5000, 5501762.0000,
        3799832.2500, 5552270.5000, 2823914.2500, 5154637.0000, 4830963.5000,
        2740498.0000, 4982567.0000, 9269272.0000, 5253521.0000, 3841105.2500,
        3389201.2500, 2853154.5000, 5506181.5000, 3044946.0000, 5103229.5000,
        5705100.5000, 3270859.7500, 6852140.5000, 3287623.5000, 2664763.0000,
        3627444.7500, 3442449.0000, 2970353.0000, 4396419.5000, 4218322.5000,
        3783192.5000, 3519654.5000, 4549999.5000, 6362029.0000, 2943844.2500,
        5442339.5000, 7377343.0000, 5118304.5000, 3605129.7500, 6251676.0000,
        4046645.5000, 2815697.7500, 4443187.5000, 5646577.0000, 3779954.5000,
        4614761.0000, 3888167.5000, 5973567.0000, 4316638.5000, 3825354.0000,
        4895820.0000, 4713888.5000, 3630593.0000, 5510821.0000, 5219010.5000,
        3124923.2500, 5700641.5000, 4321036.0000, 6355472.5000, 6227298.0000,
        2776287.7500, 5651628.0000, 5114264.0000, 3464761.0000, 3343663.0000,
        4568362.0000, 3431598.2500, 3431944.0000, 2432561.7500, 3764887.5000,
        4213123.0000, 4038599.0000, 5218829.0000, 3995344.0000, 4329558.5000,
        3131099.5000, 5298590.0000, 2482613.7500, 4180348.5000, 4980629.0000,
        6546138.0000, 4215353.5000, 4232471.0000, 4049692.0000, 2695417.5000,
        5740941.5000, 5940786.0000, 3211198.2500, 8033602.5000, 3568249.0000,
        3342167.7500])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 55251.5000, 134813.0156, 369757.2500,  ..., 476189.7188,
         40634.2773, 111296.8984])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 979629.3125,  567135.5000,  517236.5312,  504125.9688,  560000.1875,
         550644.7500,  399354.5000,  515215.8750,  826391.0000, 1224163.8750,
         822383.4375,  474531.3750,  498869.6250,  607111.8125,  602720.8750,
        1153432.8750,  605453.8750,  912207.3125,  879597.5625,  779324.6250,
         523760.3125, 1230896.8750,  526252.2500,  918613.3125,  372500.5312,
         574338.0625,  283151.8125,  406799.6875,  575601.8750,  295570.9062,
         652028.2500, 1062389.0000,  461118.4688,  608297.1250,  448041.9375,
         716583.3750, 1199883.2500,  553800.2500,  543287.2500, 1159779.2500,
         822649.3125,  637826.6875,  617704.6875,  681944.4375,  367775.0625,
         667013.0625,  789624.3125,  841500.6250,  737170.6875,  618176.6875,
         398779.5000,  827768.5000,  885433.9375,  830868.6875,  521617.6562,
         887058.9375,  455595.6875,  240926.8906,  498161.8125,  771270.1875,
         708787.8125,  501167.5938,  943816.5625,  396950.5625,  259853.8125,
         596104.0625,  843698.3125,  444266.0000,  695691.1875,  608627.1875,
         617286.3125,  740302.5000,  462587.9375,  427073.3438,  562128.3750,
         416373.2500,  587517.8125,  848921.8750,  341573.0625,  656504.4375,
         619725.2500,  425910.0625,  702719.9375,  430532.0938,  830399.7500,
         818152.8750,  460012.6250,  415981.9688,  355065.1562,  560790.5000,
         691033.1875,  538501.8750,  320420.3438,  558056.5000,  609740.0625,
         457005.7500,  939272.8125,  613135.9375,  613083.7500,  736265.5000,
         925641.8125,  625450.2500,  583654.5000,  596205.5000,  539386.5000,
         578413.4375,  545238.7500,  477723.6875,  347392.7188,  564234.6250,
         529312.2500,  869344.5000,  897276.8125, 1286497.2500,  528917.8125,
         600138.5625,  515045.9688,  473069.5312,  714322.0625,  539653.7500,
         488361.5625,  772374.1250,  895426.1875,  701200.8750,  270836.5938,
         517153.3438,  728883.1875,  619092.3125,  436557.6250,  584342.0625,
         445684.1250,  768484.4375,  728331.0625,  845929.8125, 1108260.3750,
         986384.5625,  590275.6250,  328635.0000, 1045938.6875,  832598.9375,
         365245.9062,  507313.1875,  536031.5625,  319288.5938,  473474.0000,
         786993.3125,  540454.9375,  273663.5000,  824119.4375,  619399.0000,
         230517.3594,  654221.4375,  428567.5625,  544798.0625,  391887.4688,
         699534.2500,  577564.3125,  597974.0000,  453592.5312,  312920.1250,
        1010659.1250,  505583.0625,  285028.3750,  650351.0625,  495738.4688,
         343742.5938,  383148.4688,  443149.9375,  770146.0000,  378144.0625,
         387560.8750,  680013.0000,  469440.5625,  788911.9375,  287971.0938,
         270501.2500,  850156.8750,  754821.9375,  846937.2500,  527936.5625,
         499047.5625,  422285.4688,  679615.6250,  931102.4375,  378634.2500,
         587932.0625,  672234.3125,  467749.0938,  548015.9375,  224396.7344,
         536215.9375,  414437.2188,  927353.5625,  650363.8750,  551492.6250,
         816648.1250,  342041.9375,  789281.5000,  561257.5625,  359745.5000,
         746449.1250,  803823.3125,  454519.0312,  553846.8750,  325043.5938,
         350722.3438,  319428.0000,  624375.5625,  769422.5625,  515367.9375,
         447727.3125, 1024752.7500,  563930.1250,  396046.7812, 1003848.2500,
         439326.5625,  315363.2500,  579440.1875,  399062.8750,  608413.6250,
         613219.6875,  550936.3750,  957003.1875,  357836.0312,  422260.4062,
         507119.2500,  717887.3750,  348628.3750,  529481.4375,  932771.8750,
         723099.8750,  725748.7500,  594171.9375,  322897.6875,  382066.7500,
         960364.9375,  442875.7188,  406798.1562,  538708.1250,  492959.7812,
         681592.3125,  228996.3438,  721746.8750,  399120.9062,  678538.0625,
         591597.0000,  887399.3125,  263086.0625, 1065662.2500,  288297.7500,
         470456.0312,  880093.1875,  639873.5000,  424557.7188,  595175.6875,
         577871.3125])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3031323.0000, 2096577.5000, 1663064.0000, 4330310.5000, 2469603.0000,
        3906643.2500, 1844056.8750, 1642505.1250, 2290887.0000, 2087151.8750,
        2613942.5000, 2571039.5000, 2195938.0000, 3467278.7500, 1312243.6250,
        3444891.5000, 3042866.2500, 4087641.5000, 3945799.7500, 2502994.2500,
        4759488.0000, 2468716.0000, 3493618.0000, 1888487.8750, 2124038.5000,
        3781472.5000, 2811934.2500, 3374733.2500, 2281098.5000, 1323796.2500,
        2465054.0000, 3080575.0000, 2232084.0000, 1424127.8750, 2545897.7500,
        2769300.7500, 2256845.7500, 3896205.0000, 2923259.7500, 1352446.6250,
        2897406.2500, 2963283.7500, 1341078.0000, 1689575.6250, 3311763.5000,
        2627428.5000, 2386522.2500, 3109729.2500, 1946479.5000, 2745515.5000,
        1382511.6250, 2667858.0000, 5168730.5000, 1686220.3750, 1858216.3750,
        2175466.2500, 3239733.0000, 4079248.0000, 3549993.0000, 3090830.5000,
        1704182.6250, 2092648.5000, 2714726.0000, 2053929.2500, 1852419.0000,
        1772708.3750, 2549870.2500, 1501314.5000, 2086157.2500, 1926410.3750,
        2556416.5000, 3837125.2500, 2685158.7500, 1975974.5000, 1625039.0000,
        1726924.1250, 1320560.8750, 3386373.7500, 1560413.0000, 1464407.8750,
        2256935.5000, 1573684.7500, 4023940.5000, 2385456.5000, 2620408.7500,
        1568803.6250, 3490337.7500, 2227386.5000, 3631075.2500, 3272158.7500,
        1293398.1250, 1410383.1250, 4323564.5000, 2198087.5000, 4306596.5000,
        1937604.8750, 2999975.2500, 3348993.0000, 2268564.0000, 3469876.0000,
        2033856.5000, 2599115.0000, 3471072.5000, 3343727.2500, 3571705.5000,
        1885390.0000, 5107688.5000, 2846852.0000, 1855622.6250, 3809836.5000,
        2233951.2500, 3198092.0000, 3108747.2500, 4249881.0000, 2616573.5000,
        3149818.7500, 4693186.5000, 2328128.5000, 1711793.7500, 1259633.3750,
        1116179.3750, 2179422.0000, 1948484.0000, 3273660.7500, 1725036.6250,
        2799654.7500, 2703766.7500, 2281577.5000, 1954783.0000, 3792637.5000,
        2081049.6250, 2125741.2500, 2137123.0000, 2415693.2500, 2929136.0000,
        2377116.0000, 2102010.2500, 2620203.5000, 3111004.5000, 2237055.5000,
        1957310.7500, 2888184.0000, 3204726.7500, 1453867.7500, 2337902.5000,
        1865562.8750, 2750154.5000, 2541641.5000, 1749541.8750, 3871435.2500,
        1138993.8750, 2132760.5000, 1863058.5000, 1549110.8750, 1264481.1250,
        3562350.5000, 2435542.0000, 1796193.6250, 1840627.5000, 1266374.5000,
        2400208.2500, 3158961.0000, 1976909.8750, 1028459.0625, 2176584.2500,
        2935071.7500, 2265597.2500, 3039367.2500, 1631522.8750, 3054200.2500,
        2262116.5000, 3820181.0000, 1215593.0000, 2570910.0000, 1335775.0000,
        1658568.5000, 1852893.6250, 3591394.2500, 3618566.2500, 2198732.7500,
        2820500.0000, 2408196.5000, 2831847.0000, 3346843.0000, 1471586.5000,
        1709162.1250, 2543966.7500, 2358248.0000, 2957605.0000, 1754386.7500,
        3247058.2500, 2425815.0000, 3229964.2500, 3121728.0000, 3365843.5000,
        1623606.2500, 2113722.7500, 2745559.7500, 1399760.7500, 1430508.0000,
        2771765.7500, 1426122.7500, 4506568.5000, 1544941.7500, 3978543.0000,
        3287094.7500, 1807872.3750, 1594578.8750, 3064575.7500, 1525476.1250,
        1805135.8750, 4603147.5000, 3924932.5000, 1949329.3750, 2679765.2500,
        3367057.7500, 2286137.5000, 4767978.5000, 3386746.0000, 2154191.7500,
        2084671.5000, 3706326.0000, 3785940.0000, 1487166.2500, 1074987.7500,
        1867154.1250, 2582420.2500, 2233724.5000, 2582520.5000, 1695072.5000,
        2563966.7500, 1429860.8750, 1126802.6250, 2798950.0000, 1903375.1250,
        2178812.0000, 2763004.7500, 3210257.7500, 1645128.5000, 1473780.7500,
        1423915.6250, 2850857.5000, 1966357.3750, 1986800.6250, 3833355.2500,
        2252075.5000, 2333719.7500, 2846601.0000, 1802827.0000, 2255159.0000,
        2748699.0000, 3458436.7500, 2136990.2500, 1166586.0000, 2222928.2500,
        1720292.2500])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 4441.8135, 95493.6250, 66166.7578,  ..., 18830.4980,  1365.6674,
        53224.8828])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([275409.7188, 176875.0000, 228618.5156, 214282.3750, 208828.9531,
        197126.5625, 120584.4688, 189657.2969, 296580.2188, 226486.8125,
        276724.7500, 246776.4219, 163870.7344, 283332.0000, 213932.6562,
        179045.3906, 277458.5625, 200038.7500, 194517.9531, 165795.7656,
        169061.8125, 209894.0156, 215373.2031, 158330.4062, 226119.8281,
        201207.0469,  97023.6016, 179517.7188, 268322.2188, 245915.2031,
        195615.1094, 277908.2188, 214094.6719, 214817.3594, 188263.1562,
        220353.4844, 155646.4062, 223920.9219, 213477.1094, 188024.6875,
        184313.1406, 230221.7656, 202005.3281, 148961.7500, 237271.5156,
        117063.8906, 176163.4375, 201012.2969, 149852.9062, 179189.1719,
        163471.8281, 191020.4531, 169155.6406, 139891.3125, 261797.8281,
        172562.4844, 201602.9375, 164582.7031, 203623.6406, 192554.8594,
        177095.1094, 247245.7188, 230740.7188, 153435.7969, 145074.2969,
        182961.6719,  94562.7031, 144829.8906, 175528.5938, 214774.2344,
        113160.1953, 156757.6094, 320221.7500, 130467.8672, 155203.4062,
        161344.3438, 159866.5156, 247157.1094, 142640.6406, 122130.8984,
        166012.0000, 254632.8906, 304344.6875, 257203.0312, 156467.2969,
        192233.0000, 202277.7031, 198894.0312, 264377.0312, 216358.6250,
        231098.8750, 206305.3750, 228540.5156, 246197.8125, 253361.3750,
        263626.1875, 217490.4219, 174896.2344, 183980.8281, 250051.8750,
        111186.2109, 244884.2031, 173754.7188, 155811.5938, 272637.9062,
        194448.7969, 184871.5312, 325781.8438, 202796.0625, 134684.2344,
        205524.0625, 169751.2656, 160055.1094, 348876.4688, 206209.4531,
        199212.6094, 189768.0781, 207933.5469, 154917.1406, 194331.1250,
        178112.1719, 133438.9219, 198322.5781, 136452.9844, 138381.0781,
         93953.3516, 184262.0156, 197188.0312, 120582.1953, 291391.4062,
        228005.8281, 168608.6875, 161098.8594, 142678.4375, 203419.6250,
        180240.3125, 162268.5469, 198075.8906, 210643.9844, 219483.8438,
        162554.6719, 217765.2969, 132124.3438, 324639.9688, 151625.3438,
        186972.3125, 205513.4688, 215053.2812, 134035.9219, 198683.9375,
        221928.2969, 252111.0938, 263250.0312, 192415.3281, 193193.8281,
        139206.7969, 135334.9688, 169880.3438, 218846.1562, 112529.5469,
        363876.5312, 216811.9219, 141971.1562, 112743.6484, 185380.0625,
        367776.5000, 217467.5312, 200304.3438, 155320.6719, 240432.4375,
        347267.2188, 208497.6562, 182741.6562, 142603.2969,  81927.8906,
        194631.3594, 179755.0625, 124772.8828, 213174.3438, 217692.2969,
        174081.7344, 109997.8750, 234994.7500, 141362.0938, 218537.7344,
        159619.8281, 194449.2031, 143592.8906, 154341.0000, 213161.5625,
        188235.1406, 173587.7500, 139805.4062, 191141.6406, 171257.7344,
        178546.8750, 148914.4375, 230466.0312, 235282.5156, 147371.5156,
        162925.7031, 152876.0781, 293041.9062, 161925.1406, 227245.8594,
        171595.9844, 123802.5859, 147089.7500, 306996.4688, 239474.9062,
        143925.8906, 200094.0938, 190546.4062, 171206.6094, 160641.3594,
        193226.0156, 158634.9219, 164291.0469, 274149.6250, 226638.8125,
        146497.1250, 131998.0781, 174156.3906, 263350.6562, 272024.9062,
        320528.0938, 123939.3594, 278033.4062, 141487.1094, 277181.8750,
        158777.2656, 128482.1094, 163509.2188, 191324.3750, 173264.9531,
        238532.7656, 274366.8125, 323331.2500, 258797.3906, 222998.7969,
        139962.7188, 301339.7188, 179282.7344, 191610.9844, 261012.7500,
        225315.5156, 242389.7188, 225198.2188, 208534.1875, 193923.3750,
        213789.7812, 262926.4062, 217744.7500, 194205.6250, 156851.4375,
        218104.5938])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 992876.5000,  740363.2500, 1136598.8750, 1338515.6250,  889050.0000,
         523594.0000,  743065.1250,  817163.6875,  931789.0000, 1118099.1250,
         674316.6250,  644854.3750,  828144.1875,  789893.8750,  694821.8750,
         991436.1250,  617148.8125, 1062383.3750,  740139.8125,  607271.8125,
        1044414.6250,  815564.8125,  844482.1875, 1901511.1250, 1191008.2500,
         708060.0625,  939233.2500, 1054266.8750, 1111961.8750,  838848.5000,
         789846.5000, 1033922.8750, 1024391.4375,  710260.1875,  874914.8750,
         756904.6250,  842239.0000, 1214096.6250,  734008.6875,  933055.9375,
         736329.9375,  743595.8750, 1284540.5000, 1092696.7500,  687699.3125,
         682883.4375,  499635.0000,  967155.3750,  587481.3125,  904567.5625,
        1006782.3125, 1072096.6250,  975442.3750, 1015798.6250, 1017554.0625,
         794681.9375, 1039870.2500,  731040.1875,  845468.2500,  767545.3750,
         704421.8750,  790893.1250,  792389.4375,  982811.0000, 1211878.1250,
         760735.8125, 1363092.1250, 1045244.6250,  934029.1875,  563324.4375,
         824687.0625,  934325.9375,  892210.9375, 1215599.6250, 1174240.0000,
         474531.0938,  892475.4375,  751686.0625,  882982.5625, 1093352.5000,
         849161.0625,  680264.2500,  916036.3125,  767698.8750,  640657.9375,
         751336.3750,  705058.3125, 1375459.8750, 1404923.5000,  579275.6875,
         741544.0625, 1013371.3750,  858721.7500,  662605.6875,  680276.1875,
         789430.8125, 1212619.5000,  623717.7500,  709879.1875,  721967.0625,
         852182.7500,  942392.3125, 1289280.5000,  650681.6250,  640821.3750,
         663933.3125, 1102110.5000,  732449.6875,  952493.0000,  396833.9688,
         664664.9375, 1051263.0000, 1236022.0000,  569020.0625,  834425.2500,
        1131069.3750,  880787.6875, 1156490.3750,  457891.7500,  962212.9375,
         888432.8750,  888625.1250,  862459.0000, 1141463.1250,  808971.0625,
         791741.8125,  785389.5000,  602142.6250,  847080.3125,  712478.5000,
         947332.1875, 1177397.7500,  706204.9375,  719695.8750, 1055458.1250,
         980320.5625,  869783.3750,  834187.8125,  665340.4375,  950770.0000,
         820994.1250, 1345238.2500,  816797.6875,  781079.8125,  543905.5625,
        1340826.2500,  651560.8750, 1507613.8750,  611443.1875,  797732.1250,
         978014.8750,  604018.9375,  982734.8750,  829688.7500,  679938.7500,
         720994.5000,  838123.8750, 1043241.8125,  606760.4375, 1199938.0000,
        1095041.6250,  855087.8750,  774038.9375,  940357.5625,  707483.9375,
         901442.1250,  546601.7500, 1100889.5000,  994289.9375,  836348.9375,
        1411060.7500,  847069.5000,  819557.1250,  883776.3750,  673492.3750,
         675779.8125, 1339088.3750, 1089867.0000,  901126.0000,  743624.8750,
         946694.3125,  906200.8125, 1230528.6250, 1523137.8750, 1315847.8750,
         861949.2500, 1089959.5000,  669193.1250,  991800.5625,  853981.6875,
        1029948.6250,  628007.1875, 1225874.2500,  486917.7812,  704715.3125,
         616328.3125, 1206087.1250,  700791.1875, 1141168.2500, 1002636.1250,
         864972.3750, 1321380.3750,  782238.0625,  552116.9375,  498896.0000,
         595221.4375, 1048688.6250,  903985.0625,  581572.3125,  855491.3750,
         687648.3125, 1016613.6250,  719839.3125,  837614.1250, 1081778.1250,
         824775.0625,  738598.1250,  833661.3125,  538445.2500,  714112.9375,
         734149.5000,  546851.1250,  728855.6875,  489523.5938,  975925.7500,
         614302.6250,  803710.1875,  995644.5000,  561276.2500,  618507.0000,
         920975.6250, 1006233.8125,  739724.1875, 1219597.7500,  808627.1250,
         624856.7500, 1019264.6875, 1553332.8750,  986260.5625,  929857.0000,
        1022740.0625,  776356.6250,  979904.7500,  921068.6875,  665227.8750,
        1116622.6250,  906827.3125, 1202339.2500,  789304.1875,  902944.3750,
        1235551.0000,  842850.3750, 1461942.6250,  634592.2500, 1120983.7500,
         677152.5625])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([  1106.3115, 136854.8281,   4985.8125,  ...,   4252.1118,
          2048.1746,   7724.9912])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 51123.9414,  53197.7148, 101629.4688,  80715.2422,  85994.7578,
         64668.3672,  54131.4883,  35357.5312,  43088.7812,  41231.4141,
         44407.9023,  38812.6836,  43173.1875,  60674.3945,  53352.3906,
         62554.6133,  86192.0859,  69324.9375,  56155.1328,  48775.7852,
         43717.0625,  77061.3047,  49194.1758,  50381.3789,  63520.2734,
         62857.5664,  64323.8438,  49799.8672,  56796.3594,  47118.6250,
         53031.9844,  54617.1094,  44257.3867,  42285.0703,  41796.2852,
         50092.4180,  86372.5156,  49907.1719,  57519.8828,  49372.2461,
         47605.6445,  54609.8828,  64437.6289,  42410.7930,  53873.1094,
         73431.6719,  33365.1914,  73067.9219,  43954.1562,  39696.5586,
         55028.4414,  33306.5625,  43756.6172,  41970.5977,  54813.9570,
         40831.9023,  45325.5273,  62513.9219,  50632.2617,  88865.7891,
         48539.1562,  26850.0625,  73137.2188,  68098.7812,  42762.7031,
         50035.5391,  53780.3438,  78158.7578,  46658.7383,  56509.6172,
         60073.7656,  58860.3203,  63954.1406,  72133.4844,  56081.0938,
         65138.9023,  53134.5547,  67598.3281,  61492.9766,  57583.8359,
         39943.8750,  41715.0586,  39716.4141,  38883.9883,  63506.4648,
         28734.5566,  56541.4648,  48731.0547,  53697.0430,  79488.0312,
         53907.4570,  43603.3633,  38587.8594,  38617.1484,  37841.9609,
         43289.8477,  34235.3789,  56251.0000,  56628.7148,  60341.0312,
         62789.2070,  62691.6211,  63885.1719,  30766.5957,  69493.4688,
         67039.4531,  46053.9141,  71668.6797,  67163.5703,  36458.5273,
         54469.1641,  52792.2695,  48262.0859,  45533.5781,  79690.7188,
         52548.3945,  26262.4727,  53363.0469,  53224.9648,  37568.6523,
         54144.0859,  34716.8555,  72178.0547,  43397.9023,  49998.1562,
         45884.3320,  51071.9258,  50331.7188,  69461.6094,  72454.7812,
         50171.7969,  47593.3711,  60036.5703,  62433.1328,  45444.9688,
         67064.8438,  46678.6953,  72735.3984,  53614.6172,  56161.7266,
         82157.3047,  78174.0312,  37248.4258,  77348.0703,  67842.9453,
         32946.0742,  56413.6367,  67004.5703,  76943.0781,  31699.2070,
         48223.1602,  78014.0000,  59053.4375,  43699.6445,  60800.3555,
         47888.5781,  70143.6328,  45456.3008,  51040.9609,  61780.8672,
         73667.5156,  46662.5078,  64430.7305,  51663.7812,  42708.9453,
         47143.6445,  75457.8750,  40149.3906,  69200.7109,  76550.9141,
         46960.3438,  42398.4844,  68935.1016,  64420.6484,  63573.4570,
         55094.9219,  50183.0156,  35323.5898,  49062.1719,  52440.7227,
         63832.3203,  41977.3398,  44506.2109,  86650.1797,  62971.3828,
         45524.8945,  43256.0664,  51342.1016,  44576.9805,  52151.7109,
         43474.2344,  52270.0273,  51748.3555,  50434.4961,  39705.5625,
         56857.6875,  39617.6758,  44980.7930,  52379.0078,  41216.2070,
         45156.4570,  48980.0547,  66421.3203, 115262.8281,  65974.4141,
         41620.5781,  45743.6211,  69992.7031,  54777.5039,  49833.1055,
         46759.7695,  52518.2500,  82784.9531,  28248.2109,  65114.5234,
         42726.7656,  60313.8047,  40686.6719,  43483.7734,  58700.3711,
         47964.4922,  35737.2461,  64259.6836,  44588.8047,  35712.4336,
         41460.1367,  52503.6250,  40833.5117,  45568.4883,  27662.3438,
         51677.6289,  61670.8594,  87307.4297,  46825.9336,  51549.5586,
         47355.3867,  50001.6562,  47205.2969,  57299.3633,  42110.7656,
         24415.7285,  87906.8828,  52825.6875,  45525.3125,  74929.6094,
         28873.0625,  65046.0898,  64218.8906,  64582.8906,  44462.8789,
         90104.2969,  69105.7109,  60061.9883,  39496.4961,  70895.2109,
         49611.9531])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([373363.5938, 369120.2812, 207360.5000, 246158.0156, 136390.8438,
        338218.6250, 270971.4688, 372735.7500, 181003.9062, 197705.8906,
        336670.9375, 235064.7812, 295249.7500, 210506.8594, 246718.8281,
        224395.7500, 182022.7031, 240317.1875, 351472.9062, 186011.8438,
        346165.9688, 255974.7500, 254117.2656, 260660.2656, 244651.2812,
        190999.4844, 152469.2344, 216001.7969, 416644.7500, 389139.4688,
        196957.1719, 209352.0625, 192755.8281, 204800.8125, 269747.4688,
        394726.6562, 246757.8438, 332266.1562, 165514.9844, 232345.4375,
        175103.4219, 179136.0156, 242309.5156, 324783.9688, 224289.8438,
        267074.8438, 274033.9688, 237527.6719, 133071.6250, 203803.8906,
        330460.4688, 284208.7188, 326742.8438, 252298.4844, 182558.8750,
        229626.0469, 263331.3438, 306347.5000, 274422.2500, 196388.3906,
        291805.7500, 233988.1250, 258140.4062, 278412.3125, 201857.6719,
        391138.6250, 212409.8906, 200037.5781, 279492.4688, 322105.0312,
        343475.0312, 275697.9688, 325560.7812, 167786.8594, 275212.0000,
        360811.1562, 207797.6250, 188115.1562, 243725.5469, 368593.0312,
        207784.0781, 223067.5625, 254793.0156, 252695.6719, 183871.1562,
        372773.6250, 243424.8906, 336021.4062, 255348.2500, 159568.8438,
        198131.1562, 377913.9375, 313026.9062, 261260.5312, 367256.5000,
        211660.7812, 296016.4375, 179231.3125, 312171.5312, 195978.6094,
        186977.8750, 210566.8281, 275518.6875, 346514.1875, 216523.9062,
        299994.6562, 216636.5938, 201987.6875, 304003.4375, 170092.7500,
        257311.6719, 228152.5469, 216432.2500, 254174.5469, 166811.5781,
        199282.8594, 288850.5625, 272226.0938, 459903.4688, 274452.8750,
        214767.0156, 285853.7500, 273565.3750, 332884.1250, 489981.7188,
        237940.7656, 190432.2031, 222195.3125, 209003.8438, 187873.3906,
        272084.8750, 196220.9844, 417069.4375, 206816.9688, 316713.3125,
        200890.9688, 224200.4688, 277286.1250, 209118.3438, 317183.9062,
        224021.5312, 267534.1875, 243818.7188, 332049.5000, 254636.2812,
        333149.3750, 217281.3750, 232449.9844, 349987.0312, 313393.1250,
        426350.7188, 318084.5312, 312756.0312, 312396.0625, 377672.9688,
        346502.3750, 219961.4375, 274847.0000, 366233.2500, 298616.2188,
        311898.9688, 294173.0312, 261997.2656, 294958.3125, 226977.3594,
        306180.4375, 319652.2812, 240777.1562, 242216.9219, 328841.8750,
        310801.1562, 193548.6406, 142494.4375, 257457.8906, 161420.4219,
        283748.3125, 292850.8750, 319889.8438, 192219.9688, 286344.2188,
        273267.1875, 276238.8438, 284275.1562, 297285.8438, 233187.6406,
        347641.5625, 156436.5938, 208449.0469, 332796.7500, 317566.9375,
        189920.2500, 212556.7500, 200597.9531, 244890.1875, 171664.2812,
        186322.2344, 299305.4375, 237306.8906, 370372.5312, 326787.9375,
        183143.9062, 236746.5156, 202802.4531, 179948.2969, 159477.8906,
        383240.2500, 338469.2500, 213250.7500, 399436.0312, 281060.2500,
        237840.9219, 358216.7812, 212641.8750, 192478.3125, 110738.1719,
        370353.0625, 237770.3750, 318342.3750, 294359.2500, 302877.4062,
        258968.2031, 202060.5156, 236840.9688, 171404.0625, 239812.5000,
        243948.2656, 239850.2500, 201671.9375, 153693.6094, 208238.3438,
        169781.0156, 255805.6719, 190010.6562, 317950.1250, 263092.5312,
        363211.0312, 232959.5312, 326537.0000, 172009.4062, 311982.8438,
        179027.5938, 289917.0000, 395065.2188, 273942.2812, 353396.2188,
        146476.8125, 257636.2031, 270970.8750, 436534.8438, 325927.2812,
        303380.8750, 249490.8438, 284601.4062, 240277.1094, 211392.6406,
        229916.0312])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 601.4564,  897.2158, 2958.4470,  ...,  204.5444,  185.9057,
        2825.0759])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1481.1685, 1541.6975, 1180.2604, 1571.4851, 1529.6744, 1507.9143,
        1456.1987, 1401.8268, 1304.4667, 1454.3334, 1285.4628, 1668.3345,
        1498.7710, 1236.7439, 1607.2211, 1611.3724, 1319.4448, 1527.0762,
        1678.7968, 1420.7614, 1243.3555, 1553.0708, 1518.3063, 1531.4305,
        1228.0811, 1215.2721, 1592.9076, 1442.4685, 1457.2096, 1143.2212,
        1300.5763, 1253.2789, 1553.4554, 1320.9956, 1488.7115, 1423.3645,
        1323.1056, 1268.1698, 1320.1100, 1353.4729, 1278.2185, 1312.3872,
        1252.9567, 1293.2224, 1779.3186, 1296.4187, 1284.5885, 1189.8966,
        1373.8884, 1499.8300, 1242.8811, 1479.2257, 1548.5323, 1397.6322,
        1456.5076, 1509.3573, 1192.8007, 1759.3989, 1453.4370, 1300.6365,
        1240.5807, 1260.1681, 1553.4827, 1255.6620, 1504.6648, 1361.1348,
        1409.8101, 1285.9086, 1424.2802, 1410.8480, 1444.5215, 1513.2609,
        1726.1978, 1273.3304, 1392.6827, 1377.8287, 1500.3032, 1531.6345,
        1437.8596, 1344.9922, 1253.7998, 1418.6083, 1381.8044, 1490.4751,
        1430.7911, 1397.2227, 1438.8300, 1574.0896, 1357.3322, 1311.1738,
        1282.3674, 1509.0128, 1344.7501, 1725.7634, 1641.1351, 1407.8334,
        1733.0160, 1679.2316, 1551.9192, 1322.2886, 1388.0061, 1227.5339,
        1284.0602, 1288.7405, 1468.6527, 1512.8763, 1396.2633, 1411.4771,
        1503.8545, 1537.9651, 1343.7412, 1527.7302, 1273.0645, 1493.0469,
        1382.3802, 1609.4353, 1512.0355, 1338.6272, 1239.2196, 1477.1521,
        1224.9955, 1437.9768, 1420.2024, 1534.3326, 1516.8069, 1367.8735,
        1127.1650, 1606.9290, 1384.3477, 1329.4447, 1428.4733, 1387.8357,
        1462.0891, 1310.3026, 1609.8322, 1471.1754, 1197.7026, 1459.1108,
        1344.9698, 1681.9218, 1453.5256, 1341.7178, 1614.8014, 1624.5096,
        1326.4089, 1188.8365, 1616.6816, 1346.4985, 1463.5913, 1199.2538,
        1306.4713, 1495.8157, 1421.1681, 1464.9468, 1419.6564, 1509.3203,
        1601.1877, 1544.9562, 1600.6591, 1343.6323, 1872.6438, 1481.8477,
        1444.7582, 1456.3846, 1501.1993, 1373.2794, 1582.8920, 1510.3737,
        1770.4254, 1343.0121, 1419.1714, 1297.0371, 1393.9032, 1260.1641,
        1316.8760, 1595.1589, 1475.0173, 1467.5902, 1584.6638, 1507.2968,
        1720.6615, 1327.7119, 1381.0344, 1116.6095, 1276.3668, 1300.7056,
        1703.4725, 1490.1135, 1204.8204, 1285.6655, 1265.1782, 1546.5569,
        1238.0413, 1570.3680, 1382.1617, 1223.8787, 1297.3802, 1397.8125,
        1321.4982, 1624.0215, 1744.7872, 1561.4833, 1622.8966, 1338.1830,
        1322.9720, 1551.6376, 1208.3427, 1367.8192, 1377.5217, 1548.1541,
        1411.6586, 1621.8086, 1471.5002, 1436.4735, 1466.3562, 1490.4838,
        1179.5869, 1429.6693, 1692.4370, 1329.9834, 1366.1592, 1707.1748,
        1322.1517, 1598.6351, 1214.7988, 1745.5154, 1337.8596, 1699.9913,
        1328.6494, 1484.7665, 1485.1622, 1460.3860, 1458.4231, 1379.9286,
        1314.6708, 1571.0679, 1155.0212, 1323.3240, 1809.8694, 1398.1024,
        1477.9768, 1329.0673, 1332.0605, 1482.9653, 1782.4083, 1354.9486,
        1602.6908, 1321.1753, 1465.6434, 1457.5847, 1456.7163, 1452.0157,
        1496.3391, 1603.9893, 1528.2994, 1583.1740, 1650.3762, 1353.5966,
        1469.7750, 1474.9564, 1207.9690, 1473.4552, 1340.8408, 1689.6757,
        1412.9188, 1671.9807, 1694.3062, 1391.5739, 1172.2621, 1312.5422,
        1175.3457, 1480.5432, 1662.0822, 1279.0856, 1444.0544, 1502.1487,
        1489.1682, 1388.2008, 1448.4607, 1728.0291, 1462.7335, 1474.1224,
        1289.8762, 1267.1570, 1618.0808, 1291.9780, 1302.5120, 1496.9484,
        1364.7423, 1295.1298, 1635.4431, 1360.4821, 1414.5723, 1465.0808,
        1732.5826, 1372.3595, 1488.8735, 1710.1642, 1435.6746, 1288.4202,
        1431.4868, 1434.9896, 1412.6484, 1475.8079, 1413.2098, 1452.7654,
        1118.1162, 1334.3298, 1474.7107, 1474.7771, 1353.8990, 1637.2968,
        1205.4818, 1380.3304, 1416.2656, 1473.3859, 1432.2798, 1514.9768,
        1603.7604, 1449.1606, 1267.3113, 1058.0029, 1602.2173, 1354.1283,
        1335.1924, 1363.0819, 1727.6500, 1432.7880, 1729.3822, 1353.2402,
        1388.3859, 1392.2264, 1190.4766, 1385.1104, 1473.9408, 1387.1539,
        1462.0317, 1425.7405, 1476.4547, 1117.4233, 1367.5260, 1388.0441,
        1534.0574, 1448.7485, 1658.1727, 1450.4542, 1182.9504, 1247.2194,
        1211.2783, 1193.8452, 1251.4033, 1360.0731, 1303.4846, 1505.3881,
        1576.5789, 1188.7169, 1328.2321, 1570.4368, 1453.4104, 1521.0612,
        1429.1484, 1414.4910, 1341.9299, 1549.5148, 1381.6622, 1291.8035,
        1299.8491, 1730.7936, 1346.4268, 1569.0802, 1447.8688, 1492.9945,
        1447.5673, 1261.8854, 1321.6382, 1512.0215, 1461.4133, 1210.2247,
        1277.5851, 1604.1896, 1379.8726, 1339.1511, 1421.4056, 1540.6954,
        1312.9625, 1386.8782, 1362.9036, 1412.0332, 1630.7908, 1312.7307,
        1322.7952, 1631.3459, 1506.1808, 1342.6509, 1547.8470, 1750.8109,
        1616.1467, 1372.8551, 1405.9910, 1533.7333, 1497.7281, 1380.5192,
        1473.0094, 1745.1127, 1436.1445, 1621.4052, 1354.4476, 1191.2366,
        1381.9871, 1576.6779, 1564.5802, 1704.6644, 1461.7043, 1292.3822,
        1270.9702, 1193.5906, 1237.2542, 1504.3031, 1245.5900, 1172.1837,
        1224.7327, 1254.8782, 1406.2371, 1508.2795, 1446.6021, 1328.3514,
        1451.6956, 1734.4484, 1348.7545, 1244.3666, 1232.7999, 1365.9309,
        1677.5461, 1430.6676, 1455.3160, 1481.9714, 1390.0204, 1251.9935,
        1595.9073, 1444.3776, 1390.7874, 1427.2938, 1263.7584, 1195.9366,
        1460.7089, 1522.4309, 1415.7770, 1412.5574, 1632.7827, 1396.9637,
        1379.9490, 1540.6053, 1353.1169, 1576.9957, 1345.3796, 1639.5526,
        1594.4764, 1251.7893, 1343.7151, 1453.9646, 1369.5621, 1377.3799,
        1560.7723, 1437.1089, 1399.5895, 1331.9955, 1473.1340, 1257.3085,
        1320.7920, 1547.1608, 1239.5852, 1210.6179, 1347.0005, 1293.0249,
        1433.6462, 1413.5059, 1196.6791, 1666.4188, 1460.6754, 1587.6465,
        1353.6982, 1380.0458, 1362.9452, 1300.0824, 1651.5056, 1530.1375,
        1275.1173, 1367.7295, 1491.1233, 1353.3479, 1248.9001, 1344.5032,
        1273.1304, 1546.5093, 1454.3269, 1444.7457, 1089.7311, 1421.6788,
        1238.5256, 1454.2821, 1444.9026, 1285.1498, 1377.9293, 1329.8584,
        1169.5251, 1529.8242, 1301.8245, 1680.8538, 1740.6638, 1463.9053,
        1366.4395, 1339.1547])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([13882.4141, 14974.7607, 13651.6738, 15236.1875, 12216.3184, 14084.9248,
        15447.4717, 13290.2334, 14682.5850, 16006.0488, 13329.4717, 15282.2734,
        15730.1035, 13212.7559, 14305.0664, 14986.7227, 13725.8916, 14207.4727,
        17392.5508, 13654.4727, 14227.2100, 14133.2559, 14424.7568, 13699.2393,
        13645.1777, 12720.6621, 13641.1758, 13189.2441, 15169.1514, 13325.1816,
        14411.4912, 14518.9814, 14033.2539, 14802.4814, 13928.9922, 13434.5186,
        11960.9062, 16281.1621, 14481.3857, 13573.4775, 13159.5420, 16374.2139,
        15225.5381, 12450.3848, 13551.3486, 14226.3271, 13147.3184, 18006.9688,
        12660.6025, 13456.2910, 16078.4512, 15081.8271, 13112.2129, 11868.8320,
        13254.0283, 14201.8604, 16352.2324, 15015.8750, 15099.8604, 15486.9688,
        13304.4336, 15149.6943, 13581.8291, 14028.3721, 13708.9219, 15321.7041,
        14024.8867, 16861.7285, 12403.0000, 12229.5820, 13532.6855, 13672.1055,
        13245.5977, 16649.6816, 13136.1260, 17320.0664, 15378.2676, 15676.6914,
        14671.1621, 13165.2754, 14748.8887, 14481.9785, 15762.2578, 12519.8096,
        11713.1221, 13681.0791, 14061.2305, 15150.4473, 14471.9951, 14847.1914,
        13965.7617, 15544.3066, 14928.7764, 13100.2412, 14694.4238, 13472.6436,
        13210.0957, 12705.2949, 18470.8848, 15946.5596, 14257.7764, 15115.4570,
        13553.2061, 13737.1123, 16007.3730, 13028.5439, 13344.7852, 14547.4648,
        15362.1484, 14692.7432, 12972.3447, 15428.4375, 12832.4023, 12800.3750,
        13510.2891, 13070.4580, 13448.9365, 15498.7725, 12786.1963, 13950.6348,
        13942.1104, 15857.0205, 13887.7197, 13480.5625, 14061.0039, 14166.8037,
        13110.7793, 14022.4346, 12880.4990, 14277.5264, 14414.1074, 12795.3350,
        13429.8721, 13882.3057, 14956.0449, 15416.6104, 17023.8359, 12974.1084,
        15299.0547, 14124.7520, 13003.3467, 15040.2861, 15193.2627, 14117.7246,
        12523.3945, 12187.5645, 15198.5957, 17914.4961, 14908.4434, 14578.4902,
        14108.9453, 13675.1455, 14006.1445, 13741.5156, 15394.3711, 13736.5107,
        13672.5010, 14452.5557, 14629.6240, 14355.2852, 17076.3438, 14489.3047,
        14234.8438, 13405.6348, 15626.9258, 12898.1074, 13493.0527, 12613.1973,
        16271.5156, 18240.8652, 14356.3350, 14713.6416, 14868.5879, 16972.4551,
        12997.3994, 14626.7031, 11713.8789, 13017.1836, 15724.4160, 13219.8047,
        13525.1074, 14807.2275, 15513.0156, 14350.8867, 15315.4150, 14572.5439,
        13764.5322, 15483.0059, 13554.4375, 14114.0771, 15073.1084, 12393.8281,
        15334.1377, 14522.2266, 13355.4199, 15657.2559, 14950.7617, 14419.9512,
        15017.3369, 14179.3955, 15548.6172, 15155.9756, 14345.4541, 14278.6533,
        13310.7090, 14558.0791, 13402.7949, 14191.1807, 14271.8164, 14542.3750,
        13803.7490, 14365.7979, 15212.6133, 15564.3584, 15270.8701, 16179.4639,
        13190.7432, 14922.9111, 15162.0400, 12375.0820, 14219.9844, 14826.0195,
        13573.5186, 16104.0381, 15732.9189, 11659.8584, 13451.3203, 16267.8184,
        14816.1904, 14179.8389, 13549.1445, 13038.7490, 14216.4414, 13957.2480,
        14628.5469, 17939.2676, 14860.8115, 13090.1729, 14691.8604, 14298.5264,
        16265.2959, 15661.0479, 13911.7881, 17760.8477, 14054.7461, 13992.2861,
        13352.1777, 13565.2344, 15571.5127, 14112.7461, 13386.3916, 15402.6064,
        15567.0605, 13995.2178, 13694.9414, 13746.2627, 14789.1797, 14853.7910,
        13585.2646, 13376.2314, 13796.5117, 14950.4707, 15394.7266, 14949.6445,
        14657.7100, 14480.0312, 13320.5967, 13261.1436, 16149.3096, 15143.8291,
        17584.7461, 15337.3037, 16328.6826, 15362.8672, 15856.5742, 14851.2354,
        12007.8555, 14920.5566, 16787.3203, 15593.5420, 13949.9990, 14147.8828,
        16987.4082, 13377.6387, 16401.7422, 14954.3594, 16538.6094, 13074.6328,
        14271.3047, 11904.7588, 14580.7295, 13584.9043, 13476.8818, 14702.2617,
        16218.0264, 15563.2920, 15585.1768, 13912.4414, 13092.6504, 13069.9912,
        11543.4385, 15852.1260, 15463.2725, 12314.5693, 15669.6299, 13514.8828,
        14917.3867, 12262.6318, 14862.0420, 12967.7490, 15574.8057, 13366.5420,
        14059.6572, 13871.0469, 14015.4336, 13031.3857, 15569.8672, 15353.6943,
        13701.6445, 14043.0215, 15451.4424, 13499.3721, 13880.5742, 13394.2715,
        14051.9463, 13148.9619, 15497.9971, 14767.4082, 16937.6035, 16757.4395,
        13536.1738, 14248.7070, 12920.3018, 14151.1348, 14211.4619, 15092.9561,
        14530.6953, 15621.1201, 12836.4971, 13905.8701, 13732.8535, 14727.8350,
        13618.3213, 15727.5547, 18864.6602, 13637.5547, 12431.9541, 15905.3330,
        13099.5674, 14675.7275, 15259.4902, 12808.6758, 13153.5059, 12439.7412,
        13489.6357, 14928.1299, 14269.3115, 14840.6680, 13268.7900, 14964.2266,
        13911.5215, 13697.1934, 16232.3164, 13810.4756, 11774.0928, 13216.2354,
        13736.6797, 12009.7354, 13316.2178, 10975.0586, 12996.8525, 14966.6309,
        15573.2676, 13931.8896, 15290.8809, 15306.9971, 12426.0352, 16098.6035,
        12214.9268, 15426.1787, 13008.2656, 15603.1963, 14869.8184, 12522.2920,
        14713.7275, 15014.2275, 14375.1992, 15702.8760, 14548.3564, 12784.9971,
        13385.5420, 14320.9717, 15467.5352, 13925.1104, 13856.7061, 13111.5947,
        15383.1465, 12181.6631, 15825.1553, 12774.9004, 13443.8125, 12761.5566,
        14159.1221, 13945.5605, 16339.6875, 12949.6289, 12751.1904, 12567.2715,
        15383.3789, 11566.8760, 14021.6221, 12104.6406, 13468.3887, 15630.9170,
        15133.1299, 13593.3730, 13674.3271, 13691.6963, 14910.1182, 15576.8555,
        12488.0332, 14117.0000, 15052.5234, 13767.9629, 12220.3750, 14995.3369,
        15108.4043, 12938.9766, 14706.7051, 14308.7773, 14077.3633, 14476.4277,
        15128.9756, 12866.9297, 13142.9268, 15024.6387, 12367.0508, 13744.7295,
        14902.2969, 15489.5205, 13094.9863, 14439.0498, 14880.3252, 13684.4326,
        13107.5391, 16544.0977, 13348.4658, 14509.2305, 12955.0791, 12073.3154,
        15892.6045, 14755.7812, 14111.7754, 15053.8057, 14636.9746, 15112.6982,
        13099.2490, 12970.5674, 13008.5908, 13886.8213, 13192.0391, 13540.6680,
        14611.1113, 12138.0117, 17804.5078, 13978.1572, 12734.5664, 13997.6260,
        14359.9639, 15159.4697, 14326.1211, 14891.6670, 12999.1230, 13818.5039,
        14930.3965, 13501.3555, 14548.5576, 14016.2998, 13447.2061, 13733.7695,
        11201.7168, 16458.0449, 14745.2646, 12643.7578, 16886.0938, 13292.7861,
        14079.6748, 16618.4922, 14912.0273, 15976.3682, 13284.2178, 14651.3398,
        17141.3086, 14748.0176, 14330.4727, 14861.8086, 14496.9180, 15158.3877,
        12883.7627, 14864.6240, 12290.5752, 14538.1338, 14540.5713, 13187.1592,
        12400.6133, 14678.3525, 16456.0332, 13059.5771, 14734.3350, 15543.8535,
        14385.5010, 13801.6875])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1201.0542,  176.2122,  558.3518,  ...,   62.1689,  471.1620,
         100.2293])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1058.4690,  768.0189,  109.1620,  ...,  265.9471, 1420.8096,
         537.2974])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 776.6277,  825.9955,  832.3126,  780.9183,  730.3774,  649.1257,
         691.6958,  792.4985,  761.8110,  753.8660,  836.9683,  734.5380,
         879.6599,  828.5114,  696.3331,  755.7083,  855.7607,  986.7932,
         776.6757,  666.9193,  726.4350,  744.6791,  770.3508,  738.9627,
         706.0098,  969.9765,  726.0608,  847.0200,  793.1948,  736.2348,
         728.1152,  686.3466,  940.0035,  882.3186,  714.0312,  752.5603,
         718.4685,  691.6483,  715.7522,  747.9448,  868.3696,  880.1681,
         728.0473,  755.6250,  640.4566,  905.8118,  754.9414,  828.9085,
         663.7220,  848.2405,  856.0197,  941.3713,  630.1328,  606.1887,
         779.2191,  659.5339,  683.9600,  715.6578,  879.6461,  712.6367,
         734.9648,  874.3381,  809.2783,  651.7063,  680.8600,  831.8503,
         729.1581,  854.1572,  836.5364,  631.6565,  699.5094,  719.8154,
         601.2701,  751.9753,  670.4847,  797.9600,  795.0824,  695.0583,
         806.3630,  927.5247,  919.7966,  820.4677,  755.2704,  734.2177,
         934.8448,  753.7555,  673.6629,  638.9374,  663.9147,  694.4557,
         815.3882,  722.3594,  832.7798,  691.6455,  871.1107,  843.8070,
         591.9038,  884.7392,  650.0824,  706.1172,  731.1603,  741.5822,
         835.7242,  891.6947,  840.7466,  640.4680,  829.5590,  781.4603,
         795.3190,  748.7960,  837.9194,  674.8453,  644.8860,  732.6155,
         828.9502,  751.4106,  722.5213,  767.1644,  877.2795,  819.0116,
         753.1464,  781.9994,  800.9044,  688.8716,  820.0111,  808.0139,
         792.8758,  746.4044,  799.6057,  938.4212,  741.2385,  905.5266,
         687.8922,  635.8490,  871.8827,  906.6976,  939.8319,  714.8607,
         774.5356,  660.3635,  762.4740,  644.3636,  842.0993,  808.7406,
         813.6329,  887.8618,  857.3969,  832.8019,  688.2109,  718.0551,
         768.6345,  934.9122, 1074.5219,  652.1376,  755.8346,  792.0986,
         612.5005,  774.8582,  796.7845,  899.6092,  730.6299,  791.8826,
         770.7352,  563.8743,  648.5532,  888.4684,  606.6561,  916.7774,
         716.4338,  585.7941,  769.9579,  810.1459,  774.5177,  818.7728,
         730.1257,  860.6787,  694.2603,  686.6030,  731.6525,  645.6877,
         778.0308,  690.2112,  755.1597,  770.7191,  868.7078,  698.3929,
         738.1288,  619.3182,  713.2900,  821.9445,  731.8275,  869.8425,
         966.7424,  780.6877,  755.5593,  718.0388,  642.7861,  761.6143,
         841.0035,  676.9929,  704.0860,  732.0560,  683.9420,  932.0916,
         652.9766,  815.2158,  720.1923,  728.7595,  874.3121,  728.3362,
         724.3381,  659.1433,  737.3065,  753.4188,  867.3800,  857.6932,
         719.6941,  913.0864,  761.6136,  763.1125,  864.1846,  841.5582,
         883.3030,  850.9577,  775.4279,  768.3214,  753.1484,  724.8083,
         762.0945,  727.6050,  743.4451,  885.7673,  789.4247,  708.6714,
         678.2994,  742.4714, 1129.2240,  692.2672,  804.4166,  717.5403,
         985.2806,  719.6984,  871.1371,  778.4000,  913.6431,  702.4207,
         768.1711,  754.5835,  709.0771,  939.9526,  750.3346,  700.4667,
         789.5045,  845.1754,  715.8660,  678.1696,  737.5585,  906.4433,
         824.2859,  841.0611,  803.1472,  627.0630,  739.8522,  743.5573,
         666.7349,  894.6035,  941.0276,  807.1178,  942.0527,  796.4857,
         963.1727,  848.7605,  754.5605,  695.0327,  628.9105,  825.4281,
         650.5355,  557.6553,  772.7193,  735.9708,  676.7297,  732.0977,
         835.7533,  651.2483,  554.4478,  783.6938,  901.7573,  886.4688,
         857.4597,  793.3918,  800.0158,  750.9963,  816.6257,  901.8637,
         644.5743,  760.3006,  757.7130,  867.7196,  721.5165,  668.6281,
         784.8287,  810.0920,  766.4694,  857.8051,  777.5206,  733.5407,
         720.3657, 1130.7472,  681.4962,  533.9062,  750.4683,  743.0759,
         792.9790,  815.5142,  768.1169,  577.3247,  773.7372,  878.0344,
         612.3705,  803.4312,  608.3451,  802.2216,  809.8618,  768.9590,
         781.0994,  865.5108,  632.4362,  766.6136,  782.4318,  853.1426,
         731.3766,  721.2182,  757.2367,  678.8746,  761.9444,  788.9266,
         705.4821,  907.4300,  689.5598,  696.3254,  977.6572,  744.0076,
         782.0170,  726.5544,  735.6000,  792.4708,  745.1165,  739.5262,
         719.5126,  745.4404,  795.8058,  675.1498,  616.9734,  873.4467,
         715.0229,  861.4647,  799.9466,  835.6002,  713.2357,  680.0800,
         862.4656,  802.3759,  629.4272,  689.0756,  839.9458,  922.4893,
         751.0078,  826.2352,  681.8511,  704.6347,  797.9070,  662.2396,
         729.0967,  668.9109,  721.6353,  694.9745,  862.6363,  763.6433,
         816.7264,  801.5405,  737.2413,  688.8470,  769.2587,  740.6345,
         642.7491,  839.5367,  746.8792,  829.3721,  826.9229,  694.6302,
         862.5615,  612.3441,  692.9663,  889.4520,  894.7447,  697.6707,
         770.0543,  623.4543,  859.8555,  846.8159,  848.0147,  838.5486,
         767.8077,  659.7734,  924.7681,  740.2605,  723.6970,  623.4234,
         812.0449,  797.6057,  588.9597,  738.7124,  774.1774,  670.9351,
         676.5388,  705.9625,  828.5374,  634.2699,  601.1275,  681.9634,
         816.5267,  687.9742,  799.6412,  856.9113,  763.6188,  726.5341,
         685.5268,  794.3166,  865.0327,  830.2166,  866.4924,  815.7249,
         731.4918,  912.4383,  805.8953,  797.1364,  871.6735,  927.3824,
         702.9838,  611.5598,  675.4728,  724.9526,  610.5457,  655.7023,
        1000.0486,  755.2230,  796.0841,  744.8057,  923.1786,  750.2293,
         880.6008,  604.2861,  917.1418,  683.8113,  859.1424,  887.6570,
         817.9506,  918.0551,  617.1069,  666.4142,  772.3520,  778.5679,
         678.7750,  764.1753,  659.4597,  747.6605,  750.9547,  757.9764,
         890.9276,  708.4608,  875.2557,  731.5143,  749.9324,  960.8629,
         842.0545,  780.8391,  955.9065,  773.0553,  850.3478,  679.4411,
         617.1843,  792.2768,  809.6699,  870.9985,  729.1776,  618.2094,
         768.1909,  804.7552,  854.2717,  835.0427,  782.0125,  761.8908,
         754.8821,  759.4012,  863.1081,  691.6410,  783.3890,  714.3857,
         778.0280,  721.3238,  799.6406,  780.1077,  910.8091,  912.5188,
         643.0995,  592.8312,  710.4607,  785.6953,  693.6209,  781.0060,
        1109.0426,  797.3663])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5740.8481, 6455.3262, 6465.4282, 6148.5840, 5167.3301, 5905.9102,
        6712.1978, 6278.7134, 5310.7798, 6107.6934, 5519.9517, 6787.2827,
        5858.0557, 6347.7295, 6636.7988, 5529.7661, 5254.8042, 6158.6362,
        5752.0859, 5598.4810, 6016.5845, 6328.3818, 7705.0635, 6769.0522,
        6806.7207, 4769.4150, 6157.7632, 5933.7646, 6721.9116, 5655.8291,
        6249.0674, 5610.3472, 5412.0674, 6587.1216, 6837.7334, 5331.1616,
        5122.5737, 7024.5244, 6538.4692, 6578.8086, 6971.7305, 5656.6431,
        7930.2192, 5352.4072, 5748.5630, 5676.9683, 6948.9297, 4786.0625,
        6056.2202, 7130.0059, 5449.7739, 6841.9375, 6116.3677, 5856.8086,
        5671.7954, 5942.0063, 6054.0889, 5336.1680, 8349.1064, 6228.9976,
        5653.7036, 5522.4360, 5422.7246, 7019.3687, 6090.3008, 5835.5576,
        4719.7998, 7163.0825, 5269.3594, 7639.4663, 8124.3496, 6509.1670,
        6655.3242, 7012.0552, 5300.5259, 5203.6084, 6095.3242, 7063.7227,
        8306.8604, 5822.8296, 6315.2471, 7061.3447, 5359.2920, 7870.5562,
        5421.6655, 6605.2134, 5958.6802, 7905.1382, 7295.0034, 8519.3076,
        6213.7935, 5702.5083, 5856.6846, 6909.1006, 5944.7832, 6543.9316,
        6112.6616, 4963.4595, 5303.0859, 7359.2188, 5216.1147, 5906.5098,
        6190.4385, 6656.3906, 5760.0562, 6392.7930, 5639.7969, 5978.5923,
        8278.9629, 6514.9404, 7099.0386, 7353.5742, 6172.0498, 5713.1855,
        6142.6538, 6416.0874, 6070.7046, 5480.0591, 6758.4141, 6730.1494,
        6524.9629, 6293.6406, 6613.9521, 6150.5811, 6100.6260, 5748.9102,
        6580.1841, 7214.4565, 6185.2461, 6404.7007, 6879.0425, 5642.6382,
        5706.1396, 5663.5166, 6093.7778, 4924.9360, 6172.9053, 6889.0317,
        5781.9609, 6427.3052, 5967.8696, 5491.0996, 6153.5996, 6788.1885,
        5814.0635, 6004.1274, 5269.1221, 4785.7656, 5799.4473, 7102.7773,
        5878.8066, 6637.9502, 4946.3389, 5029.3125, 6118.2754, 6918.8345,
        7823.7612, 6557.4971, 7306.5938, 5903.0361, 5490.4971, 6342.6240,
        6449.5479, 6339.1875, 5812.7207, 5651.6768, 6985.4961, 5745.1724,
        6931.4839, 5243.9600, 6285.5361, 5577.8677, 6085.2373, 6825.5649,
        6656.0508, 6133.2319, 6172.6431, 6735.9717, 6706.5151, 6424.2378,
        6433.7964, 7211.4736, 6560.8022, 5774.2119, 5167.5923, 5346.2231,
        7285.2998, 6306.4751, 4846.6499, 7640.0913, 7200.6479, 7976.0181,
        5301.5713, 6976.2534, 6286.0332, 6730.3301, 5742.2847, 6238.4219,
        7197.5381, 5437.2476, 7427.8755, 6533.7227, 6077.7085, 5988.8228,
        6105.9629, 4947.0967, 6236.0498, 5301.9546, 6318.2432, 5890.9165,
        6012.2002, 6312.3979, 7253.1143, 7652.4702, 5662.0923, 5890.9043,
        5730.1782, 4825.0835, 7590.2046, 5979.4629, 6597.2300, 5571.6626,
        6495.1318, 6662.5352, 7046.6001, 5606.1499, 6429.2471, 6107.5396,
        5036.9751, 5096.7788, 5288.3521, 5567.0034, 5377.0034, 5987.7822,
        6277.7422, 5458.9277, 7141.7354, 5809.8442, 6281.7988, 6487.4424,
        5303.3027, 6576.7456, 7127.1172, 5361.8022, 6062.7661, 6100.0737,
        6296.5698, 5957.8965, 5896.9233, 6174.5781, 6672.0586, 5348.2383,
        5962.8589, 6980.2808, 7258.7622, 5682.0361, 5422.1821, 6409.2363,
        6868.4897, 4821.5679, 6398.2705, 5563.2705, 6764.6768, 6251.3696,
        6317.4863, 5683.6021, 6683.2139, 5367.9961, 6353.9863, 5863.5054,
        5975.6270, 6122.5063, 5338.6025, 6763.5103, 5988.9927, 6921.6821,
        5690.7510, 5938.3760, 5082.9609, 5289.9287, 5961.9829, 6037.8125,
        7483.0503, 5374.1646, 7055.3330, 6539.4272, 4819.8135, 5009.6025,
        7161.5674, 6391.9658, 5274.7354, 5865.8613, 6304.7476, 8265.4893,
        6456.9844, 5157.7349, 6775.8970, 6646.2642, 7138.2358, 5701.3291,
        8569.7295, 6519.9214, 5506.3486, 6451.6973, 6355.5522, 6228.2319,
        6467.7651, 5912.9980, 7407.5161, 6801.7305, 7841.3940, 6162.4849,
        7577.1201, 6111.2832, 5531.0874, 5203.2031, 5318.1606, 6495.6021,
        6608.1626, 6252.2349, 7421.3960, 5824.9561, 6845.7598, 6330.5010,
        5108.1079, 6115.6733, 6697.5864, 5268.9756, 6774.2793, 6026.7485,
        5842.9453, 5841.4199, 5952.8501, 6551.2607, 7022.7002, 6332.7041,
        6626.4937, 6379.3813, 7113.3169, 6484.7725, 7881.2480, 6729.4277,
        6962.6064, 5978.9170, 5931.5884, 7154.0591, 5936.7627, 6810.0889,
        7856.6025, 6321.1968, 7123.0923, 5945.6499, 5841.3540, 5644.1289,
        6177.4170, 6856.9790, 6337.0420, 6170.3945, 6518.4995, 5262.7061,
        7142.8716, 6586.6777, 5178.1997, 6084.3198, 8359.6406, 5970.3496,
        4830.8530, 6076.8853, 5275.5522, 7445.0889, 5545.2256, 4940.5889,
        5770.8521, 5936.8110, 6323.0898, 7071.8232, 6316.9131, 5624.7896,
        6093.8633, 6451.2520, 5091.9351, 6579.6133, 5010.3813, 5019.4590,
        5626.9238, 6544.9355, 5659.7456, 5116.1323, 6199.4980, 5565.8027,
        5843.5444, 6298.0312, 7019.2715, 6447.8535, 6521.0991, 6605.7773,
        5441.2900, 6716.1069, 5882.1196, 7086.8359, 7297.5464, 7015.1147,
        5188.6265, 7874.4585, 5393.3159, 6609.0513, 6286.0278, 7312.0156,
        5810.7988, 7486.5215, 6062.7178, 8525.7656, 6634.7227, 6875.3882,
        5537.2632, 6012.0488, 6246.5542, 6023.8779, 6953.5728, 4555.6479,
        5179.4448, 5208.6987, 5299.1641, 5620.3115, 7619.0283, 6385.4536,
        5388.7212, 6185.3066, 6303.1553, 6528.0718, 6216.1235, 6527.9014,
        7089.2192, 5835.1299, 6614.2729, 4975.3027, 5353.2837, 6205.1396,
        7612.4565, 6414.2520, 5301.0757, 6872.7305, 5502.2139, 5070.2275,
        6054.2754, 5720.6763, 5527.6377, 6398.2036, 6065.3540, 6077.7310,
        6676.2148, 5968.7739, 6128.8340, 5943.2881, 5581.5220, 7096.2231,
        7830.9395, 6190.7212, 5231.5767, 5888.7944, 7698.0405, 5588.7495,
        6956.4619, 6355.0928, 6091.0786, 6417.2031, 5698.4453, 5669.6899,
        5990.1309, 6110.1616, 5541.1416, 5177.3335, 6750.2622, 6543.3140,
        5512.0146, 6724.6655, 5766.9502, 6329.3101, 5781.4800, 5819.4902,
        7763.0137, 6403.2197, 5776.1616, 7527.0659, 6384.7588, 6707.2686,
        5766.6343, 5607.4355, 6680.1196, 7322.9038, 5278.4336, 5521.3428,
        6240.7798, 6336.7578, 6729.8564, 6528.1328, 6283.5752, 6035.6870,
        6608.8574, 6088.2471, 7029.0981, 6333.8999, 7149.3257, 5073.4668,
        6278.8174, 6367.0005, 4441.7627, 6204.3838, 5698.9429, 6164.4873,
        5964.8203, 7186.3984])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([109.0389, 159.4646, 147.3676,  ...,  71.8118, 551.4339, 232.9208])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([470.2477, 641.6368, 579.5131, 568.2422, 547.8565, 519.2325, 479.9659,
        685.0443, 519.6346, 550.3881, 621.8926, 491.3281, 468.3921, 746.4861,
        530.9678, 575.0374, 668.7186, 540.7168, 552.9404, 529.5953, 632.5514,
        760.9730, 654.7049, 552.3589, 671.2916, 608.7229, 467.7624, 627.3784,
        442.4521, 555.8320, 482.8125, 519.1278, 452.4753, 472.3592, 563.4373,
        558.5077, 538.9420, 547.0225, 721.3325, 498.2408, 565.4637, 621.6111,
        532.3961, 645.7729, 542.5579, 554.5133, 483.3927, 500.5154, 703.9332,
        443.2089, 582.2706, 519.1707, 549.2836, 530.2459, 600.1313, 551.7120,
        497.5178, 651.2515, 494.5598, 496.0673, 681.6111, 643.2472, 594.6715,
        572.3347, 488.5010, 419.6917, 463.7231, 610.0581, 569.4933, 637.0517,
        535.7254, 456.6694, 629.8389, 476.9812, 714.6871, 674.2436, 669.9018,
        563.7139, 504.5391, 485.2003, 503.4735, 525.7940, 463.2115, 468.3589,
        574.5237, 535.1201, 581.6798, 614.3207, 604.7811, 594.4283, 596.4948,
        565.3461, 526.0072, 626.6971, 634.8718, 472.2509, 606.7767, 579.8256,
        487.5038, 612.5862, 559.5010, 735.5324, 558.6074, 497.0340, 652.4756,
        549.8707, 598.4390, 485.6230, 585.1354, 502.8169, 550.4133, 597.7310,
        544.5042, 701.1876, 598.8081, 543.8591, 662.5458, 529.6845, 567.9774,
        491.4216, 609.0676, 691.8635, 636.3335, 500.0847, 595.2775, 701.9822,
        498.0949, 612.5444, 498.2510, 557.9539, 645.6840, 686.4606, 568.3355,
        525.8890, 481.4393, 648.9962, 571.3868, 630.2330, 596.6746, 702.4846,
        476.3295, 530.9798, 708.3864, 519.7369, 578.2359, 652.3007, 695.1506,
        422.9432, 528.6444, 505.7653, 565.7061, 572.1659, 568.6961, 499.5830,
        572.5806, 678.7168, 614.6127, 536.5870, 428.5032, 514.0027, 655.0212,
        617.0554, 476.4321, 513.7422, 640.6880, 516.7947, 534.4849, 523.8897,
        581.7627, 484.1541, 536.5378, 549.1649, 636.3414, 503.7180, 555.4878,
        507.6898, 506.9038, 523.6049, 571.5330, 620.7136, 583.5073, 594.7491,
        623.3748, 627.8347, 621.5011, 612.7332, 608.8198, 632.0014, 599.2964,
        509.8911, 491.1533, 562.9301, 556.5113, 649.4042, 452.2209, 462.2438,
        544.5007, 574.8467, 474.3138, 573.5749, 665.1404, 537.8054, 657.8049,
        492.6634, 502.4774, 619.9764, 404.7207, 494.2815, 554.9095, 503.3590,
        533.5748, 618.2647, 496.9999, 680.8544, 673.0237, 628.4004, 581.9153,
        686.0078, 503.7628, 509.6570, 637.7340, 524.4776, 719.9705, 511.7310,
        550.6404, 691.3826, 468.4301, 511.0478, 563.6080, 514.1555, 532.7155,
        522.7796, 477.5735, 686.5714, 552.4463, 481.2061, 551.5421, 569.3076,
        509.3645, 533.8105, 425.9911, 564.6602, 692.1460, 554.9004, 590.7650,
        608.3663, 505.0529, 412.0606, 472.9020, 674.2641, 674.8553, 414.1733,
        470.2982, 619.0027, 541.0142, 659.9599, 761.2629, 478.8141, 523.2870,
        634.8198, 489.6976, 724.8941, 522.1850, 432.6024, 597.3925, 565.5787,
        598.1408, 698.1619, 673.3958, 566.8491, 546.2558, 524.4331, 539.0587,
        564.3654, 589.2942, 500.8007, 535.0901, 515.1921, 631.6541, 559.8436,
        713.3771, 592.9703, 491.6361, 459.4611, 471.8099, 489.8615, 543.9388,
        606.1661, 487.6882, 542.4368, 654.8118, 706.0417, 512.3369, 580.8768,
        609.6835, 476.8098, 521.6031, 574.7307, 525.4197, 578.6802, 512.6653,
        733.3705, 726.5496, 583.3402, 542.7453, 420.8904, 604.6083, 603.4612,
        584.7307, 623.5291, 525.7924, 475.3974, 487.5170, 475.6002, 643.6295,
        573.9971, 535.7525, 571.5770, 642.6871, 485.4475, 572.0179, 585.1161,
        501.6092, 672.8825, 435.2624, 477.0491, 623.6717, 612.9517, 749.5013,
        505.3436, 556.6402, 558.7584, 586.2955, 601.0706, 472.9691, 565.7926,
        478.6210, 470.0183, 575.3395, 510.3047, 475.8369, 650.6603, 568.7711,
        560.4948, 453.8445, 580.9042, 533.3928, 516.9508, 659.8333, 563.7376,
        595.9543, 552.7582, 617.1950, 489.6177, 494.2093, 593.6730, 459.4553,
        549.5667, 567.6225, 563.3489, 572.1457, 510.7314, 465.1477, 535.6703,
        465.9247, 567.6426, 504.0859, 648.6960, 548.5211, 560.0197, 640.6720,
        558.0493, 541.5741, 563.6216, 538.1435, 550.9355, 641.3447, 544.1254,
        503.0911, 551.5396, 500.3618, 559.9971, 532.9788, 504.5738, 549.1116,
        573.8024, 447.6433, 629.7917, 611.8224, 596.0387, 467.7932, 609.1319,
        544.0569, 523.7393, 525.1296, 554.3791, 631.0515, 657.7748, 473.8507,
        503.4773, 676.4943, 583.0669, 520.3376, 541.5711, 603.4446, 548.8314,
        768.7141, 557.4908, 568.2824, 596.0725, 411.3727, 552.9128, 519.9846,
        580.5509, 435.7609, 518.0803, 521.1372, 507.9639, 653.4616, 627.5694,
        531.8461, 641.3904, 558.7683, 541.8585, 544.8193, 756.5698, 489.0397,
        586.8584, 647.5820, 458.8993, 664.6913, 647.7656, 517.4657, 614.2603,
        544.3490, 588.3249, 824.7972, 515.9449, 530.6113, 566.7516, 545.7506,
        609.7394, 576.3173, 504.5940, 459.1354, 499.6164, 517.1854, 511.7598,
        445.3344, 505.0651, 513.3170, 624.6180, 658.4753, 585.5270, 567.2029,
        572.8482, 654.3751, 421.1624, 572.7807, 488.9946, 699.0685, 480.8158,
        425.8760, 639.7302, 609.1935, 602.9902, 362.5288, 586.8752, 500.3947,
        488.5028, 584.0437, 703.3379, 571.9460, 630.6246, 571.3785, 633.1512,
        549.8704, 548.6688, 396.5102, 460.4843, 468.1359, 604.4046, 585.4978,
        586.4028, 408.0803, 612.9938, 638.7477, 577.0834, 458.4762, 626.0264,
        512.7319, 782.9904, 506.0815, 605.0164, 541.0148, 571.2139, 593.2764,
        629.4058, 554.5458, 571.2394, 554.4157, 745.6034, 490.6963, 694.4289,
        505.5137, 530.5862, 615.4930, 503.9990, 579.0709, 552.4634, 504.8870,
        537.3618])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6372.9414, 5708.9272, 7103.0708, 5955.8325, 6036.8403, 5812.5410,
        5053.7271, 5841.6782, 7155.9219, 6910.9707, 5487.6143, 4674.3872,
        6147.0645, 5537.8105, 5791.9834, 5654.6523, 6243.6909, 6134.5171,
        6194.7461, 5382.7231, 5760.1694, 5752.8345, 6459.9346, 6359.2930,
        5260.4785, 6382.3843, 4635.6895, 5252.7285, 5585.3179, 5318.1587,
        6184.3706, 5191.0098, 6113.3491, 6235.9697, 6024.7729, 6266.2056,
        4704.7153, 5554.4814, 5865.5991, 6591.9443, 4122.2012, 5848.6816,
        6693.3706, 5341.9995, 5748.5068, 7361.9858, 5223.7212, 5828.3755,
        5269.9180, 5589.5298, 4923.2656, 5225.9092, 4588.9316, 5736.7227,
        7534.2197, 6776.9888, 5900.2432, 6295.3027, 5851.6367, 6622.1904,
        4855.3335, 7776.4741, 6332.0850, 5448.2651, 6762.4272, 5022.1841,
        6122.5576, 6935.3999, 5406.8608, 6303.9116, 4821.9565, 5815.7295,
        5196.1890, 4702.7632, 6307.5591, 5441.5425, 5224.0918, 5330.6621,
        5826.8486, 5871.1616, 5043.1875, 6325.9429, 5569.2065, 4650.9771,
        5399.0747, 6420.3647, 5330.5415, 6340.0679, 6093.0088, 5422.4072,
        5078.7998, 6236.9863, 6320.3174, 6236.7085, 5307.4136, 6002.0801,
        5124.9019, 6020.5068, 5074.5449, 4921.9961, 4987.2720, 5587.5767,
        4955.8862, 7050.7427, 5995.9746, 5914.5698, 4700.4263, 7172.5635,
        4980.3633, 6734.4390, 8104.3608, 5289.4321, 5641.4917, 5487.1772,
        8366.4404, 6393.6177, 6244.7783, 5367.4526, 5635.3413, 4578.9912,
        5705.7817, 6307.2974, 5063.3755, 5176.5161, 4358.7109, 6581.5752,
        5875.3091, 5962.5991, 4966.9561, 5780.8374, 5990.5742, 9334.3604,
        5812.5063, 5446.7603, 6547.9805, 6096.9072, 5419.6406, 5381.3706,
        6567.9761, 6228.6074, 4617.3530, 7460.7153, 6226.7495, 5513.7314,
        6370.1426, 6644.3447, 5669.9644, 5731.0552, 6001.1089, 7251.2300,
        4679.9688, 5411.5049, 5584.3799, 4883.0439, 5568.9160, 5906.0010,
        5770.9697, 6448.6855, 4821.7563, 5114.7090, 5813.2480, 6122.0239,
        5073.1021, 5643.8872, 5979.7842, 6050.3975, 5633.7383, 5045.6816,
        5492.4849, 5942.6821, 5854.3149, 5680.5791, 5691.7573, 6513.5034,
        5926.0044, 5962.9272, 4876.5122, 5289.3477, 7074.3345, 6381.5762,
        5654.9604, 6690.5688, 5208.4673, 4992.9995, 6792.7148, 6107.5479,
        5646.6499, 6171.7334, 5997.3643, 5482.7153, 4746.0859, 6362.5762,
        7540.1118, 7223.4214, 6134.7197, 5995.4375, 5870.3213, 5010.5273,
        6986.5640, 6967.0234, 6746.9463, 5703.0020, 6755.3188, 6513.5649,
        7380.4883, 5667.5620, 6033.7788, 5778.0127, 6500.1431, 5811.7793,
        5832.5815, 5837.4209, 6095.4731, 6613.2202, 6039.3638, 5284.4751,
        4996.7783, 5563.9878, 5574.4805, 6219.2686, 5785.1758, 5205.0581,
        5895.8267, 5623.0254, 4730.8213, 6873.2935, 5786.1934, 6449.6670,
        3915.7185, 5876.4399, 7029.4946, 6669.5723, 5231.2114, 5225.4888,
        6700.8999, 5711.8096, 4010.6777, 6814.0869, 6271.5259, 5591.2739,
        8331.0146, 5203.1763, 5227.5132, 5081.8545, 7059.1841, 5390.4434,
        7959.2407, 7243.7295, 6960.4526, 4851.4634, 5565.9116, 5076.9780,
        6902.3130, 6376.0972, 4626.5156, 6633.8833, 5872.0913, 6732.3491,
        3781.7419, 5902.0688, 5940.7812, 6023.3662, 7636.1768, 4711.5947,
        6046.4507, 5793.5498, 5549.6655, 6645.3774, 5549.6582, 5711.0518,
        4622.3374, 5895.0283, 6763.1128, 6129.3115, 5772.5869, 5733.3408,
        7958.1592, 5906.5425, 4557.2290, 4898.7295, 5176.0015, 8500.1719,
        6133.2827, 5623.4639, 6089.4019, 5429.0811, 5772.2891, 5689.7412,
        4256.3057, 5900.3511, 6432.7446, 6379.4951, 5772.9243, 5247.4961,
        5893.1143, 7154.8203, 5219.4189, 5750.6079, 6607.9473, 3818.1433,
        8460.6865, 5123.0010, 6148.0474, 7708.5596, 6586.3569, 5682.9336,
        5587.2632, 7232.4717, 5189.8521, 5790.0537, 6010.7139, 6637.8335,
        5905.7817, 6081.2173, 5364.7573, 6308.1992, 5044.4771, 4967.5449,
        5240.2559, 6593.0610, 6279.4595, 6178.7529, 5446.4121, 4795.3545,
        6222.2451, 6320.1104, 4789.7173, 4981.2178, 6237.4946, 6664.8789,
        6631.1675, 4995.0234, 5919.4312, 5702.6865, 6843.6055, 6742.4092,
        4889.5947, 6342.7529, 4768.8867, 6311.1582, 6668.5376, 5765.0244,
        6077.9595, 6639.8237, 4555.9111, 6302.0244, 6215.6450, 5631.4766,
        6226.2676, 6759.2227, 6460.0581, 4965.0381, 4943.9937, 6530.3389,
        5075.7573, 5474.2871, 6115.9214, 6089.2095, 6136.5352, 6703.1514,
        4766.9941, 4481.8447, 6086.8193, 6072.3496, 5312.4355, 5725.5508,
        5137.2271, 5945.1826, 5158.9902, 4484.1445, 4871.4346, 6576.0166,
        5798.6011, 7046.3154, 5436.9199, 6834.9448, 5344.1040, 5665.0400,
        5866.6470, 5246.4497, 5790.3184, 9302.9922, 6180.1230, 5675.7793,
        5523.5020, 6028.3516, 4857.3555, 6077.4644, 5862.0967, 4970.5557,
        5240.7705, 4532.3882, 6633.0918, 6232.0820, 7531.8579, 6134.6816,
        4762.7769, 6683.9146, 5250.2827, 5154.4121, 4801.7324, 6453.3105,
        5336.1484, 5212.4097, 5520.8936, 5183.0034, 5794.7544, 5959.6318,
        4574.6777, 5554.4614, 5453.6201, 5224.3911, 5086.8281, 6875.4717,
        5493.2402, 6085.5308, 6309.4531, 5168.6567, 6165.9106, 5555.3931,
        5921.6606, 6885.9282, 6167.2876, 5563.5259, 4617.3359, 5669.8467,
        6683.0967, 7074.7764, 6274.9150, 4633.8013, 5709.2363, 6112.1143,
        7530.8203, 4812.1729, 6503.4365, 5770.7891, 4683.5962, 5348.9609,
        6146.5171, 7758.5894, 5851.4917, 5675.8306, 4923.4595, 6978.1382,
        6275.3394, 6132.4360, 5933.7871, 5677.0835, 7312.5879, 6083.7612,
        5440.1025, 5987.8975, 4514.5093, 8180.4868, 6028.8223, 5172.3188,
        6543.5659, 8775.6777, 8184.8770, 5753.6733, 4164.1572, 5166.7769,
        6605.0112, 5029.8672, 7453.1514, 6741.8804, 6107.3008, 5571.0688,
        5569.4937, 5151.4570, 6885.9897, 5671.4136, 6531.2222, 5707.8643,
        6830.0508, 6468.1001, 5808.5405, 5746.8975, 6505.9868, 5188.6201,
        6613.2344, 4386.5352, 5335.0674, 5354.2432, 4951.3247, 5090.0259,
        6801.3013, 6254.4453, 6488.3486, 4863.3188, 4658.6670, 4404.5093,
        7255.6890, 6088.6938, 5349.4048, 5816.9727, 5405.6318, 4897.8306,
        5550.2593, 6986.0400, 5882.1084, 5507.8130, 5841.5195, 6557.7451,
        5383.2412, 5407.4092, 4531.2925, 6122.9121, 6152.1904, 6732.2803,
        4943.8613, 6008.6006])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([  2.8319, 247.0447, 177.6317,  ...,   0.0000, 222.6777,  39.4738])
[DEBUG] Global concept maps computed with 53 layers.
Layer: conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0')}}
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
[DEBUG] Global pruning mask: OrderedDict([('conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.0.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}), ('encoder.4.0.conv3', {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.0.downsample.0', {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.1.conv1', {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0')}}), ('encoder.4.1.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}), ('encoder.4.1.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.2.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.2.conv2', {'Conv2d': {'weight': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.0.conv1', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.0.conv3', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.0.downsample.0', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.1.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.1.conv2', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1.], device='cuda:0')}}), ('encoder.5.1.conv3', {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.2.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.2.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}}), ('encoder.5.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.3.conv1', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.3.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.3.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}), ('encoder.6.0.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0.], device='cuda:0')}}), ('encoder.6.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.0.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.0.downsample.0', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.1.conv1', {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.1.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.1.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.2.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.2.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.], device='cuda:0')}}), ('encoder.6.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}}), ('encoder.6.3.conv1', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.3.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.3.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.4.conv1', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.4.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.4.conv3', {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.5.conv1', {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.5.conv2', {'Conv2d': {'weight': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.5.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.downsample.0', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.1.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}), ('encoder.7.1.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}}), ('encoder.7.1.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.2.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.2.conv2', {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}})])
Full profiling results saved to /home/paul/projects/CV4RS-main/pruning_callgraph.txt
=== Round 2/5 ===
Applying pruning mask for Round 2...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
Training and communication for Round 2...
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
=== Round 3/5 ===
Applying pruning mask for Round 3...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
Training and communication for Round 3...
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
=== Round 4/5 ===
Applying pruning mask for Round 4...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
Training and communication for Round 4...
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
=== Round 5/5 ===
Applying pruning mask for Round 5...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Binding mask to module: conv1, Target: weight
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 10, 7, 7])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[INFO] Removed re-parametrization for weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: conv1, Target: bias
  Module weight shape: torch.Size([64, 10, 7, 7])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Removed re-parametrization for bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: weight
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv1, Target: bias
  Module weight shape: torch.Size([64, 64, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.0.downsample.0, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.1.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: weight
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv1, Target: bias
  Module weight shape: torch.Size([64, 256, 1, 1])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: weight
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64, 64, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv2, Target: bias
  Module weight shape: torch.Size([64, 64, 3, 3])
  Module bias shape: torch.Size([64])
  Mask shape: torch.Size([64])
[INFO] Mask reshaped to match bias dimensions: torch.Size([64])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: weight
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 64, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.4.2.conv3, Target: bias
  Module weight shape: torch.Size([256, 64, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: weight
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv1, Target: bias
  Module weight shape: torch.Size([128, 256, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: weight
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.0.downsample.0, Target: bias
  Module weight shape: torch.Size([512, 256, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.1.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.2.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: weight
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv1, Target: bias
  Module weight shape: torch.Size([128, 512, 1, 1])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: weight
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128, 128, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv2, Target: bias
  Module weight shape: torch.Size([128, 128, 3, 3])
  Module bias shape: torch.Size([128])
  Mask shape: torch.Size([128])
[INFO] Mask reshaped to match bias dimensions: torch.Size([128])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: weight
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 128, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.5.3.conv3, Target: bias
  Module weight shape: torch.Size([512, 128, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: weight
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv1, Target: bias
  Module weight shape: torch.Size([256, 512, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: weight
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.0.downsample.0, Target: bias
  Module weight shape: torch.Size([1024, 512, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.1.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.2.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.3.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.4.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: weight
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv1, Target: bias
  Module weight shape: torch.Size([256, 1024, 1, 1])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: weight
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256, 256, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv2, Target: bias
  Module weight shape: torch.Size([256, 256, 3, 3])
  Module bias shape: torch.Size([256])
  Mask shape: torch.Size([256])
[INFO] Mask reshaped to match bias dimensions: torch.Size([256])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: weight
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.6.5.conv3, Target: bias
  Module weight shape: torch.Size([1024, 256, 1, 1])
  Module bias shape: torch.Size([1024])
  Mask shape: torch.Size([1024])
[INFO] Mask reshaped to match bias dimensions: torch.Size([1024])
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: weight
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv1, Target: bias
  Module weight shape: torch.Size([512, 1024, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: weight
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.0.downsample.0, Target: bias
  Module weight shape: torch.Size([2048, 1024, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Removed re-parametrization for bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.1.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: weight
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv1, Target: bias
  Module weight shape: torch.Size([512, 2048, 1, 1])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: weight
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512, 512, 3, 3])
[INFO] Mask reshaped to match weight dimensions: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv2, Target: bias
  Module weight shape: torch.Size([512, 512, 3, 3])
  Module bias shape: torch.Size([512])
  Mask shape: torch.Size([512])
[INFO] Mask reshaped to match bias dimensions: torch.Size([512])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: weight
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Mask reshaped to match weight dimensions: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[DEBUG] Binding mask to module: encoder.7.2.conv3, Target: bias
  Module weight shape: torch.Size([2048, 512, 1, 1])
  Module bias shape: torch.Size([2048])
  Mask shape: torch.Size([2048])
[INFO] Mask reshaped to match bias dimensions: torch.Size([2048])
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Removed re-parametrization for bias in layer: encoder.7.2.conv3
Training and communication for Round 5...
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
Training completed in 604.66 seconds.
[{'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}]
