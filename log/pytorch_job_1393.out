Lines that potentially need to be canonized 309
Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initializing LRP Pruning...
LRP initialized successfully.
=== Runde 1/5 ===
Training and communication for Round 1...
Epoch 1/1
----------
Batch 0:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 1:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 2:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 3:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 4:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 5:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 6:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 7:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 8:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 9:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 10:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 11:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 12:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 13:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 14:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 15:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 16:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 17:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 18:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 19:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 20:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 21:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 22:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 23:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 24:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 25:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 26:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 27:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 28:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 29:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 30:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 31:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 32:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 33:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 34:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 35:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 36:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 37:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 38:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 39:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 40:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 41:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 42:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 43:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 44:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 45:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 46:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 47:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 48:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 49:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 50:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 51:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 52:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 53:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 54:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 55:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 56:
  Data shape: torch.Size([256, 10, 120, 120])
  Labels shape: torch.Size([256, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Batch 57:
  Data shape: torch.Size([213, 10, 120, 120])
  Labels shape: torch.Size([213, 19])
  Conv1 weight shape: torch.Size([64, 10, 7, 7])
Starting validation after Round 1...
Bug fix for empty classification report.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.4630
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.2682

Führe LRP-Pruning in Runde 1 durch...
Berechne LRP-Pruning-Maske für Land: Finland
Erstelle DataLoader für Land: Finland
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([5])
Unexpected input shape for LRP: torch.Size([5]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 5])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 5
Input shape before relevance computation: torch.Size([5, 10, 1, 5])
Model output shape: torch.Size([5, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([5, 19]) to 5 for one-hot encoding.
Targets shape: torch.Size([5])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([5, 19])
Grad output shape: torch.Size([5, 19])
Relevance computation successful. Relevance shape: torch.Size([5, 10, 1, 5])
Relevanzkarten berechnet: ['conv1', 'encoder.4.0.conv1', 'encoder.4.0.conv2', 'encoder.4.0.conv3', 'encoder.4.0.downsample.0', 'encoder.4.1.conv1', 'encoder.4.1.conv2', 'encoder.4.1.conv3', 'encoder.4.2.conv1', 'encoder.4.2.conv2', 'encoder.4.2.conv3', 'encoder.5.0.conv1', 'encoder.5.0.conv2', 'encoder.5.0.conv3', 'encoder.5.0.downsample.0', 'encoder.5.1.conv1', 'encoder.5.1.conv2', 'encoder.5.1.conv3', 'encoder.5.2.conv1', 'encoder.5.2.conv2', 'encoder.5.2.conv3', 'encoder.5.3.conv1', 'encoder.5.3.conv2', 'encoder.5.3.conv3', 'encoder.6.0.conv1', 'encoder.6.0.conv2', 'encoder.6.0.conv3', 'encoder.6.0.downsample.0', 'encoder.6.1.conv1', 'encoder.6.1.conv2', 'encoder.6.1.conv3', 'encoder.6.2.conv1', 'encoder.6.2.conv2', 'encoder.6.2.conv3', 'encoder.6.3.conv1', 'encoder.6.3.conv2', 'encoder.6.3.conv3', 'encoder.6.4.conv1', 'encoder.6.4.conv2', 'encoder.6.4.conv3', 'encoder.6.5.conv1', 'encoder.6.5.conv2', 'encoder.6.5.conv3', 'encoder.7.0.conv1', 'encoder.7.0.conv2', 'encoder.7.0.conv3', 'encoder.7.0.downsample.0', 'encoder.7.1.conv1', 'encoder.7.1.conv2', 'encoder.7.1.conv3', 'encoder.7.2.conv1', 'encoder.7.2.conv2', 'encoder.7.2.conv3']
Calling generate_global_pruning_mask with pruning_rate: 0.3
Global concept maps: OrderedDict([('conv1', tensor([3.2314e+19, 3.5805e+19, 1.4629e+19, 1.2658e+19, 4.2344e+19, 1.8549e+19,
        1.2640e+19, 1.2985e+19, 1.3692e+19, 5.0046e+18, 2.1519e+19, 2.1277e+19,
        1.7939e+19, 1.2902e+19, 1.2407e+19, 1.2219e+19, 1.0130e+19, 1.9789e+19,
        1.6667e+19, 3.5499e+18, 3.4053e+19, 3.0003e+19, 3.5260e+18, 1.7298e+19,
        2.3862e+19, 2.1117e+19, 1.4362e+19, 3.2309e+19, 3.8834e+18, 6.2625e+18,
        1.1033e+19, 1.3773e+19, 1.9995e+19, 4.2334e+19, 4.8003e+18, 9.7432e+18,
        1.0578e+19, 2.2304e+19, 2.2904e+19, 2.1785e+19, 1.6737e+19, 2.3531e+19,
        1.8101e+19, 1.9817e+19, 9.2796e+18, 4.7686e+18, 1.4602e+19, 1.0193e+19,
        2.0177e+19, 2.3931e+19, 1.8345e+19, 1.4242e+19, 1.7145e+19, 9.6107e+18,
        7.2855e+18, 4.5643e+18, 2.2064e+19, 1.3290e+19, 2.6670e+19, 2.9231e+19,
        3.9362e+18, 1.1573e+19, 3.6896e+18, 1.0198e+19])), ('encoder.4.0.conv1', tensor([7.2569e+18, 6.1900e+18, 7.2848e+18, 4.7911e+18, 6.2889e+18, 6.8001e+18,
        2.8140e+18, 2.1002e+18, 9.4393e+18, 5.7817e+18, 5.7186e+18, 2.7892e+18,
        1.3855e+18, 5.3747e+18, 8.4641e+18, 7.2404e+18, 9.1113e+18, 9.2866e+18,
        6.6538e+18, 9.9027e+18, 1.3395e+19, 2.4616e+18, 4.0083e+18, 6.0455e+18,
        4.0390e+18, 9.3744e+18, 5.3382e+18, 9.9308e+18, 7.1110e+18, 1.3965e+19,
        3.4579e+18, 5.4717e+18, 1.3259e+18, 2.8285e+18, 7.1897e+18, 4.1442e+18,
        5.7034e+18, 1.7507e+18, 4.4656e+18, 1.4288e+19, 4.9288e+18, 2.8243e+18,
        8.6387e+18, 3.6032e+18, 7.1170e+18, 4.6726e+18, 1.3356e+18, 3.0999e+18,
        1.4154e+19, 3.3254e+18, 9.0210e+18, 1.3639e+19, 6.9838e+18, 2.9162e+18,
        1.7240e+18, 5.1722e+18, 1.7671e+18, 1.2288e+19, 3.0227e+18, 9.3829e+18,
        6.3996e+18, 3.9480e+18, 1.6263e+18, 4.7708e+18])), ('encoder.4.0.conv2', tensor([8.0498e+18, 1.5625e+19, 5.5404e+18, 7.4757e+18, 4.2303e+18, 1.3091e+18,
        2.2355e+18, 4.6734e+18, 1.0675e+19, 6.0149e+18, 6.8029e+18, 7.4071e+18,
        6.2562e+18, 2.4444e+18, 5.6345e+18, 6.5620e+18, 7.5129e+18, 3.4932e+18,
        3.6984e+18, 2.4940e+18, 2.3990e+18, 2.6850e+18, 2.7913e+18, 3.3118e+18,
        9.7041e+18, 5.3106e+18, 2.6681e+18, 7.8152e+18, 2.4961e+18, 1.7641e+19,
        3.0076e+18, 1.5056e+19, 1.9835e+18, 9.4995e+18, 2.8697e+18, 7.4949e+18,
        5.9493e+18, 1.9029e+18, 3.8704e+18, 3.1287e+18, 1.1884e+19, 8.1859e+18,
        2.0313e+19, 9.1063e+18, 3.2119e+18, 2.3443e+18, 1.1169e+19, 7.4063e+18,
        7.9933e+18, 1.6759e+18, 9.2811e+18, 9.9128e+18, 1.4426e+19, 2.5692e+18,
        1.7155e+18, 2.3338e+18, 2.2945e+18, 1.5486e+18, 8.1236e+18, 4.7254e+18,
        9.1633e+18, 3.4136e+18, 9.5356e+18, 4.3144e+18])), ('encoder.4.0.conv3', tensor([1.0325e+16, 1.4248e+16, 3.9735e+16, 7.8514e+17, 1.6313e+16, 1.2490e+16,
        7.5830e+16, 1.0608e+17, 1.9965e+17, 1.0612e+17, 8.5782e+15, 2.0693e+16,
        9.7102e+16, 6.7654e+16, 1.3040e+16, 1.7925e+17, 8.9773e+15, 1.5196e+17,
        5.4967e+16, 1.8356e+18, 3.4007e+16, 7.3757e+16, 3.1172e+17, 8.2856e+16,
        3.7761e+17, 6.2476e+15, 8.8391e+16, 6.0978e+16, 1.2215e+17, 2.6284e+17,
        4.0717e+16, 1.9375e+16, 1.0283e+17, 1.0182e+17, 3.8164e+16, 5.0474e+17,
        1.5145e+16, 5.8232e+15, 2.0198e+17, 4.9186e+17, 1.9203e+17, 4.3433e+16,
        1.5719e+17, 4.9175e+16, 6.7234e+15, 9.0540e+16, 1.0882e+16, 2.2424e+16,
        2.3047e+16, 7.4446e+16, 1.0872e+16, 3.1568e+16, 6.3659e+16, 3.2734e+16,
        1.5169e+16, 1.9554e+16, 6.7038e+16, 2.0484e+17, 6.5984e+16, 2.8819e+16,
        1.6587e+16, 1.9889e+16, 8.6035e+16, 1.5263e+16, 1.9246e+16, 2.3227e+16,
        1.3443e+16, 4.2271e+16, 3.3196e+17, 2.3277e+16, 7.0518e+16, 2.7552e+16,
        1.7290e+17, 5.9402e+16, 2.6855e+16, 1.4744e+16, 7.9565e+16, 1.7359e+16,
        3.2020e+16, 2.2116e+16, 4.6015e+16, 3.2248e+16, 1.0779e+17, 1.0516e+16,
        1.2247e+17, 5.8926e+16, 3.6564e+15, 1.5175e+17, 6.9591e+16, 6.9249e+16,
        2.6784e+17, 2.9407e+17, 3.2386e+16, 1.5565e+17, 2.1759e+16, 4.5881e+16,
        2.6591e+16, 1.8980e+16, 5.8948e+16, 1.3101e+17, 3.6067e+17, 4.0252e+16,
        1.9997e+16, 2.2590e+17, 3.0600e+17, 5.4639e+16, 6.5065e+16, 7.1567e+15,
        6.9084e+17, 6.9145e+16, 8.6643e+15, 1.6383e+17, 1.9468e+16, 1.3228e+17,
        1.8877e+17, 2.0145e+17, 6.9600e+15, 2.0075e+16, 2.0125e+17, 2.2067e+16,
        1.8993e+17, 1.1970e+16, 4.6368e+16, 2.1562e+18, 1.3138e+16, 1.1398e+16,
        5.3069e+16, 1.7625e+16, 5.5824e+16, 3.0739e+16, 2.3166e+17, 4.8713e+16,
        1.7991e+17, 2.8149e+17, 1.3900e+17, 4.2760e+16, 6.1413e+16, 1.0996e+18,
        7.8883e+16, 3.1206e+16, 1.4650e+16, 2.9108e+17, 5.3972e+16, 3.9918e+17,
        1.9107e+15, 2.0419e+17, 6.5191e+16, 3.5526e+16, 2.9809e+16, 2.7170e+16,
        1.0602e+17, 6.9530e+16, 2.0898e+16, 5.6156e+17, 3.9181e+16, 1.0337e+16,
        5.2436e+16, 5.1093e+16, 4.0906e+16, 6.9671e+15, 1.6491e+16, 4.2658e+16,
        4.0243e+16, 5.1399e+16, 1.3084e+17, 3.4330e+17, 9.1619e+17, 7.6904e+16,
        8.3558e+16, 5.8074e+16, 2.4384e+17, 2.9310e+16, 1.4730e+16, 1.8104e+16,
        1.7855e+16, 7.2025e+16, 4.2037e+16, 1.0628e+18, 2.3727e+16, 1.6095e+17,
        6.4004e+16, 2.3716e+17, 9.4542e+15, 2.8564e+17, 2.1666e+16, 4.2103e+15,
        1.9846e+17, 1.2167e+17, 8.0965e+16, 2.9159e+16, 6.1590e+16, 1.8009e+16,
        1.0703e+17, 4.4084e+16, 6.0743e+16, 2.9653e+19, 1.5720e+16, 6.1317e+16,
        5.5828e+16, 3.4071e+17, 1.3771e+16, 4.1752e+16, 3.6445e+16, 4.1382e+16,
        7.2200e+16, 1.8347e+16, 1.5461e+17, 1.2148e+18, 6.4671e+17, 1.2524e+16,
        2.9485e+17, 1.4338e+16, 1.2832e+16, 7.4884e+15, 1.1346e+16, 1.0296e+16,
        2.3633e+16, 1.9047e+17, 1.3644e+16, 4.6905e+16, 1.2983e+17, 1.6636e+16,
        2.4488e+17, 1.3833e+17, 6.1948e+16, 1.5076e+16, 1.1340e+17, 1.5676e+16,
        3.5666e+16, 5.6024e+16, 2.2022e+16, 1.2659e+16, 7.6892e+16, 1.9292e+16,
        4.0431e+17, 2.9732e+16, 3.7723e+15, 2.1488e+16, 3.8944e+16, 8.0033e+16,
        5.6548e+17, 1.5976e+16, 2.3395e+17, 7.3629e+16, 2.1736e+17, 4.5542e+17,
        1.3708e+16, 4.4080e+16, 1.1049e+16, 4.5571e+16, 1.5146e+17, 3.9939e+16,
        1.1420e+16, 2.0440e+16, 4.6547e+16, 1.0158e+17])), ('encoder.4.0.downsample.0', tensor([6.8428e+16, 5.9809e+16, 1.0892e+16, 7.4913e+17, 2.3523e+16, 2.8907e+16,
        5.8408e+16, 9.1281e+16, 2.0260e+17, 1.3566e+17, 3.6751e+16, 3.2434e+16,
        1.1943e+17, 6.4803e+16, 3.1399e+16, 1.4793e+17, 4.1966e+16, 1.2616e+17,
        9.7563e+16, 1.8843e+18, 4.0696e+16, 6.5223e+16, 3.1545e+17, 6.1531e+16,
        3.8849e+17, 3.6692e+16, 6.8142e+16, 3.2221e+16, 2.0122e+17, 2.0438e+17,
        1.0582e+17, 8.6052e+16, 7.3853e+16, 4.8036e+16, 4.2117e+16, 4.8758e+17,
        5.0745e+16, 3.5443e+16, 2.1834e+17, 4.9909e+17, 1.8799e+17, 9.4077e+15,
        1.5766e+17, 3.7873e+16, 2.5397e+16, 1.3210e+17, 3.4276e+16, 3.0380e+16,
        3.1778e+15, 7.6013e+16, 2.6991e+16, 4.8981e+16, 2.7192e+16, 1.3343e+16,
        2.1455e+16, 2.6418e+16, 8.5941e+16, 1.6518e+17, 8.3939e+16, 1.2667e+16,
        1.9165e+16, 6.7510e+16, 1.1564e+17, 4.6066e+16, 2.3392e+16, 3.6986e+16,
        8.2217e+16, 7.1471e+16, 3.6984e+17, 2.8373e+16, 5.8829e+16, 3.0949e+16,
        1.4702e+17, 1.0907e+16, 2.4299e+16, 2.8917e+16, 2.8365e+16, 6.8740e+16,
        3.9720e+16, 3.7988e+16, 8.1126e+16, 7.2247e+16, 8.3656e+16, 3.3651e+16,
        9.6205e+16, 5.0295e+16, 5.3485e+16, 1.6394e+17, 7.9536e+16, 1.0809e+17,
        2.4208e+17, 2.9095e+17, 4.8392e+16, 1.7672e+17, 3.1620e+16, 8.0736e+16,
        2.2020e+16, 4.1317e+16, 5.4114e+16, 1.9364e+17, 4.1865e+17, 3.3156e+16,
        2.1423e+16, 2.7531e+17, 3.0398e+17, 1.0649e+16, 1.0976e+17, 5.0533e+16,
        6.5395e+17, 9.3348e+16, 6.9333e+16, 1.4170e+17, 3.7239e+16, 7.5044e+16,
        1.9898e+17, 2.0322e+17, 4.0201e+16, 2.8252e+16, 1.5746e+17, 3.9734e+16,
        1.7315e+17, 2.4850e+16, 9.3724e+15, 2.1755e+18, 2.1268e+16, 3.3375e+16,
        8.1072e+16, 8.2255e+16, 5.7459e+16, 1.0050e+17, 2.8557e+17, 6.7049e+16,
        1.7341e+17, 2.2597e+17, 9.9699e+16, 9.5011e+16, 4.4178e+16, 1.1127e+18,
        1.4497e+17, 7.4849e+16, 3.5625e+16, 2.7529e+17, 3.9441e+16, 4.3961e+17,
        2.0832e+16, 1.9290e+17, 5.6474e+16, 3.6697e+16, 6.1645e+16, 3.2203e+16,
        7.8083e+16, 4.7013e+16, 3.9943e+16, 5.7235e+17, 2.9680e+16, 3.6481e+16,
        5.9613e+16, 1.1875e+16, 2.2585e+16, 3.6691e+16, 1.6166e+16, 3.0368e+16,
        2.3124e+16, 2.8228e+16, 1.7974e+17, 3.5208e+17, 9.5928e+17, 5.8275e+16,
        5.6973e+16, 5.7615e+16, 2.4596e+17, 5.4474e+16, 4.3979e+16, 2.7546e+16,
        4.4901e+16, 1.0239e+17, 7.5384e+16, 1.1185e+18, 1.3103e+16, 1.3462e+17,
        4.8535e+16, 2.6507e+17, 4.4754e+16, 2.5462e+17, 3.5022e+16, 4.2862e+16,
        1.8681e+17, 1.1385e+17, 7.6910e+16, 2.4013e+16, 1.2226e+17, 1.8138e+16,
        1.2131e+17, 1.3583e+16, 6.3302e+16, 2.9701e+19, 1.9951e+16, 1.1178e+17,
        8.5038e+16, 3.3648e+17, 3.7263e+16, 5.9974e+16, 1.4331e+16, 2.2727e+16,
        1.4663e+16, 4.8507e+16, 1.4594e+17, 1.1723e+18, 6.9586e+17, 2.6782e+16,
        2.9153e+17, 3.5106e+16, 8.2299e+16, 3.3584e+16, 3.9772e+16, 2.5607e+16,
        3.9831e+16, 1.9719e+17, 3.8430e+16, 9.5355e+16, 7.1515e+16, 2.6874e+16,
        2.3009e+17, 9.1178e+16, 7.3076e+16, 2.6263e+16, 1.5363e+17, 1.3401e+16,
        8.1530e+16, 3.7798e+16, 1.2855e+16, 2.8575e+16, 1.0822e+17, 4.8881e+16,
        3.8373e+17, 7.0919e+16, 4.6473e+16, 4.8149e+16, 2.0202e+16, 8.1104e+16,
        5.6819e+17, 8.6155e+16, 2.5258e+17, 5.8398e+16, 2.6723e+17, 4.9486e+17,
        1.9824e+16, 2.9207e+16, 4.5772e+16, 7.4019e+16, 1.5338e+17, 4.4297e+16,
        3.2899e+16, 6.8287e+16, 1.3950e+16, 1.4013e+17])), ('encoder.4.1.conv1', tensor([6.0323e+16, 5.4394e+16, 8.3825e+16, 8.4575e+16, 8.7046e+16, 7.0540e+16,
        5.8123e+16, 8.3868e+16, 8.7411e+16, 7.4910e+16, 2.7025e+16, 3.7328e+16,
        1.1909e+17, 8.4903e+16, 2.4820e+16, 1.2555e+17, 7.5938e+16, 7.9428e+16,
        5.5404e+16, 9.3830e+16, 5.5943e+16, 7.2785e+16, 7.8397e+16, 7.0813e+16,
        8.4321e+16, 8.8067e+16, 6.5559e+16, 5.7749e+16, 8.1661e+16, 8.0807e+16,
        1.0021e+17, 5.9844e+16, 6.6889e+16, 9.3993e+16, 5.2819e+16, 6.8540e+16,
        9.4998e+16, 1.2262e+17, 1.0121e+17, 1.7537e+17, 1.0874e+17, 5.3705e+16,
        5.4270e+16, 4.2799e+16, 1.0746e+17, 7.8006e+16, 5.2647e+16, 8.1024e+16,
        1.0143e+17, 8.0373e+16, 8.8003e+16, 1.3065e+17, 1.1717e+17, 7.2233e+16,
        9.9573e+16, 6.4993e+16, 6.5979e+16, 9.9442e+16, 6.9982e+16, 6.5461e+16,
        8.6593e+16, 2.9704e+16, 6.9088e+16, 9.7666e+16])), ('encoder.4.1.conv2', tensor([6.8878e+16, 6.9696e+16, 5.5724e+16, 8.0391e+16, 6.1746e+16, 1.1170e+17,
        1.1268e+17, 5.6350e+16, 9.9817e+16, 6.0751e+16, 6.3293e+16, 3.5268e+16,
        6.6228e+16, 7.0984e+16, 8.0027e+16, 5.7513e+16, 5.9958e+16, 6.5345e+16,
        6.7055e+16, 7.5581e+16, 6.5159e+16, 6.6858e+16, 7.1065e+16, 1.2632e+17,
        9.5924e+16, 4.6394e+16, 1.1614e+17, 1.0560e+17, 6.5262e+16, 1.1653e+17,
        7.2785e+16, 5.4595e+16, 6.2647e+16, 6.2928e+16, 9.0936e+16, 6.1007e+16,
        1.2461e+17, 5.4496e+16, 6.1487e+16, 5.6300e+16, 1.3461e+17, 5.4899e+16,
        1.3416e+17, 6.0014e+16, 1.0623e+17, 1.4778e+17, 5.6939e+16, 9.5768e+16,
        1.4771e+17, 1.2196e+17, 1.0629e+17, 1.3127e+17, 4.7361e+16, 1.3449e+17,
        1.3126e+17, 9.8803e+16, 9.3584e+16, 4.6521e+16, 6.9194e+16, 9.8773e+16,
        4.9271e+16, 6.7785e+16, 7.2913e+16, 4.8324e+16])), ('encoder.4.1.conv3', tensor([1.8116e+16, 3.1458e+15, 5.2973e+14, 9.6556e+15, 2.9624e+15, 2.5356e+14,
        2.5211e+15, 1.9169e+15, 2.5966e+15, 1.0123e+15, 1.9006e+15, 1.9801e+15,
        1.5347e+15, 3.4612e+15, 1.8246e+15, 1.4136e+15, 4.1171e+15, 4.8347e+15,
        6.5953e+15, 3.0583e+15, 3.6927e+15, 4.2449e+15, 1.8954e+15, 4.3519e+15,
        9.7379e+14, 1.6767e+15, 2.3522e+16, 2.8864e+15, 6.8170e+15, 8.6457e+14,
        1.5826e+15, 6.7355e+14, 1.5421e+15, 9.1473e+14, 2.5015e+15, 4.5835e+15,
        2.2916e+16, 7.3959e+14, 5.2912e+15, 1.1124e+16, 1.0299e+16, 5.1051e+15,
        1.2800e+15, 7.4365e+15, 1.0491e+15, 3.1243e+15, 2.4627e+15, 1.0949e+16,
        6.4448e+14, 2.0967e+15, 1.9843e+15, 2.1142e+15, 1.9974e+15, 1.7814e+15,
        1.3325e+15, 6.5519e+14, 7.9403e+15, 1.0201e+15, 2.2238e+15, 8.3734e+14,
        2.5877e+15, 8.3355e+15, 3.1981e+15, 1.4959e+14, 3.0125e+15, 2.2340e+15,
        2.1590e+15, 1.9157e+15, 1.2444e+15, 3.2473e+15, 1.6982e+15, 2.1327e+15,
        1.5722e+15, 2.5010e+15, 4.3455e+14, 7.0727e+15, 2.5242e+15, 3.7331e+15,
        1.8663e+15, 1.7758e+15, 2.1166e+15, 2.0511e+15, 4.1177e+15, 2.6471e+15,
        3.4644e+15, 2.8222e+15, 2.2091e+15, 1.6635e+15, 2.9448e+15, 2.0008e+15,
        3.4112e+15, 2.9620e+15, 1.8866e+15, 1.0576e+15, 2.6095e+15, 2.1464e+16,
        2.3421e+15, 1.9227e+15, 1.4163e+15, 2.5697e+15, 1.3074e+15, 3.0287e+15,
        3.0737e+15, 4.1340e+15, 2.4119e+15, 1.9595e+15, 9.1701e+15, 1.9111e+15,
        3.4503e+15, 3.0947e+15, 1.6632e+15, 1.6435e+15, 1.1008e+15, 1.5917e+15,
        2.5353e+15, 3.7671e+15, 3.1401e+15, 2.8269e+14, 2.3595e+15, 1.0541e+15,
        2.1084e+15, 1.4270e+15, 1.5375e+15, 3.0744e+15, 4.9753e+14, 2.1307e+14,
        2.4144e+16, 1.5983e+15, 1.7461e+15, 4.2171e+16, 1.8781e+16, 1.1740e+16,
        1.0633e+15, 3.2837e+15, 3.3820e+15, 1.9901e+15, 3.0169e+15, 2.4031e+15,
        2.2259e+15, 6.2282e+15, 2.1315e+15, 2.2362e+15, 2.7607e+15, 1.0573e+16,
        9.7921e+14, 1.6097e+15, 1.0701e+15, 7.1879e+15, 5.3376e+15, 1.1518e+15,
        1.3723e+15, 6.9663e+15, 4.0923e+14, 8.6871e+15, 2.2225e+15, 2.1199e+14,
        1.9917e+15, 2.8065e+15, 1.7039e+15, 3.5633e+14, 6.2643e+14, 2.1512e+15,
        6.5448e+14, 3.1298e+15, 2.4257e+15, 2.5172e+15, 8.1185e+14, 2.4188e+15,
        3.9579e+15, 2.5776e+15, 1.9895e+15, 1.1056e+15, 2.7077e+15, 6.5820e+14,
        1.4991e+15, 8.8161e+14, 1.8183e+15, 1.4969e+16, 2.5426e+15, 5.1349e+15,
        4.5786e+15, 3.3648e+15, 1.5673e+16, 1.4377e+16, 2.6713e+15, 2.2899e+15,
        8.7207e+15, 6.4701e+14, 2.8256e+15, 3.0478e+15, 2.1863e+15, 2.9521e+14,
        1.0109e+16, 5.0741e+14, 2.8425e+15, 3.2886e+15, 4.1033e+15, 1.7793e+17,
        8.7224e+14, 2.7631e+15, 1.2864e+15, 1.2101e+15, 1.8352e+16, 2.4132e+15,
        6.6932e+15, 4.2584e+15, 1.1545e+15, 2.5692e+15, 1.5666e+15, 7.9443e+14,
        2.2856e+15, 1.6293e+15, 1.7255e+16, 7.4476e+14, 1.5581e+15, 5.2909e+14,
        1.9896e+15, 9.6544e+15, 3.0715e+15, 1.1647e+15, 1.8338e+15, 2.2765e+14,
        2.6008e+15, 6.1713e+15, 2.5408e+15, 6.6625e+14, 3.4055e+15, 2.6832e+15,
        1.6568e+16, 2.5013e+15, 2.0598e+15, 3.1506e+14, 4.3545e+15, 2.6338e+15,
        8.8629e+16, 3.2889e+15, 9.3709e+14, 3.9390e+15, 9.3228e+14, 1.3990e+15,
        1.9906e+15, 2.3276e+15, 2.3877e+15, 2.7609e+15, 1.9947e+16, 1.0027e+16,
        1.4322e+16, 2.7454e+16, 1.9339e+15, 2.3985e+15, 7.9418e+14, 1.1328e+16,
        5.6489e+15, 4.4544e+14, 9.7548e+14, 8.6529e+15])), ('encoder.4.2.conv1', tensor([2.4843e+15, 5.1629e+15, 5.2447e+15, 6.4197e+15, 7.5570e+15, 4.5077e+15,
        5.7541e+15, 4.0019e+15, 2.2520e+15, 4.9729e+15, 8.8151e+15, 3.6249e+15,
        3.2199e+15, 3.7732e+15, 3.5212e+15, 5.8680e+15, 2.6658e+15, 4.8251e+15,
        3.7401e+15, 2.1388e+15, 3.7808e+15, 4.5342e+15, 5.4285e+15, 4.8592e+15,
        7.6248e+15, 2.7270e+15, 5.1743e+15, 4.2986e+15, 4.7613e+15, 6.0287e+15,
        4.3019e+15, 3.0099e+15, 3.0286e+15, 4.3241e+15, 2.4166e+15, 5.5378e+15,
        4.9992e+15, 3.9423e+15, 5.8374e+15, 5.3096e+15, 5.4075e+15, 3.1253e+15,
        4.5766e+15, 1.9770e+15, 2.5228e+15, 5.0245e+15, 5.9657e+15, 3.9073e+15,
        5.9166e+15, 4.5932e+15, 4.7732e+15, 5.0890e+15, 3.5660e+15, 5.8312e+15,
        4.0385e+15, 4.1032e+15, 4.5178e+15, 4.0670e+15, 4.8589e+15, 4.3320e+15,
        4.7859e+15, 5.4007e+15, 3.8441e+15, 2.5647e+15])), ('encoder.4.2.conv2', tensor([5.1379e+15, 3.6346e+15, 5.1447e+15, 8.3811e+15, 4.9835e+15, 7.3477e+15,
        7.0458e+15, 5.5424e+15, 7.7461e+15, 4.6632e+15, 4.9155e+15, 3.6122e+15,
        4.1333e+15, 3.2027e+15, 3.9982e+15, 6.5348e+15, 8.1215e+15, 4.3368e+15,
        5.3686e+15, 5.8007e+15, 5.6083e+15, 6.8635e+15, 4.1164e+15, 6.7329e+15,
        4.6300e+15, 3.7840e+15, 7.0161e+15, 7.3861e+15, 4.1296e+15, 7.6545e+15,
        4.5724e+15, 5.9888e+15, 6.7467e+15, 6.0653e+15, 6.1000e+15, 4.2624e+15,
        3.5640e+15, 7.9773e+15, 4.1644e+15, 5.8969e+15, 4.2028e+15, 6.4889e+15,
        3.9553e+15, 3.1056e+15, 3.9120e+15, 6.5224e+15, 5.4105e+15, 5.0070e+15,
        8.2998e+15, 5.3175e+15, 4.5781e+15, 4.9975e+15, 4.2233e+15, 5.5551e+15,
        6.1623e+15, 5.2799e+15, 4.2597e+15, 7.8799e+15, 7.2808e+15, 4.7210e+15,
        3.9503e+15, 6.4145e+15, 5.8289e+15, 4.0703e+15])), ('encoder.4.2.conv3', tensor([2.5613e+13, 1.3721e+14, 5.5796e+13, 2.2808e+14, 8.6542e+13, 3.9800e+13,
        1.0039e+14, 8.0636e+13, 8.3411e+14, 1.6939e+14, 1.1542e+14, 1.3783e+14,
        6.4839e+13, 2.3758e+14, 2.2448e+14, 2.9404e+14, 1.0046e+14, 1.4729e+14,
        5.8095e+15, 3.5625e+14, 3.0386e+14, 1.2408e+14, 1.3237e+14, 2.9736e+15,
        3.6097e+14, 1.5182e+14, 7.6705e+13, 1.3885e+15, 1.1208e+14, 5.0299e+13,
        4.5862e+13, 8.4381e+13, 1.4669e+14, 1.4556e+14, 1.6826e+14, 8.0565e+13,
        1.0856e+14, 1.5918e+13, 4.0833e+13, 3.9218e+14, 1.0254e+14, 8.6692e+13,
        1.3197e+14, 7.2669e+13, 2.0373e+13, 2.2860e+14, 1.2344e+14, 7.8996e+13,
        6.7554e+13, 3.0923e+14, 1.0630e+14, 1.2735e+14, 7.3208e+14, 2.0322e+14,
        1.2488e+14, 1.5117e+13, 9.9334e+13, 5.6339e+13, 1.1485e+14, 1.5006e+14,
        1.3804e+14, 5.7218e+15, 8.1571e+13, 2.4150e+13, 1.3191e+14, 2.4745e+14,
        1.0467e+14, 1.5100e+14, 2.8331e+14, 1.6153e+14, 5.1847e+13, 1.5861e+14,
        4.9205e+13, 6.6876e+13, 1.6726e+14, 2.4189e+14, 1.0799e+14, 8.4938e+13,
        1.6577e+14, 4.8780e+13, 2.4486e+14, 1.3253e+14, 3.5198e+14, 8.8716e+13,
        1.4239e+14, 2.6202e+14, 3.8231e+13, 2.2280e+14, 3.7951e+14, 4.6633e+14,
        4.3880e+14, 2.7740e+14, 7.4928e+13, 5.3126e+13, 1.2988e+14, 1.4367e+14,
        1.2961e+14, 6.3170e+13, 4.4427e+13, 2.6531e+13, 1.2462e+14, 1.2022e+14,
        5.0465e+13, 4.9989e+13, 1.7280e+14, 8.5785e+13, 6.1447e+14, 1.1319e+14,
        1.0884e+15, 1.0426e+14, 1.8720e+14, 4.3148e+13, 5.7319e+13, 3.5002e+14,
        6.4295e+13, 2.5306e+14, 6.9886e+13, 7.3216e+13, 6.0404e+13, 1.0563e+14,
        1.0634e+14, 1.0575e+14, 3.5654e+14, 1.3545e+14, 7.2114e+14, 8.3076e+12,
        1.1135e+15, 3.9443e+14, 8.7732e+13, 1.9183e+14, 1.1638e+14, 1.6504e+14,
        4.2183e+13, 2.0674e+14, 1.4414e+15, 1.4039e+14, 1.8286e+14, 9.4951e+13,
        5.5728e+14, 4.0319e+15, 9.1825e+13, 2.4811e+14, 3.1170e+14, 2.7102e+14,
        2.3684e+13, 5.7317e+14, 5.9878e+13, 1.2938e+14, 2.1578e+14, 4.7753e+14,
        2.5431e+14, 1.3016e+14, 8.3628e+13, 8.4213e+13, 9.2737e+13, 8.3156e+13,
        2.5918e+14, 5.0459e+14, 1.2093e+14, 1.5895e+14, 1.0204e+15, 1.7166e+14,
        4.4146e+13, 9.0652e+13, 6.2679e+14, 7.5535e+13, 6.5155e+13, 4.1390e+13,
        2.3999e+15, 5.0816e+13, 4.5445e+13, 4.0028e+14, 5.6919e+14, 5.7402e+13,
        2.0888e+14, 1.1784e+14, 1.0447e+14, 1.0114e+14, 3.1409e+13, 3.8309e+14,
        9.5431e+13, 7.3175e+13, 1.0607e+14, 1.4150e+14, 9.7332e+13, 3.4130e+14,
        9.7141e+13, 6.0102e+13, 3.4593e+14, 8.0071e+14, 7.7128e+13, 3.5939e+13,
        1.1486e+14, 4.0252e+13, 4.6045e+14, 1.5563e+14, 5.5318e+13, 6.3470e+13,
        2.2990e+14, 1.1889e+14, 1.9298e+14, 4.6157e+13, 2.9584e+14, 6.8857e+14,
        1.5166e+14, 1.6445e+15, 3.8895e+14, 2.7924e+14, 4.8562e+13, 9.6055e+13,
        2.3869e+14, 6.1527e+13, 2.1341e+14, 2.0083e+14, 1.0997e+14, 1.0683e+13,
        8.6041e+13, 1.7550e+15, 3.6306e+14, 4.3352e+13, 2.9334e+14, 3.6245e+13,
        4.2131e+15, 3.9274e+14, 9.2613e+13, 8.4457e+13, 1.5759e+15, 3.7723e+13,
        2.3061e+14, 3.2485e+13, 8.3751e+13, 1.8411e+13, 2.8605e+14, 7.1736e+13,
        2.3230e+15, 2.0840e+15, 1.5937e+13, 1.4982e+15, 3.5856e+13, 6.2128e+14,
        1.0239e+14, 1.0041e+14, 1.4954e+15, 1.6611e+14, 1.3349e+14, 2.4132e+14,
        1.2765e+14, 3.6684e+14, 8.1710e+13, 1.4635e+14, 3.7105e+13, 2.3045e+14,
        1.1641e+14, 4.9188e+14, 1.2148e+14, 1.6351e+14])), ('encoder.5.0.conv1', tensor([1.4576e+14, 1.8020e+14, 1.3748e+14, 1.4002e+14, 8.4466e+13, 1.2987e+14,
        1.3027e+14, 1.0982e+14, 2.4899e+14, 1.3567e+14, 1.1786e+14, 8.4668e+13,
        7.2419e+13, 1.2344e+14, 1.8515e+14, 1.5601e+14, 2.4792e+14, 1.3794e+14,
        8.5458e+13, 2.1477e+14, 9.9301e+13, 1.1880e+14, 1.6412e+14, 1.6501e+14,
        1.7985e+14, 1.3728e+14, 1.3113e+14, 1.8742e+14, 1.2058e+14, 2.2672e+14,
        1.9054e+14, 1.2285e+14, 1.1380e+14, 1.8769e+14, 1.7315e+14, 1.7446e+14,
        1.5342e+14, 1.3809e+14, 1.8652e+14, 1.7053e+14, 1.9596e+14, 1.6987e+14,
        1.3277e+14, 1.4368e+14, 2.1577e+14, 7.8447e+13, 2.5797e+14, 7.5744e+13,
        1.7536e+14, 1.4888e+14, 1.9560e+14, 1.2291e+14, 9.1334e+13, 1.3298e+14,
        1.8030e+14, 1.3258e+14, 2.9343e+14, 6.7698e+13, 9.5571e+13, 1.8033e+14,
        1.5360e+14, 2.3335e+14, 1.9438e+14, 1.7868e+14, 1.3425e+14, 1.7066e+14,
        7.6663e+13, 1.4899e+14, 1.5088e+14, 1.4933e+14, 1.3640e+14, 1.3859e+14,
        8.0195e+13, 9.4011e+13, 1.4174e+14, 1.5448e+14, 9.7236e+13, 2.1739e+14,
        1.8004e+14, 1.3389e+14, 1.0799e+14, 1.4084e+14, 1.6967e+14, 1.3829e+14,
        1.2106e+14, 1.7092e+14, 1.8342e+14, 1.1996e+14, 1.6645e+14, 8.7264e+13,
        9.6145e+13, 1.4964e+14, 2.2791e+14, 1.5942e+14, 1.9604e+14, 1.9573e+14,
        1.8269e+14, 1.2674e+14, 8.9433e+13, 2.2218e+14, 1.9738e+14, 2.1719e+14,
        1.1805e+14, 1.1610e+14, 1.2490e+14, 1.0667e+14, 1.1744e+14, 1.5153e+14,
        1.2701e+14, 1.2353e+14, 1.4014e+14, 8.2443e+13, 1.6171e+14, 1.8793e+14,
        1.0442e+14, 1.6107e+14, 1.6779e+14, 1.6645e+14, 2.2223e+14, 1.8594e+14,
        1.9650e+14, 2.4991e+14, 1.0683e+14, 8.3719e+13, 1.2089e+14, 1.0044e+14,
        7.8599e+13, 1.7190e+14])), ('encoder.5.0.conv2', tensor([1.9027e+14, 1.8587e+14, 1.3047e+14, 2.6510e+14, 1.7814e+14, 3.1120e+14,
        2.0690e+14, 2.3203e+14, 2.0606e+14, 1.5756e+14, 2.1373e+14, 1.7897e+14,
        2.6931e+14, 1.9658e+14, 1.1052e+14, 1.8144e+14, 2.0994e+14, 2.0413e+14,
        1.3456e+14, 1.7404e+14, 1.1161e+14, 1.7252e+14, 2.1614e+14, 2.5172e+14,
        1.3164e+14, 1.6723e+14, 1.8653e+14, 2.1398e+14, 2.0638e+14, 1.4417e+14,
        1.7455e+14, 2.1173e+14, 1.4891e+14, 2.3973e+14, 2.0106e+14, 1.9198e+14,
        1.4998e+14, 1.7373e+14, 1.6803e+14, 2.5231e+14, 2.3098e+14, 2.0853e+14,
        1.8620e+14, 1.3082e+14, 1.5255e+14, 2.4470e+14, 1.8914e+14, 1.8636e+14,
        2.3957e+14, 1.4605e+14, 2.0805e+14, 1.8551e+14, 1.9619e+14, 1.7093e+14,
        2.0709e+14, 1.7323e+14, 1.9973e+14, 1.1120e+14, 2.1139e+14, 1.5035e+14,
        2.5589e+14, 2.1271e+14, 1.9698e+14, 2.4310e+14, 2.0335e+14, 2.8693e+14,
        1.3084e+14, 1.5479e+14, 1.1103e+14, 3.7720e+14, 3.0251e+14, 2.5449e+14,
        2.0976e+14, 1.6239e+14, 2.4644e+14, 2.3799e+14, 2.1414e+14, 1.8523e+14,
        1.7905e+14, 1.4819e+14, 1.4817e+14, 1.9074e+14, 2.1405e+14, 1.2528e+14,
        3.1203e+14, 2.7317e+14, 2.2934e+14, 1.2534e+14, 2.5420e+14, 2.8382e+14,
        2.0828e+14, 1.6313e+14, 1.9095e+14, 1.9656e+14, 2.0398e+14, 1.5495e+14,
        1.4014e+14, 2.2070e+14, 1.4689e+14, 2.2138e+14, 2.4383e+14, 2.9469e+14,
        2.7293e+14, 2.2362e+14, 1.8030e+14, 2.5287e+14, 1.6559e+14, 2.0931e+14,
        1.7335e+14, 1.2082e+14, 1.9709e+14, 1.2987e+14, 2.6076e+14, 2.6357e+14,
        3.0753e+14, 2.1291e+14, 2.5843e+14, 2.4043e+14, 2.3889e+14, 2.4846e+14,
        2.2119e+14, 2.8896e+14, 1.8576e+14, 1.8599e+14, 1.3341e+14, 1.6925e+14,
        2.6648e+14, 2.7824e+14])), ('encoder.5.0.conv3', tensor([1.3915e+13, 7.6043e+11, 5.5281e+12, 1.1985e+13, 1.1604e+12, 2.4453e+12,
        1.6008e+12, 1.6389e+12, 1.6589e+13, 6.8030e+12, 3.4855e+12, 1.9459e+12,
        2.0334e+12, 1.1091e+14, 5.9113e+12, 8.1020e+11, 1.5693e+13, 4.7916e+14,
        1.6966e+12, 2.1393e+12, 7.1035e+12, 6.2717e+11, 3.6291e+12, 1.5250e+12,
        4.1707e+12, 1.1415e+12, 1.8062e+13, 8.5982e+11, 2.1154e+12, 7.8564e+11,
        4.3119e+11, 4.0173e+12, 2.4022e+12, 2.9908e+12, 8.0399e+11, 2.3599e+12,
        3.5653e+12, 5.0549e+13, 1.5377e+13, 2.3220e+12, 5.0148e+12, 2.0063e+12,
        2.8407e+13, 1.2066e+12, 4.7261e+12, 1.1497e+13, 6.4852e+12, 1.4175e+12,
        2.6783e+12, 8.0259e+11, 9.2005e+11, 1.9604e+12, 1.3986e+12, 5.4468e+12,
        5.2974e+12, 2.3291e+12, 1.2104e+13, 2.0536e+12, 9.3139e+11, 9.5213e+11,
        3.5502e+12, 7.8372e+11, 1.5479e+12, 1.8970e+13, 2.2844e+13, 8.4819e+12,
        4.8022e+12, 1.1236e+13, 1.6119e+12, 1.8017e+13, 9.4258e+12, 5.1335e+12,
        3.1352e+13, 8.7626e+11, 1.0787e+12, 6.2161e+11, 3.6076e+12, 2.5322e+13,
        8.2782e+12, 1.3388e+13, 4.7782e+11, 2.3444e+12, 2.0820e+12, 7.8902e+11,
        2.2213e+13, 4.2280e+12, 1.2578e+13, 8.5188e+11, 1.4101e+12, 4.9547e+13,
        1.9610e+12, 6.4247e+11, 7.7881e+12, 8.3775e+11, 2.8506e+12, 9.2453e+10,
        1.0398e+13, 3.4420e+12, 4.8814e+12, 1.7200e+12, 6.9181e+11, 2.9476e+12,
        8.2408e+12, 8.4693e+12, 1.7564e+12, 5.6123e+12, 1.8682e+12, 1.3279e+13,
        7.9134e+13, 7.8307e+11, 3.6315e+12, 4.4621e+12, 2.2075e+12, 1.8823e+11,
        2.2099e+13, 8.7112e+13, 6.1167e+11, 9.5355e+11, 1.8230e+13, 3.7488e+13,
        1.3790e+12, 9.0864e+12, 1.0689e+12, 1.0214e+12, 4.4346e+12, 3.3963e+12,
        9.7268e+11, 8.1555e+11, 3.3225e+12, 1.7622e+12, 1.7462e+12, 3.7245e+11,
        5.4844e+12, 2.6208e+12, 2.3370e+13, 3.2503e+12, 1.0313e+12, 3.9170e+12,
        2.9543e+12, 4.5244e+12, 1.8223e+12, 5.2867e+12, 4.6361e+12, 6.7536e+12,
        1.8030e+12, 3.6222e+11, 5.5090e+12, 4.7737e+11, 8.9615e+12, 7.7966e+12,
        3.0921e+11, 9.5698e+12, 3.1893e+12, 2.4343e+12, 2.3603e+12, 2.2110e+12,
        8.1381e+12, 4.7836e+12, 1.1475e+13, 1.8841e+12, 1.2016e+12, 1.2543e+14,
        1.5669e+12, 2.0556e+13, 5.5306e+12, 8.1079e+11, 7.1608e+12, 1.6219e+12,
        3.1309e+12, 3.3046e+13, 5.4670e+12, 1.1278e+12, 3.9290e+12, 1.7285e+12,
        1.4981e+13, 2.1822e+13, 7.2197e+11, 2.7905e+12, 5.5746e+13, 5.6714e+12,
        4.2782e+12, 3.4384e+12, 2.0326e+12, 1.2838e+13, 4.5680e+11, 1.4487e+13,
        5.8110e+12, 1.1382e+12, 6.6496e+12, 7.7021e+11, 2.5310e+13, 1.7144e+12,
        7.6899e+11, 6.4682e+11, 1.3275e+12, 1.3951e+12, 9.0551e+11, 3.9979e+12,
        6.8951e+12, 4.0379e+12, 1.3848e+13, 6.7329e+11, 2.4992e+13, 3.1655e+12,
        2.3155e+12, 1.5950e+12, 5.5904e+12, 2.8090e+12, 2.7803e+12, 1.7737e+12,
        1.4320e+12, 3.9054e+12, 6.3282e+12, 5.3813e+11, 4.6938e+13, 2.7419e+12,
        5.6449e+12, 1.2343e+12, 2.2792e+12, 8.6132e+11, 6.8094e+12, 6.9485e+12,
        1.0121e+12, 5.2798e+12, 2.5736e+12, 3.8004e+12, 1.8153e+12, 9.5844e+11,
        1.1335e+13, 5.1681e+11, 5.8653e+12, 2.5345e+14, 6.9479e+11, 3.2472e+12,
        2.5741e+12, 6.9671e+12, 3.4422e+12, 7.1519e+12, 7.5617e+11, 7.3276e+13,
        3.6987e+12, 5.5156e+12, 4.0490e+12, 2.7339e+12, 2.7794e+11, 8.1553e+11,
        2.9381e+12, 2.1456e+12, 6.0538e+11, 7.2559e+12, 4.3186e+12, 1.6842e+12,
        5.0934e+11, 8.5194e+11, 3.3978e+12, 8.6639e+12, 2.6040e+12, 3.1548e+12,
        2.8838e+12, 3.4385e+13, 1.5101e+12, 2.4866e+11, 4.7838e+12, 2.3245e+12,
        8.2447e+11, 1.5039e+13, 5.8894e+12, 8.7920e+11, 3.4305e+12, 2.0829e+12,
        4.0184e+12, 1.2673e+12, 1.3414e+12, 3.5010e+12, 9.5391e+11, 1.4876e+12,
        2.7499e+12, 9.7646e+12, 4.8949e+12, 2.0451e+12, 1.9185e+13, 2.4608e+12,
        3.6465e+13, 1.4450e+12, 1.7546e+12, 4.0552e+12, 3.9518e+12, 1.1521e+13,
        2.2813e+13, 4.0186e+12, 3.8432e+12, 2.6249e+12, 7.2244e+11, 7.0502e+11,
        2.1013e+13, 2.5340e+11, 3.1416e+12, 3.0644e+12, 1.6075e+12, 1.6182e+12,
        1.3455e+12, 7.4220e+11, 6.5985e+12, 3.1397e+12, 2.6568e+12, 4.1871e+12,
        1.2402e+12, 2.7986e+13, 8.4497e+11, 4.0629e+12, 1.4828e+12, 4.0030e+12,
        1.5740e+12, 9.4954e+11, 3.6941e+12, 2.6581e+12, 3.8567e+12, 2.4011e+11,
        4.1305e+12, 4.5916e+12, 8.0253e+12, 1.0839e+13, 2.2926e+13, 1.7606e+13,
        2.1124e+12, 1.0987e+13, 5.9169e+13, 2.7454e+12, 7.4654e+12, 2.9945e+11,
        7.5082e+12, 2.1398e+12, 2.9958e+13, 4.0435e+12, 1.1597e+12, 5.9513e+12,
        4.2319e+12, 2.1290e+12, 1.1029e+12, 1.4776e+13, 2.3894e+12, 1.0216e+12,
        2.7206e+12, 7.6874e+13, 4.2105e+12, 5.3215e+12, 5.6229e+11, 1.4963e+12,
        3.3077e+12, 4.0302e+12, 1.6369e+12, 2.3069e+12, 7.2498e+11, 3.3849e+12,
        9.9214e+12, 8.3049e+12, 8.8064e+11, 2.3017e+12, 3.1178e+12, 6.7081e+12,
        3.4016e+11, 1.8486e+12, 1.1908e+12, 8.1154e+12, 1.0444e+12, 3.5709e+12,
        8.2214e+12, 4.6793e+12, 3.5629e+12, 3.8365e+12, 5.4227e+11, 5.5198e+12,
        1.0598e+12, 6.0951e+11, 2.2503e+12, 1.9596e+13, 1.2781e+13, 2.7220e+11,
        5.3736e+12, 1.7685e+13, 9.9702e+11, 1.8412e+11, 1.2928e+12, 9.5621e+11,
        4.0450e+11, 1.2916e+12, 1.0235e+13, 1.5245e+14, 5.5615e+12, 3.5349e+12,
        2.9913e+12, 1.5319e+13, 6.3338e+12, 2.6043e+12, 1.7962e+12, 3.2660e+12,
        5.9433e+12, 4.3593e+12, 1.3337e+12, 3.3048e+12, 7.0339e+11, 2.5738e+11,
        8.2709e+12, 1.7176e+12, 4.5618e+12, 1.6471e+13, 4.3927e+12, 7.9603e+11,
        6.4646e+12, 2.0809e+12, 3.9551e+13, 1.2097e+13, 6.5871e+12, 6.5073e+11,
        8.1364e+11, 2.5313e+12, 5.8754e+11, 5.2723e+12, 3.3778e+11, 3.1755e+11,
        1.5101e+13, 7.8649e+11, 3.2373e+12, 1.2677e+13, 1.6002e+12, 5.5869e+12,
        3.3859e+12, 2.9324e+12, 4.6691e+12, 4.3599e+12, 3.6069e+13, 9.5150e+12,
        1.8347e+13, 9.3236e+11, 5.5919e+12, 2.9400e+11, 4.3679e+12, 1.3998e+13,
        1.0654e+13, 1.3696e+12, 4.8203e+12, 1.5054e+14, 8.1086e+12, 3.8398e+12,
        5.7523e+12, 7.3872e+11, 1.0387e+12, 1.1200e+13, 3.9106e+12, 2.3338e+12,
        3.7405e+12, 1.1376e+14, 7.2079e+12, 6.9066e+12, 9.3675e+11, 6.9356e+12,
        1.0344e+13, 1.9573e+12, 1.3556e+12, 7.6252e+12, 2.6696e+12, 1.7762e+12,
        8.7974e+12, 2.4920e+12, 3.9463e+12, 4.0124e+12, 1.5094e+12, 4.0913e+12,
        3.8771e+12, 1.7968e+12, 5.0161e+13, 1.4369e+12, 2.4106e+12, 7.9280e+12,
        3.7490e+11, 9.6952e+11, 5.8216e+11, 2.4114e+12, 2.0464e+13, 2.1639e+12,
        2.9772e+12, 4.9222e+12, 1.5833e+11, 3.9764e+13, 3.5411e+11, 7.8691e+11,
        8.0790e+11, 2.0083e+11, 4.1106e+12, 1.8826e+12, 9.7972e+11, 1.5327e+12,
        1.2973e+12, 3.8507e+11, 7.5729e+12, 1.7863e+12, 1.1890e+13, 1.5190e+13,
        2.5472e+12, 3.3481e+12, 3.6780e+12, 3.3543e+13, 9.1297e+11, 6.4927e+13,
        4.3659e+12, 3.3476e+12, 1.2913e+12, 3.5133e+12, 2.7326e+11, 1.3483e+12,
        1.6894e+13, 1.3031e+12])), ('encoder.5.0.downsample.0', tensor([1.6927e+13, 4.1863e+12, 7.5801e+12, 1.1552e+13, 1.5216e+12, 2.2103e+12,
        2.5346e+12, 3.9221e+12, 1.5853e+13, 5.3152e+12, 3.2639e+12, 9.5351e+11,
        3.0614e+12, 1.0887e+14, 7.7958e+12, 1.9899e+12, 2.1145e+13, 4.8036e+14,
        2.1839e+12, 3.9605e+12, 8.0463e+12, 3.5748e+12, 3.8553e+12, 2.7407e+12,
        4.2637e+12, 9.9780e+11, 2.3385e+13, 2.7141e+12, 3.5030e+12, 1.5082e+12,
        4.7860e+12, 6.5866e+12, 5.3604e+12, 5.5558e+12, 3.2141e+12, 3.3752e+12,
        5.7128e+12, 4.9274e+13, 1.4998e+13, 5.7040e+12, 4.9279e+12, 3.9555e+12,
        2.7669e+13, 1.9694e+12, 5.3834e+12, 1.1005e+13, 9.2046e+12, 2.6832e+12,
        4.3266e+12, 2.1631e+12, 1.2853e+12, 5.3764e+12, 3.9304e+12, 7.7420e+12,
        3.9424e+12, 5.1153e+12, 1.1825e+13, 1.8852e+12, 1.5435e+12, 2.6732e+12,
        3.4548e+12, 2.8171e+12, 4.6555e+12, 2.1267e+13, 2.3811e+13, 8.9255e+12,
        6.3960e+12, 1.3000e+13, 1.3788e+12, 1.5599e+13, 1.1471e+13, 5.8703e+12,
        3.2183e+13, 3.1022e+12, 4.3797e+12, 2.3452e+12, 5.5040e+12, 2.5946e+13,
        9.3434e+12, 1.3874e+13, 2.7387e+12, 3.1045e+12, 1.5207e+12, 1.5362e+12,
        2.1135e+13, 3.5119e+12, 1.1541e+13, 1.6792e+12, 2.2063e+12, 5.0688e+13,
        2.8316e+12, 3.3922e+12, 6.4780e+12, 4.3936e+12, 4.1865e+12, 1.4375e+12,
        1.1239e+13, 3.5828e+12, 4.2903e+12, 1.7097e+12, 2.2573e+12, 4.7520e+12,
        4.5395e+12, 9.4805e+12, 2.7095e+12, 6.1695e+12, 3.6637e+12, 1.0260e+13,
        8.0451e+13, 2.4233e+12, 2.1941e+12, 1.5762e+12, 1.8694e+12, 1.9810e+12,
        2.4436e+13, 8.7243e+13, 9.3124e+11, 3.7243e+12, 1.6445e+13, 3.9537e+13,
        9.6291e+11, 7.5063e+12, 3.2717e+12, 2.0161e+12, 5.6291e+12, 6.3463e+12,
        2.3821e+12, 3.0037e+12, 5.6146e+12, 1.9607e+12, 5.6810e+11, 9.0999e+11,
        5.2897e+12, 1.7946e+12, 2.3219e+13, 1.6916e+12, 1.3262e+12, 4.6178e+12,
        2.6154e+12, 8.3811e+12, 3.8644e+12, 7.2303e+12, 5.1412e+12, 5.8251e+12,
        8.5105e+11, 2.3912e+12, 4.6709e+12, 1.8471e+12, 5.7023e+12, 1.0254e+13,
        4.3042e+12, 1.0261e+13, 4.5600e+12, 1.3918e+12, 3.6646e+12, 4.1554e+12,
        6.7691e+12, 3.4727e+12, 1.0357e+13, 1.8675e+12, 2.6136e+12, 1.2494e+14,
        2.0488e+12, 2.1037e+13, 5.1278e+12, 1.1567e+12, 5.6359e+12, 2.8917e+12,
        3.1089e+12, 3.2305e+13, 6.5792e+12, 1.7700e+12, 3.7582e+12, 2.5075e+12,
        1.0518e+13, 2.1162e+13, 3.1249e+12, 1.7746e+12, 5.6589e+13, 6.1236e+12,
        8.5058e+12, 1.3465e+12, 1.7570e+12, 1.7202e+13, 4.2635e+12, 1.3745e+13,
        7.2000e+12, 1.9386e+12, 5.6211e+12, 2.3321e+12, 2.3953e+13, 3.1584e+12,
        1.1240e+12, 4.5539e+12, 1.4312e+12, 1.8113e+12, 2.8854e+12, 6.1163e+12,
        7.3123e+12, 4.2795e+12, 1.3616e+13, 3.5446e+12, 2.5778e+13, 4.7127e+12,
        2.2473e+12, 2.5729e+12, 7.3807e+12, 3.6718e+12, 3.6042e+12, 1.7668e+12,
        2.1299e+12, 2.1727e+12, 7.6975e+12, 2.1429e+12, 4.9917e+13, 3.5312e+12,
        5.8260e+12, 1.1095e+12, 2.4861e+12, 3.0877e+12, 6.9124e+12, 9.0510e+12,
        1.8933e+12, 3.5740e+12, 2.6793e+12, 3.8224e+12, 2.3603e+12, 1.4334e+12,
        1.1727e+13, 2.6176e+12, 5.7778e+12, 2.5798e+14, 2.2051e+12, 3.1446e+12,
        1.7291e+12, 7.5314e+12, 4.0863e+12, 6.3756e+12, 3.3478e+12, 7.1870e+13,
        1.4112e+12, 4.7895e+12, 5.4342e+12, 4.7238e+12, 2.1013e+12, 2.9396e+12,
        6.0303e+12, 4.5377e+12, 1.9267e+12, 7.6300e+12, 5.8889e+12, 2.3786e+12,
        2.8481e+12, 1.2191e+12, 5.6356e+12, 7.2968e+12, 3.0130e+12, 3.8550e+12,
        1.9745e+12, 3.6155e+13, 9.2966e+11, 1.4096e+12, 8.6008e+12, 3.4166e+12,
        3.5423e+12, 1.4172e+13, 6.7599e+12, 2.6430e+12, 6.0004e+12, 4.7064e+12,
        8.3903e+12, 2.9476e+12, 2.0443e+12, 6.1909e+12, 2.6139e+12, 1.5206e+12,
        1.7591e+12, 9.2415e+12, 3.2287e+12, 2.1478e+12, 1.8572e+13, 2.2263e+12,
        3.3673e+13, 6.0044e+12, 3.1275e+12, 8.1454e+12, 4.7047e+12, 1.3540e+13,
        2.3883e+13, 3.9655e+12, 5.2838e+12, 2.5284e+12, 3.8001e+12, 2.0729e+12,
        2.0660e+13, 1.8519e+12, 5.5927e+12, 3.4038e+12, 1.6954e+12, 1.7376e+12,
        6.4317e+11, 2.1865e+12, 4.3870e+12, 2.7194e+12, 2.8755e+12, 3.1996e+12,
        3.0341e+12, 3.0549e+13, 8.5661e+11, 3.4859e+12, 3.4986e+12, 5.7411e+12,
        4.5410e+12, 9.6931e+11, 3.5343e+12, 2.7727e+12, 3.3246e+12, 1.9965e+12,
        3.8515e+12, 5.8822e+12, 9.4540e+12, 9.3722e+12, 2.3748e+13, 1.9953e+13,
        1.6557e+12, 7.2725e+12, 7.5945e+13, 5.4017e+12, 8.4682e+12, 3.0631e+12,
        6.3133e+12, 1.7776e+12, 2.7662e+13, 3.7544e+12, 1.9616e+12, 6.7218e+12,
        5.9313e+12, 4.2027e+12, 1.5774e+12, 1.6197e+13, 1.6513e+12, 1.0719e+12,
        3.4951e+12, 7.9568e+13, 8.9710e+12, 6.6263e+12, 2.3344e+12, 2.3096e+12,
        6.7525e+12, 3.6288e+12, 1.1657e+12, 7.4521e+12, 1.8517e+12, 4.4358e+12,
        9.6905e+12, 9.6325e+12, 2.1798e+12, 3.6359e+12, 1.3376e+12, 5.1877e+12,
        3.8670e+12, 2.4527e+12, 4.5218e+12, 1.6424e+13, 4.7479e+12, 3.8173e+12,
        6.5261e+12, 3.2297e+12, 4.1698e+12, 2.2050e+12, 1.7542e+12, 7.0166e+12,
        3.1597e+12, 1.2717e+12, 3.3381e+12, 2.0394e+13, 1.2030e+13, 2.1628e+12,
        6.3169e+12, 1.8935e+13, 1.5233e+12, 1.7198e+12, 5.3422e+12, 1.6950e+12,
        1.5882e+12, 8.8375e+11, 1.1972e+13, 1.5096e+14, 4.6923e+12, 2.6724e+12,
        4.7164e+12, 1.4800e+13, 7.0961e+12, 2.2319e+12, 6.5369e+11, 2.4766e+12,
        5.0595e+12, 6.2556e+12, 2.0937e+12, 2.4856e+12, 2.3654e+12, 2.5921e+12,
        6.9158e+12, 1.8632e+12, 5.4893e+12, 1.9550e+13, 6.0589e+12, 2.3225e+12,
        6.7430e+12, 2.1374e+12, 3.8803e+13, 9.3296e+12, 7.1717e+12, 2.1396e+12,
        1.4154e+12, 4.6700e+12, 1.9977e+12, 4.8021e+12, 1.9071e+12, 1.3212e+12,
        1.9866e+13, 2.1046e+12, 3.4634e+12, 1.2261e+13, 1.4805e+12, 8.2447e+12,
        4.4391e+12, 1.5959e+12, 5.2898e+12, 6.3705e+12, 3.3317e+13, 1.0180e+13,
        1.7555e+13, 3.6749e+12, 9.0401e+12, 3.8225e+12, 2.4797e+12, 1.5019e+13,
        1.2525e+13, 1.4414e+12, 7.8120e+12, 1.5094e+14, 5.1842e+12, 4.8703e+12,
        3.7973e+12, 4.8305e+12, 4.5373e+12, 8.9414e+12, 4.3899e+12, 1.7826e+12,
        6.1245e+12, 1.1250e+14, 8.3504e+12, 6.7585e+12, 2.5225e+12, 8.1670e+12,
        1.2772e+13, 2.8315e+12, 1.7964e+12, 8.4607e+12, 2.7387e+12, 4.1160e+12,
        7.4532e+12, 1.5991e+12, 3.9765e+12, 6.7076e+12, 4.0625e+12, 6.0144e+12,
        6.6554e+12, 4.4654e+12, 5.0583e+13, 1.2949e+12, 2.5976e+12, 7.0271e+12,
        1.5165e+12, 1.8590e+12, 2.9124e+12, 1.5785e+12, 1.9290e+13, 2.9516e+12,
        7.3683e+12, 3.9891e+12, 2.3408e+12, 4.0187e+13, 2.5687e+12, 3.8152e+12,
        2.5138e+12, 2.3636e+12, 7.8045e+12, 2.5199e+12, 5.1027e+11, 1.8238e+12,
        2.8557e+12, 1.9626e+12, 7.2174e+12, 4.8407e+12, 1.0812e+13, 1.6268e+13,
        3.2201e+12, 3.7794e+12, 2.1349e+12, 3.3692e+13, 2.7988e+12, 6.4739e+13,
        2.8222e+12, 3.8505e+12, 1.0530e+12, 3.3287e+12, 1.8872e+12, 2.9712e+12,
        1.8668e+13, 1.9516e+12])), ('encoder.5.1.conv1', tensor([3.8969e+12, 3.5823e+12, 9.7652e+12, 3.4164e+12, 3.6870e+12, 4.3007e+12,
        3.8597e+12, 4.6661e+12, 4.0078e+12, 3.8443e+12, 5.9137e+12, 4.5866e+12,
        6.2080e+12, 4.5322e+12, 8.1063e+12, 7.0085e+12, 1.0491e+13, 3.1723e+12,
        7.1683e+12, 5.4895e+12, 5.4473e+12, 3.6044e+12, 4.0831e+12, 4.1149e+12,
        4.1092e+12, 2.5173e+12, 5.2892e+12, 6.0413e+12, 6.0431e+12, 8.7883e+12,
        3.1364e+12, 2.9752e+12, 3.3496e+12, 5.4954e+12, 6.8206e+12, 3.5609e+12,
        5.8402e+12, 5.5681e+12, 4.4504e+12, 7.3528e+12, 2.4340e+12, 4.3208e+12,
        3.3923e+12, 3.6518e+12, 5.0860e+12, 5.0395e+12, 8.2802e+12, 6.4962e+12,
        4.4574e+12, 7.4507e+12, 6.4342e+12, 7.9307e+12, 9.4966e+12, 4.2240e+12,
        3.4451e+12, 6.6111e+12, 1.9107e+12, 5.2339e+12, 6.9872e+12, 3.2556e+12,
        2.6848e+12, 2.3734e+12, 7.3241e+12, 3.8774e+12, 6.8026e+12, 2.9196e+12,
        5.6514e+12, 4.3263e+12, 2.8909e+12, 4.7380e+12, 3.1381e+12, 6.3836e+12,
        2.0110e+12, 4.2653e+12, 2.6988e+12, 5.8457e+12, 3.5426e+12, 5.5694e+12,
        2.7959e+12, 4.8426e+12, 1.9264e+12, 5.2436e+12, 6.2134e+12, 4.3195e+12,
        6.9795e+12, 4.1212e+12, 4.0379e+12, 4.9785e+12, 3.6307e+12, 3.7411e+12,
        5.7968e+12, 4.7241e+12, 5.3819e+12, 1.9530e+12, 4.5708e+12, 5.5539e+12,
        5.7699e+12, 4.4513e+12, 5.2134e+12, 1.1925e+13, 4.3429e+12, 4.4816e+12,
        6.6509e+12, 6.7707e+12, 3.9397e+12, 2.8992e+12, 5.6017e+12, 3.6149e+12,
        2.4325e+12, 1.0703e+13, 3.4386e+12, 6.8436e+12, 4.3897e+12, 5.3585e+12,
        6.9225e+12, 6.1777e+12, 7.8145e+12, 5.3077e+12, 4.2749e+12, 4.0620e+12,
        5.3322e+12, 4.5493e+12, 5.6188e+12, 8.6987e+12, 6.8056e+12, 5.3471e+12,
        4.6896e+12, 9.2821e+12])), ('encoder.5.1.conv2', tensor([4.6877e+12, 8.0107e+12, 6.9354e+12, 5.3668e+12, 7.7250e+12, 9.4895e+12,
        3.1485e+12, 4.9284e+12, 1.1648e+13, 2.6967e+12, 4.1320e+12, 1.5143e+13,
        3.0146e+12, 4.3251e+12, 6.6642e+12, 6.6367e+12, 1.0372e+13, 7.6250e+12,
        3.1208e+12, 7.5313e+12, 3.0869e+12, 3.8713e+12, 8.8127e+12, 5.0814e+12,
        8.7004e+12, 3.5137e+12, 6.9852e+12, 8.6767e+12, 5.6416e+12, 3.7473e+12,
        4.9203e+12, 3.0503e+12, 1.0484e+13, 7.4781e+12, 1.0067e+13, 5.0464e+12,
        5.2793e+12, 5.1226e+12, 1.4730e+13, 8.1186e+12, 9.7064e+12, 9.2681e+12,
        4.7992e+12, 1.1874e+13, 5.5444e+12, 1.1376e+13, 8.6601e+12, 1.0826e+13,
        9.4415e+12, 7.5158e+12, 6.5756e+12, 7.6983e+12, 4.2196e+12, 5.2223e+12,
        5.1184e+12, 6.8653e+12, 6.7430e+12, 7.2137e+12, 1.2830e+13, 4.1918e+12,
        5.7785e+12, 8.1517e+12, 3.5437e+12, 9.3156e+12, 2.8611e+12, 2.6621e+12,
        4.9352e+12, 7.3814e+12, 5.5571e+12, 7.3810e+12, 6.1958e+12, 7.8257e+12,
        8.1408e+12, 7.1049e+12, 8.3380e+12, 8.3975e+12, 1.0418e+13, 9.1391e+12,
        6.5683e+12, 5.8803e+12, 4.4505e+12, 6.1243e+12, 6.0393e+12, 2.7788e+12,
        9.5798e+12, 5.5504e+12, 5.4231e+12, 4.6468e+12, 5.0529e+12, 4.6494e+12,
        8.8011e+12, 6.3749e+12, 6.5466e+12, 8.5406e+12, 5.5632e+12, 6.8302e+12,
        7.2486e+12, 1.2850e+13, 7.5964e+12, 7.3040e+12, 3.3078e+12, 8.2584e+12,
        1.0624e+13, 5.7767e+12, 3.9633e+12, 1.7884e+13, 6.5988e+12, 6.1761e+12,
        1.2832e+13, 1.0142e+13, 6.5531e+12, 1.1915e+13, 4.8444e+12, 4.9429e+12,
        7.8310e+12, 3.7665e+12, 5.4795e+12, 3.7337e+12, 4.9041e+12, 1.2118e+13,
        7.7034e+12, 8.2325e+12, 9.2813e+12, 8.7764e+12, 1.0820e+13, 4.4829e+12,
        3.7835e+12, 5.5928e+12])), ('encoder.5.1.conv3', tensor([5.3429e+11, 3.8977e+10, 3.7067e+11, 5.5250e+11, 2.0276e+12, 7.1708e+11,
        6.1993e+10, 5.5913e+11, 5.8914e+11, 5.7568e+11, 2.8996e+11, 1.9130e+11,
        2.0617e+11, 8.2975e+11, 5.8094e+11, 1.8670e+11, 3.8626e+12, 7.4945e+11,
        7.3528e+10, 7.7871e+11, 5.4719e+11, 1.4795e+11, 4.1109e+11, 3.3508e+11,
        5.7809e+11, 7.0532e+11, 7.0397e+12, 3.8392e+11, 2.0625e+12, 5.3711e+11,
        9.5319e+11, 4.1778e+11, 1.2157e+12, 1.4138e+11, 2.8433e+11, 2.2246e+11,
        2.0988e+11, 7.3112e+11, 6.8454e+11, 6.3837e+11, 4.2790e+11, 1.9592e+12,
        6.2347e+11, 8.0634e+11, 1.0715e+12, 3.7818e+11, 4.6031e+11, 1.1526e+12,
        7.1302e+11, 2.0797e+11, 2.0965e+11, 3.8549e+11, 4.2339e+11, 9.7908e+11,
        2.0683e+11, 5.3217e+11, 6.7115e+11, 5.3428e+11, 1.3976e+11, 1.5893e+11,
        1.0400e+12, 9.6172e+11, 1.3184e+11, 6.4743e+11, 4.7247e+11, 5.9486e+11,
        5.6362e+11, 2.9296e+11, 2.8925e+11, 6.6778e+11, 4.4297e+11, 4.5262e+11,
        1.1144e+12, 4.8150e+11, 4.3811e+11, 6.5992e+11, 6.1364e+11, 4.3053e+11,
        9.2229e+11, 2.3206e+11, 6.1819e+11, 1.9857e+11, 7.0399e+11, 8.9661e+10,
        5.9507e+11, 1.7208e+12, 6.7809e+11, 5.4219e+11, 1.2938e+11, 3.3122e+11,
        3.5206e+11, 2.6032e+11, 7.2364e+11, 4.3494e+11, 2.0442e+11, 5.5793e+11,
        5.7013e+11, 5.7389e+11, 1.0727e+12, 5.9904e+11, 2.5358e+11, 9.7431e+10,
        8.0126e+11, 1.7074e+12, 1.7551e+11, 2.4357e+11, 4.5752e+10, 1.9519e+11,
        2.2601e+11, 1.7427e+11, 3.7538e+11, 4.1120e+11, 5.0452e+11, 3.7667e+10,
        1.3372e+12, 7.0484e+11, 4.3765e+11, 3.1296e+11, 4.9795e+11, 1.9942e+12,
        3.9228e+11, 2.2651e+11, 2.7205e+11, 2.2718e+11, 5.1755e+12, 1.7829e+12,
        5.3584e+11, 4.3935e+11, 1.1192e+12, 2.6300e+11, 1.2632e+11, 6.5868e+11,
        3.5703e+11, 4.0358e+12, 6.0462e+11, 1.8996e+11, 1.0654e+11, 2.7534e+11,
        5.8897e+11, 3.4121e+11, 1.0132e+12, 1.6115e+12, 8.2352e+11, 7.7106e+11,
        8.7169e+10, 7.2449e+11, 2.1571e+11, 1.9019e+11, 1.4829e+11, 6.9830e+11,
        8.6920e+11, 1.2939e+12, 6.7426e+11, 2.0961e+12, 1.4241e+11, 1.5033e+12,
        8.2006e+11, 4.9431e+12, 8.7740e+11, 1.0861e+11, 5.7712e+11, 3.9686e+11,
        5.0496e+11, 2.9261e+12, 4.8498e+11, 3.4990e+10, 3.3501e+11, 2.3508e+11,
        3.2975e+11, 1.5744e+12, 6.1116e+11, 8.2047e+11, 4.8242e+11, 2.2786e+11,
        3.1452e+12, 1.5955e+12, 5.6255e+11, 2.0608e+11, 3.1971e+11, 2.3099e+11,
        3.2969e+11, 4.8731e+11, 6.5222e+11, 1.3266e+12, 1.1219e+12, 4.5186e+11,
        3.2182e+11, 1.0942e+11, 1.0722e+12, 6.1550e+11, 1.2550e+12, 2.1696e+11,
        5.8149e+11, 6.4120e+10, 5.6704e+11, 6.4595e+11, 6.9772e+10, 8.6412e+11,
        2.1660e+11, 7.7448e+11, 6.2686e+11, 4.9986e+11, 3.6654e+11, 1.3756e+12,
        5.8437e+11, 2.7602e+11, 3.7463e+11, 1.0295e+12, 3.2660e+11, 9.3796e+11,
        6.5936e+11, 8.2182e+11, 5.5848e+11, 5.7097e+11, 3.4814e+11, 7.8407e+11,
        3.0177e+11, 4.4760e+11, 9.0895e+11, 2.1751e+11, 4.6991e+11, 7.9506e+11,
        3.8465e+11, 4.3145e+11, 3.0530e+11, 5.0118e+11, 4.0626e+11, 4.0687e+11,
        1.4746e+12, 5.6143e+11, 6.2048e+11, 1.3543e+12, 5.4969e+11, 9.2066e+11,
        3.1210e+11, 3.4078e+11, 1.7317e+11, 9.7055e+11, 6.4343e+11, 2.0309e+11,
        6.6854e+11, 1.0264e+11, 4.0026e+11, 7.8882e+11, 6.6437e+11, 7.0939e+11,
        6.2851e+11, 3.3981e+11, 5.1248e+11, 3.8474e+11, 9.1011e+11, 2.0884e+11,
        1.1973e+12, 9.2115e+11, 3.6678e+11, 5.2725e+11, 7.9274e+11, 7.2960e+11,
        4.0622e+11, 2.9906e+11, 1.6962e+12, 1.7429e+11, 8.9405e+11, 7.7990e+11,
        7.3598e+11, 4.4013e+11, 4.3653e+11, 1.3732e+11, 2.2169e+11, 1.1403e+12,
        1.2451e+12, 6.7959e+10, 6.3686e+11, 8.8299e+11, 2.6609e+11, 6.9033e+11,
        1.8895e+12, 6.7244e+11, 1.4379e+12, 1.0838e+11, 3.5396e+11, 7.5983e+11,
        4.7254e+11, 3.9203e+11, 2.0729e+12, 1.5361e+11, 3.4504e+11, 7.6450e+11,
        3.0849e+11, 1.4895e+11, 4.8969e+11, 5.6523e+11, 6.1183e+10, 2.6034e+11,
        4.0213e+11, 5.9946e+11, 1.7433e+11, 7.0955e+11, 2.0905e+11, 6.2710e+11,
        9.5909e+11, 4.0875e+10, 8.3589e+11, 2.0290e+11, 4.2182e+11, 4.7654e+12,
        8.9657e+11, 6.6798e+11, 8.9454e+10, 5.6223e+11, 6.1668e+11, 7.7458e+11,
        1.3651e+11, 6.2166e+11, 1.5597e+11, 2.0147e+12, 3.9682e+11, 5.9993e+11,
        5.0130e+11, 5.9878e+11, 1.6135e+12, 7.9842e+11, 2.2308e+11, 8.2719e+11,
        4.9415e+11, 7.2972e+11, 1.5865e+13, 6.2505e+11, 4.1841e+11, 4.6410e+11,
        4.8005e+11, 4.1024e+11, 1.7355e+12, 8.7171e+11, 7.0599e+11, 7.8756e+11,
        2.7342e+11, 1.8412e+11, 9.8267e+10, 1.5850e+11, 7.6168e+11, 4.8979e+11,
        1.2889e+12, 1.5453e+12, 7.7959e+12, 6.6148e+11, 9.2989e+11, 4.2589e+11,
        3.6522e+11, 1.2138e+12, 4.3831e+11, 5.3614e+12, 6.1864e+11, 4.6307e+11,
        1.4909e+12, 2.1372e+12, 8.3891e+11, 2.8445e+12, 3.1977e+12, 1.0920e+12,
        8.4787e+11, 6.8695e+11, 6.5793e+11, 7.1577e+12, 5.0853e+11, 4.8476e+11,
        6.6249e+11, 6.7201e+11, 7.6793e+11, 1.6574e+12, 1.0681e+11, 2.1158e+11,
        1.9070e+11, 8.4494e+11, 1.4044e+11, 6.6942e+11, 3.4327e+11, 7.9423e+11,
        8.8785e+11, 6.2608e+11, 2.0966e+10, 7.4846e+11, 1.1899e+11, 3.1425e+11,
        7.0555e+10, 9.2483e+11, 7.6020e+11, 4.7334e+11, 5.0163e+11, 2.3805e+11,
        2.0600e+12, 7.5335e+11, 3.8466e+12, 5.9377e+11, 6.1341e+10, 4.5287e+11,
        7.1562e+11, 7.7663e+11, 4.6335e+10, 1.2326e+11, 6.0790e+11, 9.9121e+10,
        9.2036e+11, 1.1593e+11, 4.6992e+11, 1.2124e+12, 3.9181e+11, 5.8400e+11,
        5.6718e+11, 1.0596e+11, 2.4761e+11, 2.0500e+11, 8.2977e+11, 3.2793e+11,
        2.4951e+11, 2.2287e+12, 5.2925e+11, 5.7810e+11, 6.3006e+10, 5.3677e+11,
        1.7854e+12, 3.8442e+11, 1.0221e+12, 3.6712e+11, 9.4598e+11, 1.5763e+11,
        9.4967e+11, 1.4160e+11, 1.7316e+12, 1.8355e+11, 4.8734e+11, 3.1176e+11,
        4.7788e+11, 6.0177e+11, 6.3619e+11, 1.1900e+11, 7.7386e+11, 5.0040e+11,
        1.5967e+11, 6.4032e+11, 4.0323e+11, 5.0895e+11, 3.6101e+11, 1.4993e+11,
        3.7752e+11, 2.2709e+12, 6.9117e+10, 6.3864e+11, 7.6869e+11, 5.4759e+11,
        1.1969e+12, 1.8804e+11, 3.3151e+11, 4.2298e+11, 5.4338e+11, 1.5918e+12,
        9.7711e+11, 5.2594e+11, 3.3640e+11, 2.0670e+11, 4.4265e+11, 1.9934e+11,
        3.0482e+11, 5.9254e+11, 2.4987e+11, 3.3011e+11, 1.0146e+12, 4.3471e+11,
        1.3961e+11, 5.4114e+11, 3.1546e+11, 5.9293e+11, 4.0638e+12, 7.4151e+11,
        5.5259e+11, 5.8823e+11, 1.9528e+11, 5.1481e+11, 7.9305e+11, 2.4254e+11,
        1.9028e+12, 4.6847e+11, 4.3113e+10, 1.0448e+12, 5.9992e+11, 4.8104e+10,
        4.9507e+10, 1.7271e+11, 5.2939e+11, 5.4347e+11, 8.8452e+11, 2.0140e+11,
        4.6652e+11, 5.0469e+11, 3.7241e+11, 2.1475e+11, 3.8083e+11, 2.6081e+11,
        3.5100e+11, 8.0002e+11, 2.2371e+12, 7.9125e+11, 8.0931e+11, 3.7113e+12,
        6.7454e+11, 7.0755e+11, 5.5646e+10, 6.2855e+11, 6.2613e+11, 6.3075e+11,
        9.7241e+11, 5.6858e+11])), ('encoder.5.2.conv1', tensor([7.5250e+11, 9.0704e+11, 8.4059e+11, 5.1749e+11, 1.2382e+12, 1.3289e+12,
        1.2458e+12, 1.2180e+12, 1.0772e+12, 8.1009e+11, 1.1312e+12, 8.4699e+11,
        1.0501e+12, 1.5986e+12, 7.8765e+11, 1.1172e+12, 1.0823e+12, 1.0190e+12,
        1.2031e+12, 9.9039e+11, 1.5034e+12, 1.4119e+12, 1.4376e+12, 9.3513e+11,
        7.2229e+11, 1.0723e+12, 9.0361e+11, 5.9744e+11, 1.2486e+12, 1.6118e+12,
        1.1526e+12, 1.1828e+12, 9.9821e+11, 1.7671e+12, 8.5879e+11, 8.4989e+11,
        1.3550e+12, 1.8152e+12, 1.1420e+12, 7.2321e+11, 1.0660e+12, 1.6021e+12,
        1.1940e+12, 1.2999e+12, 7.7575e+11, 1.8891e+12, 7.1424e+11, 7.7082e+11,
        9.3796e+11, 1.5261e+12, 8.2338e+11, 1.5964e+12, 1.2554e+12, 1.5576e+12,
        1.3194e+12, 6.1295e+11, 1.1686e+12, 9.7959e+11, 1.5019e+12, 6.4685e+11,
        2.3017e+12, 4.0129e+11, 5.8718e+11, 1.0372e+12, 1.6975e+12, 1.3382e+12,
        7.9260e+11, 1.5324e+12, 2.0008e+12, 1.1744e+12, 1.4331e+12, 1.8520e+12,
        2.2265e+12, 1.3485e+12, 9.2809e+11, 1.3126e+12, 1.0979e+12, 1.4925e+12,
        2.0141e+12, 1.5878e+12, 1.5320e+12, 1.3707e+12, 1.6780e+12, 1.0942e+12,
        7.7370e+11, 2.2515e+12, 2.9025e+12, 1.4990e+12, 9.8306e+11, 5.3799e+11,
        1.3284e+12, 9.4001e+11, 2.1316e+12, 7.0562e+11, 5.5648e+11, 1.6648e+12,
        1.1175e+12, 1.5448e+12, 1.2342e+12, 1.2637e+12, 1.0958e+12, 1.0103e+12,
        1.3503e+12, 1.5224e+12, 4.6308e+11, 1.1732e+12, 1.1196e+12, 1.7440e+12,
        9.6733e+11, 1.3135e+12, 9.4853e+11, 1.9035e+12, 1.5022e+12, 2.2050e+12,
        9.0531e+11, 1.2274e+12, 1.7677e+12, 1.8080e+12, 6.6759e+11, 7.4579e+11,
        9.7545e+11, 2.0557e+12, 6.9717e+11, 5.5608e+11, 1.5773e+12, 8.3334e+11,
        1.2017e+12, 5.7186e+11])), ('encoder.5.2.conv2', tensor([1.9201e+12, 2.8949e+12, 1.0796e+12, 1.8290e+12, 9.2065e+11, 1.6813e+12,
        1.0601e+12, 1.6526e+12, 2.5850e+12, 1.4494e+12, 1.1405e+12, 1.5230e+12,
        1.0517e+12, 1.8198e+12, 1.9344e+12, 1.5999e+12, 2.3782e+12, 3.2200e+12,
        1.4457e+12, 1.2914e+12, 7.5020e+11, 1.4013e+12, 1.9726e+12, 1.1905e+12,
        1.3376e+12, 1.7658e+12, 1.4695e+12, 6.7459e+11, 2.0616e+12, 1.6034e+12,
        3.3300e+12, 7.7178e+11, 2.0560e+12, 1.2380e+12, 1.4478e+12, 1.7434e+12,
        2.5497e+12, 1.7894e+12, 1.9279e+12, 1.7155e+12, 2.0529e+12, 1.3715e+12,
        1.3339e+12, 1.1078e+12, 1.1995e+12, 2.3591e+12, 1.3845e+12, 3.0797e+12,
        2.1698e+12, 2.7002e+12, 1.1573e+12, 2.6066e+12, 3.1311e+12, 1.0615e+12,
        2.3671e+12, 1.2253e+12, 2.0796e+12, 1.4998e+12, 1.1452e+12, 2.3734e+12,
        2.7037e+12, 2.8627e+12, 1.2471e+12, 3.2446e+12, 1.6990e+12, 2.7351e+12,
        9.0541e+11, 1.5056e+12, 1.6308e+12, 2.8810e+12, 2.7190e+12, 9.4842e+11,
        6.9010e+11, 2.4937e+12, 1.9852e+12, 6.2473e+11, 1.4366e+12, 2.9396e+12,
        2.3051e+12, 2.8132e+12, 1.8466e+12, 1.4287e+12, 2.0431e+12, 2.9805e+12,
        1.2438e+12, 1.6805e+12, 2.4226e+12, 2.5641e+12, 2.8587e+12, 1.6483e+12,
        1.1442e+12, 1.9166e+12, 9.4761e+11, 1.5910e+12, 2.0917e+12, 2.9112e+12,
        2.3030e+12, 1.0963e+12, 1.9212e+12, 1.0072e+12, 1.9892e+12, 1.9091e+12,
        1.2735e+12, 1.9102e+12, 1.4318e+12, 1.8499e+12, 2.4284e+12, 2.0445e+12,
        9.7608e+11, 2.1286e+12, 2.2520e+12, 1.8608e+12, 8.9477e+11, 1.6341e+12,
        1.4445e+12, 1.7398e+12, 2.6086e+12, 2.7165e+12, 1.4206e+12, 1.8005e+12,
        2.3383e+12, 1.4012e+12, 2.4025e+12, 1.9565e+12, 2.5648e+12, 1.3797e+12,
        9.7125e+11, 8.6318e+11])), ('encoder.5.2.conv3', tensor([3.6243e+10, 5.0808e+09, 4.2552e+09, 3.6344e+09, 6.8526e+10, 1.6409e+11,
        4.6586e+09, 6.5581e+10, 1.2736e+11, 3.9574e+10, 1.9431e+10, 1.6801e+10,
        4.4016e+10, 6.8354e+10, 8.8156e+10, 3.1255e+10, 1.1301e+11, 2.1422e+10,
        3.1405e+09, 7.9010e+10, 1.7871e+11, 2.2708e+10, 3.9392e+10, 4.3851e+10,
        7.9272e+09, 7.7601e+10, 9.8771e+10, 6.2758e+09, 7.1220e+10, 1.2794e+11,
        2.2330e+10, 2.5030e+10, 5.2053e+10, 1.7559e+10, 3.0385e+10, 3.8105e+10,
        1.3222e+10, 4.6127e+10, 1.7231e+10, 5.5048e+10, 3.3490e+10, 2.0929e+12,
        1.6528e+10, 3.4416e+10, 5.9927e+10, 2.5330e+10, 6.4056e+10, 4.3463e+10,
        6.7366e+10, 6.5663e+10, 1.5215e+10, 5.9349e+10, 4.3962e+10, 9.6575e+11,
        8.9056e+09, 4.8298e+10, 5.4301e+10, 1.8559e+10, 1.0384e+10, 6.7966e+09,
        5.4096e+10, 1.5601e+11, 4.4022e+09, 7.6776e+10, 6.9350e+10, 8.1795e+10,
        2.2317e+10, 4.4445e+10, 2.8024e+11, 6.8085e+10, 2.1702e+10, 1.5481e+10,
        3.3905e+11, 1.5401e+10, 8.2261e+10, 6.6356e+10, 1.8590e+10, 1.9125e+11,
        3.8000e+10, 6.0156e+10, 9.1377e+10, 9.9701e+09, 5.2476e+09, 2.8298e+09,
        1.3185e+11, 5.9655e+10, 1.1351e+11, 7.4983e+10, 2.9562e+10, 1.2045e+11,
        6.1860e+10, 3.8516e+10, 5.2876e+10, 1.5144e+10, 4.1715e+10, 3.4900e+10,
        2.2655e+10, 3.9493e+10, 6.1810e+10, 2.9212e+10, 1.7335e+10, 4.9756e+09,
        8.0300e+10, 7.5973e+10, 1.5295e+11, 9.7501e+10, 4.2178e+09, 1.9521e+10,
        5.8721e+10, 6.1405e+10, 3.6024e+10, 3.1125e+10, 5.6102e+10, 2.4102e+09,
        3.0986e+10, 2.6164e+10, 1.0663e+11, 1.6610e+10, 7.2849e+10, 1.9238e+11,
        3.6353e+10, 2.7163e+10, 2.0369e+11, 9.5238e+09, 1.3180e+11, 2.8304e+10,
        6.9727e+09, 8.0330e+10, 3.7511e+10, 7.7319e+09, 6.1313e+09, 1.6992e+10,
        1.1489e+11, 2.2366e+11, 3.1331e+10, 1.2911e+10, 3.5879e+09, 1.0989e+11,
        2.1597e+10, 2.0677e+10, 1.5107e+10, 2.0628e+10, 3.1257e+10, 5.8461e+10,
        3.7638e+09, 1.4979e+10, 2.3576e+10, 1.1114e+10, 8.2335e+10, 2.6311e+11,
        9.8281e+10, 2.7470e+10, 4.8106e+10, 8.5058e+10, 4.7796e+10, 1.5719e+10,
        1.1966e+11, 1.0790e+12, 3.1305e+10, 4.3201e+09, 1.9470e+11, 2.7309e+11,
        7.6970e+10, 3.0027e+10, 2.3529e+10, 4.5850e+09, 1.2631e+11, 3.7071e+10,
        1.8590e+11, 3.4272e+10, 2.2264e+10, 4.6477e+11, 6.6304e+10, 1.6936e+10,
        3.8028e+10, 3.7410e+10, 1.7121e+10, 1.9524e+10, 5.8230e+10, 1.6298e+10,
        1.1979e+10, 1.7214e+10, 5.8470e+10, 2.2851e+10, 3.5043e+10, 3.9869e+10,
        3.0595e+10, 3.4154e+10, 6.5613e+10, 1.3364e+10, 6.2981e+10, 1.4860e+10,
        6.7799e+10, 3.0422e+09, 7.1363e+10, 1.6738e+11, 1.5510e+10, 1.5979e+10,
        4.4738e+10, 1.8851e+10, 5.0519e+10, 3.3309e+10, 1.1798e+10, 9.6681e+10,
        1.3878e+11, 1.2852e+11, 1.7292e+11, 1.1899e+11, 1.1803e+10, 6.1060e+10,
        1.8360e+10, 4.4315e+10, 4.1137e+10, 1.7843e+11, 3.8881e+10, 6.9272e+10,
        3.9996e+10, 1.4316e+11, 1.4600e+11, 3.5166e+10, 9.6409e+10, 6.1646e+10,
        5.3988e+10, 5.3737e+10, 4.3275e+10, 3.5019e+10, 7.2279e+10, 1.1812e+11,
        3.4193e+10, 3.7018e+10, 2.5062e+10, 9.5082e+10, 6.3450e+10, 2.5288e+10,
        3.1432e+11, 8.0764e+09, 2.6814e+10, 3.6172e+10, 3.4210e+10, 1.7412e+10,
        5.5426e+10, 2.3870e+10, 2.7771e+10, 4.4504e+10, 6.4950e+10, 4.6536e+10,
        2.2189e+10, 4.1805e+10, 1.0035e+11, 7.7928e+10, 4.2344e+11, 1.4690e+10,
        7.7957e+10, 2.5719e+10, 4.8865e+10, 4.6997e+10, 1.7331e+12, 5.9120e+10,
        3.0621e+10, 2.8093e+10, 1.3008e+12, 1.6852e+10, 3.2534e+10, 2.8371e+11,
        7.4179e+10, 1.8142e+10, 2.2086e+10, 4.6732e+09, 2.8969e+10, 2.2905e+10,
        1.4035e+12, 9.7093e+09, 4.1590e+10, 1.8854e+11, 3.4829e+10, 7.8363e+10,
        1.4101e+12, 1.2276e+10, 9.4958e+10, 6.9786e+10, 3.3413e+10, 4.6452e+10,
        7.3901e+09, 3.7538e+10, 6.3486e+09, 1.3915e+10, 1.6447e+10, 1.2730e+11,
        2.4293e+11, 2.8276e+10, 3.7959e+10, 6.9300e+10, 1.6091e+10, 4.2126e+10,
        7.6484e+10, 5.3323e+10, 9.4666e+09, 3.2902e+10, 2.4791e+09, 4.7753e+10,
        3.1658e+11, 5.0579e+09, 4.7554e+10, 7.3366e+10, 3.2302e+10, 1.9064e+10,
        6.9869e+10, 3.9061e+10, 1.9125e+09, 4.4362e+10, 3.7109e+10, 1.0034e+11,
        2.5552e+10, 6.0308e+10, 4.6286e+10, 1.0436e+11, 1.3045e+10, 4.5982e+10,
        7.8160e+10, 4.1949e+10, 6.0918e+10, 1.5144e+11, 5.0844e+10, 2.9003e+10,
        5.9031e+10, 5.3201e+10, 1.1555e+10, 1.6342e+10, 3.3543e+10, 1.7879e+10,
        8.3010e+10, 2.0812e+11, 4.8597e+10, 3.5961e+10, 4.6605e+10, 8.6419e+10,
        2.4714e+10, 2.4877e+10, 6.3191e+09, 2.3299e+10, 1.1302e+10, 3.0114e+10,
        5.1616e+10, 2.9240e+10, 3.4031e+11, 5.0549e+10, 1.9584e+10, 8.4938e+10,
        1.4654e+10, 4.8710e+11, 3.5346e+10, 2.5860e+10, 1.1444e+11, 2.0710e+10,
        2.5143e+10, 2.8286e+10, 4.2672e+10, 1.5934e+10, 6.0427e+10, 1.2657e+11,
        7.1079e+10, 3.1177e+10, 2.7010e+10, 1.7064e+10, 1.6089e+10, 5.9563e+10,
        6.2810e+10, 2.2969e+10, 4.0933e+10, 1.2860e+12, 2.4434e+09, 2.1473e+10,
        7.1588e+09, 2.0083e+10, 5.0392e+10, 1.7583e+10, 4.2724e+09, 5.3969e+10,
        6.1347e+10, 2.0796e+10, 9.9551e+09, 4.9488e+10, 4.7157e+10, 2.0386e+11,
        3.1475e+09, 5.0696e+10, 2.9077e+10, 2.0012e+10, 2.4914e+10, 3.1941e+10,
        2.2174e+10, 1.7748e+10, 3.4207e+12, 6.5141e+10, 2.7093e+09, 3.2794e+10,
        1.9195e+10, 3.5094e+10, 6.3976e+09, 9.4135e+10, 1.4416e+10, 2.2504e+09,
        4.6264e+10, 1.7712e+09, 9.8739e+10, 3.1116e+10, 2.3172e+10, 2.1033e+11,
        5.1703e+11, 1.5076e+10, 4.9658e+10, 4.0073e+10, 6.8983e+10, 3.1997e+10,
        1.4563e+10, 2.4210e+10, 1.1881e+11, 5.8101e+10, 1.7335e+09, 2.9349e+10,
        6.2213e+10, 3.8279e+10, 2.8346e+10, 2.6767e+10, 4.1322e+11, 2.6581e+10,
        5.0862e+10, 8.2260e+11, 3.8196e+10, 8.2254e+10, 2.8507e+10, 1.7634e+10,
        9.3968e+10, 7.9992e+10, 6.5545e+10, 3.9121e+09, 3.7568e+10, 2.4779e+10,
        3.2776e+10, 6.4704e+10, 1.0999e+11, 6.1511e+10, 5.6389e+10, 6.3386e+10,
        1.0136e+11, 1.4154e+10, 4.3759e+09, 2.4093e+10, 2.9348e+10, 2.6463e+10,
        4.3138e+10, 3.9133e+10, 7.1911e+10, 4.7845e+11, 4.5798e+10, 1.6408e+10,
        2.3103e+11, 2.0578e+10, 5.4089e+10, 1.3405e+10, 3.3644e+10, 1.3819e+10,
        4.8537e+10, 2.8998e+10, 1.1162e+10, 9.3071e+10, 6.9009e+10, 9.2165e+10,
        1.4495e+10, 8.0246e+09, 1.9328e+10, 1.6544e+10, 3.5389e+12, 5.0184e+10,
        3.1747e+10, 1.1366e+11, 4.6238e+10, 8.5564e+09, 2.4678e+10, 3.2682e+10,
        9.9874e+10, 2.9124e+10, 5.1268e+09, 1.0906e+10, 5.1900e+09, 7.2690e+09,
        7.6788e+09, 2.2641e+09, 5.1306e+10, 6.4241e+10, 2.4318e+10, 8.2971e+10,
        1.5286e+10, 7.2184e+10, 1.3767e+11, 4.6394e+09, 5.7479e+10, 2.2933e+10,
        3.1801e+11, 3.0686e+10, 1.0826e+11, 1.7483e+10, 9.5034e+10, 1.9466e+10,
        3.9985e+10, 3.0657e+10, 4.2124e+09, 7.9724e+10, 2.9909e+10, 7.0176e+10,
        1.6669e+11, 2.2187e+10])), ('encoder.5.3.conv1', tensor([1.2476e+11, 1.7125e+11, 1.1709e+11, 1.7827e+11, 1.9262e+11, 1.8284e+11,
        2.0537e+11, 7.4499e+10, 1.1265e+11, 3.8859e+10, 1.0752e+11, 2.0609e+11,
        1.7822e+11, 2.1636e+11, 1.6187e+11, 6.3392e+10, 6.3654e+10, 6.7124e+10,
        1.1158e+11, 1.6809e+11, 1.4626e+11, 6.3792e+10, 9.5801e+10, 1.3545e+11,
        1.3324e+11, 1.0480e+11, 5.4551e+10, 1.2397e+11, 8.1391e+10, 4.0856e+10,
        7.9691e+10, 1.1516e+11, 1.1620e+11, 8.6108e+10, 1.1021e+11, 1.2787e+11,
        1.0396e+11, 9.6456e+10, 1.2131e+11, 1.1068e+11, 7.9167e+10, 1.0859e+11,
        1.4543e+11, 3.1119e+11, 9.7466e+10, 4.4926e+10, 1.3863e+11, 1.2192e+11,
        1.9651e+11, 8.2717e+10, 5.4607e+10, 1.7022e+11, 1.0034e+11, 2.2081e+11,
        1.1272e+11, 1.4061e+11, 1.1576e+11, 1.0738e+11, 5.9660e+10, 1.7352e+11,
        5.0319e+10, 5.7838e+10, 6.9096e+10, 1.7735e+11, 9.7009e+10, 1.1710e+11,
        6.2515e+10, 1.3425e+11, 1.3822e+11, 7.4623e+10, 6.2792e+10, 5.1158e+10,
        8.9916e+10, 8.4713e+10, 1.1450e+11, 5.5622e+10, 1.6591e+11, 1.9226e+11,
        4.6268e+10, 1.8643e+11, 1.8230e+11, 1.0059e+11, 1.8257e+11, 8.3504e+10,
        9.0937e+10, 1.1815e+11, 1.2235e+11, 9.8027e+10, 6.6157e+10, 5.3887e+10,
        1.4508e+11, 5.7823e+10, 4.9278e+10, 1.4239e+11, 1.2391e+11, 6.9711e+10,
        9.9431e+10, 8.6406e+10, 1.4854e+11, 9.0883e+10, 9.4528e+10, 7.0400e+10,
        2.2279e+11, 1.8688e+11, 1.9974e+11, 1.1340e+11, 1.4098e+11, 8.8940e+10,
        1.2141e+11, 8.8094e+10, 1.8212e+11, 2.1584e+11, 7.1601e+10, 1.3916e+11,
        1.5523e+11, 8.7381e+10, 1.1474e+11, 4.3891e+10, 1.2939e+11, 1.3407e+11,
        9.8977e+10, 4.3017e+10, 9.4083e+10, 1.9046e+11, 9.3899e+10, 7.3229e+10,
        1.5025e+11, 1.5455e+11])), ('encoder.5.3.conv2', tensor([2.4353e+11, 9.4629e+10, 1.0331e+11, 1.7257e+11, 1.8958e+11, 1.1819e+11,
        2.5810e+11, 1.4016e+11, 1.8512e+11, 9.2424e+10, 1.0038e+11, 2.6506e+11,
        7.9020e+10, 2.0741e+11, 6.3176e+10, 1.7179e+11, 1.8460e+11, 1.7218e+11,
        1.5151e+11, 2.3216e+11, 1.1585e+11, 9.4744e+10, 1.8501e+11, 2.5622e+11,
        2.5050e+11, 7.2287e+10, 9.8474e+10, 2.2787e+11, 1.4194e+11, 2.4015e+11,
        9.8847e+10, 1.5285e+11, 5.5754e+10, 1.0885e+11, 7.1363e+10, 2.0433e+11,
        2.2451e+11, 1.6223e+11, 3.4096e+11, 2.3493e+11, 1.7360e+11, 1.4022e+11,
        8.2297e+10, 1.6362e+11, 7.0350e+10, 8.2322e+10, 2.6960e+11, 2.2398e+11,
        2.7901e+11, 1.9235e+11, 1.4707e+11, 8.7716e+10, 2.3689e+11, 1.8175e+11,
        1.1257e+11, 2.2528e+11, 1.4391e+11, 1.8760e+11, 1.0064e+11, 1.2632e+11,
        2.7798e+11, 1.5017e+11, 2.4819e+11, 9.1485e+10, 1.6593e+11, 2.6358e+11,
        2.2650e+11, 1.4187e+11, 1.4299e+11, 2.3769e+11, 2.1096e+11, 1.2263e+11,
        8.4487e+10, 3.0757e+11, 9.1084e+10, 1.8175e+11, 1.9541e+11, 2.1817e+11,
        1.4502e+11, 1.2194e+11, 9.7813e+10, 9.1917e+10, 1.5520e+11, 7.3789e+10,
        1.2070e+11, 2.7691e+11, 1.6496e+11, 1.6062e+11, 2.6140e+11, 2.0838e+11,
        8.5774e+10, 1.7339e+11, 1.5695e+11, 1.1202e+11, 1.1185e+11, 2.6759e+11,
        2.9589e+11, 2.2589e+11, 1.3894e+11, 2.3752e+11, 1.0796e+11, 9.5327e+10,
        2.1886e+11, 1.1865e+11, 3.2974e+11, 3.5220e+11, 1.5094e+11, 1.2413e+11,
        1.5062e+11, 7.5411e+10, 2.1620e+11, 2.2338e+11, 3.4304e+11, 1.0565e+11,
        8.5405e+10, 1.6231e+11, 4.9104e+10, 1.6855e+11, 2.4602e+11, 3.6199e+11,
        2.5685e+11, 2.2233e+11, 1.6276e+11, 1.0976e+11, 3.0820e+11, 2.7421e+11,
        2.4497e+11, 1.3486e+11])), ('encoder.5.3.conv3', tensor([5.4719e+09, 1.8789e+09, 1.3816e+09, 4.3494e+09, 1.5775e+09, 4.2257e+09,
        1.6402e+09, 1.6775e+10, 1.5832e+10, 2.0361e+09, 2.7659e+09, 4.8419e+09,
        2.3133e+09, 3.6518e+09, 1.0686e+10, 4.2256e+09, 3.0520e+09, 1.9960e+09,
        3.2533e+09, 5.1565e+09, 2.2895e+09, 6.4428e+08, 3.9229e+10, 5.0234e+10,
        2.8535e+09, 1.7720e+10, 3.8711e+09, 3.6593e+09, 4.4690e+09, 2.1499e+10,
        4.1243e+09, 1.9891e+09, 8.3663e+09, 4.3729e+08, 1.3634e+09, 1.5113e+09,
        7.9062e+08, 5.2331e+08, 3.3550e+09, 1.1502e+10, 2.6866e+09, 2.3076e+09,
        9.3627e+09, 4.7214e+09, 1.5652e+09, 9.1093e+08, 1.9201e+10, 1.6152e+09,
        6.9036e+09, 4.5291e+09, 3.1778e+09, 5.3244e+09, 4.8031e+09, 9.5157e+09,
        2.8324e+10, 6.9282e+09, 6.0386e+09, 6.3906e+09, 5.6053e+09, 2.1904e+08,
        1.1751e+10, 6.1410e+09, 3.7469e+08, 3.0516e+09, 1.1716e+09, 1.0870e+09,
        3.4150e+09, 2.5305e+09, 7.0011e+09, 2.4839e+10, 9.4875e+08, 3.1066e+09,
        6.7614e+09, 4.5668e+08, 9.8948e+09, 3.7629e+09, 6.4671e+08, 1.3943e+10,
        4.2172e+09, 7.1078e+10, 2.5792e+10, 2.1206e+10, 1.2033e+09, 3.9509e+08,
        1.7909e+10, 3.3374e+10, 3.2082e+09, 4.0881e+09, 1.1427e+09, 4.2872e+09,
        5.5622e+09, 5.7499e+09, 4.5788e+09, 2.8363e+09, 1.6561e+09, 3.4953e+09,
        4.3599e+09, 6.9392e+09, 9.9861e+09, 7.7058e+09, 3.2331e+09, 1.0159e+09,
        4.5479e+09, 5.6746e+09, 1.0885e+09, 2.5028e+09, 6.3469e+08, 2.9346e+09,
        6.0640e+09, 1.8463e+10, 1.3601e+09, 2.5485e+09, 7.8198e+09, 8.1479e+08,
        2.6141e+09, 5.4138e+09, 8.4627e+09, 1.7739e+09, 4.0945e+09, 2.1688e+09,
        1.4293e+09, 6.7888e+08, 9.3665e+09, 4.6399e+09, 2.1163e+09, 6.2589e+08,
        7.0510e+09, 4.3078e+09, 1.8165e+09, 1.4435e+09, 3.4667e+09, 1.0418e+10,
        9.4136e+09, 2.7482e+09, 8.0048e+09, 1.4521e+09, 6.9358e+08, 5.2833e+09,
        1.6172e+09, 4.9895e+09, 6.3239e+09, 3.2606e+09, 3.3015e+09, 7.0533e+08,
        1.7846e+09, 3.1252e+09, 2.5032e+08, 8.9333e+08, 4.0438e+09, 4.1853e+09,
        6.2866e+09, 7.8260e+09, 6.8736e+09, 3.6197e+09, 4.4605e+09, 1.9233e+10,
        5.0655e+09, 2.2469e+09, 4.1767e+09, 2.1832e+09, 4.7877e+09, 5.2026e+09,
        2.1076e+10, 4.2802e+10, 3.1361e+09, 5.8083e+08, 1.0777e+10, 1.0488e+10,
        9.1814e+09, 1.0212e+10, 1.0988e+10, 1.0145e+10, 3.1067e+09, 1.4146e+09,
        1.4837e+09, 5.5543e+09, 9.2660e+09, 6.1925e+09, 6.6110e+09, 2.7399e+09,
        1.0588e+09, 2.5757e+09, 2.3623e+09, 1.9719e+09, 2.5859e+09, 3.6952e+09,
        5.1621e+10, 2.8271e+08, 2.4055e+09, 1.3850e+10, 1.1067e+10, 1.7971e+09,
        2.1644e+09, 2.2204e+08, 1.0916e+10, 1.0177e+11, 4.4812e+09, 1.6556e+09,
        2.0304e+10, 4.2563e+09, 1.4305e+09, 1.3408e+09, 2.7258e+09, 3.0165e+10,
        1.6920e+10, 9.8963e+09, 1.5207e+10, 8.2365e+10, 8.7204e+09, 3.9590e+09,
        3.3358e+09, 2.7935e+09, 4.1652e+09, 7.4542e+09, 1.0417e+10, 2.2343e+10,
        1.2992e+10, 2.1404e+09, 3.5283e+09, 1.1340e+09, 4.3575e+09, 2.5075e+10,
        3.3892e+09, 2.9438e+09, 1.0288e+11, 1.6021e+10, 7.0885e+09, 1.2808e+10,
        7.5000e+09, 5.4085e+09, 1.5479e+10, 1.2957e+10, 1.2130e+10, 1.1708e+10,
        1.5682e+09, 4.6047e+09, 3.1218e+09, 8.5640e+09, 3.4920e+09, 3.3307e+09,
        5.3996e+09, 2.8366e+09, 2.1050e+09, 7.2501e+09, 2.7426e+10, 1.0089e+10,
        2.1113e+09, 1.0471e+09, 6.4784e+09, 3.9679e+10, 4.6318e+09, 6.7852e+08,
        3.2071e+09, 1.0628e+10, 2.8145e+10, 1.6127e+09, 5.8046e+09, 1.6166e+09,
        4.3126e+09, 3.2630e+09, 3.0262e+09, 3.0013e+08, 4.9525e+09, 6.3283e+09,
        1.4542e+10, 2.3507e+10, 3.1565e+08, 2.2569e+08, 5.0091e+08, 4.7933e+09,
        1.5015e+10, 9.4336e+08, 2.2684e+10, 4.6853e+09, 1.1006e+09, 2.6889e+09,
        9.2168e+08, 3.8737e+09, 2.9272e+09, 2.9981e+09, 3.2617e+09, 1.3478e+10,
        2.0694e+10, 8.1011e+08, 5.9792e+08, 1.9554e+10, 8.5932e+09, 2.1786e+09,
        4.1632e+11, 2.5605e+09, 3.0426e+09, 5.3214e+09, 1.3621e+09, 5.5648e+09,
        1.7816e+10, 4.7686e+09, 3.5002e+09, 1.1475e+09, 7.0265e+08, 3.7713e+09,
        4.0782e+11, 4.5578e+08, 8.2814e+09, 4.0878e+09, 7.7290e+09, 6.4976e+09,
        2.4537e+09, 1.0031e+10, 3.8225e+08, 4.9339e+09, 1.4446e+09, 1.7871e+10,
        1.5188e+09, 5.3544e+09, 1.1926e+11, 9.2479e+09, 2.6086e+09, 1.5256e+10,
        8.3914e+09, 4.0019e+09, 8.4619e+09, 9.2089e+09, 1.5177e+10, 9.3312e+09,
        1.1115e+10, 7.5147e+09, 6.3642e+09, 5.7478e+09, 3.8742e+10, 5.1634e+09,
        1.8655e+10, 3.5781e+10, 2.5923e+09, 1.1017e+10, 5.2541e+09, 1.2115e+10,
        4.6020e+09, 2.8470e+09, 1.4719e+09, 1.1496e+10, 4.2263e+09, 4.8209e+09,
        4.0065e+09, 1.1843e+09, 4.0256e+09, 4.4308e+09, 3.8930e+10, 4.0199e+09,
        2.2896e+09, 7.8057e+09, 1.8887e+10, 1.9863e+10, 6.9133e+10, 1.1092e+10,
        2.2855e+09, 4.7000e+09, 1.1769e+10, 3.8194e+09, 2.9821e+10, 2.5415e+09,
        7.5553e+09, 2.5950e+09, 1.3967e+10, 8.1342e+09, 2.8329e+09, 3.0965e+09,
        4.3632e+09, 1.9151e+09, 5.9606e+09, 6.8139e+09, 1.2707e+08, 2.7465e+09,
        1.1638e+09, 6.2393e+09, 1.0669e+09, 1.0382e+10, 1.6895e+10, 2.4786e+09,
        8.7841e+09, 7.9609e+09, 3.9871e+08, 3.1864e+09, 1.1792e+11, 1.2435e+10,
        8.2440e+08, 4.3893e+09, 9.9321e+09, 9.6306e+08, 5.8050e+09, 2.4121e+09,
        2.1927e+09, 6.9494e+09, 5.9671e+09, 9.6135e+09, 4.5171e+09, 2.3872e+09,
        6.7704e+09, 6.7036e+09, 6.2228e+08, 5.0963e+10, 3.7771e+09, 7.5022e+08,
        1.0496e+10, 6.4758e+08, 3.0863e+09, 1.1205e+10, 9.6002e+08, 2.1554e+09,
        1.8039e+11, 8.2203e+08, 8.4538e+09, 6.6946e+09, 2.1135e+10, 2.0048e+09,
        1.1762e+09, 6.8961e+09, 7.5267e+09, 5.5860e+09, 3.6355e+08, 6.1273e+10,
        1.5195e+10, 8.6585e+09, 3.4943e+09, 4.8685e+09, 3.8245e+09, 9.1615e+08,
        2.4046e+09, 5.8131e+09, 5.1555e+09, 2.7597e+09, 9.1838e+09, 2.3273e+09,
        2.7638e+09, 4.7308e+09, 6.1594e+09, 1.2079e+09, 5.9653e+09, 8.7575e+09,
        1.4175e+09, 5.7776e+10, 2.7894e+09, 7.2430e+09, 2.7041e+09, 7.1312e+09,
        4.7419e+09, 8.5696e+09, 9.8040e+08, 2.8064e+09, 4.5394e+09, 6.8159e+09,
        1.5003e+09, 4.3293e+09, 6.3006e+10, 2.2071e+10, 6.6811e+09, 4.4063e+09,
        1.0177e+10, 2.0433e+09, 1.1331e+10, 1.1799e+09, 3.3187e+10, 9.0492e+08,
        1.0745e+09, 1.9865e+11, 5.5817e+09, 3.0127e+09, 8.4674e+09, 2.8793e+09,
        1.0155e+09, 1.2007e+09, 8.2780e+09, 1.0736e+10, 4.1126e+09, 2.8970e+09,
        9.3455e+09, 1.1161e+10, 7.1033e+09, 2.8114e+09, 9.1457e+09, 1.2386e+10,
        1.1741e+10, 1.0637e+10, 4.6501e+08, 3.1840e+09, 7.0111e+08, 1.4226e+09,
        7.4320e+08, 1.2937e+09, 1.3364e+10, 8.7881e+09, 1.2569e+10, 1.1496e+10,
        2.6879e+09, 1.2248e+09, 4.2322e+09, 3.1065e+09, 1.3485e+10, 1.3883e+10,
        1.7062e+11, 2.2791e+09, 1.1332e+09, 5.0153e+09, 1.4667e+09, 1.6327e+09,
        4.9307e+09, 3.2260e+10, 1.2391e+09, 1.8722e+09, 3.8159e+09, 2.8924e+09,
        1.3250e+10, 9.0203e+08])), ('encoder.6.0.conv1', tensor([4.7442e+09, 1.7817e+10, 1.8603e+09, 2.3550e+10, 3.8930e+09, 8.5960e+09,
        4.0611e+09, 5.3770e+09, 2.5121e+09, 1.2748e+10, 4.4603e+09, 8.3415e+09,
        5.4596e+09, 2.0480e+10, 1.5530e+10, 9.3761e+09, 7.3532e+09, 5.2622e+09,
        9.5002e+09, 9.2264e+09, 1.0354e+10, 1.4786e+10, 8.1823e+09, 3.0852e+09,
        7.0758e+09, 5.3523e+09, 6.1356e+09, 5.3959e+09, 2.4482e+09, 1.9349e+10,
        1.6749e+10, 5.7506e+09, 2.2809e+10, 1.3733e+10, 9.9938e+09, 7.5052e+09,
        6.0105e+09, 7.2093e+09, 2.4017e+10, 9.7701e+09, 5.0264e+09, 2.7958e+09,
        5.4242e+09, 3.4290e+09, 1.9551e+10, 1.4243e+10, 1.3667e+10, 1.5860e+09,
        2.1316e+09, 3.2691e+09, 3.9086e+09, 1.3117e+10, 4.9456e+09, 7.1826e+09,
        1.5116e+10, 1.1515e+10, 1.1435e+10, 7.7108e+09, 1.2661e+10, 2.0099e+10,
        3.9167e+09, 1.0251e+10, 8.1124e+09, 2.6919e+10, 6.7992e+09, 1.4383e+10,
        5.0011e+09, 1.5649e+10, 2.2082e+10, 3.8719e+09, 2.0439e+09, 6.0647e+09,
        5.6852e+09, 3.6169e+09, 1.4620e+09, 1.1445e+10, 1.3302e+10, 7.6646e+09,
        1.1971e+10, 6.8997e+09, 6.2478e+09, 1.2730e+10, 5.5585e+09, 1.7741e+10,
        1.0934e+10, 1.0491e+10, 1.1793e+10, 6.9024e+09, 5.0897e+09, 3.3512e+09,
        3.4348e+09, 2.5285e+09, 1.6557e+10, 2.7335e+09, 7.2815e+09, 5.3533e+09,
        6.8525e+09, 1.3591e+10, 1.0359e+10, 4.3032e+09, 4.1098e+09, 3.1094e+09,
        4.2408e+09, 4.6509e+09, 8.7981e+09, 1.6636e+10, 2.3590e+10, 1.6367e+10,
        4.6783e+09, 1.2684e+10, 4.4543e+09, 3.8485e+09, 5.3179e+09, 3.6550e+09,
        1.6547e+10, 2.9236e+09, 9.4013e+09, 1.2935e+10, 6.6535e+09, 1.4597e+10,
        5.2737e+09, 2.0422e+09, 7.2209e+09, 6.9515e+09, 1.6381e+10, 1.6958e+10,
        1.4599e+10, 1.6781e+10, 1.0299e+10, 1.7433e+10, 1.7535e+10, 9.2400e+09,
        1.9234e+10, 4.5005e+09, 3.2828e+09, 1.9897e+10, 1.4256e+10, 7.7594e+09,
        9.5184e+09, 1.5981e+10, 4.1517e+09, 1.3431e+10, 4.1616e+09, 5.6567e+09,
        2.3513e+09, 1.5490e+10, 1.4604e+10, 9.1337e+09, 1.3753e+10, 9.7072e+09,
        7.7915e+09, 6.0476e+09, 2.8563e+09, 6.0095e+09, 6.6627e+09, 2.1643e+10,
        3.0621e+10, 3.3107e+09, 6.5941e+09, 2.8898e+09, 1.8204e+10, 3.4627e+09,
        1.6920e+10, 1.0188e+10, 1.0830e+10, 7.4423e+09, 1.7636e+10, 7.7470e+09,
        9.0907e+09, 1.0295e+10, 8.7734e+09, 1.0237e+10, 1.2599e+10, 8.2660e+09,
        2.1008e+10, 6.3539e+09, 8.3826e+09, 2.1376e+10, 6.1604e+09, 5.5339e+09,
        1.7187e+10, 1.7151e+10, 6.6329e+09, 2.3654e+10, 1.0683e+10, 1.8012e+09,
        7.4611e+09, 1.0214e+10, 5.3420e+09, 9.4455e+09, 1.3724e+10, 2.4175e+09,
        1.2793e+09, 1.1164e+10, 1.2681e+10, 1.0430e+10, 1.2337e+10, 4.6862e+09,
        6.6917e+09, 4.7404e+09, 2.0977e+09, 1.2649e+10, 2.9854e+09, 1.2022e+10,
        2.6215e+09, 1.4094e+10, 1.4422e+10, 2.1346e+09, 5.7265e+09, 5.6732e+09,
        8.2506e+09, 2.8352e+09, 4.0578e+09, 4.2731e+09, 1.5412e+10, 3.8514e+09,
        5.0201e+09, 1.2326e+10, 2.6250e+10, 3.1889e+09, 8.0321e+09, 1.6121e+10,
        1.8907e+09, 1.0310e+10, 4.2385e+09, 1.0745e+09, 1.2105e+10, 9.9892e+09,
        1.0131e+10, 8.5661e+09, 7.3205e+09, 9.4326e+09, 1.2856e+10, 1.3267e+10,
        4.8886e+09, 2.0886e+10, 1.8753e+10, 1.1504e+09, 3.2428e+09, 9.9887e+09,
        4.5997e+09, 8.1562e+09, 7.7351e+09, 1.7822e+10, 2.9414e+09, 2.2865e+09,
        7.0039e+09, 2.0298e+10, 9.5358e+09, 6.7687e+09, 3.6603e+09, 6.1353e+09,
        6.6147e+09, 4.8275e+09, 1.4948e+10, 1.2435e+10])), ('encoder.6.0.conv2', tensor([1.5647e+10, 1.5908e+10, 1.6170e+10, 1.5548e+10, 1.1780e+10, 1.0887e+10,
        8.1589e+09, 2.0568e+10, 2.7561e+10, 1.9077e+10, 1.4678e+10, 4.9084e+09,
        1.2618e+10, 1.1114e+10, 6.5842e+09, 3.5590e+09, 3.7869e+10, 3.2157e+09,
        1.2458e+10, 5.1543e+09, 3.4675e+10, 5.3451e+09, 2.1271e+10, 1.0078e+10,
        8.4215e+09, 8.4731e+09, 1.8498e+10, 1.5888e+10, 1.6987e+10, 6.4787e+09,
        1.9143e+10, 1.0955e+10, 1.2589e+10, 3.0251e+10, 2.1699e+10, 3.1230e+09,
        6.7863e+09, 2.4676e+10, 5.9392e+09, 1.2707e+10, 5.5757e+09, 1.9878e+10,
        1.3430e+10, 1.1816e+10, 9.3189e+09, 1.0701e+10, 3.6324e+09, 1.6114e+10,
        2.7408e+10, 1.4001e+10, 1.3991e+10, 3.7560e+10, 1.7972e+10, 1.9009e+10,
        1.3580e+10, 1.8421e+10, 4.1600e+09, 1.9090e+10, 1.2538e+10, 5.2121e+09,
        1.1693e+10, 1.1658e+10, 1.9531e+10, 2.5139e+10, 8.5730e+09, 2.2509e+10,
        3.5347e+10, 5.2169e+09, 4.1510e+10, 3.0276e+10, 3.9812e+10, 5.8091e+09,
        4.4859e+09, 7.7405e+09, 1.5580e+10, 1.3288e+10, 1.4040e+10, 1.0536e+10,
        3.3502e+09, 3.4297e+09, 2.0058e+10, 2.6488e+10, 3.5381e+10, 1.0469e+10,
        1.0042e+10, 1.2430e+10, 1.9289e+10, 2.3552e+10, 1.5837e+10, 1.8056e+10,
        4.3992e+09, 1.5002e+10, 6.3535e+09, 7.4395e+09, 5.3637e+09, 2.3938e+10,
        6.8598e+09, 1.2501e+10, 9.1757e+09, 8.4526e+09, 1.5879e+10, 1.0139e+10,
        2.0121e+10, 1.3432e+10, 1.6000e+10, 1.0581e+10, 4.5320e+09, 1.4560e+10,
        7.8952e+09, 1.2087e+10, 1.2915e+10, 1.8533e+10, 2.2412e+10, 9.7543e+09,
        1.8158e+10, 1.1840e+10, 1.5641e+10, 1.2484e+10, 8.6099e+09, 2.4354e+10,
        2.1867e+10, 1.1045e+10, 2.7840e+10, 1.1627e+10, 1.4314e+10, 1.6465e+10,
        5.5983e+09, 2.4279e+10, 3.3292e+09, 1.8565e+10, 1.5683e+10, 1.3373e+10,
        4.1361e+10, 3.2691e+10, 1.0243e+10, 1.5781e+10, 7.8313e+09, 2.9235e+10,
        1.8591e+09, 2.1587e+10, 3.3851e+10, 1.6225e+10, 1.5745e+10, 9.5995e+09,
        1.1339e+10, 6.5591e+09, 2.1620e+10, 7.4422e+09, 4.5019e+09, 9.6717e+09,
        1.6383e+10, 7.4789e+09, 3.3673e+10, 7.5758e+09, 8.7613e+09, 1.7562e+10,
        2.3769e+09, 1.7078e+10, 1.5132e+10, 6.6646e+09, 2.4759e+10, 6.0597e+09,
        1.0222e+10, 4.1104e+09, 7.8779e+09, 4.6077e+10, 1.5268e+10, 1.6840e+10,
        3.0219e+10, 7.3950e+09, 1.3005e+10, 1.3458e+10, 1.2547e+10, 1.3828e+10,
        5.6139e+09, 7.5810e+09, 1.5223e+10, 7.1111e+09, 8.4269e+09, 8.7839e+09,
        8.7675e+09, 1.7615e+10, 3.3615e+10, 3.9209e+10, 6.0101e+09, 1.5000e+10,
        5.8829e+09, 1.1489e+10, 1.3673e+10, 1.1630e+10, 2.2738e+10, 5.1742e+09,
        1.3168e+10, 1.0727e+10, 2.0972e+10, 3.1163e+10, 2.3363e+10, 7.0321e+09,
        1.4032e+10, 7.4219e+09, 1.8426e+10, 2.2439e+10, 1.1241e+10, 1.4151e+10,
        3.8995e+10, 2.2204e+10, 5.9830e+09, 2.2284e+10, 1.6767e+10, 1.8389e+10,
        1.0806e+10, 3.6189e+10, 6.5360e+09, 1.1191e+10, 1.5184e+10, 2.9185e+10,
        1.1850e+10, 5.8354e+09, 3.5817e+09, 8.4091e+09, 2.0926e+10, 3.7266e+09,
        2.4088e+10, 2.1643e+10, 2.4624e+10, 1.5623e+10, 3.8502e+10, 6.4178e+09,
        5.3954e+10, 1.5676e+10, 2.5701e+10, 1.0781e+10, 4.2867e+10, 2.3617e+10,
        6.4262e+09, 6.3291e+09, 2.2230e+10, 4.3125e+10, 1.7271e+10, 6.9763e+09,
        6.2105e+09, 1.6986e+10, 6.9139e+09, 1.3929e+10, 6.5159e+09, 1.2944e+10,
        1.2820e+10, 1.0448e+10, 1.2533e+10, 4.1849e+09, 1.7152e+10, 1.2368e+10,
        3.9134e+10, 1.2709e+10, 2.5025e+10, 1.9287e+10])), ('encoder.6.0.conv3', tensor([2.9634e+07, 3.7422e+07, 4.3639e+08,  ..., 9.8253e+07, 3.3843e+07,
        1.1345e+07])), ('encoder.6.0.downsample.0', tensor([4.9683e+07, 6.5885e+07, 2.3609e+08,  ..., 3.8622e+08, 9.2630e+08,
        8.5680e+07])), ('encoder.6.1.conv1', tensor([1.1493e+09, 2.0863e+09, 5.0901e+08, 8.2955e+08, 1.5191e+09, 6.1642e+08,
        7.6596e+08, 1.5558e+09, 1.2468e+09, 7.5021e+07, 3.8384e+08, 2.4673e+08,
        1.4989e+09, 7.1440e+08, 1.5597e+08, 9.4168e+08, 7.0912e+08, 1.6326e+08,
        1.1564e+09, 8.7366e+07, 2.1145e+08, 4.1209e+08, 4.6154e+08, 1.2333e+09,
        1.3086e+08, 1.4356e+09, 3.8191e+08, 1.1875e+08, 3.9730e+08, 9.5169e+08,
        5.6038e+08, 1.4325e+09, 1.3752e+09, 1.8336e+08, 1.0536e+09, 9.1507e+08,
        7.7100e+08, 3.2961e+08, 8.6883e+08, 4.7106e+08, 7.1296e+08, 2.8098e+09,
        1.2475e+09, 6.4053e+07, 8.8752e+08, 1.1834e+09, 9.2208e+08, 3.0321e+08,
        4.2536e+08, 1.2584e+09, 1.4490e+09, 5.5637e+08, 4.9301e+08, 1.6740e+08,
        6.6652e+08, 1.9138e+09, 1.7093e+09, 3.2780e+08, 3.1231e+08, 2.8180e+08,
        6.6076e+08, 6.9866e+08, 3.0333e+08, 1.2049e+08, 9.0018e+08, 1.5218e+09,
        1.9006e+09, 4.5137e+08, 2.6159e+08, 2.0639e+09, 1.3122e+09, 5.3513e+08,
        4.7418e+08, 7.7618e+08, 1.4666e+08, 4.1769e+08, 5.3504e+08, 5.6000e+08,
        7.2644e+08, 7.3843e+08, 1.3486e+09, 9.7957e+08, 6.1114e+08, 4.4392e+08,
        1.1932e+09, 1.2758e+09, 2.6258e+08, 5.9030e+08, 4.3846e+08, 2.4569e+09,
        4.9754e+08, 4.2611e+08, 6.7727e+08, 1.3762e+08, 3.7167e+08, 6.8743e+08,
        1.9783e+09, 6.3131e+08, 1.0267e+09, 2.5819e+09, 4.8427e+08, 2.3118e+08,
        1.5538e+08, 9.3294e+08, 7.6500e+08, 4.2658e+08, 9.3155e+08, 7.5964e+08,
        6.3563e+08, 4.5892e+08, 8.4865e+08, 6.7340e+07, 1.0553e+08, 3.1570e+08,
        4.3131e+08, 1.4759e+09, 1.1971e+09, 1.4845e+09, 4.2918e+08, 3.1552e+08,
        4.4798e+08, 3.0170e+08, 1.2626e+09, 2.4350e+08, 1.9996e+08, 6.2835e+08,
        6.3772e+08, 7.6573e+08, 1.5203e+09, 8.1277e+08, 7.0866e+08, 1.4624e+09,
        1.3156e+09, 1.1137e+08, 7.2823e+08, 5.7457e+08, 9.4149e+08, 1.8923e+08,
        4.5368e+08, 6.6606e+08, 2.7205e+08, 1.7726e+09, 1.8138e+08, 7.0053e+08,
        9.0071e+08, 1.5730e+09, 4.9340e+08, 8.7839e+08, 3.5758e+08, 1.4868e+08,
        8.1293e+08, 5.9534e+08, 3.4717e+08, 1.8737e+08, 8.7630e+08, 4.6943e+08,
        1.5936e+09, 8.3822e+08, 1.1981e+09, 1.2547e+09, 6.0278e+08, 3.1140e+08,
        7.0564e+08, 1.3256e+09, 2.5627e+08, 6.8901e+07, 4.0381e+08, 5.8867e+08,
        3.3910e+08, 1.3038e+09, 6.4709e+08, 8.8294e+08, 1.5065e+09, 1.6883e+09,
        3.0394e+08, 1.0234e+09, 6.2121e+08, 1.4699e+08, 5.9556e+08, 3.8405e+08,
        7.2498e+08, 1.7494e+09, 1.2174e+09, 7.4318e+08, 4.9052e+08, 2.2347e+08,
        4.5947e+08, 2.5318e+08, 1.7379e+09, 3.8935e+08, 6.0969e+08, 8.4808e+08,
        9.1497e+08, 2.2439e+08, 4.8243e+08, 3.7348e+08, 4.8386e+08, 1.5001e+09,
        1.4083e+09, 1.5487e+09, 3.4436e+08, 7.9776e+08, 3.5948e+08, 1.7856e+09,
        1.4219e+08, 6.3817e+08, 1.0383e+08, 1.0963e+09, 1.7658e+09, 3.7898e+08,
        1.4743e+09, 3.2881e+08, 1.3567e+09, 6.1489e+08, 3.3481e+08, 3.5791e+08,
        1.6899e+08, 4.2432e+08, 1.5312e+09, 5.2622e+08, 9.8836e+08, 2.3058e+08,
        1.0921e+09, 1.1710e+09, 5.0384e+08, 1.8016e+09, 1.1066e+09, 5.5018e+08,
        8.4124e+07, 7.0669e+08, 3.4564e+08, 1.9894e+08, 6.1802e+08, 4.4603e+08,
        6.6393e+08, 6.7849e+08, 7.2611e+08, 2.7799e+08, 1.3955e+09, 1.3081e+09,
        7.5148e+08, 4.1579e+08, 1.0490e+09, 8.2880e+08, 1.1361e+08, 1.3100e+09,
        7.7341e+07, 6.8079e+08, 2.9853e+08, 1.5331e+08, 1.5812e+09, 5.5681e+08,
        9.3080e+08, 1.2952e+09, 1.4122e+09, 5.4083e+08])), ('encoder.6.1.conv2', tensor([2.8947e+09, 3.2423e+09, 3.9822e+09, 9.5211e+08, 2.9446e+09, 2.6656e+09,
        2.2522e+09, 4.3133e+08, 2.2830e+09, 4.7042e+09, 1.5906e+09, 4.1649e+09,
        6.1692e+08, 6.3530e+08, 8.6800e+08, 3.0365e+08, 2.4629e+09, 7.6868e+08,
        1.6466e+09, 2.0762e+09, 9.5716e+08, 1.3093e+09, 3.1401e+09, 8.2924e+08,
        1.8452e+09, 2.6111e+09, 1.0071e+09, 8.7840e+08, 3.1032e+08, 5.9963e+08,
        6.9603e+08, 2.8423e+09, 1.1543e+09, 1.8772e+09, 2.1134e+09, 1.7823e+09,
        1.4351e+09, 4.8112e+09, 1.3511e+08, 9.2052e+08, 4.0968e+08, 2.7163e+09,
        3.3159e+09, 4.9888e+09, 1.3813e+09, 6.2495e+08, 1.8928e+09, 9.7818e+08,
        1.1463e+09, 1.3764e+09, 3.9044e+09, 3.0443e+09, 1.0832e+09, 3.5493e+09,
        2.1438e+09, 1.0008e+09, 9.3838e+08, 2.5306e+09, 1.6328e+09, 6.1556e+08,
        1.0460e+09, 5.8950e+08, 9.2513e+08, 9.7114e+08, 2.8779e+09, 1.5981e+09,
        1.1376e+09, 2.9808e+08, 1.4719e+09, 2.2171e+09, 1.9368e+09, 4.5809e+08,
        4.9000e+09, 1.4252e+09, 3.2320e+09, 2.7644e+09, 1.1663e+09, 5.5379e+08,
        1.2704e+09, 5.6571e+08, 2.1802e+08, 1.9549e+09, 1.6217e+09, 2.0372e+09,
        1.0763e+09, 3.4585e+08, 9.5437e+08, 1.4083e+09, 4.0087e+08, 4.0067e+09,
        5.8348e+08, 2.6149e+08, 2.7018e+09, 3.2085e+09, 7.1406e+08, 3.3982e+09,
        3.5466e+09, 2.9164e+09, 2.3343e+08, 1.6164e+09, 1.2682e+09, 4.1161e+08,
        2.9160e+08, 9.0510e+08, 1.9183e+08, 4.8051e+08, 1.7954e+09, 2.0328e+09,
        1.8678e+09, 1.4003e+09, 6.0928e+08, 1.6298e+08, 4.8578e+08, 2.4769e+09,
        3.0063e+09, 2.0699e+09, 1.3130e+09, 1.4716e+09, 6.2601e+08, 3.7869e+09,
        5.9457e+09, 3.5175e+09, 1.0012e+09, 1.3722e+09, 2.9397e+08, 4.9754e+08,
        4.8011e+09, 1.5357e+09, 1.0434e+09, 6.7207e+08, 3.9236e+08, 7.3799e+08,
        1.2685e+09, 1.0452e+09, 2.0427e+09, 1.9673e+09, 4.4936e+08, 2.8198e+09,
        1.7589e+09, 1.5327e+09, 2.5368e+09, 3.0952e+09, 2.3174e+09, 1.8197e+09,
        6.8330e+08, 2.7210e+09, 8.9432e+08, 1.0275e+09, 1.2937e+09, 1.3059e+09,
        1.2634e+09, 4.3047e+09, 1.2442e+09, 2.5815e+09, 1.8164e+09, 2.4130e+08,
        8.8007e+08, 2.5970e+08, 2.2690e+09, 1.4694e+09, 1.5456e+08, 3.8771e+09,
        1.1761e+09, 1.4593e+09, 3.4908e+09, 8.0071e+08, 1.9746e+09, 6.6473e+08,
        1.7170e+09, 9.8856e+08, 1.3986e+09, 2.5885e+09, 1.7078e+09, 2.3304e+09,
        3.9226e+09, 1.8361e+09, 7.9293e+08, 1.1326e+09, 1.0152e+09, 5.5485e+08,
        2.7571e+09, 1.2429e+09, 3.5571e+09, 4.4583e+08, 1.2819e+09, 4.5770e+09,
        2.8700e+09, 1.8382e+09, 3.1830e+08, 2.6037e+09, 3.6790e+09, 7.8636e+08,
        1.6122e+09, 1.8304e+09, 3.7298e+08, 3.2207e+09, 3.3521e+09, 1.7881e+09,
        1.5715e+09, 4.2512e+09, 1.3366e+09, 1.2073e+09, 3.3012e+09, 5.9818e+08,
        2.3613e+09, 1.4076e+09, 3.0363e+09, 2.4720e+09, 1.1716e+09, 5.4364e+08,
        1.1354e+09, 4.4753e+08, 7.9083e+08, 6.5705e+08, 1.3212e+09, 1.0703e+09,
        7.3969e+08, 6.4746e+08, 1.9611e+08, 1.3000e+08, 1.8678e+09, 7.3507e+08,
        1.8384e+09, 2.9205e+08, 2.9999e+09, 1.1814e+09, 1.5806e+09, 2.1297e+09,
        2.0746e+08, 1.2041e+09, 3.0541e+09, 3.9654e+09, 1.7113e+09, 1.7480e+09,
        1.1573e+09, 1.4549e+09, 7.0498e+08, 5.9962e+08, 2.8692e+08, 2.9429e+08,
        2.4754e+09, 4.2023e+08, 4.0006e+08, 9.5685e+08, 2.1978e+09, 4.7305e+09,
        2.5898e+09, 2.2019e+09, 4.2953e+09, 1.0813e+09, 3.0615e+08, 1.3106e+08,
        1.5582e+09, 1.6240e+09, 3.8081e+08, 2.3252e+09])), ('encoder.6.1.conv3', tensor([6576852.5000, 4484106.0000, 4148861.2500,  ..., 7000342.5000,
        4341533.5000, 4537876.0000])), ('encoder.6.2.conv1', tensor([16034983.0000, 17466978.0000,  8954526.0000, 20185046.0000,
         9779728.0000, 13677611.0000, 12726552.0000, 10138397.0000,
        15536695.0000, 13145756.0000,  9247293.0000, 13511160.0000,
        11173430.0000, 10402017.0000, 12657198.0000, 14677674.0000,
        17068536.0000,  9870653.0000, 14191294.0000, 14044079.0000,
        12375866.0000, 13344132.0000, 15413450.0000, 12053161.0000,
        14461882.0000, 11654406.0000, 15290078.0000, 15560062.0000,
        13996884.0000,  9470907.0000, 16960488.0000, 16119774.0000,
        12956908.0000, 10385204.0000, 11304752.0000, 12169575.0000,
        15117727.0000, 16492781.0000, 15361164.0000, 19182536.0000,
        13491005.0000, 11051763.0000, 17715330.0000, 11401442.0000,
        15156190.0000, 15803184.0000, 10403684.0000, 14311471.0000,
        12223766.0000, 16268574.0000, 13703420.0000, 16252360.0000,
        14286931.0000,  9585887.0000, 17006782.0000, 15875756.0000,
        11006932.0000, 12362345.0000, 14097531.0000, 12822163.0000,
        14657388.0000, 14597866.0000, 16788778.0000, 14718688.0000,
        11843918.0000, 15360248.0000, 14116257.0000, 12253472.0000,
         9612027.0000, 10711397.0000, 13793478.0000, 15461882.0000,
        13745394.0000, 14491338.0000, 13084292.0000, 11922440.0000,
        12611041.0000, 12558973.0000,  8251409.5000, 15895433.0000,
        12284799.0000, 10571374.0000, 15833554.0000, 10098401.0000,
        10965605.0000, 13812383.0000, 15310079.0000, 20132980.0000,
        15030361.0000, 12514733.0000, 14874084.0000, 16579041.0000,
         9630587.0000, 16161885.0000, 16681842.0000, 14233644.0000,
        14792244.0000, 12447513.0000, 10561272.0000,  9085677.0000,
        13409584.0000, 10373280.0000, 11015327.0000, 18997352.0000,
        10780228.0000, 11931023.0000, 13654949.0000,  9189343.0000,
        14181781.0000, 12918659.0000, 13475446.0000, 14582415.0000,
        12736896.0000, 16651327.0000, 23073244.0000,  9700119.0000,
        12321986.0000, 20609736.0000, 13603391.0000,  9942572.0000,
        15928700.0000, 15726529.0000, 14166188.0000, 13425070.0000,
        11466977.0000, 10927670.0000, 12833569.0000, 10757558.0000,
         9926945.0000, 12558852.0000, 12586395.0000,  8621323.0000,
        15613087.0000, 12634149.0000, 16881994.0000, 10776885.0000,
        19002924.0000, 13239182.0000, 16529834.0000, 14037869.0000,
        17444506.0000, 11921283.0000, 11350355.0000, 11701372.0000,
        11887858.0000, 12031944.0000, 14175424.0000,  9977756.0000,
        13091203.0000, 10677050.0000, 13424499.0000, 17409938.0000,
         9877431.0000, 11459860.0000, 16468564.0000, 11736231.0000,
        10667581.0000, 13513334.0000, 15252026.0000, 10688618.0000,
        15280753.0000, 20063850.0000, 13780631.0000, 11231963.0000,
        12879951.0000, 15191597.0000, 20388228.0000,  9085836.0000,
        10341576.0000, 17506116.0000, 14652919.0000, 13201158.0000,
        13273901.0000, 16515153.0000, 11263305.0000, 12061863.0000,
        15670809.0000,  9789337.0000, 13603661.0000, 16184452.0000,
        13483880.0000, 12373401.0000, 16920040.0000, 11645254.0000,
        13559797.0000, 12739156.0000,  9668785.0000,  9717434.0000,
        12131615.0000, 12974957.0000, 12912737.0000, 12615417.0000,
        14200044.0000, 18061802.0000, 12620651.0000, 11282124.0000,
        18797260.0000, 16637924.0000, 12841802.0000, 10137545.0000,
        12633707.0000, 17257564.0000, 16114136.0000, 10053126.0000,
        12017656.0000, 16735362.0000, 16647038.0000, 14538823.0000,
        21336814.0000, 12375369.0000, 12596741.0000, 21804822.0000,
        14401662.0000, 12090341.0000,  9108432.0000, 17083870.0000,
        16049677.0000, 14239127.0000, 13971164.0000, 15495866.0000,
        16604687.0000, 10785565.0000, 14901345.0000, 15767218.0000,
        13480599.0000, 10280738.0000, 15627968.0000, 17595614.0000,
        12937991.0000, 12298456.0000, 15186230.0000, 12981942.0000,
        18235782.0000, 12966354.0000, 12473576.0000, 12895194.0000,
        10738116.0000, 12587555.0000, 16849700.0000, 13297841.0000,
         9605243.0000, 10873312.0000, 12914798.0000, 17042980.0000,
        11884018.0000,  9978182.0000, 12288698.0000, 11299398.0000,
        12910069.0000, 12853382.0000, 11742000.0000, 10946547.0000,
        19127406.0000, 11425301.0000, 12131465.0000, 10076581.0000])), ('encoder.6.2.conv2', tensor([43097484., 32518168., 45709232., 48172628., 38756892., 33447968.,
        25666314., 38421804., 32758270., 32486800., 31800002., 28964140.,
        35487732., 37393780., 33635548., 29996342., 46584544., 31159522.,
        39329032., 56237028., 34513180., 32331626., 36283732., 50380904.,
        31976258., 34637896., 37664072., 32260974., 41236352., 38467032.,
        39369320., 46408652., 42533500., 33910956., 28462582., 41026136.,
        51794728., 36994300., 38352160., 37448176., 30021464., 37014728.,
        38915932., 50755408., 29680188., 34156920., 42727332., 41471968.,
        40985376., 50200632., 39949172., 34468216., 29219426., 44161568.,
        33045156., 35876432., 59282080., 25672920., 35583132., 28479682.,
        41091080., 31908740., 43672736., 41508124., 36722680., 38695680.,
        29772946., 48341908., 36898352., 44089720., 40927928., 48439428.,
        32500596., 39294312., 39435432., 39934568., 33053010., 36767920.,
        47389316., 52242648., 44349728., 39135672., 32675292., 30010384.,
        42807704., 34244892., 42597328., 42586320., 50541016., 28592330.,
        38313336., 58130760., 26327820., 33500858., 35596212., 51562740.,
        42409124., 37335264., 39673280., 29188460., 35813336., 37275384.,
        31389288., 26698582., 44404344., 43979616., 38719876., 31338552.,
        34284712., 34940564., 27833752., 53935300., 30491264., 43381952.,
        39581264., 50002156., 41343492., 28177064., 29777342., 40494848.,
        38322288., 34514840., 34290156., 33048224., 44011264., 43178708.,
        35577716., 35111744., 28702216., 37571132., 29886626., 32259074.,
        43219524., 40344380., 40090740., 24906424., 32190012., 37449868.,
        34119236., 44491712., 38591584., 44426436., 34580740., 28607718.,
        31425968., 35935684., 35658104., 37251428., 51782712., 51306164.,
        45650344., 35722668., 37364332., 31587046., 46184268., 33312690.,
        30471842., 37711264., 38830640., 29863144., 48728132., 31709346.,
        33031884., 26758464., 39142512., 40679324., 37588420., 45563916.,
        46836588., 26507520., 41299232., 33260224., 33989464., 44509500.,
        36736028., 29935218., 42081584., 44117656., 42874036., 29133106.,
        31096176., 49200296., 31541124., 31267556., 40107800., 44357168.,
        28064322., 34430060., 56026596., 33953060., 35118092., 45244396.,
        58721584., 34487988., 53605864., 60021368., 49494348., 32221914.,
        33162240., 44007044., 58626784., 33786264., 47648188., 41665228.,
        48691624., 38947000., 33056516., 35324628., 35934468., 42166108.,
        46341456., 34007584., 38814220., 29224436., 53026732., 34641416.,
        31599988., 48099156., 38022700., 25343080., 33695864., 30678574.,
        50038052., 44454588., 30098444., 37811232., 41741788., 41216996.,
        33711960., 31634438., 48752616., 46517412., 55454512., 41125072.,
        31581444., 32694318., 34429388., 38358320., 46835400., 32665420.,
        33408878., 29441326., 34219960., 33365474., 40315296., 33801064.,
        32442494., 30601074., 29186192., 37928800., 36820016., 45922964.,
        36863200., 51166004., 43271636., 34902256.])), ('encoder.6.2.conv3', tensor([1209428.7500, 1381443.0000, 4952838.0000,  ..., 3394313.5000,
        1397687.6250, 1361260.1250])), ('encoder.6.3.conv1', tensor([3971064.7500, 4468497.0000, 3676265.2500, 3940946.5000, 4877441.5000,
        4522447.5000, 4962284.0000, 5094618.5000, 4772664.0000, 4286001.5000,
        3760692.0000, 3333885.0000, 4181930.0000, 5952282.0000, 4253936.5000,
        4680566.0000, 3613356.0000, 3701026.7500, 4344196.5000, 3817836.5000,
        4501802.5000, 4246140.0000, 4157496.2500, 5190362.0000, 4111049.2500,
        4504762.0000, 4729843.5000, 4074086.7500, 3581297.7500, 4564016.5000,
        6031070.0000, 4558910.0000, 4518987.0000, 3366809.7500, 4678944.5000,
        3201760.0000, 4573583.5000, 3611900.5000, 3853512.2500, 5595534.5000,
        4254706.0000, 4652733.5000, 4077691.2500, 4112029.5000, 3172284.0000,
        4962919.0000, 3448157.7500, 3485871.5000, 4206112.0000, 5081678.0000,
        4369018.5000, 5440866.0000, 3498206.7500, 3538311.0000, 5013048.0000,
        4385097.5000, 5303411.5000, 3571709.2500, 4316236.0000, 3968200.7500,
        5709151.5000, 4523501.5000, 5644565.5000, 3444648.5000, 6084855.0000,
        3725908.2500, 3445882.2500, 4764470.5000, 2606761.7500, 5085527.0000,
        4687264.0000, 4993967.0000, 5209314.0000, 3705668.5000, 5997120.5000,
        3644106.0000, 4050830.7500, 5847458.5000, 3417141.5000, 5869400.0000,
        3980118.7500, 4906476.5000, 3530744.0000, 4855831.0000, 4120405.5000,
        2897526.5000, 3567385.0000, 6137230.5000, 5033586.5000, 5321277.5000,
        5183380.5000, 4728276.0000, 4221904.5000, 4457618.0000, 3773782.7500,
        3582552.0000, 5610588.0000, 3450714.0000, 3887617.5000, 4064712.0000,
        4902105.5000, 3516400.7500, 2756477.2500, 4107942.2500, 4242045.0000,
        3022283.2500, 5225498.0000, 4307456.5000, 3970201.5000, 3593228.2500,
        3252602.7500, 3726390.2500, 5517416.5000, 3279538.7500, 4600018.0000,
        4739872.5000, 5132569.0000, 4899987.5000, 4955883.5000, 3888285.7500,
        4642009.0000, 3991861.7500, 4207356.5000, 4838128.5000, 5123875.0000,
        5396079.5000, 3948676.0000, 3607908.5000, 4498898.5000, 5266343.0000,
        4213636.0000, 3528545.2500, 4265262.5000, 3431015.5000, 3431515.7500,
        4598482.0000, 3640344.7500, 4822781.0000, 4355051.0000, 4857417.0000,
        4021120.7500, 5390102.5000, 3618568.0000, 3179225.0000, 3807188.2500,
        3891174.2500, 4266960.5000, 3739810.0000, 4023668.7500, 5643302.0000,
        3765207.5000, 4378982.0000, 3458029.2500, 3888157.2500, 3867762.5000,
        5848378.0000, 3364339.2500, 3635421.0000, 3453473.2500, 3362144.5000,
        5704477.0000, 4607823.0000, 3650948.0000, 2520414.7500, 3441583.5000,
        6835705.0000, 3510500.7500, 3210810.5000, 4057869.5000, 3371792.0000,
        3272890.7500, 3206162.7500, 4455048.0000, 4216246.0000, 3985424.2500,
        5570997.5000, 3775086.0000, 2882427.0000, 3832209.7500, 4039889.5000,
        3708720.2500, 4359528.5000, 5477215.0000, 5398447.0000, 5265635.5000,
        6034566.5000, 3543876.5000, 3307330.2500, 3850627.7500, 5150853.5000,
        4009608.0000, 3577115.0000, 4267275.5000, 5786770.5000, 3607305.7500,
        4546738.5000, 4843032.5000, 4498635.0000, 4474961.0000, 3837243.7500,
        4882427.5000, 6230549.5000, 4747198.0000, 4303058.5000, 4846588.5000,
        3319490.0000, 4122082.5000, 4649022.0000, 3529509.2500, 5729097.5000,
        4292119.0000, 4714960.5000, 4803493.0000, 5229099.0000, 4154451.7500,
        5338475.5000, 3397444.7500, 3225982.7500, 4581270.5000, 4302931.0000,
        5591036.5000, 4023640.7500, 3756557.2500, 3777130.7500, 3723978.2500,
        4281230.0000, 5300099.5000, 3279253.0000, 5050521.5000, 4437344.0000,
        2623742.5000, 3188526.2500, 4760866.5000, 5447783.0000, 2812902.7500,
        4015063.7500, 5040710.5000, 3792291.7500, 4076508.7500, 4403602.5000,
        4515993.5000, 4332424.0000, 6724047.0000, 3810956.0000, 5488517.0000,
        3939308.2500, 4547008.5000, 6384878.5000, 4081959.7500, 6246894.5000,
        3273745.2500, 3827639.5000, 4888879.0000, 4058867.5000, 4155508.7500,
        5296749.0000])), ('encoder.6.3.conv2', tensor([10384628.0000, 13950858.0000, 13333130.0000, 15803794.0000,
        10191700.0000, 13189224.0000, 17095904.0000, 13892946.0000,
        14350061.0000,  9877559.0000, 10244448.0000, 14235225.0000,
         7917004.0000, 11159336.0000,  8463193.0000, 11223273.0000,
        17421362.0000, 10055262.0000, 10349312.0000, 13724027.0000,
        12236170.0000,  8117153.5000, 16033557.0000, 13512655.0000,
        12046718.0000,  8975969.0000, 16805468.0000, 13641424.0000,
        10096193.0000, 12328220.0000,  9244685.0000, 10627621.0000,
        16336675.0000, 11311012.0000, 14724199.0000, 15253160.0000,
         9592411.0000, 12583128.0000, 12246521.0000, 12320104.0000,
        10905229.0000,  9661470.0000,  9542931.0000, 10345108.0000,
        14299214.0000, 14757575.0000, 10627689.0000, 10543660.0000,
         8868158.0000, 11729881.0000, 15278023.0000, 14306917.0000,
        14584041.0000, 16778894.0000, 16209965.0000, 11389805.0000,
        14394116.0000, 13911823.0000,  9475587.0000, 14011152.0000,
        11163605.0000, 11635596.0000, 13402760.0000, 12004292.0000,
        10422283.0000,  9621683.0000, 13654422.0000, 10675917.0000,
        15529079.0000, 14314840.0000, 10962501.0000, 10403392.0000,
        11601868.0000, 13334440.0000, 12690966.0000, 10990079.0000,
        16081028.0000, 12164005.0000, 12980181.0000, 14342251.0000,
        10200883.0000, 14979808.0000, 11882480.0000, 12174685.0000,
        14001602.0000,  9674670.0000, 15940091.0000, 18781376.0000,
        11892981.0000,  9383199.0000, 11908602.0000, 10724849.0000,
        15711982.0000, 12775835.0000, 16106164.0000,  9469479.0000,
        14326072.0000, 11496406.0000, 16204261.0000, 10173150.0000,
        10032593.0000, 12514332.0000, 11079364.0000, 12361027.0000,
        10062480.0000,  9649341.0000, 14377362.0000, 12710515.0000,
        15670145.0000, 11799980.0000, 13534230.0000,  9429743.0000,
        13876621.0000, 14288215.0000, 11537241.0000, 13180520.0000,
        11351577.0000, 11089935.0000, 13195386.0000, 12760881.0000,
        12933412.0000, 10310806.0000, 11661104.0000, 12985647.0000,
        12990158.0000, 15107404.0000, 20467310.0000, 13169778.0000,
        13628400.0000, 13452023.0000, 10129608.0000,  8818999.0000,
        18623602.0000, 14460474.0000, 13372310.0000, 10646827.0000,
        13490127.0000, 13597144.0000, 10045164.0000, 14398667.0000,
        13692948.0000, 11310677.0000, 11968500.0000, 10235546.0000,
        11242630.0000, 13836010.0000, 10277214.0000, 17233068.0000,
        11763683.0000, 10478709.0000,  9589669.0000, 10214503.0000,
        13642758.0000, 12292394.0000, 12895533.0000, 13233902.0000,
        13353284.0000, 14477313.0000,  9043986.0000, 16327320.0000,
        11802600.0000, 14587898.0000, 14116680.0000, 10898913.0000,
        14974615.0000, 13864890.0000, 15715467.0000, 11383974.0000,
        13702064.0000,  9236946.0000, 11181813.0000, 15678118.0000,
        13177989.0000, 12216984.0000, 16106510.0000, 11430039.0000,
        12933130.0000, 12759274.0000, 11471806.0000, 11840283.0000,
         9365838.0000,  9190781.0000, 10398311.0000, 10859581.0000,
        13424901.0000, 11055075.0000, 11545704.0000, 15950583.0000,
        15576649.0000, 14913642.0000, 17140326.0000,  9916505.0000,
        14646266.0000, 13914433.0000, 13422191.0000, 15705424.0000,
        15136594.0000,  9901729.0000, 12218194.0000, 10950108.0000,
        13343834.0000,  9646286.0000,  9194894.0000, 13620777.0000,
        15238528.0000, 12737259.0000, 11125509.0000, 12459798.0000,
        10288843.0000, 11497538.0000, 12307613.0000, 13306403.0000,
        10574108.0000, 13349044.0000, 12855272.0000,  8606782.0000,
        11423737.0000, 13249576.0000, 14355378.0000, 14071487.0000,
        14491252.0000, 11328369.0000,  8874083.0000, 13934141.0000,
        12160727.0000, 13589491.0000, 12149332.0000, 14930968.0000,
        13589871.0000, 13633677.0000, 14872609.0000, 14758946.0000,
        13019967.0000, 15804031.0000, 13426083.0000, 10274653.0000,
        15560305.0000,  9869239.0000, 11482879.0000, 10039226.0000,
        13186163.0000, 12216402.0000, 11207439.0000, 16079979.0000,
        13852296.0000, 14600708.0000, 12162676.0000, 13432835.0000,
        19888066.0000, 15675658.0000, 15442942.0000, 11234613.0000,
        15091604.0000, 19659828.0000, 12911475.0000, 12280010.0000])), ('encoder.6.3.conv3', tensor([1773430.6250,  188454.0156,  274021.3438,  ...,  114462.2266,
         185877.0781,  264852.9688])), ('encoder.6.4.conv1', tensor([678277.5625, 632371.1250, 638760.8750, 649987.6875, 744903.8125,
        806921.9375, 437209.7500, 685021.3125, 607453.7500, 441861.3125,
        561522.2500, 542851.0000, 716490.5000, 657024.6875, 546201.8125,
        618621.9375, 437582.7500, 432087.4688, 672427.1250, 716100.3750,
        568582.8125, 723239.6875, 703324.3750, 572868.9375, 640316.1875,
        462919.1562, 638403.7500, 555439.2500, 634862.8125, 848906.8125,
        622572.5000, 483947.0000, 795076.8750, 677109.5625, 458424.7188,
        612599.8125, 497816.2500, 603920.0625, 445335.6875, 586140.0000,
        516224.3438, 584429.4375, 483982.5625, 588645.6875, 629560.1250,
        680741.5625, 585264.1875, 643206.3750, 453565.6562, 544948.8125,
        603218.6875, 602697.6875, 845974.5625, 653176.2500, 603825.3750,
        586098.3750, 730670.1875, 461473.0000, 471521.2500, 493712.1875,
        817101.4375, 704040.6875, 637851.0000, 746621.0000, 611561.9375,
        595536.4375, 705793.1875, 712759.1250, 585553.3125, 629605.3125,
        698424.6875, 556966.7500, 715350.5000, 674072.3750, 699312.3750,
        546873.8750, 712350.5625, 542729.7500, 712407.8750, 625303.6250,
        726660.0625, 560005.7500, 615880.9375, 483107.5625, 667684.5000,
        507071.1562, 669788.6250, 471543.9688, 589793.2500, 653362.4375,
        451589.2188, 674611.2500, 730634.8750, 685678.0000, 600030.5625,
        621950.0625, 613333.2500, 521962.9688, 644050.1875, 510063.9375,
        482472.3438, 520795.7188, 709837.6250, 626334.6250, 580262.4375,
        608414.0000, 566534.2500, 568905.6250, 519568.5625, 766018.3125,
        526494.6250, 522467.2188, 536328.0000, 518527.1250, 688072.0000,
        544536.5625, 606844.3125, 501516.2812, 424104.8438, 695026.8125,
        830279.4375, 572729.5625, 647833.5625, 694126.5000, 554009.2500,
        570564.7500, 427201.3125, 676825.6875, 585269.5625, 640817.3125,
        687495.1875, 681824.2500, 745215.0625, 616085.9375, 719096.8750,
        528009.2500, 475098.1250, 529598.9375, 732961.0625, 702943.7500,
        606952.1875, 664456.7500, 581070.8125, 749830.5000, 709803.6250,
        548839.3125, 536119.8125, 697825.0625, 716646.8125, 539145.8750,
        555781.7500, 686529.3125, 841988.9375, 567051.8125, 646836.6875,
        685318.4375, 527472.3750, 430525.6562, 671262.4375, 711157.1875,
        566694.0625, 559949.8125, 510394.8125, 530682.7500, 621325.9375,
        512494.4688, 513592.6875, 638217.6875, 685292.1250, 456244.6562,
        595127.0000, 525858.1875, 620159.9375, 638200.0000, 649985.4375,
        525427.0000, 628625.1250, 648708.7500, 623428.8125, 564148.7500,
        485405.0625, 594150.3750, 717141.8125, 678323.7500, 690292.6250,
        678385.4375, 717539.1875, 636843.8750, 579408.5000, 659313.8125,
        591997.0000, 747625.5000, 564180.5000, 587140.8750, 629122.9375,
        591448.1875, 544664.9375, 643624.8125, 708432.8750, 601961.4375,
        576695.3125, 575253.0000, 699015.6250, 653925.4375, 636261.6250,
        620961.4375, 570823.6250, 636616.3125, 710538.6875, 476677.3125,
        558753.5625, 556356.8750, 653178.9375, 604735.3750, 685594.0000,
        486857.8438, 539265.8125, 610522.1250, 550267.6250, 688076.2500,
        456151.0938, 643877.6250, 801524.6875, 660476.5000, 808399.6875,
        666723.3125, 607493.6250, 536697.8125, 651546.8750, 536228.1875,
        587150.8750, 653043.0625, 568202.6875, 614966.0000, 449807.0000,
        615536.8750, 549872.9375, 501087.2500, 481977.8125, 622708.9375,
        601055.7500, 571084.3750, 559163.7500, 777917.2500, 659626.2500,
        565952.4375, 535235.2500, 623945.5000, 471929.5000, 659170.1875,
        516912.4688, 506598.7500, 754259.8750, 657355.8125, 539076.5625,
        769702.1250])), ('encoder.6.4.conv2', tensor([1724959.1250, 2154120.5000, 1812856.6250, 2157347.5000, 2078927.6250,
        1690896.5000, 1565810.6250, 1909294.1250, 1910942.6250, 2400947.2500,
        2123220.2500, 2253922.2500, 2177828.2500, 2114336.5000, 1887848.3750,
        2157702.0000, 2467214.7500, 2144216.0000, 1872148.6250, 2256657.2500,
        2079467.1250, 2030529.7500, 2113560.0000, 1867925.6250, 1983018.7500,
        1731732.3750, 2095912.3750, 2236351.5000, 1782474.1250, 1915556.2500,
        1765452.2500, 2205814.0000, 2287637.2500, 1865555.7500, 1697205.7500,
        1819083.1250, 1892735.8750, 1932732.1250, 1525804.7500, 1722959.2500,
        1953353.8750, 1967510.8750, 2277216.5000, 1926679.2500, 2338754.2500,
        2310649.5000, 2147620.0000, 2022852.2500, 2012100.5000, 2228592.2500,
        1875627.2500, 2328702.7500, 2066729.1250, 1954988.2500, 1792386.6250,
        1822110.6250, 2023777.0000, 2211199.5000, 1777397.2500, 2156881.7500,
        1890028.6250, 1824838.2500, 1698025.7500, 1875794.5000, 1812782.8750,
        2280137.5000, 1648534.3750, 2167870.0000, 2676481.5000, 1891136.5000,
        2135185.0000, 2169591.0000, 1670396.5000, 2465544.7500, 1666111.1250,
        2529834.0000, 1920796.0000, 1828109.0000, 2313707.2500, 2205996.0000,
        2805885.0000, 1851161.5000, 2116262.0000, 1770374.7500, 1533810.3750,
        2070074.2500, 1615375.2500, 2249060.7500, 2149689.5000, 1728095.5000,
        2057075.1250, 1696659.3750, 1832054.5000, 2002667.8750, 1489763.8750,
        1934227.7500, 2576150.5000, 2060827.1250, 1637236.2500, 1968458.7500,
        1896786.6250, 2092755.5000, 1778727.7500, 1734690.1250, 2439842.2500,
        1571050.0000, 1775902.7500, 1946461.3750, 1844720.8750, 2545907.2500,
        2101624.5000, 1611822.5000, 1450881.6250, 2094225.3750, 1954089.7500,
        1908191.3750, 1519513.5000, 2340293.5000, 2254580.5000, 1742860.2500,
        1819869.1250, 2171092.2500, 1879431.6250, 1806325.8750, 1704749.8750,
        1892894.3750, 1763482.3750, 1849955.1250, 2143037.7500, 2278327.7500,
        1796546.8750, 1471529.5000, 1561792.1250, 1728197.7500, 1509039.0000,
        2036446.0000, 2097340.0000, 1863962.1250, 2066410.5000, 2450372.2500,
        1829888.1250, 2660029.2500, 2062761.0000, 1629662.1250, 1929364.3750,
        1801495.0000, 2141515.2500, 2418375.5000, 1693721.0000, 1828545.2500,
        1691972.5000, 2586082.7500, 1925564.3750, 1997459.1250, 2345314.0000,
        2079325.3750, 2174061.2500, 1801081.3750, 2164295.5000, 1861916.0000,
        1795924.2500, 2051496.1250, 2228724.7500, 2338543.0000, 2341405.5000,
        2059332.0000, 1981059.5000, 1863487.2500, 1869137.6250, 1933849.3750,
        1997471.5000, 2190860.5000, 1733682.6250, 2042423.5000, 2032226.6250,
        2198551.7500, 2092019.6250, 2191547.5000, 2334906.5000, 2311188.0000,
        2674322.7500, 1912975.1250, 2067588.2500, 1623235.0000, 2207087.7500,
        2289676.7500, 1677193.1250, 1849809.8750, 1651349.8750, 1637253.7500,
        2308704.2500, 2187616.2500, 2004966.8750, 2092686.7500, 3141229.5000,
        1917233.3750, 1789922.1250, 2421317.5000, 2188132.5000, 1779249.5000,
        2108380.5000, 1755933.1250, 2133260.7500, 2376096.2500, 2225073.0000,
        1944690.7500, 2010750.0000, 2207980.0000, 1977879.1250, 1786930.3750,
        1756020.8750, 1857836.8750, 1989413.7500, 2265903.5000, 2081900.1250,
        2021482.5000, 1742683.1250, 2055327.3750, 1828450.0000, 1851200.3750,
        2517524.7500, 2452698.2500, 1606698.1250, 1737861.3750, 1972093.6250,
        2382618.2500, 1908250.7500, 1480238.1250, 2051182.3750, 2095225.5000,
        2094198.2500, 2285775.7500, 2138614.5000, 2274299.0000, 1474161.8750,
        2139893.2500, 2265250.0000, 2182317.5000, 1663641.0000, 2029277.3750,
        2136082.0000, 1743522.6250, 2326053.0000, 1742231.8750, 2224848.0000,
        1965228.3750, 1718707.6250, 1835519.6250, 2050049.6250, 1720164.0000,
        2400897.2500, 1705400.2500, 1819666.1250, 2008096.3750, 1951592.1250,
        1991103.8750])), ('encoder.6.4.conv3', tensor([599017.0625, 112349.6719,  88660.8750,  ...,  14260.8154,
         33764.6562, 212406.7031])), ('encoder.6.5.conv1', tensor([127749.4375, 164336.6719, 126991.4766, 170542.8594, 151272.1562,
        172194.3594, 155869.0312, 160201.6406, 119490.3750, 110774.9141,
        151416.8438, 155306.1250, 165569.0000, 168515.9375, 205580.0781,
        187604.2500, 101061.4688,  90461.0547, 172456.3906, 102777.7266,
        188631.8906,  96355.6641, 185628.9219, 156411.8281, 160075.9531,
        146178.2656, 156993.4844, 174975.4688, 194274.0781, 134218.2031,
        133042.0625, 180177.0938, 103259.1094, 190090.6875, 149969.6250,
        169887.1562, 187982.4375, 165907.6719, 131100.5625, 149545.4219,
        152851.8594, 136907.6875, 135920.0625,  86916.6328, 164121.3438,
        161590.0938, 177236.3438, 145334.3281, 192864.7031, 163298.3750,
        197838.0000, 173343.7656, 169780.2500, 137541.2031, 161588.0312,
        181548.9844, 181239.2031, 153893.0469, 123159.4922, 160776.0469,
        164142.2656, 129270.2188, 158771.3281, 155139.2500, 170378.9844,
        128691.9219, 156985.3906, 136969.6562, 140543.2656, 128017.9609,
        161043.4688, 176889.9062, 133198.7812, 126947.4453, 142850.0781,
        189991.8906, 197180.9688, 136959.7344, 155342.1406, 172268.8281,
        172213.5781, 128618.1250, 154554.6406, 126818.5781, 138880.7031,
        154968.7500, 158877.0781, 170575.0000, 155060.8281, 124341.2500,
        170039.3281, 143008.0000, 139448.3438, 208438.3594, 136722.8906,
        136233.1250, 106770.8594, 141940.6719, 124877.1250, 185697.1719,
         94883.3594, 142744.7812, 116134.0156, 116572.1328, 159311.6562,
        251583.8438, 229469.5469, 123315.9453, 139116.1875, 176230.8125,
        120187.5000, 238495.8125, 164217.4844, 110615.2891, 150640.6719,
        156523.5625, 142381.5000, 116112.5156, 189776.2812, 124316.6562,
        149972.3906, 183489.3125, 139694.7344, 103427.8047, 158563.2031,
        175249.8438, 188727.1094, 170525.5000, 123917.9375, 132708.2969,
         99373.3203, 147593.7500, 123227.9609, 189273.4844, 143556.2500,
        200451.6875, 114695.1484, 137331.4375, 141150.1719, 156038.0938,
        118296.2109, 243798.4219, 167210.6875, 145923.0469, 148708.8125,
        202544.3906, 179335.0312, 135865.9844, 146840.5000, 126858.2578,
        138342.1562, 155346.2656, 130894.0234, 157298.9688, 213349.9531,
        146473.7344, 144486.9844, 176297.7188, 123144.2578, 140419.4062,
        162997.7500, 215429.5000, 150827.5938, 160450.3594, 102616.7891,
        132846.1094, 137056.1406, 128593.1016, 195303.9688, 146607.7500,
        150598.5781, 109663.3828, 176170.0938, 295239.4375, 174314.1406,
        216416.5938, 123867.8594, 127721.8438, 200220.7031, 149376.9375,
        155688.5312, 165786.6406, 156966.5469, 133149.4844, 174307.7812,
        160428.7344, 136101.1719, 111979.6719, 152196.5156, 125495.6250,
        159872.4375, 182667.4844, 186103.2969, 198835.8906, 153063.9844,
        149361.9844, 165642.8438, 120547.8203, 152635.6094, 188482.0938,
        103814.2578, 190287.1562, 150043.5938, 136919.1875, 165731.9531,
        105857.9297, 123924.6172, 159166.7812, 146012.2344, 128846.5859,
        174424.5312, 135297.2656, 176190.6250, 190342.0000, 172093.4531,
        156396.3906, 127909.0234, 175823.8438, 170266.0625, 126102.2578,
        157418.1875, 144761.3906, 165989.1406, 162799.0156, 123984.9453,
        108108.7891, 135633.3594, 178462.8125, 162097.6875, 170752.8438,
        181396.8594, 124411.4766, 166558.9531, 156834.2812, 139288.5781,
        123885.0312, 158840.4688, 144722.2500, 112015.9375, 135427.8281,
        170315.3125, 154694.6875, 131630.0781, 150578.8750, 150618.6094,
        217008.2656, 144180.9219, 141110.2031, 135755.2500, 157458.0938,
        164454.6719, 217434.2656, 155021.8438, 154063.2344, 152432.2656,
        171414.7656])), ('encoder.6.5.conv2', tensor([749708.3750, 790442.0625, 620372.5625, 594553.7500, 674432.0000,
        764868.6875, 876941.7500, 438640.6562, 491704.2500, 438095.7812,
        540858.5000, 392073.0000, 585023.7500, 582763.4375, 601679.5625,
        681541.8750, 596667.0625, 547488.4375, 556849.3750, 542599.7500,
        441998.0625, 613939.0000, 681294.3750, 824880.8750, 621768.1875,
        458895.8750, 606008.0000, 687744.1875, 634388.5000, 502180.7500,
        786274.9375, 399004.0312, 494544.2812, 644777.3125, 585001.2500,
        608246.3750, 481299.7188, 544420.3750, 565070.9375, 694269.2500,
        639362.7500, 556622.0625, 623299.3750, 758970.7500, 724636.8750,
        591496.3125, 543391.7500, 575739.8125, 668113.6875, 334629.0000,
        661044.1250, 518288.6250, 618042.6250, 588755.6250, 671621.8125,
        611356.5000, 662689.1875, 439219.3125, 538074.1250, 471727.5312,
        572825.3125, 632458.0000, 558902.4375, 549156.4375, 531424.7500,
        601455.3125, 643325.1250, 491818.7812, 591414.0000, 509209.6250,
        607311.6875, 485329.2812, 760098.2500, 630677.8125, 571293.6250,
        498585.7812, 610812.9375, 436098.2500, 744239.6250, 525581.8750,
        607650.7500, 533765.1875, 689389.9375, 657305.8125, 505226.6875,
        550842.1250, 483068.5312, 356129.7500, 582901.3125, 438361.0312,
        708981.1250, 691055.4375, 527223.8125, 733337.5000, 760557.3125,
        663347.0625, 469918.2500, 621873.6250, 575424.8125, 656436.6875,
        511988.3125, 665403.3125, 695486.3750, 781874.5000, 644973.3125,
        688926.1875, 592390.1250, 541352.7500, 599109.2500, 621864.6250,
        592606.4375, 473769.1250, 585138.0000, 513880.2188, 866854.4375,
        820670.9375, 415388.4062, 420375.2812, 660666.0625, 554049.0625,
        770466.9375, 653748.6250, 583318.8750, 719030.5000, 412015.4688,
        546339.9375, 669566.7500, 582726.3125, 538941.1875, 517172.7188,
        521557.4375, 728805.1875, 650634.2500, 752518.3125, 718683.5000,
        511120.3125, 479438.0000, 548525.3125, 633866.8750, 639664.6875,
        626166.0625, 768820.0625, 611949.6875, 664917.8750, 563607.6875,
        629615.2500, 553654.5000, 604873.8125, 771578.6875, 519040.0938,
        493751.4688, 600686.1250, 511096.8125, 544219.0625, 921946.1875,
        495491.7812, 621126.3750, 654010.6250, 579777.9375, 505139.7812,
        666741.5000, 548747.1875, 589373.1250, 592843.7500, 689343.9375,
        641358.7500, 988638.5000, 912118.6875, 682064.3750, 460309.7500,
        628354.3125, 818467.6250, 531210.5000, 717274.8750, 676372.6250,
        559878.5625, 463994.9062, 604149.0625, 588365.6250, 633934.3125,
        486434.1250, 623025.8750, 499698.5312, 456730.2188, 580970.3750,
        545950.9375, 594555.8125, 583283.6875, 494242.3750, 516207.1250,
        726033.4375, 781917.0625, 580319.0625, 509865.7188, 542085.1875,
        601176.0625, 606581.6875, 773637.4375, 581835.9375, 551350.8750,
        646411.1875, 518177.2812, 540595.1250, 781444.1875, 438094.3750,
        619209.3125, 597624.7500, 507933.0000, 611958.5625, 716457.1875,
        400556.9062, 683610.6250, 453439.5625, 420470.9375, 580964.0625,
        511244.4688, 489511.5938, 586080.7500, 639285.6875, 603081.3125,
        531960.9375, 610026.3750, 604644.5625, 633984.3125, 573372.6250,
        533391.8750, 699106.8125, 581806.6875, 743584.9375, 772617.7500,
        698537.3125, 404105.6250, 707499.8750, 671512.3125, 646847.6875,
        405592.5000, 611755.0000, 588972.1875, 518215.8438, 767526.2500,
        751222.0000, 583112.9375, 578919.1250, 498694.1250, 520101.5000,
        550799.5625, 626423.9375, 490721.5625, 555856.6250, 566920.0000,
        524793.9375, 900176.5000, 583336.1875, 655350.8125, 607252.7500,
        511235.3750])), ('encoder.6.5.conv3', tensor([1.0183e+06, 9.6288e+04, 9.0334e+03,  ..., 9.0190e+02, 2.3890e+03,
        1.0350e+04])), ('encoder.7.0.conv1', tensor([ 9150.8438, 15218.6240,  9198.0879, 13229.0684,  7001.5410,  8680.9355,
        12502.3594, 12720.2061, 11391.0049, 16153.2861, 11466.7725, 10384.6514,
        11243.1074, 14734.3945, 11181.8672, 13162.9229, 11440.3252, 13150.4014,
         9279.4131, 12145.1602, 11447.4258, 12255.7559, 10344.9053, 14480.7344,
        13081.3555, 11008.2236, 12328.8750, 13646.4004, 12328.9277, 13514.6113,
        10593.7129, 13250.5371,  9762.6367, 11098.0762, 15406.0947, 16727.3223,
        11355.8301,  8952.9385, 10156.9492, 12332.7500,  7600.3447, 10459.6230,
        17134.2461, 11626.6025, 15225.3066, 13922.0645, 13654.8896, 23176.3086,
        20305.3281, 15492.9580, 18179.1621, 10828.8740, 10713.0107, 10289.6924,
        10742.8838,  8116.6816, 20502.5234, 13822.4346, 15580.2549, 10272.2061,
        11455.9873, 14903.6572, 14003.8223, 11347.9189, 12988.8994, 14031.9346,
        13912.1768, 13427.4014, 10023.7783, 13383.1387, 14766.5000, 12987.0098,
        13079.3408, 13964.1943, 10616.7041, 13270.1484, 10725.9199,  8655.7402,
        15360.1562, 11999.8164, 13526.8545, 14439.9834, 14866.9424, 16143.5449,
         9915.9775, 14471.1416, 12637.2520, 10284.6572, 14573.4961, 13387.8232,
        13496.3506,  9799.4561, 16140.5811, 12255.6885, 10804.2393, 12369.3428,
        13526.2178, 14452.7168, 14287.0908,  8281.7100, 13793.3750, 17979.0938,
        11955.1660, 12128.3271,  9947.1914, 23679.8184,  9866.2305, 17762.3535,
        14349.7178, 14251.7832,  7471.7007,  9872.4922, 14186.0811, 13854.5596,
        15995.0537, 11365.2930, 13121.1904,  9274.6914, 14736.0518, 20259.2012,
        14867.0996,  9380.5615, 10105.2637,  9687.2207, 15906.0264,  8639.2285,
        11341.3555, 15411.5107, 14866.4395, 19023.3145, 16114.8086,  9224.1885,
         7301.9536, 13149.6309,  8292.9453, 12930.3223, 11870.2363, 16673.8672,
        22986.1211,  7752.5205, 17559.2461, 10370.4600, 13764.3564,  9593.5332,
        19507.4199, 10689.7080, 12937.3418, 14013.7080, 10290.3789, 13988.7275,
        16382.9180, 15392.3008, 11769.0215, 12818.9258,  9446.3721, 14582.8701,
        12599.9473,  7754.9824, 12569.1807,  9357.1426, 13102.7520, 10910.4785,
        12521.0107, 10177.0508, 15184.8838, 11036.2715, 15675.7549, 13481.6455,
         9657.2803, 23042.9238, 16473.5840, 16284.1826, 14055.9775, 19631.3184,
        14846.2656,  9043.0352, 10536.8271, 11576.8018, 16941.7188, 12663.6113,
        14479.6523, 15665.0059, 11702.7275,  9823.9170, 12231.0410, 16893.4824,
        10029.0732, 16122.5400, 11090.8672, 14066.7539, 11096.6943, 10950.8203,
         8784.3115,  9323.5957,  8204.6562, 13835.3096, 10020.4551,  9139.0273,
         9124.8027, 13472.0137, 10552.1895,  9850.1084, 15030.2422, 15704.8721,
        13013.0410, 16120.6895, 15591.6807, 11766.6191,  9050.8154, 13249.5312,
         7695.6514, 10135.9668, 10484.6230, 10635.0898, 10640.5947, 13341.6963,
        14241.9414, 15381.4541, 14555.8916, 15153.7783, 14600.4570, 13018.7578,
         9990.2080,  8452.3633, 14253.1064, 11974.8760, 10833.3486, 17152.8125,
        10598.8926, 16985.0977, 10589.2529, 13607.5762, 11239.0859,  9752.1025,
        17916.5254,  8155.2056, 10466.6201,  9763.8975, 11316.6680, 11263.7998,
        17497.6816, 13125.7500, 11851.1094, 15718.2871, 11146.3564, 16377.4727,
        18193.3379, 14807.1748, 11657.9248, 17874.4707, 11766.8818, 10326.3691,
         8852.2461, 11633.0146, 11606.4209, 15055.7510, 12268.9746, 14726.7646,
         9335.0928, 13773.7529, 14013.7998, 10427.3750, 11891.4834, 22129.3965,
        12885.8623,  9286.0469, 13764.9678, 12391.6494, 12667.9326, 12481.6299,
        10696.1035, 12031.6973, 11248.0205, 11080.4307, 10836.1309,  8700.5098,
        12463.4131, 16156.2422, 11383.6240,  9571.2129,  9783.3350,  6757.4229,
        15724.7842, 13471.7490, 14535.8643, 12288.4629, 10087.9053, 11726.9268,
        13396.6719, 10118.1855, 12349.6455, 12101.6299, 18232.6309, 15537.3779,
        10126.6865, 15286.5752, 10478.0127, 11450.8281, 11857.3252, 12164.9512,
        15706.0557,  7523.2256, 10132.7051, 11689.8662, 14439.3389, 14384.6211,
         6645.3350, 25108.0762, 12287.5635, 16853.2285,  9358.0547, 12728.1338,
        12606.1133, 10234.7334, 18253.0488,  9704.1777, 17663.4629, 12559.9180,
         9214.1582,  9039.3027, 18442.7305, 15252.9541, 17424.7773,  8436.4512,
        12114.4404, 15913.4697,  7263.3745, 10685.3027, 12789.0586, 16920.1680,
        15540.2002,  8651.5996, 10985.5400, 10552.4580, 18402.9023,  8560.1729,
        10061.0781, 14584.5342, 11637.1660,  9775.3799, 17835.6816, 12958.7090,
        17531.1172, 13689.3809, 13515.0410, 15581.4590, 15919.0127, 19290.7656,
        15070.3516, 10998.3574, 16580.2578,  8478.5693, 10602.6094, 14619.2754,
        15698.0400, 12672.7168, 16596.3105, 10601.8486,  9614.1934, 15722.2598,
        14839.0488, 15249.8350, 12920.6289, 10778.7256, 13072.8223, 10977.0410,
        10354.6318, 17111.4590, 12191.9590,  9717.3926, 14819.3301, 14618.8232,
        13639.2246, 16746.9238, 13159.5068, 11230.9453, 15709.1465, 12056.3877,
        14688.3604, 11408.1680, 16635.4453, 14529.1953, 14073.3262, 12145.6055,
        14537.4277, 13968.7041, 21463.8496, 13052.9980,  9433.5967,  8840.1768,
        12494.6992, 13774.0771,  9350.4561,  9757.0312, 16258.3916, 10589.3496,
        10185.4629,  9272.6191, 11361.0039, 12414.7725, 11004.3516, 13110.5469,
        11101.9512,  9382.5312, 10432.7910, 14393.8379, 11754.7617, 13172.6016,
        15384.4756, 12961.6348, 11016.0205, 14777.8701, 15905.8535,  6983.9790,
        15500.7705,  9835.3936, 17591.4707, 10635.7812, 16073.9697, 13087.0137,
        17011.9629, 16650.4746, 19330.1582, 12970.7373, 18257.7617, 14753.6201,
        12570.7422, 13856.3936, 15552.2822, 11311.4170, 11839.8223, 11212.1406,
        18078.3027, 11073.6992, 10383.8408, 12508.8457, 15736.1504, 10997.5273,
        15371.7852,  9827.3379, 11197.8496, 19547.9043, 11736.2979, 10616.4590,
        11000.3848, 16303.9033, 13922.6436, 13205.7559, 18784.7617, 10120.5957,
        12555.6572,  9527.1338, 13321.4258, 14439.8350, 12552.2480, 18406.2090,
        13506.7295, 14103.0342, 17231.8281,  9653.9385, 10208.0273, 14179.2100,
        11633.5752,  7840.8711, 10430.2432, 13179.7158,  7562.8281, 14414.3105,
        16724.7324, 12184.9326, 11655.8184, 10992.2139, 10514.2031, 18780.3633,
        15332.0010, 16993.4570, 15476.5547, 10393.3760, 13914.6660, 15549.1152,
        10521.2295, 14409.1748, 13740.8281, 12678.2295,  9872.1514, 11411.9590,
        17043.9238, 13664.4922, 10837.8174, 12873.7363, 10490.8770, 13687.9971,
        14377.8223, 13960.1387,  7705.2812, 14822.2773,  9349.5488, 17428.5820,
        11054.8555, 13506.6904, 16397.5918, 20126.1113,  9062.3027, 13152.4111,
        11762.2549, 10394.0635, 16556.5859, 13951.9697, 12596.4219, 19002.9180,
        12433.5029, 10409.5801])), ('encoder.7.0.conv2', tensor([121551.2812,  96714.3203,  81713.2500,  65935.3359,  76275.3906,
         87096.7344,  68474.7891,  75338.4531, 104842.6172, 122747.6250,
        105417.0000,  84695.5234,  85668.9141,  94991.0547,  83713.0312,
         74674.9453, 119345.5078, 106133.2734,  83692.8203, 105759.3594,
         76667.5938,  74954.9375, 141094.9531,  62733.3906,  86613.8359,
        101169.7969, 111390.8047, 120255.2812,  59494.8867,  47009.0391,
         79127.8281,  98201.8906,  90402.9609,  97359.4062,  87418.8906,
        116999.3125,  53158.1914,  67777.7109, 102068.9531, 143238.0625,
         85147.6641, 123558.8281,  97762.0547, 125250.1328,  96872.1797,
        104319.6094,  94168.2891, 109239.3828,  78999.3594,  79780.9453,
         93417.9766, 120446.4844,  90027.4922,  84230.6875,  99328.0859,
        108043.2578,  77188.9141, 125330.5703,  73633.1719, 118457.3672,
         64123.0664,  94096.8125,  98428.6094,  66055.5781,  85531.2578,
         53208.7461, 143108.8594, 134414.5312, 104471.0625,  73103.8516,
         95837.1406,  95519.3672,  98010.2656, 125780.4844,  99985.5781,
         87333.5000,  95480.4375, 104499.2578, 106603.8516,  97980.7969,
        109224.7109,  75298.6250,  94226.4688, 128678.1562,  95396.0625,
         73072.6484,  74749.9922,  93285.6484,  73636.9531, 103347.4609,
         90814.9297,  97165.5859, 124680.5547, 105565.9609,  71765.7188,
         59188.8828, 113856.3125,  91179.7734, 112473.0078, 111176.4141,
         74061.1797,  68361.0156, 103085.9297, 131684.2188,  75842.5391,
        100414.0703,  51380.9023,  75575.4297,  71349.4375, 115502.6484,
         75304.2734,  79980.6328,  94210.6719, 107256.6328,  76078.0625,
         82971.6484,  81849.1875,  59999.3320, 109102.3047,  79297.1875,
        122900.3594, 116743.3125,  82646.1328, 108578.5938,  62409.6562,
         84718.8125,  70441.0547, 155454.8125,  94797.7109,  87281.8125,
         94930.8672, 109188.1719, 128633.0938,  80790.4375,  86063.7812,
         90271.3516,  56183.1719,  96708.0625,  95276.5781, 104238.2422,
         84952.2656,  83212.0625,  67661.7344,  99564.1719,  91450.9766,
         96561.0000, 108116.4922,  79136.4141, 128277.7031,  67587.4453,
        145029.4688, 113204.8828,  90674.4062,  88559.4844,  92145.0938,
         94977.5312,  85898.9922, 119426.4453,  72715.4453,  67282.6875,
        113494.4219, 101603.8594, 112230.9844,  95393.1953,  74082.5469,
        102871.6797, 106932.9766,  95563.9219,  71053.8047, 112464.3750,
        105244.4922,  80532.9922, 108904.7188,  75105.1875,  79637.2031,
        119917.2578,  59895.4141,  79952.5859, 121903.5547,  77057.1172,
         93555.6016,  99404.4297, 115952.9062,  67186.4844,  74054.8125,
         93391.8203,  84852.5859, 154232.2812,  60991.2266, 126089.2812,
         59965.1875,  63784.2266,  86347.5859, 111873.1562,  93139.1094,
        108001.2969,  85087.5938,  97023.9297,  98269.6406,  96258.3594,
         97582.6250,  79774.8828,  73330.1797, 112660.1875,  87305.5156,
         64163.6953, 149734.9219, 136339.0938,  81369.3516,  83378.7891,
         96044.1406,  78450.8125, 114452.4609,  84336.2422, 100488.5234,
         83115.5312, 103325.1406, 111556.2578, 113914.2891, 120993.9609,
         75460.8203,  93806.3516, 128025.7891, 125640.1797,  74741.7578,
        136530.6406,  86530.0781,  59568.5977,  97084.4844, 128371.7656,
         83203.8359,  78833.1484,  76059.6094,  77345.5625, 106607.0078,
         90545.8672,  97935.9531,  86569.4609, 136849.4375, 101466.4922,
        115677.0156, 101606.2109, 106514.7344, 111006.1562,  73351.6406,
         68661.2969,  80271.7969,  87372.0781, 106617.7031,  75560.7422,
         82831.3438,  90203.3125,  81222.9062, 125430.1875, 114826.8594,
        113340.6172,  96508.0703,  65725.4453, 122624.7344,  88996.8828,
         69411.8047,  96994.2266,  73987.1172, 120384.7109,  80276.7891,
         85623.1953,  80450.3594, 101306.9688,  81100.0156,  83828.3594,
         92255.1953,  93595.0234, 119352.3281,  78639.8438,  76816.8828,
        109051.3281,  86179.7422,  84846.3438,  71807.6172,  70507.8125,
         87515.6641,  67368.3594,  93369.8984,  76814.9141, 131546.1094,
        110771.1250, 100156.3359, 105944.3203,  57325.9570, 113366.3359,
         72090.6875,  84207.8516, 114631.7969,  64675.5469,  93853.8750,
         77916.9062,  67769.6406,  70938.5625, 106644.3359,  98673.2656,
         89977.1094,  55711.8945,  81917.5000,  97887.0781, 102290.5156,
         75481.1094, 116751.9609,  80754.9297,  87058.9531,  83950.4297,
         91260.0156, 107949.2188, 150479.9062, 118145.8438, 123872.1484,
         87610.2344,  90230.0156, 110740.9141,  86969.9766,  86774.2500,
         89132.8594, 102813.2344, 100930.7891,  57013.4570,  61069.2539,
         91528.1719, 123696.8203,  97503.9219, 108459.3906,  87746.4844,
        129253.6172, 127792.9453,  63676.7734,  84599.3047,  87590.6406,
        106362.8203,  91875.7266,  84779.5078, 128574.6250, 146898.9375,
        159120.8438,  82741.5547,  63158.8789,  99033.4609,  91050.7266,
        105507.1875, 118460.5312,  85958.9688,  69748.5703, 116092.0312,
        101146.5234,  70102.5625,  96892.7422, 100640.7031,  95548.6484,
         88550.1797,  75917.7891, 105934.7812,  83048.8281,  85493.8438,
         69152.6250,  79394.6641,  72332.2188,  66733.3438,  72216.0703,
         76552.2266,  52251.0273,  69058.3281,  79504.1641,  68276.5078,
        138592.2031,  61643.9336,  72714.0547,  79824.7969, 112023.9531,
        111658.5312, 110308.0000, 106453.2109,  71610.3281, 102078.6016,
         89688.6406, 155355.9062,  68100.4062,  75622.0391,  76352.0391,
         88376.6875, 101404.0156,  62580.3516,  98483.9531,  92411.3438,
        134871.5625, 108033.1016,  67592.7422,  85561.7031,  78285.0625,
         70529.3828, 123006.0469,  89108.6562,  84168.0938, 138099.1719,
         89163.0703,  82604.1641,  81078.3047, 114014.6875,  78514.7969,
        104867.8594,  87242.7031,  71010.8125,  71552.6094,  63764.7148,
        127322.6172,  96273.3281, 112083.4453,  82934.1484,  87524.1797,
        105351.4844,  85004.8203,  85667.2812, 135293.3438,  98028.8594,
        132268.0156,  63162.6055,  85508.1484,  96148.0156,  79978.8672,
        127524.5391, 104438.7188,  71377.8047,  75841.8281,  65247.6562,
         79615.1953,  97026.5000,  97394.6172, 119070.6797,  88877.8125,
         70280.2266,  87832.4062, 113252.8438,  92616.9141,  72111.1562,
         68485.5000,  69027.6953, 112961.7734, 136189.5156, 102238.6953,
         89890.5156,  80871.3672,  91852.6875,  84488.5234,  65405.1758,
         92371.7109,  98997.2578,  95342.3672,  68867.2969,  67322.7891,
         65788.3516,  61145.9141,  92010.2031,  99442.6406, 101015.4609,
         79215.2812,  85681.8594, 138492.1250,  86183.7031,  60259.4141,
        113249.3359,  87593.0312, 106926.3203,  71349.1953, 119687.3125,
         99949.3281,  84468.1484,  86218.9844,  57187.6992,  85345.1719,
         97552.4453,  98846.7812, 122323.0781, 114098.8516, 106558.0859,
         92605.7109,  78530.9922,  82811.6641, 138491.0312,  85438.2109,
         99893.1328, 110605.1953,  84051.8906, 116132.0547, 106259.8281,
         61965.7500,  62104.6523, 107790.6953, 116008.4375, 113900.5000,
         78512.7188, 142708.7500,  92352.9141,  50164.8281,  71225.7656,
         99167.1875, 118730.2891,  95283.7109,  86095.9766,  83779.3594,
         73683.0625,  81930.3438,  77207.3516, 109723.8594,  94143.0000,
         76453.0781,  78764.1172])), ('encoder.7.0.conv3', tensor([2643.1755,  227.5357, 4500.2861,  ...,  180.9651,  668.9054,
         536.1365])), ('encoder.7.0.downsample.0', tensor([2387.0881, 1008.4975, 3920.9746,  ...,  832.0339, 1337.5762,
        1244.3019])), ('encoder.7.1.conv1', tensor([ 5636.5503,  4238.7749,  3602.3176,  4106.3027,  5557.5786,  4196.8286,
         3908.3074,  1580.0211,  4736.1064,  2710.4526,  1560.3495,  1742.9128,
         3372.6848,  4758.9990,  2051.2693,  2073.8901,  8817.0303,  1688.6205,
         2650.6018,  2382.2468,  5044.4736,  1773.3074,  8493.4561,  2024.1598,
         4274.7261,  4697.1304,  4721.6187,  2253.2031,  2847.8389,  3470.2058,
         6415.4868,  2444.0879,  5260.1875,  1981.2445,  2858.6621,  4661.0835,
         2792.5293,  2467.8040,  3008.0081,  4266.4717,  4167.7378,  1865.7822,
         1652.2537,  2349.0498,  3856.6274,  3936.4263,  2067.8064,  3697.8174,
         2702.0527,  1994.8722,  4872.9561,  2686.0322,  2140.3550,  4647.6836,
         2151.8889,  3730.0281,  2216.8428,  1636.7418,  7817.5220,  3180.9614,
        11906.8662,  2092.3901,  4872.1812,  2404.1895,  3833.8333,  3001.7043,
         7519.8291,  7926.5015,  2255.8535,  3818.1870,  2973.6770,  2965.8748,
         6924.4463,  4540.3818,  4088.5750,  3492.1160,  6265.7026,  6502.8882,
         3220.6284,  3227.3713,  3107.7368,  2115.3872,  3545.2024,  6433.1089,
         2811.5793,  2240.0000,  6644.3179,  1831.5011,  4587.4824,  2340.6421,
         6610.0820,  5123.0698,  3941.6482,  2901.7903,  2403.5649,  2987.3901,
         5979.2246,  2096.0232,  4736.4888,  4979.9570,  5106.4292,  1874.7662,
         3589.9238,  4605.2998,  4125.6948,  2334.5303,  2332.3237,  6933.2051,
         5702.0688,  2219.8296,  3688.6980,  2582.5999,  3188.5591,  3136.0520,
         2784.7705,  5954.5547,  3112.0352,  5727.2720,  2502.1528,  3553.7371,
         3356.0178,  2720.6831,  3983.5281,  4150.0381,  5203.5752,  3981.8794,
         4792.4751,  1891.6798,  5025.6494,  1847.5530,  2227.2461,  6215.0986,
         4493.0586,  3992.4258,  3013.5693,  2662.9194,  4923.7910,  5765.7012,
         1613.8489,  1681.2737,  8226.4932,  1958.7522,  6317.5938,  1860.4271,
         5598.5366,  2488.2063,  5857.7397,  5929.7910,  1711.5636,  3071.3228,
         4476.7974,  4560.9839,  3965.1345,  2407.2227,  3898.6721,  3090.0146,
         2539.2021,  1577.6576,  1637.8231,  4785.4941,  6365.8955, 10879.5938,
         8239.0996,  1769.0859,  3249.0867,  2475.1619,  3554.5479,  4975.4688,
         3277.2817,  5649.2178,  4976.8022,  6400.0776,  9402.5625,  5603.0522,
         2102.8882,  2095.4553,  1953.2385,  6101.1958,  1691.6101,  1957.7531,
         1909.8706, 11224.7275,  4119.5156,  5032.7520,  4515.7456,  2355.6011,
         3490.4009,  2929.7881,  4127.8745,  2205.9280,  2910.2046,  4635.2793,
         2247.8870,  3067.6389,  1644.4956,  8991.9912,  6115.0879,  2284.9009,
         2975.8552,  8487.6758,  2655.7573,  5410.4868,  5042.7734,  6338.7524,
         2817.2900,  8729.1377,  8407.5420,  6184.8511,  5298.8765,  3078.4812,
         3664.8159,  3180.2781,  2359.9180,  4554.2671,  4077.8706,  2390.6965,
         3630.3171,  6066.9604,  7813.8276,  1715.7477,  1678.2961,  2412.0715,
         6859.4731,  3440.6013,  5513.1440,  1673.6681,  5636.1362,  3777.5396,
         9576.5410,  2574.7151,  4849.4155,  3105.8020,  4696.9043,  2165.0037,
         3075.2510,  3067.5161,  6124.8047,  6063.4448,  4079.0371,  1801.6010,
         4072.6758,  2259.0342,  8488.7432,  7327.3311,  2146.8511,  4267.6309,
         3827.0054,  2010.0753,  3048.1821,  2737.8013,  6688.2920,  4246.5972,
         7434.7729,  3943.3728,  4897.5215,  4493.3877,  2695.0337,  3082.1379,
         6714.8716,  1952.8448,  4385.5806,  4623.4653,  3322.6943,  4113.6392,
         6621.5649,  2201.8813,  8817.8301,  5000.5469,  2181.4109,  1638.3285,
         2901.4014,  3495.8635,  4661.2437,  7090.1787,  6455.2593,  1555.3684,
         2997.1838,  3796.5059,  7089.0845,  2518.4802,  7260.6670,  7937.0093,
         2219.0713,  3896.5530,  8055.3057,  2939.0278,  3195.0442,  7046.3652,
         3978.4216,  2582.3911,  2993.3738,  4736.9263,  5428.1162,  5055.8154,
         2017.2577,  2721.2375,  1398.0188,  5387.7002,  3594.5481,  6860.5376,
         3506.8213,  4835.5640,  2718.0305,  5182.4497,  7519.8896,  9184.2061,
         4647.9458,  3889.3870,  6957.8159,  7649.7178,  6260.4150,  3379.9661,
         5406.7153,  2192.0825,  3075.0583,  7636.6299,  1860.4019,  6906.1021,
         9805.9092,  5226.7974,  6583.3340,  5398.1118,  4293.7896,  1879.7367,
         6106.0112,  5501.7349,  1938.2727,  3535.3308,  2178.0332,  2900.7905,
         1910.7220,  3922.2485,  7361.5518,  3937.1277,  1813.0524,  3576.5334,
         5581.5088,  3235.6799,  2767.5674,  3813.6531,  4214.1680,  6050.4775,
         2904.8086,  7445.7310,  2366.1165,  5689.2529,  3908.7769,  6242.1738,
         4523.7407,  2503.6106,  3144.6677,  8181.0801,  2283.6641, 10126.5166,
         2958.4045,  7271.8804,  3931.1755,  2718.9915,  4017.8640,  3341.6506,
         3399.6560,  2111.3220,  7202.9629,  1658.0393,  5478.8062,  2749.6631,
         6952.8892,  3275.1367,  4836.3794,  2339.6328,  3391.6875,  7983.4551,
         1992.2980,  5351.8721,  4208.6650,  3426.5955,  8568.2578,  1563.0728,
         5565.1426,  3915.9138,  3875.6094,  4710.3428,  7510.5513,  3245.6343,
         3921.8354,  2386.7468,  2636.0227,  5876.1040,  5123.4492,  1946.8906,
         2122.0439,  3221.1301,  3805.2454,  3403.8623,  3064.3369,  5015.5679,
         3148.6248,  3938.5779,  3941.2456,  6729.9756,  3783.8352,  7816.6089,
         2263.4382,  1672.3000,  2136.6287,  2518.9622,  1447.9270,  3681.4421,
         1612.0524,  7731.7632,  2093.8479,  1880.2148,  5662.8071,  4324.0381,
         4726.3589,  7830.9307,  1869.8328,  5740.9951,  6919.4585,  7456.4746,
         2553.1599,  4923.8262,  4191.1147,  4032.3701,  6018.9995,  2040.0817,
         4641.0288,  3245.9817,  2624.7229,  5338.9976,  2604.9250,  5210.2383,
         7212.7642,  3786.2502,  2741.5396,  4377.8687,  4864.1807,  2647.9783,
         1598.4061,  4101.7671,  3009.8828,  2643.0945,  5034.9238,  5953.0957,
         6120.3311,  2383.5249,  7185.0542,  5842.8716,  2392.7139,  6089.1040,
         9350.9482,  1739.9183,  1705.2183,  6541.2969,  2366.5864,  8346.0586,
         2643.0300,  1851.0438,  6360.8223,  5772.1460,  5327.6387,  6165.7949,
         6482.1084,  5262.6172,  5549.0054,  4945.7583,  7197.1694,  4387.9629,
         2862.9744,  3396.1829,  1883.9109,  4586.9878,  3401.5645,  9810.7832,
         3629.1670,  4098.5137,  3679.4944,  1992.0582,  4750.0806,  2037.0291,
         2508.3792,  3455.3010,  2105.1628,  4542.3481,  4419.0371,  3930.2659,
         3975.0210,  3008.1350,  3918.7542,  4148.0366,  3097.5369,  1751.8494,
         3383.7551,  4149.2485,  2400.9995,  3304.9773,  5769.7446,  2440.3606,
         5513.1333,  3115.4519,  5986.1489,  5058.2314,  4977.8149,  6264.3359,
         2157.3362,  2107.6023,  4376.5718,  2850.5994,  5825.4731,  5748.7354,
         6432.0625,  2323.0583])), ('encoder.7.1.conv2', tensor([39332.7500, 21413.4473, 16820.6289, 19164.7734, 24274.3164, 30159.1445,
        49060.2891, 29287.4316, 28144.9668, 12862.6299, 21372.6777, 19115.9258,
        25240.2207, 84337.2812, 26962.8926, 32083.9590, 42583.4375, 41668.7695,
        43505.1445, 14943.4932, 38535.4141, 31673.3457, 34976.1367, 35579.7031,
        24965.5469, 46226.2656, 14576.9102, 36368.8203, 31761.6445, 11877.6543,
        26021.0605, 14932.3184, 18761.5215, 38309.8477, 15835.9902, 43874.1641,
        40852.8359, 54442.8867, 20408.6641, 28335.1777, 11145.4531, 15591.5537,
        27729.2090, 53394.7188, 23987.3359, 30163.6426, 39974.5781, 52971.6289,
        30236.9883, 16735.6270, 25241.9590, 17604.5781, 33660.5859, 26010.8789,
        31018.4453, 42373.9844, 14319.3281, 16951.1992, 27154.8145, 19833.9590,
        17584.5762, 35420.7930, 39923.0859, 23625.3301, 16749.4355, 76760.0859,
        19398.6074, 20742.6562, 25979.0801, 13254.9980, 16876.1367, 21374.2598,
        37305.8242, 43723.9844, 38881.8047, 12706.3574, 18875.2988, 16041.1387,
        22326.6230, 44811.3594, 13908.9580, 16945.1406, 17562.7207, 44433.4531,
        18897.4219, 29282.4238, 28709.1797, 21162.6523, 18808.0234, 54193.6055,
        16349.2295, 33362.7812, 21412.0430, 33322.1797, 30036.9336, 27465.5195,
        27448.1172, 19029.6992, 17739.8125, 27551.7012, 11789.5479, 23831.6875,
        58615.4062, 16801.5996, 28732.5586, 37213.6328, 49873.8633, 23764.8730,
        39483.8008, 25991.6055, 33122.8164, 28062.7891, 11437.5654, 38332.0977,
        12482.8809, 18517.4512, 29755.7812, 19961.5625, 29287.2793, 12364.0059,
        15323.9873, 44999.7031, 36900.4141, 22119.0742, 18159.9258, 51248.9531,
        51789.2695, 26472.1973, 38779.0195, 30993.1797, 48940.7969, 19700.2676,
        54136.1992, 52086.2734, 61664.6836, 41650.3750, 48170.3438, 22633.5293,
        14636.3438, 22436.6445, 43713.4727, 21205.7930, 46311.9766, 35793.5820,
        50039.1445, 43995.4688, 16407.2363, 18709.7480, 30574.1250, 29484.6699,
        37106.1055, 43452.5625, 46674.7266, 25667.1895, 28050.0215, 43669.3828,
        15272.9688, 29436.8867, 38464.9336, 16558.2051, 32597.6172, 20273.5352,
        53192.4805, 14822.9658, 16132.4258, 17295.9531, 35675.2617, 20511.3242,
        14660.4502, 14595.4170, 39187.1953, 39541.0469, 23201.0684, 17467.4902,
        22922.7852, 67069.6250, 20120.4844, 36559.8398, 23589.9844, 34510.9141,
        34833.2695, 36082.7969, 14830.8184, 37355.7930, 38458.2852, 19784.0508,
        43782.6797, 44765.2500, 18555.3145, 21838.5625, 12563.4229, 13078.0762,
        40599.2695, 11386.1885, 35885.5859, 24233.2520, 19880.3242, 22962.8945,
        37523.0742, 15177.9326, 20367.9844, 45660.3008, 18703.7090, 21068.7402,
        25844.9902, 17408.7285, 13745.1309, 32553.2695, 33563.8945, 19006.6426,
        28476.9395, 25749.8027, 22674.8301, 36842.3594, 22986.5625, 30488.7988,
        73134.3125, 26865.5273, 49696.4258, 20881.1543, 24088.1895, 16653.8770,
        17993.8301, 55211.2031, 15993.7793, 40322.7852, 63423.1406, 52426.8047,
        17555.1582, 17158.5156, 17458.3926, 20772.2188, 41681.1719, 15989.5156,
        45335.5508, 34471.6562, 35309.9297, 18140.0723, 12615.2656, 19914.2188,
        16789.0684, 21019.0566, 31291.8809, 22102.7383, 13565.3115, 35964.0156,
        18793.9961, 38002.7617, 28772.7520, 15589.7920, 35633.3203, 24308.7734,
        13610.6719, 57026.6367, 13208.7607, 32529.1758, 41700.2930, 18191.8105,
        33984.5703, 10474.4355, 26523.4375, 41081.6641, 25184.2559, 32251.8906,
        31790.5078, 29843.8516, 20777.1309, 10460.9346, 54343.1562, 18685.5312,
        37562.3516, 17134.6602, 48731.0938, 15468.1934, 22553.9473, 20316.6348,
        47304.2422, 19225.5664, 28094.4883, 13363.6973, 42042.7500, 24890.5098,
        16127.0117, 13859.2178, 18720.9844, 42167.3477, 16387.3301, 14079.0098,
        13574.9033, 38930.2812, 24312.4570, 23994.5156, 49601.2500, 27439.2402,
        34003.7969, 38055.5742, 15481.0400, 40020.8945, 21155.7227, 18378.5176,
        20962.0332, 24430.2520, 15843.3545, 26888.5508, 29524.2793, 29684.6680,
        32378.8184, 25854.4375, 45531.3945, 40583.5039, 14292.1123, 61260.3086,
        11713.6299, 17928.8496, 38699.1484, 23786.2832, 28037.1094, 16235.1396,
        21976.3672, 53537.1367, 37710.4766, 49568.0273, 47847.4609, 26622.2012,
        57326.3008, 39016.5078, 22552.0625, 24859.7168, 24980.9375, 21902.7773,
        29745.2637, 27105.2227, 16024.0537, 15325.5547, 29736.7969, 59406.1523,
        24584.3867, 20946.9121, 34022.3789, 20549.2988, 93955.2578, 58908.9648,
        55508.7891, 11804.5400, 22499.4727, 33780.4336, 16637.7012, 68113.7656,
        16569.1855, 37864.4102, 38229.4062, 39769.4492, 73104.1094, 16453.3887,
        24642.2344, 44930.3164, 30190.0605, 18341.7773, 17422.5781, 21352.6602,
        47914.0117, 24611.4922, 40431.7891, 51657.7852, 44457.7031, 25033.9395,
        13520.6709, 13955.7852, 48160.5781, 22913.3555, 61716.2109, 14183.8711,
        20024.4355, 35113.7305, 54069.0859, 25994.0020, 39225.1367, 10561.8730,
        14346.3906, 41588.8359, 15334.4912, 18376.8262, 27548.8945, 15378.7080,
        48300.3242, 19077.5801, 11786.7344, 27178.8867, 25456.3105, 12652.0322,
        22144.3633, 53709.9180, 14632.5508, 22087.5996, 19066.1465, 19705.0996,
        28143.0312, 15990.8379, 48663.9336, 15531.2783, 40252.1367, 51569.9336,
        18789.3184, 73962.6719, 17028.3242, 18515.8359, 17906.9980, 29776.3398,
        35851.0664, 14296.8955, 15807.3994, 62715.1250, 25015.5000, 21800.0254,
        20526.6348, 27930.5684, 19289.1133, 20711.8848, 25915.3242, 47643.9141,
        21920.7090, 16862.4785, 22975.3438, 27166.4961, 34829.3828, 30988.2520,
        43751.5781, 27298.5215, 50109.5586, 18621.8262, 15456.1250, 50672.2539,
        43839.2969, 19183.6113, 32290.0859, 25910.8809, 23347.7344, 32104.5742,
        42600.9844, 18403.9258, 12193.7275, 33952.3672, 12520.8516, 23151.3711,
        50193.7617, 17190.3457, 36426.1484, 32065.1055, 30487.1406, 21543.8945,
        11316.0918, 20889.2559, 34044.3438, 13337.5703, 19000.6992, 32375.7676,
        19545.3418, 24192.8730, 18014.3262, 30607.1836, 10254.2803, 36973.6367,
        21546.2598, 22972.3301, 20615.2520, 27431.5195, 22460.9434, 27644.0938,
        28456.9531, 27643.6055, 25794.2012, 11781.8418, 67436.3281, 20234.7539,
        27799.9102, 27063.0645, 35062.3477, 20228.4805, 40691.1484, 13183.3057,
        21858.8809, 44373.2891, 12770.3105, 30217.6992, 15533.6436, 25098.7363,
        36980.8984, 14883.9062, 52452.1953, 48159.9453, 16821.7363, 15000.1494,
        29150.7266, 33066.5977, 36803.4531, 30912.6465, 25379.6738, 33047.4609,
        23674.8555, 45434.0391, 14079.6152, 26368.5410, 29304.8848, 17423.2344,
        30345.8027, 22671.3594, 34940.2344, 32783.4180, 21930.5000, 30288.0957,
        26996.4473, 32486.6699])), ('encoder.7.1.conv3', tensor([673.7173, 330.7315, 555.3526,  ..., 307.8333, 428.1603,  71.3900])), ('encoder.7.2.conv1', tensor([ 908.9851, 1052.9326, 1137.4417, 1168.7910, 1249.3496, 1074.3599,
        1067.6840, 1069.4735, 1165.2957, 1243.1046, 1240.7616, 1020.3049,
        1029.7075, 1028.4705, 1178.0962, 1275.3163,  996.3589, 1081.0974,
        1316.4407, 1315.4647,  885.1060, 1434.1099, 1381.6210, 1643.9188,
        1121.5206, 1055.5017,  974.8655, 1095.1807, 1080.5449, 1281.2683,
        1045.8352,  969.4578, 1049.6537, 1145.7970, 1212.2424,  934.4459,
         937.6204, 1168.2743, 1077.4398, 1052.1229, 1020.9143,  860.0512,
        1460.3142, 1048.3595, 1358.3960,  939.4860,  996.2874,  980.3635,
        1139.2852,  938.1548, 1072.8966,  924.1608, 1129.5580,  835.5394,
        1176.9166,  891.6208, 1134.7677,  821.2092, 1018.0504, 1174.7546,
        1083.0834, 1077.0841, 1015.8795, 1131.7958,  894.2665,  852.9818,
         923.8540, 1131.4125,  908.9639,  817.8337, 1046.8521, 1107.1854,
        1350.8521, 1182.8474, 1233.7158,  825.4125, 1198.6967, 1195.6278,
        1052.4106, 1277.7810, 1024.5947, 1041.3444,  944.4081, 1106.8914,
        1093.7311,  987.2054, 1201.9264, 1328.0490, 1010.3322,  997.5281,
        1030.7684, 1198.5907,  944.0950, 1187.4901, 1114.4207, 1051.9215,
        1066.2738,  923.2122,  888.6346,  935.8571, 1240.9025,  990.7483,
        1104.1398, 1123.0286,  960.6469, 1083.4734,  910.1321, 1351.8135,
         797.9078, 1127.6995,  996.7054, 1030.9343, 1236.6495,  986.6932,
        1109.3965, 1429.2866, 1078.5074,  953.7487, 1561.4496, 1278.2568,
        1046.4492, 1355.7799, 1320.0770,  985.6058,  942.4161, 1178.9561,
        1220.8925, 1221.7516, 1116.2976, 1009.0317, 1288.2882,  971.6391,
        1121.4258, 1295.4935,  971.7899,  910.3958, 1374.9751, 1088.5726,
         923.5143, 1014.4186, 1065.4583,  929.4498, 1037.0037, 1032.6731,
         981.7714,  897.1117,  978.7474, 1216.2911, 1266.5278, 1011.6924,
        1060.2416,  903.7209, 1541.6053,  873.1550, 1108.9919,  927.0458,
        1359.7830, 1028.7168, 1178.5151, 1165.6255, 1229.5690, 1009.8839,
        1159.4318,  988.2964, 1115.3518,  993.1502, 1088.1798, 1419.2273,
        1322.5995, 1036.7970, 1059.5084, 1122.5037,  912.7463,  966.8884,
         797.9628, 1059.7209,  908.2231, 1295.4955,  980.5077, 1008.2115,
        1243.9139, 1134.0575, 1302.7003,  979.2341, 1185.3158, 1471.5542,
        1327.1075, 1216.2343,  928.7143, 1347.7938, 1062.0089,  920.5989,
        1043.0380, 1373.5114, 1206.6866,  914.1388, 1076.2878, 1284.4213,
         862.0456,  945.1613, 1017.5190, 1089.5751, 1161.6105,  997.3276,
         915.9681, 1178.1539, 1117.7198, 1051.9485, 1270.1606,  959.9137,
        1168.2157,  957.7725, 1205.3217, 1155.5690, 1202.6198, 1141.9890,
        1091.9695, 1151.3217,  996.9715, 1033.6085,  939.2624, 1103.9471,
        1156.2532,  908.8936, 1063.1267, 1188.9517, 1094.1716, 1281.0601,
        1036.1891,  903.9162, 1021.7873, 1385.1836, 1000.7253, 1205.4323,
        1060.3752,  935.2252, 1333.1216, 1293.0934, 1170.7952, 1266.1014,
        1058.4303, 1016.0644, 1362.4700, 1195.8655,  970.6550,  981.1481,
        1080.9558, 1304.3009, 1108.8722,  869.1024, 1214.9492, 1147.1561,
        1058.5759, 1251.9913,  971.9529, 1158.9137,  972.9745, 1048.2935,
        1264.5245, 1217.6328, 1078.5142, 1191.5985,  932.6845, 1043.0505,
         976.1418, 1028.4966,  859.7867, 1249.4532,  974.1320,  873.0170,
        1079.4442, 1139.9216,  962.6364, 1041.7305, 1454.0935,  796.9671,
        1221.2693, 1278.1556,  913.3233, 1264.5476, 1038.7268,  821.9335,
        1258.5609, 1063.4922, 1103.8512, 1002.6389,  843.0253, 1238.8253,
        1194.6986, 1158.0376, 1319.1047, 1145.3607, 1027.6199, 1127.4730,
        1126.2811, 1090.4244,  955.4818, 1521.7876, 1039.0787,  882.5187,
        1320.9250, 1181.9993, 1174.7623, 1160.1781, 1290.2255,  857.0258,
         928.9918,  857.5051, 1176.6934,  926.4363,  956.2319, 1129.6216,
         909.2811, 1092.7406, 1184.4242, 1130.3094, 1337.8643, 1120.7062,
        1056.5814,  986.8859, 1108.3774, 1310.1027, 1120.7299, 1127.1631,
        1135.1703,  926.0480, 1130.5535, 1239.1975, 1229.1278,  907.9861,
        1098.5509, 1087.7889, 1131.2402, 1009.8328, 1193.1707, 1005.3529,
        1257.1346, 1117.6594,  995.6501, 1002.5248, 1310.7576, 1099.9990,
        1020.0032, 1157.9390,  946.2336,  959.3562, 1064.3628, 1237.3262,
        1324.7208, 1083.3888, 1049.1615,  909.6954, 1232.9316, 1429.6925,
        1277.5150, 1111.6943, 1329.8901, 1112.7433, 1173.1801,  832.7074,
         896.1268, 1013.6624, 1117.5308, 1090.6685, 1137.1160, 1175.1047,
         922.2974, 1015.7467, 1020.7711, 1170.8131, 1496.7585, 1182.3112,
        1138.8925, 1073.4604,  978.3097, 1142.3528, 1060.1356, 1149.4731,
         954.3833,  842.4988, 1066.4982, 1183.9055,  901.0702, 1032.3599,
        1071.6194,  914.6154, 1013.5911, 1040.7545, 1150.8757, 1140.5216,
        1015.6245, 1228.8312,  943.4739, 1084.7600,  846.6843, 1054.6147,
        1026.3138, 1006.7280, 1070.1393, 1092.9215, 1199.4233, 1126.5934,
         913.5775, 1184.3049, 1101.3103,  913.6838, 1148.7036, 1107.8142,
        1142.9026,  938.3917, 1213.5172, 1112.1818, 1312.0530, 1036.5078,
        1154.7021, 1201.2712, 1271.8055, 1347.6204, 1151.7701, 1246.2349,
        1148.0302, 1269.2209, 1576.4780,  851.9376, 1309.7461, 1014.0394,
         972.9189, 1292.3452, 1086.5315, 1269.7926,  998.7484, 1106.8536,
        1283.7135,  971.8422, 1086.6499, 1075.0929, 1230.3881, 1170.8159,
        1132.0586, 1134.8328, 1026.9181, 1200.2761,  756.5006, 1140.1648,
        1065.0612, 1266.9827, 1021.4365,  978.7738, 1061.2494, 1147.5758,
        1096.9810, 1145.8601, 1263.3035, 1057.6881,  963.7272, 1135.7123,
         963.4730, 1130.9871, 1180.7163, 1169.5665, 1184.4734, 1307.3961,
        1008.3345,  939.7734, 1024.2158,  965.8685, 1162.0087, 1079.9702,
        1153.5060,  907.7572, 1298.9363,  957.5343,  831.0596,  942.4391,
        1043.7086, 1157.4459, 1010.8845, 1193.8887, 1150.7479, 1118.6826,
        1171.6864, 1185.2327, 1159.4927,  962.6597, 1075.1299, 1297.3069,
        1287.0920, 1052.2905, 1181.6036, 1030.4131, 1124.6188,  848.4858,
        1091.0729, 1289.9686,  970.7209, 1088.2662,  909.2570, 1041.9601,
         923.5124, 1197.2502, 1207.5752, 1068.8209, 1068.5085, 1002.0996,
        1158.2416, 1150.0956,  880.9117,  994.8238, 1180.0548,  982.5902,
         799.1051, 1023.6804])), ('encoder.7.2.conv2', tensor([ 8128.7363,  8490.5381,  8587.9473,  8881.3203,  6770.2354, 10531.9014,
         8724.7529,  8447.0752,  8711.1357,  8640.6055,  8549.1426, 10570.6074,
        10530.3467,  9737.4482,  9120.4941,  9330.5293,  9414.3604, 12121.7373,
         8361.3008,  8028.9604,  8949.2139,  9816.2061,  7872.4409, 10916.8457,
         9383.5400,  9257.8115, 11589.3604,  9659.0527,  8883.5967,  9144.1621,
        10250.2246,  8704.1094,  9679.3438,  8412.9775,  7936.8794,  8405.4072,
         8632.3330,  8296.3916,  8906.4912, 10327.2227,  8786.8643, 11031.5713,
        10474.2266,  9950.4775,  9950.6338,  8923.1387,  8895.1992,  8631.5918,
         9046.0977,  7862.2568, 10273.1797,  8923.8486,  9570.3711, 10054.8760,
        10039.4863,  8677.0146,  9481.5312,  9742.0732,  9605.2500, 10467.2363,
        10150.0576,  9555.1904, 10222.9648,  8350.8711,  7418.4707,  9761.1562,
         8667.4307,  7789.3291,  7724.5728,  7477.6973,  9060.0303,  8977.0977,
         9752.3926, 10570.9307,  8867.1357,  8829.4023,  9325.4414,  9634.8037,
        11556.6699,  9356.3750,  8248.8574,  8344.2861,  8490.7383, 10071.7637,
         7450.9590,  9863.5986,  8285.0127,  9505.2852,  7981.2144, 10251.1113,
         9150.8018,  8721.2734,  7603.8169,  7080.5415, 10903.5195,  9844.3008,
         9663.5332,  8955.7314, 10080.2881,  9477.5264,  9484.1240,  8474.5869,
         8033.0610,  8993.3711,  9241.0938,  7651.9810, 10033.7119,  8760.6982,
         9032.4980,  7426.6929,  7790.8354,  8805.2266, 10403.3613,  9938.7646,
        11190.4346, 10665.1074,  8143.4771,  8042.8730, 11098.1416,  9766.6514,
         8854.8857, 10849.2217,  8616.8926, 10523.4893,  9667.7383,  8796.8936,
         9485.3945, 10933.5332, 10757.5107,  9759.9346,  9696.9082,  8615.1309,
         9980.3682,  8711.8350, 10364.5117,  8880.4805,  8303.6377,  9498.4072,
         9960.1006,  9344.1826, 10224.9170,  9329.8008,  9744.1504,  9274.0371,
        11091.8887,  9354.0420, 10000.7637, 12988.3457,  8360.5869, 10488.6719,
         8125.9888,  9192.0469, 10031.5596,  8017.1904,  8112.6768,  8107.3745,
        10530.1602,  9309.3105,  6804.5103,  9236.6602,  8344.6299, 10500.2119,
         8681.3643,  7813.6025,  9941.2842,  7741.4551,  9833.1973,  9067.5439,
        11417.1680,  7365.7466,  7657.6343,  8302.2461,  9584.8418,  8526.6182,
         9234.1904,  9110.7725,  6729.0562,  8525.6895,  9895.3691,  7808.1948,
        10091.6748,  8486.0439,  9930.4902, 10795.2207, 10030.9932,  9019.0918,
         8658.6426,  8606.0918,  9650.5361,  8150.2832,  8348.0674, 10011.4346,
         9566.7979,  9043.7002,  9117.1484, 11909.7314,  7938.6929,  9185.6582,
         8212.9521,  9019.4014,  8519.0859, 12525.2568,  9110.6562,  8609.7012,
         9991.1553, 10516.9717,  8802.0322, 10222.1504,  7983.2490,  8529.8428,
         7640.4263,  9668.8037,  8215.2246,  9519.1416, 13146.6006, 10114.9170,
        11158.1260,  9952.8936, 10532.4570, 10549.1113,  8505.2012,  8783.1592,
         8063.1016, 10089.5820,  8355.1846,  9273.6475, 11310.6924,  8293.3232,
        10857.3838,  8928.5381,  9399.5273, 10637.7793,  7404.2290,  8966.1357,
         7198.0801,  8490.0469,  9509.1982, 10904.7500,  8944.5039,  9599.9590,
         8623.7998,  8240.4873,  7541.1128,  9049.9785,  9618.9492, 10661.3574,
         9901.7568,  8039.8848,  8355.9932,  8196.7969,  9191.7080,  9800.8975,
         9390.5312,  8581.1748, 10493.4121,  8697.8047,  8106.4624, 10398.5947,
        12875.3496,  7727.4727,  8275.8252,  8738.3193,  6614.7065, 11685.8359,
        10271.2314,  8493.9346,  9697.0098, 10037.3223,  8821.1035,  8724.0908,
         9192.3896,  8400.6465,  7236.6123,  9683.7305,  8998.9229, 10047.2832,
         8963.8623,  9966.2246, 12241.5674,  8444.0420, 12052.3906,  8855.2051,
         7393.5601,  9654.6553, 12008.8066,  8838.7959, 12400.3369, 11037.5098,
         9048.6152,  9078.6650, 10168.1660,  8924.8691,  9555.7158,  9340.5703,
         9552.8369,  9332.8604,  9947.0830,  9414.0605,  8062.8423, 12282.5283,
         8942.8682,  7932.3301,  9921.1064,  8852.7070,  9465.4893,  9611.6016,
         7851.6777,  7449.7793, 10060.1875, 11292.7725,  9400.6875, 12388.0771,
         8328.9385,  9873.5078,  9635.6992,  8952.2432,  8937.2725,  8279.1426,
        10476.0420,  9031.8027, 10502.9053,  9110.4980,  9842.2705, 10155.2471,
         8934.0830, 10063.9033,  9340.2246,  8525.0938,  9785.6250, 10928.1953,
         9083.3975, 10471.5869, 11105.7041, 11377.2178,  7903.1704,  9172.9678,
        10010.9238, 11782.2090,  8769.8438,  8368.2559,  7443.1196,  8736.3418,
         8676.5000,  9712.7012, 10046.7764,  8761.0625,  7628.1162,  8478.8535,
         9048.4082,  9652.9619, 10345.8623,  7518.1108,  7922.7876, 13100.4922,
         7793.6064,  9032.7637,  8141.1372,  8815.0908,  8116.7524,  7962.3369,
         8363.1797,  8169.0508, 11912.3945,  9038.6270, 12083.6182,  8911.3086,
         8870.6113,  8683.4453, 10076.9658, 10478.3818,  9759.0908, 10243.6406,
         8702.9072,  8584.5557,  8640.5361, 11841.1758,  8193.5889,  7570.8564,
         9156.5430,  7957.0337,  9699.7100,  9486.0850, 12188.1885,  9832.2109,
         7797.3975, 10473.7314,  9536.7354, 11018.7734,  9681.0771, 10672.4121,
         8631.6113, 11318.4746, 10699.9922,  8050.2690, 10977.9766,  9833.9990,
        11249.3525,  9107.6504,  8862.3066,  8976.1201,  8431.7646,  9170.5029,
         7465.8994,  8906.1807,  7967.2593, 10219.1631,  9768.9521, 10834.1348,
         9216.0674,  9108.1182,  9446.0703,  7710.5815,  7308.9976, 10012.2168,
         9234.3027,  9714.2148,  8790.5049,  9320.5439,  9026.2852,  9372.7900,
         9028.1914,  6837.5063,  8562.7422, 12051.0225, 10044.3564,  9188.2031,
         8766.9307,  8102.9844,  8876.4365, 10235.2666, 10051.9229, 13505.9121,
         8522.2197, 11080.9785,  8632.0469,  9151.3105,  8507.8164,  9626.1943,
        11717.3477,  8984.3877,  7699.1807, 10846.1260,  9458.5615,  8848.9141,
        10356.7070, 11327.1904,  9265.5146,  8450.8271,  8689.5557,  8158.0806,
         9913.7109, 10745.2012, 10197.4775, 10031.5723, 10084.5195,  8782.1338,
        10923.2666, 10223.1504, 10098.6357,  8984.9707, 10447.0508, 10423.1719,
        12301.2803,  8509.6963,  7539.7837,  9575.7715,  7287.3726,  9671.7822,
         8434.6533, 11266.4854,  9369.0215,  7549.7969,  9351.5850, 10147.7324,
        11299.8857,  9169.0137,  9926.1240,  9967.0645,  8764.0068,  9369.3809,
        10410.7529,  8536.3906,  9190.8604,  9915.6172,  8743.0605, 12431.0000,
         8311.7383,  7938.3799,  9228.6680,  9093.0908,  9259.1240, 12857.0957,
         8989.4473,  6715.4243, 10952.9414,  7873.0552, 12208.5635, 12549.2773,
         9184.1865, 10135.6426,  8140.5645, 11060.5938, 11640.0996,  7682.9268,
        11027.5010, 10421.6289,  9022.8730,  9438.1709,  7671.7974,  9448.7930,
         8752.3623,  8891.0459])), ('encoder.7.2.conv3', tensor([433.6976,  59.8468, 205.6327,  ...,   7.7433,  63.7847, 221.5422]))])
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2314e+19, 3.5805e+19, 1.4629e+19, 1.2658e+19, 4.2344e+19, 1.8549e+19,
        1.2640e+19, 1.2985e+19, 1.3692e+19, 5.0046e+18, 2.1519e+19, 2.1277e+19,
        1.7939e+19, 1.2902e+19, 1.2407e+19, 1.2219e+19, 1.0130e+19, 1.9789e+19,
        1.6667e+19, 3.5499e+18, 3.4053e+19, 3.0003e+19, 3.5260e+18, 1.7298e+19,
        2.3862e+19, 2.1117e+19, 1.4362e+19, 3.2309e+19, 3.8834e+18, 6.2625e+18,
        1.1033e+19, 1.3773e+19, 1.9995e+19, 4.2334e+19, 4.8003e+18, 9.7432e+18,
        1.0578e+19, 2.2304e+19, 2.2904e+19, 2.1785e+19, 1.6737e+19, 2.3531e+19,
        1.8101e+19, 1.9817e+19, 9.2796e+18, 4.7686e+18, 1.4602e+19, 1.0193e+19,
        2.0177e+19, 2.3931e+19, 1.8345e+19, 1.4242e+19, 1.7145e+19, 9.6107e+18,
        7.2855e+18, 4.5643e+18, 2.2064e+19, 1.3290e+19, 2.6670e+19, 2.9231e+19,
        3.9362e+18, 1.1573e+19, 3.6896e+18, 1.0198e+19])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.2569e+18, 6.1900e+18, 7.2848e+18, 4.7911e+18, 6.2889e+18, 6.8001e+18,
        2.8140e+18, 2.1002e+18, 9.4393e+18, 5.7817e+18, 5.7186e+18, 2.7892e+18,
        1.3855e+18, 5.3747e+18, 8.4641e+18, 7.2404e+18, 9.1113e+18, 9.2866e+18,
        6.6538e+18, 9.9027e+18, 1.3395e+19, 2.4616e+18, 4.0083e+18, 6.0455e+18,
        4.0390e+18, 9.3744e+18, 5.3382e+18, 9.9308e+18, 7.1110e+18, 1.3965e+19,
        3.4579e+18, 5.4717e+18, 1.3259e+18, 2.8285e+18, 7.1897e+18, 4.1442e+18,
        5.7034e+18, 1.7507e+18, 4.4656e+18, 1.4288e+19, 4.9288e+18, 2.8243e+18,
        8.6387e+18, 3.6032e+18, 7.1170e+18, 4.6726e+18, 1.3356e+18, 3.0999e+18,
        1.4154e+19, 3.3254e+18, 9.0210e+18, 1.3639e+19, 6.9838e+18, 2.9162e+18,
        1.7240e+18, 5.1722e+18, 1.7671e+18, 1.2288e+19, 3.0227e+18, 9.3829e+18,
        6.3996e+18, 3.9480e+18, 1.6263e+18, 4.7708e+18])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.0498e+18, 1.5625e+19, 5.5404e+18, 7.4757e+18, 4.2303e+18, 1.3091e+18,
        2.2355e+18, 4.6734e+18, 1.0675e+19, 6.0149e+18, 6.8029e+18, 7.4071e+18,
        6.2562e+18, 2.4444e+18, 5.6345e+18, 6.5620e+18, 7.5129e+18, 3.4932e+18,
        3.6984e+18, 2.4940e+18, 2.3990e+18, 2.6850e+18, 2.7913e+18, 3.3118e+18,
        9.7041e+18, 5.3106e+18, 2.6681e+18, 7.8152e+18, 2.4961e+18, 1.7641e+19,
        3.0076e+18, 1.5056e+19, 1.9835e+18, 9.4995e+18, 2.8697e+18, 7.4949e+18,
        5.9493e+18, 1.9029e+18, 3.8704e+18, 3.1287e+18, 1.1884e+19, 8.1859e+18,
        2.0313e+19, 9.1063e+18, 3.2119e+18, 2.3443e+18, 1.1169e+19, 7.4063e+18,
        7.9933e+18, 1.6759e+18, 9.2811e+18, 9.9128e+18, 1.4426e+19, 2.5692e+18,
        1.7155e+18, 2.3338e+18, 2.2945e+18, 1.5486e+18, 8.1236e+18, 4.7254e+18,
        9.1633e+18, 3.4136e+18, 9.5356e+18, 4.3144e+18])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0325e+16, 1.4248e+16, 3.9735e+16, 7.8514e+17, 1.6313e+16, 1.2490e+16,
        7.5830e+16, 1.0608e+17, 1.9965e+17, 1.0612e+17, 8.5782e+15, 2.0693e+16,
        9.7102e+16, 6.7654e+16, 1.3040e+16, 1.7925e+17, 8.9773e+15, 1.5196e+17,
        5.4967e+16, 1.8356e+18, 3.4007e+16, 7.3757e+16, 3.1172e+17, 8.2856e+16,
        3.7761e+17, 6.2476e+15, 8.8391e+16, 6.0978e+16, 1.2215e+17, 2.6284e+17,
        4.0717e+16, 1.9375e+16, 1.0283e+17, 1.0182e+17, 3.8164e+16, 5.0474e+17,
        1.5145e+16, 5.8232e+15, 2.0198e+17, 4.9186e+17, 1.9203e+17, 4.3433e+16,
        1.5719e+17, 4.9175e+16, 6.7234e+15, 9.0540e+16, 1.0882e+16, 2.2424e+16,
        2.3047e+16, 7.4446e+16, 1.0872e+16, 3.1568e+16, 6.3659e+16, 3.2734e+16,
        1.5169e+16, 1.9554e+16, 6.7038e+16, 2.0484e+17, 6.5984e+16, 2.8819e+16,
        1.6587e+16, 1.9889e+16, 8.6035e+16, 1.5263e+16, 1.9246e+16, 2.3227e+16,
        1.3443e+16, 4.2271e+16, 3.3196e+17, 2.3277e+16, 7.0518e+16, 2.7552e+16,
        1.7290e+17, 5.9402e+16, 2.6855e+16, 1.4744e+16, 7.9565e+16, 1.7359e+16,
        3.2020e+16, 2.2116e+16, 4.6015e+16, 3.2248e+16, 1.0779e+17, 1.0516e+16,
        1.2247e+17, 5.8926e+16, 3.6564e+15, 1.5175e+17, 6.9591e+16, 6.9249e+16,
        2.6784e+17, 2.9407e+17, 3.2386e+16, 1.5565e+17, 2.1759e+16, 4.5881e+16,
        2.6591e+16, 1.8980e+16, 5.8948e+16, 1.3101e+17, 3.6067e+17, 4.0252e+16,
        1.9997e+16, 2.2590e+17, 3.0600e+17, 5.4639e+16, 6.5065e+16, 7.1567e+15,
        6.9084e+17, 6.9145e+16, 8.6643e+15, 1.6383e+17, 1.9468e+16, 1.3228e+17,
        1.8877e+17, 2.0145e+17, 6.9600e+15, 2.0075e+16, 2.0125e+17, 2.2067e+16,
        1.8993e+17, 1.1970e+16, 4.6368e+16, 2.1562e+18, 1.3138e+16, 1.1398e+16,
        5.3069e+16, 1.7625e+16, 5.5824e+16, 3.0739e+16, 2.3166e+17, 4.8713e+16,
        1.7991e+17, 2.8149e+17, 1.3900e+17, 4.2760e+16, 6.1413e+16, 1.0996e+18,
        7.8883e+16, 3.1206e+16, 1.4650e+16, 2.9108e+17, 5.3972e+16, 3.9918e+17,
        1.9107e+15, 2.0419e+17, 6.5191e+16, 3.5526e+16, 2.9809e+16, 2.7170e+16,
        1.0602e+17, 6.9530e+16, 2.0898e+16, 5.6156e+17, 3.9181e+16, 1.0337e+16,
        5.2436e+16, 5.1093e+16, 4.0906e+16, 6.9671e+15, 1.6491e+16, 4.2658e+16,
        4.0243e+16, 5.1399e+16, 1.3084e+17, 3.4330e+17, 9.1619e+17, 7.6904e+16,
        8.3558e+16, 5.8074e+16, 2.4384e+17, 2.9310e+16, 1.4730e+16, 1.8104e+16,
        1.7855e+16, 7.2025e+16, 4.2037e+16, 1.0628e+18, 2.3727e+16, 1.6095e+17,
        6.4004e+16, 2.3716e+17, 9.4542e+15, 2.8564e+17, 2.1666e+16, 4.2103e+15,
        1.9846e+17, 1.2167e+17, 8.0965e+16, 2.9159e+16, 6.1590e+16, 1.8009e+16,
        1.0703e+17, 4.4084e+16, 6.0743e+16, 2.9653e+19, 1.5720e+16, 6.1317e+16,
        5.5828e+16, 3.4071e+17, 1.3771e+16, 4.1752e+16, 3.6445e+16, 4.1382e+16,
        7.2200e+16, 1.8347e+16, 1.5461e+17, 1.2148e+18, 6.4671e+17, 1.2524e+16,
        2.9485e+17, 1.4338e+16, 1.2832e+16, 7.4884e+15, 1.1346e+16, 1.0296e+16,
        2.3633e+16, 1.9047e+17, 1.3644e+16, 4.6905e+16, 1.2983e+17, 1.6636e+16,
        2.4488e+17, 1.3833e+17, 6.1948e+16, 1.5076e+16, 1.1340e+17, 1.5676e+16,
        3.5666e+16, 5.6024e+16, 2.2022e+16, 1.2659e+16, 7.6892e+16, 1.9292e+16,
        4.0431e+17, 2.9732e+16, 3.7723e+15, 2.1488e+16, 3.8944e+16, 8.0033e+16,
        5.6548e+17, 1.5976e+16, 2.3395e+17, 7.3629e+16, 2.1736e+17, 4.5542e+17,
        1.3708e+16, 4.4080e+16, 1.1049e+16, 4.5571e+16, 1.5146e+17, 3.9939e+16,
        1.1420e+16, 2.0440e+16, 4.6547e+16, 1.0158e+17])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.8428e+16, 5.9809e+16, 1.0892e+16, 7.4913e+17, 2.3523e+16, 2.8907e+16,
        5.8408e+16, 9.1281e+16, 2.0260e+17, 1.3566e+17, 3.6751e+16, 3.2434e+16,
        1.1943e+17, 6.4803e+16, 3.1399e+16, 1.4793e+17, 4.1966e+16, 1.2616e+17,
        9.7563e+16, 1.8843e+18, 4.0696e+16, 6.5223e+16, 3.1545e+17, 6.1531e+16,
        3.8849e+17, 3.6692e+16, 6.8142e+16, 3.2221e+16, 2.0122e+17, 2.0438e+17,
        1.0582e+17, 8.6052e+16, 7.3853e+16, 4.8036e+16, 4.2117e+16, 4.8758e+17,
        5.0745e+16, 3.5443e+16, 2.1834e+17, 4.9909e+17, 1.8799e+17, 9.4077e+15,
        1.5766e+17, 3.7873e+16, 2.5397e+16, 1.3210e+17, 3.4276e+16, 3.0380e+16,
        3.1778e+15, 7.6013e+16, 2.6991e+16, 4.8981e+16, 2.7192e+16, 1.3343e+16,
        2.1455e+16, 2.6418e+16, 8.5941e+16, 1.6518e+17, 8.3939e+16, 1.2667e+16,
        1.9165e+16, 6.7510e+16, 1.1564e+17, 4.6066e+16, 2.3392e+16, 3.6986e+16,
        8.2217e+16, 7.1471e+16, 3.6984e+17, 2.8373e+16, 5.8829e+16, 3.0949e+16,
        1.4702e+17, 1.0907e+16, 2.4299e+16, 2.8917e+16, 2.8365e+16, 6.8740e+16,
        3.9720e+16, 3.7988e+16, 8.1126e+16, 7.2247e+16, 8.3656e+16, 3.3651e+16,
        9.6205e+16, 5.0295e+16, 5.3485e+16, 1.6394e+17, 7.9536e+16, 1.0809e+17,
        2.4208e+17, 2.9095e+17, 4.8392e+16, 1.7672e+17, 3.1620e+16, 8.0736e+16,
        2.2020e+16, 4.1317e+16, 5.4114e+16, 1.9364e+17, 4.1865e+17, 3.3156e+16,
        2.1423e+16, 2.7531e+17, 3.0398e+17, 1.0649e+16, 1.0976e+17, 5.0533e+16,
        6.5395e+17, 9.3348e+16, 6.9333e+16, 1.4170e+17, 3.7239e+16, 7.5044e+16,
        1.9898e+17, 2.0322e+17, 4.0201e+16, 2.8252e+16, 1.5746e+17, 3.9734e+16,
        1.7315e+17, 2.4850e+16, 9.3724e+15, 2.1755e+18, 2.1268e+16, 3.3375e+16,
        8.1072e+16, 8.2255e+16, 5.7459e+16, 1.0050e+17, 2.8557e+17, 6.7049e+16,
        1.7341e+17, 2.2597e+17, 9.9699e+16, 9.5011e+16, 4.4178e+16, 1.1127e+18,
        1.4497e+17, 7.4849e+16, 3.5625e+16, 2.7529e+17, 3.9441e+16, 4.3961e+17,
        2.0832e+16, 1.9290e+17, 5.6474e+16, 3.6697e+16, 6.1645e+16, 3.2203e+16,
        7.8083e+16, 4.7013e+16, 3.9943e+16, 5.7235e+17, 2.9680e+16, 3.6481e+16,
        5.9613e+16, 1.1875e+16, 2.2585e+16, 3.6691e+16, 1.6166e+16, 3.0368e+16,
        2.3124e+16, 2.8228e+16, 1.7974e+17, 3.5208e+17, 9.5928e+17, 5.8275e+16,
        5.6973e+16, 5.7615e+16, 2.4596e+17, 5.4474e+16, 4.3979e+16, 2.7546e+16,
        4.4901e+16, 1.0239e+17, 7.5384e+16, 1.1185e+18, 1.3103e+16, 1.3462e+17,
        4.8535e+16, 2.6507e+17, 4.4754e+16, 2.5462e+17, 3.5022e+16, 4.2862e+16,
        1.8681e+17, 1.1385e+17, 7.6910e+16, 2.4013e+16, 1.2226e+17, 1.8138e+16,
        1.2131e+17, 1.3583e+16, 6.3302e+16, 2.9701e+19, 1.9951e+16, 1.1178e+17,
        8.5038e+16, 3.3648e+17, 3.7263e+16, 5.9974e+16, 1.4331e+16, 2.2727e+16,
        1.4663e+16, 4.8507e+16, 1.4594e+17, 1.1723e+18, 6.9586e+17, 2.6782e+16,
        2.9153e+17, 3.5106e+16, 8.2299e+16, 3.3584e+16, 3.9772e+16, 2.5607e+16,
        3.9831e+16, 1.9719e+17, 3.8430e+16, 9.5355e+16, 7.1515e+16, 2.6874e+16,
        2.3009e+17, 9.1178e+16, 7.3076e+16, 2.6263e+16, 1.5363e+17, 1.3401e+16,
        8.1530e+16, 3.7798e+16, 1.2855e+16, 2.8575e+16, 1.0822e+17, 4.8881e+16,
        3.8373e+17, 7.0919e+16, 4.6473e+16, 4.8149e+16, 2.0202e+16, 8.1104e+16,
        5.6819e+17, 8.6155e+16, 2.5258e+17, 5.8398e+16, 2.6723e+17, 4.9486e+17,
        1.9824e+16, 2.9207e+16, 4.5772e+16, 7.4019e+16, 1.5338e+17, 4.4297e+16,
        3.2899e+16, 6.8287e+16, 1.3950e+16, 1.4013e+17])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.0323e+16, 5.4394e+16, 8.3825e+16, 8.4575e+16, 8.7046e+16, 7.0540e+16,
        5.8123e+16, 8.3868e+16, 8.7411e+16, 7.4910e+16, 2.7025e+16, 3.7328e+16,
        1.1909e+17, 8.4903e+16, 2.4820e+16, 1.2555e+17, 7.5938e+16, 7.9428e+16,
        5.5404e+16, 9.3830e+16, 5.5943e+16, 7.2785e+16, 7.8397e+16, 7.0813e+16,
        8.4321e+16, 8.8067e+16, 6.5559e+16, 5.7749e+16, 8.1661e+16, 8.0807e+16,
        1.0021e+17, 5.9844e+16, 6.6889e+16, 9.3993e+16, 5.2819e+16, 6.8540e+16,
        9.4998e+16, 1.2262e+17, 1.0121e+17, 1.7537e+17, 1.0874e+17, 5.3705e+16,
        5.4270e+16, 4.2799e+16, 1.0746e+17, 7.8006e+16, 5.2647e+16, 8.1024e+16,
        1.0143e+17, 8.0373e+16, 8.8003e+16, 1.3065e+17, 1.1717e+17, 7.2233e+16,
        9.9573e+16, 6.4993e+16, 6.5979e+16, 9.9442e+16, 6.9982e+16, 6.5461e+16,
        8.6593e+16, 2.9704e+16, 6.9088e+16, 9.7666e+16])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.8878e+16, 6.9696e+16, 5.5724e+16, 8.0391e+16, 6.1746e+16, 1.1170e+17,
        1.1268e+17, 5.6350e+16, 9.9817e+16, 6.0751e+16, 6.3293e+16, 3.5268e+16,
        6.6228e+16, 7.0984e+16, 8.0027e+16, 5.7513e+16, 5.9958e+16, 6.5345e+16,
        6.7055e+16, 7.5581e+16, 6.5159e+16, 6.6858e+16, 7.1065e+16, 1.2632e+17,
        9.5924e+16, 4.6394e+16, 1.1614e+17, 1.0560e+17, 6.5262e+16, 1.1653e+17,
        7.2785e+16, 5.4595e+16, 6.2647e+16, 6.2928e+16, 9.0936e+16, 6.1007e+16,
        1.2461e+17, 5.4496e+16, 6.1487e+16, 5.6300e+16, 1.3461e+17, 5.4899e+16,
        1.3416e+17, 6.0014e+16, 1.0623e+17, 1.4778e+17, 5.6939e+16, 9.5768e+16,
        1.4771e+17, 1.2196e+17, 1.0629e+17, 1.3127e+17, 4.7361e+16, 1.3449e+17,
        1.3126e+17, 9.8803e+16, 9.3584e+16, 4.6521e+16, 6.9194e+16, 9.8773e+16,
        4.9271e+16, 6.7785e+16, 7.2913e+16, 4.8324e+16])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.8116e+16, 3.1458e+15, 5.2973e+14, 9.6556e+15, 2.9624e+15, 2.5356e+14,
        2.5211e+15, 1.9169e+15, 2.5966e+15, 1.0123e+15, 1.9006e+15, 1.9801e+15,
        1.5347e+15, 3.4612e+15, 1.8246e+15, 1.4136e+15, 4.1171e+15, 4.8347e+15,
        6.5953e+15, 3.0583e+15, 3.6927e+15, 4.2449e+15, 1.8954e+15, 4.3519e+15,
        9.7379e+14, 1.6767e+15, 2.3522e+16, 2.8864e+15, 6.8170e+15, 8.6457e+14,
        1.5826e+15, 6.7355e+14, 1.5421e+15, 9.1473e+14, 2.5015e+15, 4.5835e+15,
        2.2916e+16, 7.3959e+14, 5.2912e+15, 1.1124e+16, 1.0299e+16, 5.1051e+15,
        1.2800e+15, 7.4365e+15, 1.0491e+15, 3.1243e+15, 2.4627e+15, 1.0949e+16,
        6.4448e+14, 2.0967e+15, 1.9843e+15, 2.1142e+15, 1.9974e+15, 1.7814e+15,
        1.3325e+15, 6.5519e+14, 7.9403e+15, 1.0201e+15, 2.2238e+15, 8.3734e+14,
        2.5877e+15, 8.3355e+15, 3.1981e+15, 1.4959e+14, 3.0125e+15, 2.2340e+15,
        2.1590e+15, 1.9157e+15, 1.2444e+15, 3.2473e+15, 1.6982e+15, 2.1327e+15,
        1.5722e+15, 2.5010e+15, 4.3455e+14, 7.0727e+15, 2.5242e+15, 3.7331e+15,
        1.8663e+15, 1.7758e+15, 2.1166e+15, 2.0511e+15, 4.1177e+15, 2.6471e+15,
        3.4644e+15, 2.8222e+15, 2.2091e+15, 1.6635e+15, 2.9448e+15, 2.0008e+15,
        3.4112e+15, 2.9620e+15, 1.8866e+15, 1.0576e+15, 2.6095e+15, 2.1464e+16,
        2.3421e+15, 1.9227e+15, 1.4163e+15, 2.5697e+15, 1.3074e+15, 3.0287e+15,
        3.0737e+15, 4.1340e+15, 2.4119e+15, 1.9595e+15, 9.1701e+15, 1.9111e+15,
        3.4503e+15, 3.0947e+15, 1.6632e+15, 1.6435e+15, 1.1008e+15, 1.5917e+15,
        2.5353e+15, 3.7671e+15, 3.1401e+15, 2.8269e+14, 2.3595e+15, 1.0541e+15,
        2.1084e+15, 1.4270e+15, 1.5375e+15, 3.0744e+15, 4.9753e+14, 2.1307e+14,
        2.4144e+16, 1.5983e+15, 1.7461e+15, 4.2171e+16, 1.8781e+16, 1.1740e+16,
        1.0633e+15, 3.2837e+15, 3.3820e+15, 1.9901e+15, 3.0169e+15, 2.4031e+15,
        2.2259e+15, 6.2282e+15, 2.1315e+15, 2.2362e+15, 2.7607e+15, 1.0573e+16,
        9.7921e+14, 1.6097e+15, 1.0701e+15, 7.1879e+15, 5.3376e+15, 1.1518e+15,
        1.3723e+15, 6.9663e+15, 4.0923e+14, 8.6871e+15, 2.2225e+15, 2.1199e+14,
        1.9917e+15, 2.8065e+15, 1.7039e+15, 3.5633e+14, 6.2643e+14, 2.1512e+15,
        6.5448e+14, 3.1298e+15, 2.4257e+15, 2.5172e+15, 8.1185e+14, 2.4188e+15,
        3.9579e+15, 2.5776e+15, 1.9895e+15, 1.1056e+15, 2.7077e+15, 6.5820e+14,
        1.4991e+15, 8.8161e+14, 1.8183e+15, 1.4969e+16, 2.5426e+15, 5.1349e+15,
        4.5786e+15, 3.3648e+15, 1.5673e+16, 1.4377e+16, 2.6713e+15, 2.2899e+15,
        8.7207e+15, 6.4701e+14, 2.8256e+15, 3.0478e+15, 2.1863e+15, 2.9521e+14,
        1.0109e+16, 5.0741e+14, 2.8425e+15, 3.2886e+15, 4.1033e+15, 1.7793e+17,
        8.7224e+14, 2.7631e+15, 1.2864e+15, 1.2101e+15, 1.8352e+16, 2.4132e+15,
        6.6932e+15, 4.2584e+15, 1.1545e+15, 2.5692e+15, 1.5666e+15, 7.9443e+14,
        2.2856e+15, 1.6293e+15, 1.7255e+16, 7.4476e+14, 1.5581e+15, 5.2909e+14,
        1.9896e+15, 9.6544e+15, 3.0715e+15, 1.1647e+15, 1.8338e+15, 2.2765e+14,
        2.6008e+15, 6.1713e+15, 2.5408e+15, 6.6625e+14, 3.4055e+15, 2.6832e+15,
        1.6568e+16, 2.5013e+15, 2.0598e+15, 3.1506e+14, 4.3545e+15, 2.6338e+15,
        8.8629e+16, 3.2889e+15, 9.3709e+14, 3.9390e+15, 9.3228e+14, 1.3990e+15,
        1.9906e+15, 2.3276e+15, 2.3877e+15, 2.7609e+15, 1.9947e+16, 1.0027e+16,
        1.4322e+16, 2.7454e+16, 1.9339e+15, 2.3985e+15, 7.9418e+14, 1.1328e+16,
        5.6489e+15, 4.4544e+14, 9.7548e+14, 8.6529e+15])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.4843e+15, 5.1629e+15, 5.2447e+15, 6.4197e+15, 7.5570e+15, 4.5077e+15,
        5.7541e+15, 4.0019e+15, 2.2520e+15, 4.9729e+15, 8.8151e+15, 3.6249e+15,
        3.2199e+15, 3.7732e+15, 3.5212e+15, 5.8680e+15, 2.6658e+15, 4.8251e+15,
        3.7401e+15, 2.1388e+15, 3.7808e+15, 4.5342e+15, 5.4285e+15, 4.8592e+15,
        7.6248e+15, 2.7270e+15, 5.1743e+15, 4.2986e+15, 4.7613e+15, 6.0287e+15,
        4.3019e+15, 3.0099e+15, 3.0286e+15, 4.3241e+15, 2.4166e+15, 5.5378e+15,
        4.9992e+15, 3.9423e+15, 5.8374e+15, 5.3096e+15, 5.4075e+15, 3.1253e+15,
        4.5766e+15, 1.9770e+15, 2.5228e+15, 5.0245e+15, 5.9657e+15, 3.9073e+15,
        5.9166e+15, 4.5932e+15, 4.7732e+15, 5.0890e+15, 3.5660e+15, 5.8312e+15,
        4.0385e+15, 4.1032e+15, 4.5178e+15, 4.0670e+15, 4.8589e+15, 4.3320e+15,
        4.7859e+15, 5.4007e+15, 3.8441e+15, 2.5647e+15])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.1379e+15, 3.6346e+15, 5.1447e+15, 8.3811e+15, 4.9835e+15, 7.3477e+15,
        7.0458e+15, 5.5424e+15, 7.7461e+15, 4.6632e+15, 4.9155e+15, 3.6122e+15,
        4.1333e+15, 3.2027e+15, 3.9982e+15, 6.5348e+15, 8.1215e+15, 4.3368e+15,
        5.3686e+15, 5.8007e+15, 5.6083e+15, 6.8635e+15, 4.1164e+15, 6.7329e+15,
        4.6300e+15, 3.7840e+15, 7.0161e+15, 7.3861e+15, 4.1296e+15, 7.6545e+15,
        4.5724e+15, 5.9888e+15, 6.7467e+15, 6.0653e+15, 6.1000e+15, 4.2624e+15,
        3.5640e+15, 7.9773e+15, 4.1644e+15, 5.8969e+15, 4.2028e+15, 6.4889e+15,
        3.9553e+15, 3.1056e+15, 3.9120e+15, 6.5224e+15, 5.4105e+15, 5.0070e+15,
        8.2998e+15, 5.3175e+15, 4.5781e+15, 4.9975e+15, 4.2233e+15, 5.5551e+15,
        6.1623e+15, 5.2799e+15, 4.2597e+15, 7.8799e+15, 7.2808e+15, 4.7210e+15,
        3.9503e+15, 6.4145e+15, 5.8289e+15, 4.0703e+15])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5613e+13, 1.3721e+14, 5.5796e+13, 2.2808e+14, 8.6542e+13, 3.9800e+13,
        1.0039e+14, 8.0636e+13, 8.3411e+14, 1.6939e+14, 1.1542e+14, 1.3783e+14,
        6.4839e+13, 2.3758e+14, 2.2448e+14, 2.9404e+14, 1.0046e+14, 1.4729e+14,
        5.8095e+15, 3.5625e+14, 3.0386e+14, 1.2408e+14, 1.3237e+14, 2.9736e+15,
        3.6097e+14, 1.5182e+14, 7.6705e+13, 1.3885e+15, 1.1208e+14, 5.0299e+13,
        4.5862e+13, 8.4381e+13, 1.4669e+14, 1.4556e+14, 1.6826e+14, 8.0565e+13,
        1.0856e+14, 1.5918e+13, 4.0833e+13, 3.9218e+14, 1.0254e+14, 8.6692e+13,
        1.3197e+14, 7.2669e+13, 2.0373e+13, 2.2860e+14, 1.2344e+14, 7.8996e+13,
        6.7554e+13, 3.0923e+14, 1.0630e+14, 1.2735e+14, 7.3208e+14, 2.0322e+14,
        1.2488e+14, 1.5117e+13, 9.9334e+13, 5.6339e+13, 1.1485e+14, 1.5006e+14,
        1.3804e+14, 5.7218e+15, 8.1571e+13, 2.4150e+13, 1.3191e+14, 2.4745e+14,
        1.0467e+14, 1.5100e+14, 2.8331e+14, 1.6153e+14, 5.1847e+13, 1.5861e+14,
        4.9205e+13, 6.6876e+13, 1.6726e+14, 2.4189e+14, 1.0799e+14, 8.4938e+13,
        1.6577e+14, 4.8780e+13, 2.4486e+14, 1.3253e+14, 3.5198e+14, 8.8716e+13,
        1.4239e+14, 2.6202e+14, 3.8231e+13, 2.2280e+14, 3.7951e+14, 4.6633e+14,
        4.3880e+14, 2.7740e+14, 7.4928e+13, 5.3126e+13, 1.2988e+14, 1.4367e+14,
        1.2961e+14, 6.3170e+13, 4.4427e+13, 2.6531e+13, 1.2462e+14, 1.2022e+14,
        5.0465e+13, 4.9989e+13, 1.7280e+14, 8.5785e+13, 6.1447e+14, 1.1319e+14,
        1.0884e+15, 1.0426e+14, 1.8720e+14, 4.3148e+13, 5.7319e+13, 3.5002e+14,
        6.4295e+13, 2.5306e+14, 6.9886e+13, 7.3216e+13, 6.0404e+13, 1.0563e+14,
        1.0634e+14, 1.0575e+14, 3.5654e+14, 1.3545e+14, 7.2114e+14, 8.3076e+12,
        1.1135e+15, 3.9443e+14, 8.7732e+13, 1.9183e+14, 1.1638e+14, 1.6504e+14,
        4.2183e+13, 2.0674e+14, 1.4414e+15, 1.4039e+14, 1.8286e+14, 9.4951e+13,
        5.5728e+14, 4.0319e+15, 9.1825e+13, 2.4811e+14, 3.1170e+14, 2.7102e+14,
        2.3684e+13, 5.7317e+14, 5.9878e+13, 1.2938e+14, 2.1578e+14, 4.7753e+14,
        2.5431e+14, 1.3016e+14, 8.3628e+13, 8.4213e+13, 9.2737e+13, 8.3156e+13,
        2.5918e+14, 5.0459e+14, 1.2093e+14, 1.5895e+14, 1.0204e+15, 1.7166e+14,
        4.4146e+13, 9.0652e+13, 6.2679e+14, 7.5535e+13, 6.5155e+13, 4.1390e+13,
        2.3999e+15, 5.0816e+13, 4.5445e+13, 4.0028e+14, 5.6919e+14, 5.7402e+13,
        2.0888e+14, 1.1784e+14, 1.0447e+14, 1.0114e+14, 3.1409e+13, 3.8309e+14,
        9.5431e+13, 7.3175e+13, 1.0607e+14, 1.4150e+14, 9.7332e+13, 3.4130e+14,
        9.7141e+13, 6.0102e+13, 3.4593e+14, 8.0071e+14, 7.7128e+13, 3.5939e+13,
        1.1486e+14, 4.0252e+13, 4.6045e+14, 1.5563e+14, 5.5318e+13, 6.3470e+13,
        2.2990e+14, 1.1889e+14, 1.9298e+14, 4.6157e+13, 2.9584e+14, 6.8857e+14,
        1.5166e+14, 1.6445e+15, 3.8895e+14, 2.7924e+14, 4.8562e+13, 9.6055e+13,
        2.3869e+14, 6.1527e+13, 2.1341e+14, 2.0083e+14, 1.0997e+14, 1.0683e+13,
        8.6041e+13, 1.7550e+15, 3.6306e+14, 4.3352e+13, 2.9334e+14, 3.6245e+13,
        4.2131e+15, 3.9274e+14, 9.2613e+13, 8.4457e+13, 1.5759e+15, 3.7723e+13,
        2.3061e+14, 3.2485e+13, 8.3751e+13, 1.8411e+13, 2.8605e+14, 7.1736e+13,
        2.3230e+15, 2.0840e+15, 1.5937e+13, 1.4982e+15, 3.5856e+13, 6.2128e+14,
        1.0239e+14, 1.0041e+14, 1.4954e+15, 1.6611e+14, 1.3349e+14, 2.4132e+14,
        1.2765e+14, 3.6684e+14, 8.1710e+13, 1.4635e+14, 3.7105e+13, 2.3045e+14,
        1.1641e+14, 4.9188e+14, 1.2148e+14, 1.6351e+14])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.4576e+14, 1.8020e+14, 1.3748e+14, 1.4002e+14, 8.4466e+13, 1.2987e+14,
        1.3027e+14, 1.0982e+14, 2.4899e+14, 1.3567e+14, 1.1786e+14, 8.4668e+13,
        7.2419e+13, 1.2344e+14, 1.8515e+14, 1.5601e+14, 2.4792e+14, 1.3794e+14,
        8.5458e+13, 2.1477e+14, 9.9301e+13, 1.1880e+14, 1.6412e+14, 1.6501e+14,
        1.7985e+14, 1.3728e+14, 1.3113e+14, 1.8742e+14, 1.2058e+14, 2.2672e+14,
        1.9054e+14, 1.2285e+14, 1.1380e+14, 1.8769e+14, 1.7315e+14, 1.7446e+14,
        1.5342e+14, 1.3809e+14, 1.8652e+14, 1.7053e+14, 1.9596e+14, 1.6987e+14,
        1.3277e+14, 1.4368e+14, 2.1577e+14, 7.8447e+13, 2.5797e+14, 7.5744e+13,
        1.7536e+14, 1.4888e+14, 1.9560e+14, 1.2291e+14, 9.1334e+13, 1.3298e+14,
        1.8030e+14, 1.3258e+14, 2.9343e+14, 6.7698e+13, 9.5571e+13, 1.8033e+14,
        1.5360e+14, 2.3335e+14, 1.9438e+14, 1.7868e+14, 1.3425e+14, 1.7066e+14,
        7.6663e+13, 1.4899e+14, 1.5088e+14, 1.4933e+14, 1.3640e+14, 1.3859e+14,
        8.0195e+13, 9.4011e+13, 1.4174e+14, 1.5448e+14, 9.7236e+13, 2.1739e+14,
        1.8004e+14, 1.3389e+14, 1.0799e+14, 1.4084e+14, 1.6967e+14, 1.3829e+14,
        1.2106e+14, 1.7092e+14, 1.8342e+14, 1.1996e+14, 1.6645e+14, 8.7264e+13,
        9.6145e+13, 1.4964e+14, 2.2791e+14, 1.5942e+14, 1.9604e+14, 1.9573e+14,
        1.8269e+14, 1.2674e+14, 8.9433e+13, 2.2218e+14, 1.9738e+14, 2.1719e+14,
        1.1805e+14, 1.1610e+14, 1.2490e+14, 1.0667e+14, 1.1744e+14, 1.5153e+14,
        1.2701e+14, 1.2353e+14, 1.4014e+14, 8.2443e+13, 1.6171e+14, 1.8793e+14,
        1.0442e+14, 1.6107e+14, 1.6779e+14, 1.6645e+14, 2.2223e+14, 1.8594e+14,
        1.9650e+14, 2.4991e+14, 1.0683e+14, 8.3719e+13, 1.2089e+14, 1.0044e+14,
        7.8599e+13, 1.7190e+14])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.9027e+14, 1.8587e+14, 1.3047e+14, 2.6510e+14, 1.7814e+14, 3.1120e+14,
        2.0690e+14, 2.3203e+14, 2.0606e+14, 1.5756e+14, 2.1373e+14, 1.7897e+14,
        2.6931e+14, 1.9658e+14, 1.1052e+14, 1.8144e+14, 2.0994e+14, 2.0413e+14,
        1.3456e+14, 1.7404e+14, 1.1161e+14, 1.7252e+14, 2.1614e+14, 2.5172e+14,
        1.3164e+14, 1.6723e+14, 1.8653e+14, 2.1398e+14, 2.0638e+14, 1.4417e+14,
        1.7455e+14, 2.1173e+14, 1.4891e+14, 2.3973e+14, 2.0106e+14, 1.9198e+14,
        1.4998e+14, 1.7373e+14, 1.6803e+14, 2.5231e+14, 2.3098e+14, 2.0853e+14,
        1.8620e+14, 1.3082e+14, 1.5255e+14, 2.4470e+14, 1.8914e+14, 1.8636e+14,
        2.3957e+14, 1.4605e+14, 2.0805e+14, 1.8551e+14, 1.9619e+14, 1.7093e+14,
        2.0709e+14, 1.7323e+14, 1.9973e+14, 1.1120e+14, 2.1139e+14, 1.5035e+14,
        2.5589e+14, 2.1271e+14, 1.9698e+14, 2.4310e+14, 2.0335e+14, 2.8693e+14,
        1.3084e+14, 1.5479e+14, 1.1103e+14, 3.7720e+14, 3.0251e+14, 2.5449e+14,
        2.0976e+14, 1.6239e+14, 2.4644e+14, 2.3799e+14, 2.1414e+14, 1.8523e+14,
        1.7905e+14, 1.4819e+14, 1.4817e+14, 1.9074e+14, 2.1405e+14, 1.2528e+14,
        3.1203e+14, 2.7317e+14, 2.2934e+14, 1.2534e+14, 2.5420e+14, 2.8382e+14,
        2.0828e+14, 1.6313e+14, 1.9095e+14, 1.9656e+14, 2.0398e+14, 1.5495e+14,
        1.4014e+14, 2.2070e+14, 1.4689e+14, 2.2138e+14, 2.4383e+14, 2.9469e+14,
        2.7293e+14, 2.2362e+14, 1.8030e+14, 2.5287e+14, 1.6559e+14, 2.0931e+14,
        1.7335e+14, 1.2082e+14, 1.9709e+14, 1.2987e+14, 2.6076e+14, 2.6357e+14,
        3.0753e+14, 2.1291e+14, 2.5843e+14, 2.4043e+14, 2.3889e+14, 2.4846e+14,
        2.2119e+14, 2.8896e+14, 1.8576e+14, 1.8599e+14, 1.3341e+14, 1.6925e+14,
        2.6648e+14, 2.7824e+14])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3915e+13, 7.6043e+11, 5.5281e+12, 1.1985e+13, 1.1604e+12, 2.4453e+12,
        1.6008e+12, 1.6389e+12, 1.6589e+13, 6.8030e+12, 3.4855e+12, 1.9459e+12,
        2.0334e+12, 1.1091e+14, 5.9113e+12, 8.1020e+11, 1.5693e+13, 4.7916e+14,
        1.6966e+12, 2.1393e+12, 7.1035e+12, 6.2717e+11, 3.6291e+12, 1.5250e+12,
        4.1707e+12, 1.1415e+12, 1.8062e+13, 8.5982e+11, 2.1154e+12, 7.8564e+11,
        4.3119e+11, 4.0173e+12, 2.4022e+12, 2.9908e+12, 8.0399e+11, 2.3599e+12,
        3.5653e+12, 5.0549e+13, 1.5377e+13, 2.3220e+12, 5.0148e+12, 2.0063e+12,
        2.8407e+13, 1.2066e+12, 4.7261e+12, 1.1497e+13, 6.4852e+12, 1.4175e+12,
        2.6783e+12, 8.0259e+11, 9.2005e+11, 1.9604e+12, 1.3986e+12, 5.4468e+12,
        5.2974e+12, 2.3291e+12, 1.2104e+13, 2.0536e+12, 9.3139e+11, 9.5213e+11,
        3.5502e+12, 7.8372e+11, 1.5479e+12, 1.8970e+13, 2.2844e+13, 8.4819e+12,
        4.8022e+12, 1.1236e+13, 1.6119e+12, 1.8017e+13, 9.4258e+12, 5.1335e+12,
        3.1352e+13, 8.7626e+11, 1.0787e+12, 6.2161e+11, 3.6076e+12, 2.5322e+13,
        8.2782e+12, 1.3388e+13, 4.7782e+11, 2.3444e+12, 2.0820e+12, 7.8902e+11,
        2.2213e+13, 4.2280e+12, 1.2578e+13, 8.5188e+11, 1.4101e+12, 4.9547e+13,
        1.9610e+12, 6.4247e+11, 7.7881e+12, 8.3775e+11, 2.8506e+12, 9.2453e+10,
        1.0398e+13, 3.4420e+12, 4.8814e+12, 1.7200e+12, 6.9181e+11, 2.9476e+12,
        8.2408e+12, 8.4693e+12, 1.7564e+12, 5.6123e+12, 1.8682e+12, 1.3279e+13,
        7.9134e+13, 7.8307e+11, 3.6315e+12, 4.4621e+12, 2.2075e+12, 1.8823e+11,
        2.2099e+13, 8.7112e+13, 6.1167e+11, 9.5355e+11, 1.8230e+13, 3.7488e+13,
        1.3790e+12, 9.0864e+12, 1.0689e+12, 1.0214e+12, 4.4346e+12, 3.3963e+12,
        9.7268e+11, 8.1555e+11, 3.3225e+12, 1.7622e+12, 1.7462e+12, 3.7245e+11,
        5.4844e+12, 2.6208e+12, 2.3370e+13, 3.2503e+12, 1.0313e+12, 3.9170e+12,
        2.9543e+12, 4.5244e+12, 1.8223e+12, 5.2867e+12, 4.6361e+12, 6.7536e+12,
        1.8030e+12, 3.6222e+11, 5.5090e+12, 4.7737e+11, 8.9615e+12, 7.7966e+12,
        3.0921e+11, 9.5698e+12, 3.1893e+12, 2.4343e+12, 2.3603e+12, 2.2110e+12,
        8.1381e+12, 4.7836e+12, 1.1475e+13, 1.8841e+12, 1.2016e+12, 1.2543e+14,
        1.5669e+12, 2.0556e+13, 5.5306e+12, 8.1079e+11, 7.1608e+12, 1.6219e+12,
        3.1309e+12, 3.3046e+13, 5.4670e+12, 1.1278e+12, 3.9290e+12, 1.7285e+12,
        1.4981e+13, 2.1822e+13, 7.2197e+11, 2.7905e+12, 5.5746e+13, 5.6714e+12,
        4.2782e+12, 3.4384e+12, 2.0326e+12, 1.2838e+13, 4.5680e+11, 1.4487e+13,
        5.8110e+12, 1.1382e+12, 6.6496e+12, 7.7021e+11, 2.5310e+13, 1.7144e+12,
        7.6899e+11, 6.4682e+11, 1.3275e+12, 1.3951e+12, 9.0551e+11, 3.9979e+12,
        6.8951e+12, 4.0379e+12, 1.3848e+13, 6.7329e+11, 2.4992e+13, 3.1655e+12,
        2.3155e+12, 1.5950e+12, 5.5904e+12, 2.8090e+12, 2.7803e+12, 1.7737e+12,
        1.4320e+12, 3.9054e+12, 6.3282e+12, 5.3813e+11, 4.6938e+13, 2.7419e+12,
        5.6449e+12, 1.2343e+12, 2.2792e+12, 8.6132e+11, 6.8094e+12, 6.9485e+12,
        1.0121e+12, 5.2798e+12, 2.5736e+12, 3.8004e+12, 1.8153e+12, 9.5844e+11,
        1.1335e+13, 5.1681e+11, 5.8653e+12, 2.5345e+14, 6.9479e+11, 3.2472e+12,
        2.5741e+12, 6.9671e+12, 3.4422e+12, 7.1519e+12, 7.5617e+11, 7.3276e+13,
        3.6987e+12, 5.5156e+12, 4.0490e+12, 2.7339e+12, 2.7794e+11, 8.1553e+11,
        2.9381e+12, 2.1456e+12, 6.0538e+11, 7.2559e+12, 4.3186e+12, 1.6842e+12,
        5.0934e+11, 8.5194e+11, 3.3978e+12, 8.6639e+12, 2.6040e+12, 3.1548e+12,
        2.8838e+12, 3.4385e+13, 1.5101e+12, 2.4866e+11, 4.7838e+12, 2.3245e+12,
        8.2447e+11, 1.5039e+13, 5.8894e+12, 8.7920e+11, 3.4305e+12, 2.0829e+12,
        4.0184e+12, 1.2673e+12, 1.3414e+12, 3.5010e+12, 9.5391e+11, 1.4876e+12,
        2.7499e+12, 9.7646e+12, 4.8949e+12, 2.0451e+12, 1.9185e+13, 2.4608e+12,
        3.6465e+13, 1.4450e+12, 1.7546e+12, 4.0552e+12, 3.9518e+12, 1.1521e+13,
        2.2813e+13, 4.0186e+12, 3.8432e+12, 2.6249e+12, 7.2244e+11, 7.0502e+11,
        2.1013e+13, 2.5340e+11, 3.1416e+12, 3.0644e+12, 1.6075e+12, 1.6182e+12,
        1.3455e+12, 7.4220e+11, 6.5985e+12, 3.1397e+12, 2.6568e+12, 4.1871e+12,
        1.2402e+12, 2.7986e+13, 8.4497e+11, 4.0629e+12, 1.4828e+12, 4.0030e+12,
        1.5740e+12, 9.4954e+11, 3.6941e+12, 2.6581e+12, 3.8567e+12, 2.4011e+11,
        4.1305e+12, 4.5916e+12, 8.0253e+12, 1.0839e+13, 2.2926e+13, 1.7606e+13,
        2.1124e+12, 1.0987e+13, 5.9169e+13, 2.7454e+12, 7.4654e+12, 2.9945e+11,
        7.5082e+12, 2.1398e+12, 2.9958e+13, 4.0435e+12, 1.1597e+12, 5.9513e+12,
        4.2319e+12, 2.1290e+12, 1.1029e+12, 1.4776e+13, 2.3894e+12, 1.0216e+12,
        2.7206e+12, 7.6874e+13, 4.2105e+12, 5.3215e+12, 5.6229e+11, 1.4963e+12,
        3.3077e+12, 4.0302e+12, 1.6369e+12, 2.3069e+12, 7.2498e+11, 3.3849e+12,
        9.9214e+12, 8.3049e+12, 8.8064e+11, 2.3017e+12, 3.1178e+12, 6.7081e+12,
        3.4016e+11, 1.8486e+12, 1.1908e+12, 8.1154e+12, 1.0444e+12, 3.5709e+12,
        8.2214e+12, 4.6793e+12, 3.5629e+12, 3.8365e+12, 5.4227e+11, 5.5198e+12,
        1.0598e+12, 6.0951e+11, 2.2503e+12, 1.9596e+13, 1.2781e+13, 2.7220e+11,
        5.3736e+12, 1.7685e+13, 9.9702e+11, 1.8412e+11, 1.2928e+12, 9.5621e+11,
        4.0450e+11, 1.2916e+12, 1.0235e+13, 1.5245e+14, 5.5615e+12, 3.5349e+12,
        2.9913e+12, 1.5319e+13, 6.3338e+12, 2.6043e+12, 1.7962e+12, 3.2660e+12,
        5.9433e+12, 4.3593e+12, 1.3337e+12, 3.3048e+12, 7.0339e+11, 2.5738e+11,
        8.2709e+12, 1.7176e+12, 4.5618e+12, 1.6471e+13, 4.3927e+12, 7.9603e+11,
        6.4646e+12, 2.0809e+12, 3.9551e+13, 1.2097e+13, 6.5871e+12, 6.5073e+11,
        8.1364e+11, 2.5313e+12, 5.8754e+11, 5.2723e+12, 3.3778e+11, 3.1755e+11,
        1.5101e+13, 7.8649e+11, 3.2373e+12, 1.2677e+13, 1.6002e+12, 5.5869e+12,
        3.3859e+12, 2.9324e+12, 4.6691e+12, 4.3599e+12, 3.6069e+13, 9.5150e+12,
        1.8347e+13, 9.3236e+11, 5.5919e+12, 2.9400e+11, 4.3679e+12, 1.3998e+13,
        1.0654e+13, 1.3696e+12, 4.8203e+12, 1.5054e+14, 8.1086e+12, 3.8398e+12,
        5.7523e+12, 7.3872e+11, 1.0387e+12, 1.1200e+13, 3.9106e+12, 2.3338e+12,
        3.7405e+12, 1.1376e+14, 7.2079e+12, 6.9066e+12, 9.3675e+11, 6.9356e+12,
        1.0344e+13, 1.9573e+12, 1.3556e+12, 7.6252e+12, 2.6696e+12, 1.7762e+12,
        8.7974e+12, 2.4920e+12, 3.9463e+12, 4.0124e+12, 1.5094e+12, 4.0913e+12,
        3.8771e+12, 1.7968e+12, 5.0161e+13, 1.4369e+12, 2.4106e+12, 7.9280e+12,
        3.7490e+11, 9.6952e+11, 5.8216e+11, 2.4114e+12, 2.0464e+13, 2.1639e+12,
        2.9772e+12, 4.9222e+12, 1.5833e+11, 3.9764e+13, 3.5411e+11, 7.8691e+11,
        8.0790e+11, 2.0083e+11, 4.1106e+12, 1.8826e+12, 9.7972e+11, 1.5327e+12,
        1.2973e+12, 3.8507e+11, 7.5729e+12, 1.7863e+12, 1.1890e+13, 1.5190e+13,
        2.5472e+12, 3.3481e+12, 3.6780e+12, 3.3543e+13, 9.1297e+11, 6.4927e+13,
        4.3659e+12, 3.3476e+12, 1.2913e+12, 3.5133e+12, 2.7326e+11, 1.3483e+12,
        1.6894e+13, 1.3031e+12])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6927e+13, 4.1863e+12, 7.5801e+12, 1.1552e+13, 1.5216e+12, 2.2103e+12,
        2.5346e+12, 3.9221e+12, 1.5853e+13, 5.3152e+12, 3.2639e+12, 9.5351e+11,
        3.0614e+12, 1.0887e+14, 7.7958e+12, 1.9899e+12, 2.1145e+13, 4.8036e+14,
        2.1839e+12, 3.9605e+12, 8.0463e+12, 3.5748e+12, 3.8553e+12, 2.7407e+12,
        4.2637e+12, 9.9780e+11, 2.3385e+13, 2.7141e+12, 3.5030e+12, 1.5082e+12,
        4.7860e+12, 6.5866e+12, 5.3604e+12, 5.5558e+12, 3.2141e+12, 3.3752e+12,
        5.7128e+12, 4.9274e+13, 1.4998e+13, 5.7040e+12, 4.9279e+12, 3.9555e+12,
        2.7669e+13, 1.9694e+12, 5.3834e+12, 1.1005e+13, 9.2046e+12, 2.6832e+12,
        4.3266e+12, 2.1631e+12, 1.2853e+12, 5.3764e+12, 3.9304e+12, 7.7420e+12,
        3.9424e+12, 5.1153e+12, 1.1825e+13, 1.8852e+12, 1.5435e+12, 2.6732e+12,
        3.4548e+12, 2.8171e+12, 4.6555e+12, 2.1267e+13, 2.3811e+13, 8.9255e+12,
        6.3960e+12, 1.3000e+13, 1.3788e+12, 1.5599e+13, 1.1471e+13, 5.8703e+12,
        3.2183e+13, 3.1022e+12, 4.3797e+12, 2.3452e+12, 5.5040e+12, 2.5946e+13,
        9.3434e+12, 1.3874e+13, 2.7387e+12, 3.1045e+12, 1.5207e+12, 1.5362e+12,
        2.1135e+13, 3.5119e+12, 1.1541e+13, 1.6792e+12, 2.2063e+12, 5.0688e+13,
        2.8316e+12, 3.3922e+12, 6.4780e+12, 4.3936e+12, 4.1865e+12, 1.4375e+12,
        1.1239e+13, 3.5828e+12, 4.2903e+12, 1.7097e+12, 2.2573e+12, 4.7520e+12,
        4.5395e+12, 9.4805e+12, 2.7095e+12, 6.1695e+12, 3.6637e+12, 1.0260e+13,
        8.0451e+13, 2.4233e+12, 2.1941e+12, 1.5762e+12, 1.8694e+12, 1.9810e+12,
        2.4436e+13, 8.7243e+13, 9.3124e+11, 3.7243e+12, 1.6445e+13, 3.9537e+13,
        9.6291e+11, 7.5063e+12, 3.2717e+12, 2.0161e+12, 5.6291e+12, 6.3463e+12,
        2.3821e+12, 3.0037e+12, 5.6146e+12, 1.9607e+12, 5.6810e+11, 9.0999e+11,
        5.2897e+12, 1.7946e+12, 2.3219e+13, 1.6916e+12, 1.3262e+12, 4.6178e+12,
        2.6154e+12, 8.3811e+12, 3.8644e+12, 7.2303e+12, 5.1412e+12, 5.8251e+12,
        8.5105e+11, 2.3912e+12, 4.6709e+12, 1.8471e+12, 5.7023e+12, 1.0254e+13,
        4.3042e+12, 1.0261e+13, 4.5600e+12, 1.3918e+12, 3.6646e+12, 4.1554e+12,
        6.7691e+12, 3.4727e+12, 1.0357e+13, 1.8675e+12, 2.6136e+12, 1.2494e+14,
        2.0488e+12, 2.1037e+13, 5.1278e+12, 1.1567e+12, 5.6359e+12, 2.8917e+12,
        3.1089e+12, 3.2305e+13, 6.5792e+12, 1.7700e+12, 3.7582e+12, 2.5075e+12,
        1.0518e+13, 2.1162e+13, 3.1249e+12, 1.7746e+12, 5.6589e+13, 6.1236e+12,
        8.5058e+12, 1.3465e+12, 1.7570e+12, 1.7202e+13, 4.2635e+12, 1.3745e+13,
        7.2000e+12, 1.9386e+12, 5.6211e+12, 2.3321e+12, 2.3953e+13, 3.1584e+12,
        1.1240e+12, 4.5539e+12, 1.4312e+12, 1.8113e+12, 2.8854e+12, 6.1163e+12,
        7.3123e+12, 4.2795e+12, 1.3616e+13, 3.5446e+12, 2.5778e+13, 4.7127e+12,
        2.2473e+12, 2.5729e+12, 7.3807e+12, 3.6718e+12, 3.6042e+12, 1.7668e+12,
        2.1299e+12, 2.1727e+12, 7.6975e+12, 2.1429e+12, 4.9917e+13, 3.5312e+12,
        5.8260e+12, 1.1095e+12, 2.4861e+12, 3.0877e+12, 6.9124e+12, 9.0510e+12,
        1.8933e+12, 3.5740e+12, 2.6793e+12, 3.8224e+12, 2.3603e+12, 1.4334e+12,
        1.1727e+13, 2.6176e+12, 5.7778e+12, 2.5798e+14, 2.2051e+12, 3.1446e+12,
        1.7291e+12, 7.5314e+12, 4.0863e+12, 6.3756e+12, 3.3478e+12, 7.1870e+13,
        1.4112e+12, 4.7895e+12, 5.4342e+12, 4.7238e+12, 2.1013e+12, 2.9396e+12,
        6.0303e+12, 4.5377e+12, 1.9267e+12, 7.6300e+12, 5.8889e+12, 2.3786e+12,
        2.8481e+12, 1.2191e+12, 5.6356e+12, 7.2968e+12, 3.0130e+12, 3.8550e+12,
        1.9745e+12, 3.6155e+13, 9.2966e+11, 1.4096e+12, 8.6008e+12, 3.4166e+12,
        3.5423e+12, 1.4172e+13, 6.7599e+12, 2.6430e+12, 6.0004e+12, 4.7064e+12,
        8.3903e+12, 2.9476e+12, 2.0443e+12, 6.1909e+12, 2.6139e+12, 1.5206e+12,
        1.7591e+12, 9.2415e+12, 3.2287e+12, 2.1478e+12, 1.8572e+13, 2.2263e+12,
        3.3673e+13, 6.0044e+12, 3.1275e+12, 8.1454e+12, 4.7047e+12, 1.3540e+13,
        2.3883e+13, 3.9655e+12, 5.2838e+12, 2.5284e+12, 3.8001e+12, 2.0729e+12,
        2.0660e+13, 1.8519e+12, 5.5927e+12, 3.4038e+12, 1.6954e+12, 1.7376e+12,
        6.4317e+11, 2.1865e+12, 4.3870e+12, 2.7194e+12, 2.8755e+12, 3.1996e+12,
        3.0341e+12, 3.0549e+13, 8.5661e+11, 3.4859e+12, 3.4986e+12, 5.7411e+12,
        4.5410e+12, 9.6931e+11, 3.5343e+12, 2.7727e+12, 3.3246e+12, 1.9965e+12,
        3.8515e+12, 5.8822e+12, 9.4540e+12, 9.3722e+12, 2.3748e+13, 1.9953e+13,
        1.6557e+12, 7.2725e+12, 7.5945e+13, 5.4017e+12, 8.4682e+12, 3.0631e+12,
        6.3133e+12, 1.7776e+12, 2.7662e+13, 3.7544e+12, 1.9616e+12, 6.7218e+12,
        5.9313e+12, 4.2027e+12, 1.5774e+12, 1.6197e+13, 1.6513e+12, 1.0719e+12,
        3.4951e+12, 7.9568e+13, 8.9710e+12, 6.6263e+12, 2.3344e+12, 2.3096e+12,
        6.7525e+12, 3.6288e+12, 1.1657e+12, 7.4521e+12, 1.8517e+12, 4.4358e+12,
        9.6905e+12, 9.6325e+12, 2.1798e+12, 3.6359e+12, 1.3376e+12, 5.1877e+12,
        3.8670e+12, 2.4527e+12, 4.5218e+12, 1.6424e+13, 4.7479e+12, 3.8173e+12,
        6.5261e+12, 3.2297e+12, 4.1698e+12, 2.2050e+12, 1.7542e+12, 7.0166e+12,
        3.1597e+12, 1.2717e+12, 3.3381e+12, 2.0394e+13, 1.2030e+13, 2.1628e+12,
        6.3169e+12, 1.8935e+13, 1.5233e+12, 1.7198e+12, 5.3422e+12, 1.6950e+12,
        1.5882e+12, 8.8375e+11, 1.1972e+13, 1.5096e+14, 4.6923e+12, 2.6724e+12,
        4.7164e+12, 1.4800e+13, 7.0961e+12, 2.2319e+12, 6.5369e+11, 2.4766e+12,
        5.0595e+12, 6.2556e+12, 2.0937e+12, 2.4856e+12, 2.3654e+12, 2.5921e+12,
        6.9158e+12, 1.8632e+12, 5.4893e+12, 1.9550e+13, 6.0589e+12, 2.3225e+12,
        6.7430e+12, 2.1374e+12, 3.8803e+13, 9.3296e+12, 7.1717e+12, 2.1396e+12,
        1.4154e+12, 4.6700e+12, 1.9977e+12, 4.8021e+12, 1.9071e+12, 1.3212e+12,
        1.9866e+13, 2.1046e+12, 3.4634e+12, 1.2261e+13, 1.4805e+12, 8.2447e+12,
        4.4391e+12, 1.5959e+12, 5.2898e+12, 6.3705e+12, 3.3317e+13, 1.0180e+13,
        1.7555e+13, 3.6749e+12, 9.0401e+12, 3.8225e+12, 2.4797e+12, 1.5019e+13,
        1.2525e+13, 1.4414e+12, 7.8120e+12, 1.5094e+14, 5.1842e+12, 4.8703e+12,
        3.7973e+12, 4.8305e+12, 4.5373e+12, 8.9414e+12, 4.3899e+12, 1.7826e+12,
        6.1245e+12, 1.1250e+14, 8.3504e+12, 6.7585e+12, 2.5225e+12, 8.1670e+12,
        1.2772e+13, 2.8315e+12, 1.7964e+12, 8.4607e+12, 2.7387e+12, 4.1160e+12,
        7.4532e+12, 1.5991e+12, 3.9765e+12, 6.7076e+12, 4.0625e+12, 6.0144e+12,
        6.6554e+12, 4.4654e+12, 5.0583e+13, 1.2949e+12, 2.5976e+12, 7.0271e+12,
        1.5165e+12, 1.8590e+12, 2.9124e+12, 1.5785e+12, 1.9290e+13, 2.9516e+12,
        7.3683e+12, 3.9891e+12, 2.3408e+12, 4.0187e+13, 2.5687e+12, 3.8152e+12,
        2.5138e+12, 2.3636e+12, 7.8045e+12, 2.5199e+12, 5.1027e+11, 1.8238e+12,
        2.8557e+12, 1.9626e+12, 7.2174e+12, 4.8407e+12, 1.0812e+13, 1.6268e+13,
        3.2201e+12, 3.7794e+12, 2.1349e+12, 3.3692e+13, 2.7988e+12, 6.4739e+13,
        2.8222e+12, 3.8505e+12, 1.0530e+12, 3.3287e+12, 1.8872e+12, 2.9712e+12,
        1.8668e+13, 1.9516e+12])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.8969e+12, 3.5823e+12, 9.7652e+12, 3.4164e+12, 3.6870e+12, 4.3007e+12,
        3.8597e+12, 4.6661e+12, 4.0078e+12, 3.8443e+12, 5.9137e+12, 4.5866e+12,
        6.2080e+12, 4.5322e+12, 8.1063e+12, 7.0085e+12, 1.0491e+13, 3.1723e+12,
        7.1683e+12, 5.4895e+12, 5.4473e+12, 3.6044e+12, 4.0831e+12, 4.1149e+12,
        4.1092e+12, 2.5173e+12, 5.2892e+12, 6.0413e+12, 6.0431e+12, 8.7883e+12,
        3.1364e+12, 2.9752e+12, 3.3496e+12, 5.4954e+12, 6.8206e+12, 3.5609e+12,
        5.8402e+12, 5.5681e+12, 4.4504e+12, 7.3528e+12, 2.4340e+12, 4.3208e+12,
        3.3923e+12, 3.6518e+12, 5.0860e+12, 5.0395e+12, 8.2802e+12, 6.4962e+12,
        4.4574e+12, 7.4507e+12, 6.4342e+12, 7.9307e+12, 9.4966e+12, 4.2240e+12,
        3.4451e+12, 6.6111e+12, 1.9107e+12, 5.2339e+12, 6.9872e+12, 3.2556e+12,
        2.6848e+12, 2.3734e+12, 7.3241e+12, 3.8774e+12, 6.8026e+12, 2.9196e+12,
        5.6514e+12, 4.3263e+12, 2.8909e+12, 4.7380e+12, 3.1381e+12, 6.3836e+12,
        2.0110e+12, 4.2653e+12, 2.6988e+12, 5.8457e+12, 3.5426e+12, 5.5694e+12,
        2.7959e+12, 4.8426e+12, 1.9264e+12, 5.2436e+12, 6.2134e+12, 4.3195e+12,
        6.9795e+12, 4.1212e+12, 4.0379e+12, 4.9785e+12, 3.6307e+12, 3.7411e+12,
        5.7968e+12, 4.7241e+12, 5.3819e+12, 1.9530e+12, 4.5708e+12, 5.5539e+12,
        5.7699e+12, 4.4513e+12, 5.2134e+12, 1.1925e+13, 4.3429e+12, 4.4816e+12,
        6.6509e+12, 6.7707e+12, 3.9397e+12, 2.8992e+12, 5.6017e+12, 3.6149e+12,
        2.4325e+12, 1.0703e+13, 3.4386e+12, 6.8436e+12, 4.3897e+12, 5.3585e+12,
        6.9225e+12, 6.1777e+12, 7.8145e+12, 5.3077e+12, 4.2749e+12, 4.0620e+12,
        5.3322e+12, 4.5493e+12, 5.6188e+12, 8.6987e+12, 6.8056e+12, 5.3471e+12,
        4.6896e+12, 9.2821e+12])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.6877e+12, 8.0107e+12, 6.9354e+12, 5.3668e+12, 7.7250e+12, 9.4895e+12,
        3.1485e+12, 4.9284e+12, 1.1648e+13, 2.6967e+12, 4.1320e+12, 1.5143e+13,
        3.0146e+12, 4.3251e+12, 6.6642e+12, 6.6367e+12, 1.0372e+13, 7.6250e+12,
        3.1208e+12, 7.5313e+12, 3.0869e+12, 3.8713e+12, 8.8127e+12, 5.0814e+12,
        8.7004e+12, 3.5137e+12, 6.9852e+12, 8.6767e+12, 5.6416e+12, 3.7473e+12,
        4.9203e+12, 3.0503e+12, 1.0484e+13, 7.4781e+12, 1.0067e+13, 5.0464e+12,
        5.2793e+12, 5.1226e+12, 1.4730e+13, 8.1186e+12, 9.7064e+12, 9.2681e+12,
        4.7992e+12, 1.1874e+13, 5.5444e+12, 1.1376e+13, 8.6601e+12, 1.0826e+13,
        9.4415e+12, 7.5158e+12, 6.5756e+12, 7.6983e+12, 4.2196e+12, 5.2223e+12,
        5.1184e+12, 6.8653e+12, 6.7430e+12, 7.2137e+12, 1.2830e+13, 4.1918e+12,
        5.7785e+12, 8.1517e+12, 3.5437e+12, 9.3156e+12, 2.8611e+12, 2.6621e+12,
        4.9352e+12, 7.3814e+12, 5.5571e+12, 7.3810e+12, 6.1958e+12, 7.8257e+12,
        8.1408e+12, 7.1049e+12, 8.3380e+12, 8.3975e+12, 1.0418e+13, 9.1391e+12,
        6.5683e+12, 5.8803e+12, 4.4505e+12, 6.1243e+12, 6.0393e+12, 2.7788e+12,
        9.5798e+12, 5.5504e+12, 5.4231e+12, 4.6468e+12, 5.0529e+12, 4.6494e+12,
        8.8011e+12, 6.3749e+12, 6.5466e+12, 8.5406e+12, 5.5632e+12, 6.8302e+12,
        7.2486e+12, 1.2850e+13, 7.5964e+12, 7.3040e+12, 3.3078e+12, 8.2584e+12,
        1.0624e+13, 5.7767e+12, 3.9633e+12, 1.7884e+13, 6.5988e+12, 6.1761e+12,
        1.2832e+13, 1.0142e+13, 6.5531e+12, 1.1915e+13, 4.8444e+12, 4.9429e+12,
        7.8310e+12, 3.7665e+12, 5.4795e+12, 3.7337e+12, 4.9041e+12, 1.2118e+13,
        7.7034e+12, 8.2325e+12, 9.2813e+12, 8.7764e+12, 1.0820e+13, 4.4829e+12,
        3.7835e+12, 5.5928e+12])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.3429e+11, 3.8977e+10, 3.7067e+11, 5.5250e+11, 2.0276e+12, 7.1708e+11,
        6.1993e+10, 5.5913e+11, 5.8914e+11, 5.7568e+11, 2.8996e+11, 1.9130e+11,
        2.0617e+11, 8.2975e+11, 5.8094e+11, 1.8670e+11, 3.8626e+12, 7.4945e+11,
        7.3528e+10, 7.7871e+11, 5.4719e+11, 1.4795e+11, 4.1109e+11, 3.3508e+11,
        5.7809e+11, 7.0532e+11, 7.0397e+12, 3.8392e+11, 2.0625e+12, 5.3711e+11,
        9.5319e+11, 4.1778e+11, 1.2157e+12, 1.4138e+11, 2.8433e+11, 2.2246e+11,
        2.0988e+11, 7.3112e+11, 6.8454e+11, 6.3837e+11, 4.2790e+11, 1.9592e+12,
        6.2347e+11, 8.0634e+11, 1.0715e+12, 3.7818e+11, 4.6031e+11, 1.1526e+12,
        7.1302e+11, 2.0797e+11, 2.0965e+11, 3.8549e+11, 4.2339e+11, 9.7908e+11,
        2.0683e+11, 5.3217e+11, 6.7115e+11, 5.3428e+11, 1.3976e+11, 1.5893e+11,
        1.0400e+12, 9.6172e+11, 1.3184e+11, 6.4743e+11, 4.7247e+11, 5.9486e+11,
        5.6362e+11, 2.9296e+11, 2.8925e+11, 6.6778e+11, 4.4297e+11, 4.5262e+11,
        1.1144e+12, 4.8150e+11, 4.3811e+11, 6.5992e+11, 6.1364e+11, 4.3053e+11,
        9.2229e+11, 2.3206e+11, 6.1819e+11, 1.9857e+11, 7.0399e+11, 8.9661e+10,
        5.9507e+11, 1.7208e+12, 6.7809e+11, 5.4219e+11, 1.2938e+11, 3.3122e+11,
        3.5206e+11, 2.6032e+11, 7.2364e+11, 4.3494e+11, 2.0442e+11, 5.5793e+11,
        5.7013e+11, 5.7389e+11, 1.0727e+12, 5.9904e+11, 2.5358e+11, 9.7431e+10,
        8.0126e+11, 1.7074e+12, 1.7551e+11, 2.4357e+11, 4.5752e+10, 1.9519e+11,
        2.2601e+11, 1.7427e+11, 3.7538e+11, 4.1120e+11, 5.0452e+11, 3.7667e+10,
        1.3372e+12, 7.0484e+11, 4.3765e+11, 3.1296e+11, 4.9795e+11, 1.9942e+12,
        3.9228e+11, 2.2651e+11, 2.7205e+11, 2.2718e+11, 5.1755e+12, 1.7829e+12,
        5.3584e+11, 4.3935e+11, 1.1192e+12, 2.6300e+11, 1.2632e+11, 6.5868e+11,
        3.5703e+11, 4.0358e+12, 6.0462e+11, 1.8996e+11, 1.0654e+11, 2.7534e+11,
        5.8897e+11, 3.4121e+11, 1.0132e+12, 1.6115e+12, 8.2352e+11, 7.7106e+11,
        8.7169e+10, 7.2449e+11, 2.1571e+11, 1.9019e+11, 1.4829e+11, 6.9830e+11,
        8.6920e+11, 1.2939e+12, 6.7426e+11, 2.0961e+12, 1.4241e+11, 1.5033e+12,
        8.2006e+11, 4.9431e+12, 8.7740e+11, 1.0861e+11, 5.7712e+11, 3.9686e+11,
        5.0496e+11, 2.9261e+12, 4.8498e+11, 3.4990e+10, 3.3501e+11, 2.3508e+11,
        3.2975e+11, 1.5744e+12, 6.1116e+11, 8.2047e+11, 4.8242e+11, 2.2786e+11,
        3.1452e+12, 1.5955e+12, 5.6255e+11, 2.0608e+11, 3.1971e+11, 2.3099e+11,
        3.2969e+11, 4.8731e+11, 6.5222e+11, 1.3266e+12, 1.1219e+12, 4.5186e+11,
        3.2182e+11, 1.0942e+11, 1.0722e+12, 6.1550e+11, 1.2550e+12, 2.1696e+11,
        5.8149e+11, 6.4120e+10, 5.6704e+11, 6.4595e+11, 6.9772e+10, 8.6412e+11,
        2.1660e+11, 7.7448e+11, 6.2686e+11, 4.9986e+11, 3.6654e+11, 1.3756e+12,
        5.8437e+11, 2.7602e+11, 3.7463e+11, 1.0295e+12, 3.2660e+11, 9.3796e+11,
        6.5936e+11, 8.2182e+11, 5.5848e+11, 5.7097e+11, 3.4814e+11, 7.8407e+11,
        3.0177e+11, 4.4760e+11, 9.0895e+11, 2.1751e+11, 4.6991e+11, 7.9506e+11,
        3.8465e+11, 4.3145e+11, 3.0530e+11, 5.0118e+11, 4.0626e+11, 4.0687e+11,
        1.4746e+12, 5.6143e+11, 6.2048e+11, 1.3543e+12, 5.4969e+11, 9.2066e+11,
        3.1210e+11, 3.4078e+11, 1.7317e+11, 9.7055e+11, 6.4343e+11, 2.0309e+11,
        6.6854e+11, 1.0264e+11, 4.0026e+11, 7.8882e+11, 6.6437e+11, 7.0939e+11,
        6.2851e+11, 3.3981e+11, 5.1248e+11, 3.8474e+11, 9.1011e+11, 2.0884e+11,
        1.1973e+12, 9.2115e+11, 3.6678e+11, 5.2725e+11, 7.9274e+11, 7.2960e+11,
        4.0622e+11, 2.9906e+11, 1.6962e+12, 1.7429e+11, 8.9405e+11, 7.7990e+11,
        7.3598e+11, 4.4013e+11, 4.3653e+11, 1.3732e+11, 2.2169e+11, 1.1403e+12,
        1.2451e+12, 6.7959e+10, 6.3686e+11, 8.8299e+11, 2.6609e+11, 6.9033e+11,
        1.8895e+12, 6.7244e+11, 1.4379e+12, 1.0838e+11, 3.5396e+11, 7.5983e+11,
        4.7254e+11, 3.9203e+11, 2.0729e+12, 1.5361e+11, 3.4504e+11, 7.6450e+11,
        3.0849e+11, 1.4895e+11, 4.8969e+11, 5.6523e+11, 6.1183e+10, 2.6034e+11,
        4.0213e+11, 5.9946e+11, 1.7433e+11, 7.0955e+11, 2.0905e+11, 6.2710e+11,
        9.5909e+11, 4.0875e+10, 8.3589e+11, 2.0290e+11, 4.2182e+11, 4.7654e+12,
        8.9657e+11, 6.6798e+11, 8.9454e+10, 5.6223e+11, 6.1668e+11, 7.7458e+11,
        1.3651e+11, 6.2166e+11, 1.5597e+11, 2.0147e+12, 3.9682e+11, 5.9993e+11,
        5.0130e+11, 5.9878e+11, 1.6135e+12, 7.9842e+11, 2.2308e+11, 8.2719e+11,
        4.9415e+11, 7.2972e+11, 1.5865e+13, 6.2505e+11, 4.1841e+11, 4.6410e+11,
        4.8005e+11, 4.1024e+11, 1.7355e+12, 8.7171e+11, 7.0599e+11, 7.8756e+11,
        2.7342e+11, 1.8412e+11, 9.8267e+10, 1.5850e+11, 7.6168e+11, 4.8979e+11,
        1.2889e+12, 1.5453e+12, 7.7959e+12, 6.6148e+11, 9.2989e+11, 4.2589e+11,
        3.6522e+11, 1.2138e+12, 4.3831e+11, 5.3614e+12, 6.1864e+11, 4.6307e+11,
        1.4909e+12, 2.1372e+12, 8.3891e+11, 2.8445e+12, 3.1977e+12, 1.0920e+12,
        8.4787e+11, 6.8695e+11, 6.5793e+11, 7.1577e+12, 5.0853e+11, 4.8476e+11,
        6.6249e+11, 6.7201e+11, 7.6793e+11, 1.6574e+12, 1.0681e+11, 2.1158e+11,
        1.9070e+11, 8.4494e+11, 1.4044e+11, 6.6942e+11, 3.4327e+11, 7.9423e+11,
        8.8785e+11, 6.2608e+11, 2.0966e+10, 7.4846e+11, 1.1899e+11, 3.1425e+11,
        7.0555e+10, 9.2483e+11, 7.6020e+11, 4.7334e+11, 5.0163e+11, 2.3805e+11,
        2.0600e+12, 7.5335e+11, 3.8466e+12, 5.9377e+11, 6.1341e+10, 4.5287e+11,
        7.1562e+11, 7.7663e+11, 4.6335e+10, 1.2326e+11, 6.0790e+11, 9.9121e+10,
        9.2036e+11, 1.1593e+11, 4.6992e+11, 1.2124e+12, 3.9181e+11, 5.8400e+11,
        5.6718e+11, 1.0596e+11, 2.4761e+11, 2.0500e+11, 8.2977e+11, 3.2793e+11,
        2.4951e+11, 2.2287e+12, 5.2925e+11, 5.7810e+11, 6.3006e+10, 5.3677e+11,
        1.7854e+12, 3.8442e+11, 1.0221e+12, 3.6712e+11, 9.4598e+11, 1.5763e+11,
        9.4967e+11, 1.4160e+11, 1.7316e+12, 1.8355e+11, 4.8734e+11, 3.1176e+11,
        4.7788e+11, 6.0177e+11, 6.3619e+11, 1.1900e+11, 7.7386e+11, 5.0040e+11,
        1.5967e+11, 6.4032e+11, 4.0323e+11, 5.0895e+11, 3.6101e+11, 1.4993e+11,
        3.7752e+11, 2.2709e+12, 6.9117e+10, 6.3864e+11, 7.6869e+11, 5.4759e+11,
        1.1969e+12, 1.8804e+11, 3.3151e+11, 4.2298e+11, 5.4338e+11, 1.5918e+12,
        9.7711e+11, 5.2594e+11, 3.3640e+11, 2.0670e+11, 4.4265e+11, 1.9934e+11,
        3.0482e+11, 5.9254e+11, 2.4987e+11, 3.3011e+11, 1.0146e+12, 4.3471e+11,
        1.3961e+11, 5.4114e+11, 3.1546e+11, 5.9293e+11, 4.0638e+12, 7.4151e+11,
        5.5259e+11, 5.8823e+11, 1.9528e+11, 5.1481e+11, 7.9305e+11, 2.4254e+11,
        1.9028e+12, 4.6847e+11, 4.3113e+10, 1.0448e+12, 5.9992e+11, 4.8104e+10,
        4.9507e+10, 1.7271e+11, 5.2939e+11, 5.4347e+11, 8.8452e+11, 2.0140e+11,
        4.6652e+11, 5.0469e+11, 3.7241e+11, 2.1475e+11, 3.8083e+11, 2.6081e+11,
        3.5100e+11, 8.0002e+11, 2.2371e+12, 7.9125e+11, 8.0931e+11, 3.7113e+12,
        6.7454e+11, 7.0755e+11, 5.5646e+10, 6.2855e+11, 6.2613e+11, 6.3075e+11,
        9.7241e+11, 5.6858e+11])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.5250e+11, 9.0704e+11, 8.4059e+11, 5.1749e+11, 1.2382e+12, 1.3289e+12,
        1.2458e+12, 1.2180e+12, 1.0772e+12, 8.1009e+11, 1.1312e+12, 8.4699e+11,
        1.0501e+12, 1.5986e+12, 7.8765e+11, 1.1172e+12, 1.0823e+12, 1.0190e+12,
        1.2031e+12, 9.9039e+11, 1.5034e+12, 1.4119e+12, 1.4376e+12, 9.3513e+11,
        7.2229e+11, 1.0723e+12, 9.0361e+11, 5.9744e+11, 1.2486e+12, 1.6118e+12,
        1.1526e+12, 1.1828e+12, 9.9821e+11, 1.7671e+12, 8.5879e+11, 8.4989e+11,
        1.3550e+12, 1.8152e+12, 1.1420e+12, 7.2321e+11, 1.0660e+12, 1.6021e+12,
        1.1940e+12, 1.2999e+12, 7.7575e+11, 1.8891e+12, 7.1424e+11, 7.7082e+11,
        9.3796e+11, 1.5261e+12, 8.2338e+11, 1.5964e+12, 1.2554e+12, 1.5576e+12,
        1.3194e+12, 6.1295e+11, 1.1686e+12, 9.7959e+11, 1.5019e+12, 6.4685e+11,
        2.3017e+12, 4.0129e+11, 5.8718e+11, 1.0372e+12, 1.6975e+12, 1.3382e+12,
        7.9260e+11, 1.5324e+12, 2.0008e+12, 1.1744e+12, 1.4331e+12, 1.8520e+12,
        2.2265e+12, 1.3485e+12, 9.2809e+11, 1.3126e+12, 1.0979e+12, 1.4925e+12,
        2.0141e+12, 1.5878e+12, 1.5320e+12, 1.3707e+12, 1.6780e+12, 1.0942e+12,
        7.7370e+11, 2.2515e+12, 2.9025e+12, 1.4990e+12, 9.8306e+11, 5.3799e+11,
        1.3284e+12, 9.4001e+11, 2.1316e+12, 7.0562e+11, 5.5648e+11, 1.6648e+12,
        1.1175e+12, 1.5448e+12, 1.2342e+12, 1.2637e+12, 1.0958e+12, 1.0103e+12,
        1.3503e+12, 1.5224e+12, 4.6308e+11, 1.1732e+12, 1.1196e+12, 1.7440e+12,
        9.6733e+11, 1.3135e+12, 9.4853e+11, 1.9035e+12, 1.5022e+12, 2.2050e+12,
        9.0531e+11, 1.2274e+12, 1.7677e+12, 1.8080e+12, 6.6759e+11, 7.4579e+11,
        9.7545e+11, 2.0557e+12, 6.9717e+11, 5.5608e+11, 1.5773e+12, 8.3334e+11,
        1.2017e+12, 5.7186e+11])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.9201e+12, 2.8949e+12, 1.0796e+12, 1.8290e+12, 9.2065e+11, 1.6813e+12,
        1.0601e+12, 1.6526e+12, 2.5850e+12, 1.4494e+12, 1.1405e+12, 1.5230e+12,
        1.0517e+12, 1.8198e+12, 1.9344e+12, 1.5999e+12, 2.3782e+12, 3.2200e+12,
        1.4457e+12, 1.2914e+12, 7.5020e+11, 1.4013e+12, 1.9726e+12, 1.1905e+12,
        1.3376e+12, 1.7658e+12, 1.4695e+12, 6.7459e+11, 2.0616e+12, 1.6034e+12,
        3.3300e+12, 7.7178e+11, 2.0560e+12, 1.2380e+12, 1.4478e+12, 1.7434e+12,
        2.5497e+12, 1.7894e+12, 1.9279e+12, 1.7155e+12, 2.0529e+12, 1.3715e+12,
        1.3339e+12, 1.1078e+12, 1.1995e+12, 2.3591e+12, 1.3845e+12, 3.0797e+12,
        2.1698e+12, 2.7002e+12, 1.1573e+12, 2.6066e+12, 3.1311e+12, 1.0615e+12,
        2.3671e+12, 1.2253e+12, 2.0796e+12, 1.4998e+12, 1.1452e+12, 2.3734e+12,
        2.7037e+12, 2.8627e+12, 1.2471e+12, 3.2446e+12, 1.6990e+12, 2.7351e+12,
        9.0541e+11, 1.5056e+12, 1.6308e+12, 2.8810e+12, 2.7190e+12, 9.4842e+11,
        6.9010e+11, 2.4937e+12, 1.9852e+12, 6.2473e+11, 1.4366e+12, 2.9396e+12,
        2.3051e+12, 2.8132e+12, 1.8466e+12, 1.4287e+12, 2.0431e+12, 2.9805e+12,
        1.2438e+12, 1.6805e+12, 2.4226e+12, 2.5641e+12, 2.8587e+12, 1.6483e+12,
        1.1442e+12, 1.9166e+12, 9.4761e+11, 1.5910e+12, 2.0917e+12, 2.9112e+12,
        2.3030e+12, 1.0963e+12, 1.9212e+12, 1.0072e+12, 1.9892e+12, 1.9091e+12,
        1.2735e+12, 1.9102e+12, 1.4318e+12, 1.8499e+12, 2.4284e+12, 2.0445e+12,
        9.7608e+11, 2.1286e+12, 2.2520e+12, 1.8608e+12, 8.9477e+11, 1.6341e+12,
        1.4445e+12, 1.7398e+12, 2.6086e+12, 2.7165e+12, 1.4206e+12, 1.8005e+12,
        2.3383e+12, 1.4012e+12, 2.4025e+12, 1.9565e+12, 2.5648e+12, 1.3797e+12,
        9.7125e+11, 8.6318e+11])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.6243e+10, 5.0808e+09, 4.2552e+09, 3.6344e+09, 6.8526e+10, 1.6409e+11,
        4.6586e+09, 6.5581e+10, 1.2736e+11, 3.9574e+10, 1.9431e+10, 1.6801e+10,
        4.4016e+10, 6.8354e+10, 8.8156e+10, 3.1255e+10, 1.1301e+11, 2.1422e+10,
        3.1405e+09, 7.9010e+10, 1.7871e+11, 2.2708e+10, 3.9392e+10, 4.3851e+10,
        7.9272e+09, 7.7601e+10, 9.8771e+10, 6.2758e+09, 7.1220e+10, 1.2794e+11,
        2.2330e+10, 2.5030e+10, 5.2053e+10, 1.7559e+10, 3.0385e+10, 3.8105e+10,
        1.3222e+10, 4.6127e+10, 1.7231e+10, 5.5048e+10, 3.3490e+10, 2.0929e+12,
        1.6528e+10, 3.4416e+10, 5.9927e+10, 2.5330e+10, 6.4056e+10, 4.3463e+10,
        6.7366e+10, 6.5663e+10, 1.5215e+10, 5.9349e+10, 4.3962e+10, 9.6575e+11,
        8.9056e+09, 4.8298e+10, 5.4301e+10, 1.8559e+10, 1.0384e+10, 6.7966e+09,
        5.4096e+10, 1.5601e+11, 4.4022e+09, 7.6776e+10, 6.9350e+10, 8.1795e+10,
        2.2317e+10, 4.4445e+10, 2.8024e+11, 6.8085e+10, 2.1702e+10, 1.5481e+10,
        3.3905e+11, 1.5401e+10, 8.2261e+10, 6.6356e+10, 1.8590e+10, 1.9125e+11,
        3.8000e+10, 6.0156e+10, 9.1377e+10, 9.9701e+09, 5.2476e+09, 2.8298e+09,
        1.3185e+11, 5.9655e+10, 1.1351e+11, 7.4983e+10, 2.9562e+10, 1.2045e+11,
        6.1860e+10, 3.8516e+10, 5.2876e+10, 1.5144e+10, 4.1715e+10, 3.4900e+10,
        2.2655e+10, 3.9493e+10, 6.1810e+10, 2.9212e+10, 1.7335e+10, 4.9756e+09,
        8.0300e+10, 7.5973e+10, 1.5295e+11, 9.7501e+10, 4.2178e+09, 1.9521e+10,
        5.8721e+10, 6.1405e+10, 3.6024e+10, 3.1125e+10, 5.6102e+10, 2.4102e+09,
        3.0986e+10, 2.6164e+10, 1.0663e+11, 1.6610e+10, 7.2849e+10, 1.9238e+11,
        3.6353e+10, 2.7163e+10, 2.0369e+11, 9.5238e+09, 1.3180e+11, 2.8304e+10,
        6.9727e+09, 8.0330e+10, 3.7511e+10, 7.7319e+09, 6.1313e+09, 1.6992e+10,
        1.1489e+11, 2.2366e+11, 3.1331e+10, 1.2911e+10, 3.5879e+09, 1.0989e+11,
        2.1597e+10, 2.0677e+10, 1.5107e+10, 2.0628e+10, 3.1257e+10, 5.8461e+10,
        3.7638e+09, 1.4979e+10, 2.3576e+10, 1.1114e+10, 8.2335e+10, 2.6311e+11,
        9.8281e+10, 2.7470e+10, 4.8106e+10, 8.5058e+10, 4.7796e+10, 1.5719e+10,
        1.1966e+11, 1.0790e+12, 3.1305e+10, 4.3201e+09, 1.9470e+11, 2.7309e+11,
        7.6970e+10, 3.0027e+10, 2.3529e+10, 4.5850e+09, 1.2631e+11, 3.7071e+10,
        1.8590e+11, 3.4272e+10, 2.2264e+10, 4.6477e+11, 6.6304e+10, 1.6936e+10,
        3.8028e+10, 3.7410e+10, 1.7121e+10, 1.9524e+10, 5.8230e+10, 1.6298e+10,
        1.1979e+10, 1.7214e+10, 5.8470e+10, 2.2851e+10, 3.5043e+10, 3.9869e+10,
        3.0595e+10, 3.4154e+10, 6.5613e+10, 1.3364e+10, 6.2981e+10, 1.4860e+10,
        6.7799e+10, 3.0422e+09, 7.1363e+10, 1.6738e+11, 1.5510e+10, 1.5979e+10,
        4.4738e+10, 1.8851e+10, 5.0519e+10, 3.3309e+10, 1.1798e+10, 9.6681e+10,
        1.3878e+11, 1.2852e+11, 1.7292e+11, 1.1899e+11, 1.1803e+10, 6.1060e+10,
        1.8360e+10, 4.4315e+10, 4.1137e+10, 1.7843e+11, 3.8881e+10, 6.9272e+10,
        3.9996e+10, 1.4316e+11, 1.4600e+11, 3.5166e+10, 9.6409e+10, 6.1646e+10,
        5.3988e+10, 5.3737e+10, 4.3275e+10, 3.5019e+10, 7.2279e+10, 1.1812e+11,
        3.4193e+10, 3.7018e+10, 2.5062e+10, 9.5082e+10, 6.3450e+10, 2.5288e+10,
        3.1432e+11, 8.0764e+09, 2.6814e+10, 3.6172e+10, 3.4210e+10, 1.7412e+10,
        5.5426e+10, 2.3870e+10, 2.7771e+10, 4.4504e+10, 6.4950e+10, 4.6536e+10,
        2.2189e+10, 4.1805e+10, 1.0035e+11, 7.7928e+10, 4.2344e+11, 1.4690e+10,
        7.7957e+10, 2.5719e+10, 4.8865e+10, 4.6997e+10, 1.7331e+12, 5.9120e+10,
        3.0621e+10, 2.8093e+10, 1.3008e+12, 1.6852e+10, 3.2534e+10, 2.8371e+11,
        7.4179e+10, 1.8142e+10, 2.2086e+10, 4.6732e+09, 2.8969e+10, 2.2905e+10,
        1.4035e+12, 9.7093e+09, 4.1590e+10, 1.8854e+11, 3.4829e+10, 7.8363e+10,
        1.4101e+12, 1.2276e+10, 9.4958e+10, 6.9786e+10, 3.3413e+10, 4.6452e+10,
        7.3901e+09, 3.7538e+10, 6.3486e+09, 1.3915e+10, 1.6447e+10, 1.2730e+11,
        2.4293e+11, 2.8276e+10, 3.7959e+10, 6.9300e+10, 1.6091e+10, 4.2126e+10,
        7.6484e+10, 5.3323e+10, 9.4666e+09, 3.2902e+10, 2.4791e+09, 4.7753e+10,
        3.1658e+11, 5.0579e+09, 4.7554e+10, 7.3366e+10, 3.2302e+10, 1.9064e+10,
        6.9869e+10, 3.9061e+10, 1.9125e+09, 4.4362e+10, 3.7109e+10, 1.0034e+11,
        2.5552e+10, 6.0308e+10, 4.6286e+10, 1.0436e+11, 1.3045e+10, 4.5982e+10,
        7.8160e+10, 4.1949e+10, 6.0918e+10, 1.5144e+11, 5.0844e+10, 2.9003e+10,
        5.9031e+10, 5.3201e+10, 1.1555e+10, 1.6342e+10, 3.3543e+10, 1.7879e+10,
        8.3010e+10, 2.0812e+11, 4.8597e+10, 3.5961e+10, 4.6605e+10, 8.6419e+10,
        2.4714e+10, 2.4877e+10, 6.3191e+09, 2.3299e+10, 1.1302e+10, 3.0114e+10,
        5.1616e+10, 2.9240e+10, 3.4031e+11, 5.0549e+10, 1.9584e+10, 8.4938e+10,
        1.4654e+10, 4.8710e+11, 3.5346e+10, 2.5860e+10, 1.1444e+11, 2.0710e+10,
        2.5143e+10, 2.8286e+10, 4.2672e+10, 1.5934e+10, 6.0427e+10, 1.2657e+11,
        7.1079e+10, 3.1177e+10, 2.7010e+10, 1.7064e+10, 1.6089e+10, 5.9563e+10,
        6.2810e+10, 2.2969e+10, 4.0933e+10, 1.2860e+12, 2.4434e+09, 2.1473e+10,
        7.1588e+09, 2.0083e+10, 5.0392e+10, 1.7583e+10, 4.2724e+09, 5.3969e+10,
        6.1347e+10, 2.0796e+10, 9.9551e+09, 4.9488e+10, 4.7157e+10, 2.0386e+11,
        3.1475e+09, 5.0696e+10, 2.9077e+10, 2.0012e+10, 2.4914e+10, 3.1941e+10,
        2.2174e+10, 1.7748e+10, 3.4207e+12, 6.5141e+10, 2.7093e+09, 3.2794e+10,
        1.9195e+10, 3.5094e+10, 6.3976e+09, 9.4135e+10, 1.4416e+10, 2.2504e+09,
        4.6264e+10, 1.7712e+09, 9.8739e+10, 3.1116e+10, 2.3172e+10, 2.1033e+11,
        5.1703e+11, 1.5076e+10, 4.9658e+10, 4.0073e+10, 6.8983e+10, 3.1997e+10,
        1.4563e+10, 2.4210e+10, 1.1881e+11, 5.8101e+10, 1.7335e+09, 2.9349e+10,
        6.2213e+10, 3.8279e+10, 2.8346e+10, 2.6767e+10, 4.1322e+11, 2.6581e+10,
        5.0862e+10, 8.2260e+11, 3.8196e+10, 8.2254e+10, 2.8507e+10, 1.7634e+10,
        9.3968e+10, 7.9992e+10, 6.5545e+10, 3.9121e+09, 3.7568e+10, 2.4779e+10,
        3.2776e+10, 6.4704e+10, 1.0999e+11, 6.1511e+10, 5.6389e+10, 6.3386e+10,
        1.0136e+11, 1.4154e+10, 4.3759e+09, 2.4093e+10, 2.9348e+10, 2.6463e+10,
        4.3138e+10, 3.9133e+10, 7.1911e+10, 4.7845e+11, 4.5798e+10, 1.6408e+10,
        2.3103e+11, 2.0578e+10, 5.4089e+10, 1.3405e+10, 3.3644e+10, 1.3819e+10,
        4.8537e+10, 2.8998e+10, 1.1162e+10, 9.3071e+10, 6.9009e+10, 9.2165e+10,
        1.4495e+10, 8.0246e+09, 1.9328e+10, 1.6544e+10, 3.5389e+12, 5.0184e+10,
        3.1747e+10, 1.1366e+11, 4.6238e+10, 8.5564e+09, 2.4678e+10, 3.2682e+10,
        9.9874e+10, 2.9124e+10, 5.1268e+09, 1.0906e+10, 5.1900e+09, 7.2690e+09,
        7.6788e+09, 2.2641e+09, 5.1306e+10, 6.4241e+10, 2.4318e+10, 8.2971e+10,
        1.5286e+10, 7.2184e+10, 1.3767e+11, 4.6394e+09, 5.7479e+10, 2.2933e+10,
        3.1801e+11, 3.0686e+10, 1.0826e+11, 1.7483e+10, 9.5034e+10, 1.9466e+10,
        3.9985e+10, 3.0657e+10, 4.2124e+09, 7.9724e+10, 2.9909e+10, 7.0176e+10,
        1.6669e+11, 2.2187e+10])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.2476e+11, 1.7125e+11, 1.1709e+11, 1.7827e+11, 1.9262e+11, 1.8284e+11,
        2.0537e+11, 7.4499e+10, 1.1265e+11, 3.8859e+10, 1.0752e+11, 2.0609e+11,
        1.7822e+11, 2.1636e+11, 1.6187e+11, 6.3392e+10, 6.3654e+10, 6.7124e+10,
        1.1158e+11, 1.6809e+11, 1.4626e+11, 6.3792e+10, 9.5801e+10, 1.3545e+11,
        1.3324e+11, 1.0480e+11, 5.4551e+10, 1.2397e+11, 8.1391e+10, 4.0856e+10,
        7.9691e+10, 1.1516e+11, 1.1620e+11, 8.6108e+10, 1.1021e+11, 1.2787e+11,
        1.0396e+11, 9.6456e+10, 1.2131e+11, 1.1068e+11, 7.9167e+10, 1.0859e+11,
        1.4543e+11, 3.1119e+11, 9.7466e+10, 4.4926e+10, 1.3863e+11, 1.2192e+11,
        1.9651e+11, 8.2717e+10, 5.4607e+10, 1.7022e+11, 1.0034e+11, 2.2081e+11,
        1.1272e+11, 1.4061e+11, 1.1576e+11, 1.0738e+11, 5.9660e+10, 1.7352e+11,
        5.0319e+10, 5.7838e+10, 6.9096e+10, 1.7735e+11, 9.7009e+10, 1.1710e+11,
        6.2515e+10, 1.3425e+11, 1.3822e+11, 7.4623e+10, 6.2792e+10, 5.1158e+10,
        8.9916e+10, 8.4713e+10, 1.1450e+11, 5.5622e+10, 1.6591e+11, 1.9226e+11,
        4.6268e+10, 1.8643e+11, 1.8230e+11, 1.0059e+11, 1.8257e+11, 8.3504e+10,
        9.0937e+10, 1.1815e+11, 1.2235e+11, 9.8027e+10, 6.6157e+10, 5.3887e+10,
        1.4508e+11, 5.7823e+10, 4.9278e+10, 1.4239e+11, 1.2391e+11, 6.9711e+10,
        9.9431e+10, 8.6406e+10, 1.4854e+11, 9.0883e+10, 9.4528e+10, 7.0400e+10,
        2.2279e+11, 1.8688e+11, 1.9974e+11, 1.1340e+11, 1.4098e+11, 8.8940e+10,
        1.2141e+11, 8.8094e+10, 1.8212e+11, 2.1584e+11, 7.1601e+10, 1.3916e+11,
        1.5523e+11, 8.7381e+10, 1.1474e+11, 4.3891e+10, 1.2939e+11, 1.3407e+11,
        9.8977e+10, 4.3017e+10, 9.4083e+10, 1.9046e+11, 9.3899e+10, 7.3229e+10,
        1.5025e+11, 1.5455e+11])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.4353e+11, 9.4629e+10, 1.0331e+11, 1.7257e+11, 1.8958e+11, 1.1819e+11,
        2.5810e+11, 1.4016e+11, 1.8512e+11, 9.2424e+10, 1.0038e+11, 2.6506e+11,
        7.9020e+10, 2.0741e+11, 6.3176e+10, 1.7179e+11, 1.8460e+11, 1.7218e+11,
        1.5151e+11, 2.3216e+11, 1.1585e+11, 9.4744e+10, 1.8501e+11, 2.5622e+11,
        2.5050e+11, 7.2287e+10, 9.8474e+10, 2.2787e+11, 1.4194e+11, 2.4015e+11,
        9.8847e+10, 1.5285e+11, 5.5754e+10, 1.0885e+11, 7.1363e+10, 2.0433e+11,
        2.2451e+11, 1.6223e+11, 3.4096e+11, 2.3493e+11, 1.7360e+11, 1.4022e+11,
        8.2297e+10, 1.6362e+11, 7.0350e+10, 8.2322e+10, 2.6960e+11, 2.2398e+11,
        2.7901e+11, 1.9235e+11, 1.4707e+11, 8.7716e+10, 2.3689e+11, 1.8175e+11,
        1.1257e+11, 2.2528e+11, 1.4391e+11, 1.8760e+11, 1.0064e+11, 1.2632e+11,
        2.7798e+11, 1.5017e+11, 2.4819e+11, 9.1485e+10, 1.6593e+11, 2.6358e+11,
        2.2650e+11, 1.4187e+11, 1.4299e+11, 2.3769e+11, 2.1096e+11, 1.2263e+11,
        8.4487e+10, 3.0757e+11, 9.1084e+10, 1.8175e+11, 1.9541e+11, 2.1817e+11,
        1.4502e+11, 1.2194e+11, 9.7813e+10, 9.1917e+10, 1.5520e+11, 7.3789e+10,
        1.2070e+11, 2.7691e+11, 1.6496e+11, 1.6062e+11, 2.6140e+11, 2.0838e+11,
        8.5774e+10, 1.7339e+11, 1.5695e+11, 1.1202e+11, 1.1185e+11, 2.6759e+11,
        2.9589e+11, 2.2589e+11, 1.3894e+11, 2.3752e+11, 1.0796e+11, 9.5327e+10,
        2.1886e+11, 1.1865e+11, 3.2974e+11, 3.5220e+11, 1.5094e+11, 1.2413e+11,
        1.5062e+11, 7.5411e+10, 2.1620e+11, 2.2338e+11, 3.4304e+11, 1.0565e+11,
        8.5405e+10, 1.6231e+11, 4.9104e+10, 1.6855e+11, 2.4602e+11, 3.6199e+11,
        2.5685e+11, 2.2233e+11, 1.6276e+11, 1.0976e+11, 3.0820e+11, 2.7421e+11,
        2.4497e+11, 1.3486e+11])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.4719e+09, 1.8789e+09, 1.3816e+09, 4.3494e+09, 1.5775e+09, 4.2257e+09,
        1.6402e+09, 1.6775e+10, 1.5832e+10, 2.0361e+09, 2.7659e+09, 4.8419e+09,
        2.3133e+09, 3.6518e+09, 1.0686e+10, 4.2256e+09, 3.0520e+09, 1.9960e+09,
        3.2533e+09, 5.1565e+09, 2.2895e+09, 6.4428e+08, 3.9229e+10, 5.0234e+10,
        2.8535e+09, 1.7720e+10, 3.8711e+09, 3.6593e+09, 4.4690e+09, 2.1499e+10,
        4.1243e+09, 1.9891e+09, 8.3663e+09, 4.3729e+08, 1.3634e+09, 1.5113e+09,
        7.9062e+08, 5.2331e+08, 3.3550e+09, 1.1502e+10, 2.6866e+09, 2.3076e+09,
        9.3627e+09, 4.7214e+09, 1.5652e+09, 9.1093e+08, 1.9201e+10, 1.6152e+09,
        6.9036e+09, 4.5291e+09, 3.1778e+09, 5.3244e+09, 4.8031e+09, 9.5157e+09,
        2.8324e+10, 6.9282e+09, 6.0386e+09, 6.3906e+09, 5.6053e+09, 2.1904e+08,
        1.1751e+10, 6.1410e+09, 3.7469e+08, 3.0516e+09, 1.1716e+09, 1.0870e+09,
        3.4150e+09, 2.5305e+09, 7.0011e+09, 2.4839e+10, 9.4875e+08, 3.1066e+09,
        6.7614e+09, 4.5668e+08, 9.8948e+09, 3.7629e+09, 6.4671e+08, 1.3943e+10,
        4.2172e+09, 7.1078e+10, 2.5792e+10, 2.1206e+10, 1.2033e+09, 3.9509e+08,
        1.7909e+10, 3.3374e+10, 3.2082e+09, 4.0881e+09, 1.1427e+09, 4.2872e+09,
        5.5622e+09, 5.7499e+09, 4.5788e+09, 2.8363e+09, 1.6561e+09, 3.4953e+09,
        4.3599e+09, 6.9392e+09, 9.9861e+09, 7.7058e+09, 3.2331e+09, 1.0159e+09,
        4.5479e+09, 5.6746e+09, 1.0885e+09, 2.5028e+09, 6.3469e+08, 2.9346e+09,
        6.0640e+09, 1.8463e+10, 1.3601e+09, 2.5485e+09, 7.8198e+09, 8.1479e+08,
        2.6141e+09, 5.4138e+09, 8.4627e+09, 1.7739e+09, 4.0945e+09, 2.1688e+09,
        1.4293e+09, 6.7888e+08, 9.3665e+09, 4.6399e+09, 2.1163e+09, 6.2589e+08,
        7.0510e+09, 4.3078e+09, 1.8165e+09, 1.4435e+09, 3.4667e+09, 1.0418e+10,
        9.4136e+09, 2.7482e+09, 8.0048e+09, 1.4521e+09, 6.9358e+08, 5.2833e+09,
        1.6172e+09, 4.9895e+09, 6.3239e+09, 3.2606e+09, 3.3015e+09, 7.0533e+08,
        1.7846e+09, 3.1252e+09, 2.5032e+08, 8.9333e+08, 4.0438e+09, 4.1853e+09,
        6.2866e+09, 7.8260e+09, 6.8736e+09, 3.6197e+09, 4.4605e+09, 1.9233e+10,
        5.0655e+09, 2.2469e+09, 4.1767e+09, 2.1832e+09, 4.7877e+09, 5.2026e+09,
        2.1076e+10, 4.2802e+10, 3.1361e+09, 5.8083e+08, 1.0777e+10, 1.0488e+10,
        9.1814e+09, 1.0212e+10, 1.0988e+10, 1.0145e+10, 3.1067e+09, 1.4146e+09,
        1.4837e+09, 5.5543e+09, 9.2660e+09, 6.1925e+09, 6.6110e+09, 2.7399e+09,
        1.0588e+09, 2.5757e+09, 2.3623e+09, 1.9719e+09, 2.5859e+09, 3.6952e+09,
        5.1621e+10, 2.8271e+08, 2.4055e+09, 1.3850e+10, 1.1067e+10, 1.7971e+09,
        2.1644e+09, 2.2204e+08, 1.0916e+10, 1.0177e+11, 4.4812e+09, 1.6556e+09,
        2.0304e+10, 4.2563e+09, 1.4305e+09, 1.3408e+09, 2.7258e+09, 3.0165e+10,
        1.6920e+10, 9.8963e+09, 1.5207e+10, 8.2365e+10, 8.7204e+09, 3.9590e+09,
        3.3358e+09, 2.7935e+09, 4.1652e+09, 7.4542e+09, 1.0417e+10, 2.2343e+10,
        1.2992e+10, 2.1404e+09, 3.5283e+09, 1.1340e+09, 4.3575e+09, 2.5075e+10,
        3.3892e+09, 2.9438e+09, 1.0288e+11, 1.6021e+10, 7.0885e+09, 1.2808e+10,
        7.5000e+09, 5.4085e+09, 1.5479e+10, 1.2957e+10, 1.2130e+10, 1.1708e+10,
        1.5682e+09, 4.6047e+09, 3.1218e+09, 8.5640e+09, 3.4920e+09, 3.3307e+09,
        5.3996e+09, 2.8366e+09, 2.1050e+09, 7.2501e+09, 2.7426e+10, 1.0089e+10,
        2.1113e+09, 1.0471e+09, 6.4784e+09, 3.9679e+10, 4.6318e+09, 6.7852e+08,
        3.2071e+09, 1.0628e+10, 2.8145e+10, 1.6127e+09, 5.8046e+09, 1.6166e+09,
        4.3126e+09, 3.2630e+09, 3.0262e+09, 3.0013e+08, 4.9525e+09, 6.3283e+09,
        1.4542e+10, 2.3507e+10, 3.1565e+08, 2.2569e+08, 5.0091e+08, 4.7933e+09,
        1.5015e+10, 9.4336e+08, 2.2684e+10, 4.6853e+09, 1.1006e+09, 2.6889e+09,
        9.2168e+08, 3.8737e+09, 2.9272e+09, 2.9981e+09, 3.2617e+09, 1.3478e+10,
        2.0694e+10, 8.1011e+08, 5.9792e+08, 1.9554e+10, 8.5932e+09, 2.1786e+09,
        4.1632e+11, 2.5605e+09, 3.0426e+09, 5.3214e+09, 1.3621e+09, 5.5648e+09,
        1.7816e+10, 4.7686e+09, 3.5002e+09, 1.1475e+09, 7.0265e+08, 3.7713e+09,
        4.0782e+11, 4.5578e+08, 8.2814e+09, 4.0878e+09, 7.7290e+09, 6.4976e+09,
        2.4537e+09, 1.0031e+10, 3.8225e+08, 4.9339e+09, 1.4446e+09, 1.7871e+10,
        1.5188e+09, 5.3544e+09, 1.1926e+11, 9.2479e+09, 2.6086e+09, 1.5256e+10,
        8.3914e+09, 4.0019e+09, 8.4619e+09, 9.2089e+09, 1.5177e+10, 9.3312e+09,
        1.1115e+10, 7.5147e+09, 6.3642e+09, 5.7478e+09, 3.8742e+10, 5.1634e+09,
        1.8655e+10, 3.5781e+10, 2.5923e+09, 1.1017e+10, 5.2541e+09, 1.2115e+10,
        4.6020e+09, 2.8470e+09, 1.4719e+09, 1.1496e+10, 4.2263e+09, 4.8209e+09,
        4.0065e+09, 1.1843e+09, 4.0256e+09, 4.4308e+09, 3.8930e+10, 4.0199e+09,
        2.2896e+09, 7.8057e+09, 1.8887e+10, 1.9863e+10, 6.9133e+10, 1.1092e+10,
        2.2855e+09, 4.7000e+09, 1.1769e+10, 3.8194e+09, 2.9821e+10, 2.5415e+09,
        7.5553e+09, 2.5950e+09, 1.3967e+10, 8.1342e+09, 2.8329e+09, 3.0965e+09,
        4.3632e+09, 1.9151e+09, 5.9606e+09, 6.8139e+09, 1.2707e+08, 2.7465e+09,
        1.1638e+09, 6.2393e+09, 1.0669e+09, 1.0382e+10, 1.6895e+10, 2.4786e+09,
        8.7841e+09, 7.9609e+09, 3.9871e+08, 3.1864e+09, 1.1792e+11, 1.2435e+10,
        8.2440e+08, 4.3893e+09, 9.9321e+09, 9.6306e+08, 5.8050e+09, 2.4121e+09,
        2.1927e+09, 6.9494e+09, 5.9671e+09, 9.6135e+09, 4.5171e+09, 2.3872e+09,
        6.7704e+09, 6.7036e+09, 6.2228e+08, 5.0963e+10, 3.7771e+09, 7.5022e+08,
        1.0496e+10, 6.4758e+08, 3.0863e+09, 1.1205e+10, 9.6002e+08, 2.1554e+09,
        1.8039e+11, 8.2203e+08, 8.4538e+09, 6.6946e+09, 2.1135e+10, 2.0048e+09,
        1.1762e+09, 6.8961e+09, 7.5267e+09, 5.5860e+09, 3.6355e+08, 6.1273e+10,
        1.5195e+10, 8.6585e+09, 3.4943e+09, 4.8685e+09, 3.8245e+09, 9.1615e+08,
        2.4046e+09, 5.8131e+09, 5.1555e+09, 2.7597e+09, 9.1838e+09, 2.3273e+09,
        2.7638e+09, 4.7308e+09, 6.1594e+09, 1.2079e+09, 5.9653e+09, 8.7575e+09,
        1.4175e+09, 5.7776e+10, 2.7894e+09, 7.2430e+09, 2.7041e+09, 7.1312e+09,
        4.7419e+09, 8.5696e+09, 9.8040e+08, 2.8064e+09, 4.5394e+09, 6.8159e+09,
        1.5003e+09, 4.3293e+09, 6.3006e+10, 2.2071e+10, 6.6811e+09, 4.4063e+09,
        1.0177e+10, 2.0433e+09, 1.1331e+10, 1.1799e+09, 3.3187e+10, 9.0492e+08,
        1.0745e+09, 1.9865e+11, 5.5817e+09, 3.0127e+09, 8.4674e+09, 2.8793e+09,
        1.0155e+09, 1.2007e+09, 8.2780e+09, 1.0736e+10, 4.1126e+09, 2.8970e+09,
        9.3455e+09, 1.1161e+10, 7.1033e+09, 2.8114e+09, 9.1457e+09, 1.2386e+10,
        1.1741e+10, 1.0637e+10, 4.6501e+08, 3.1840e+09, 7.0111e+08, 1.4226e+09,
        7.4320e+08, 1.2937e+09, 1.3364e+10, 8.7881e+09, 1.2569e+10, 1.1496e+10,
        2.6879e+09, 1.2248e+09, 4.2322e+09, 3.1065e+09, 1.3485e+10, 1.3883e+10,
        1.7062e+11, 2.2791e+09, 1.1332e+09, 5.0153e+09, 1.4667e+09, 1.6327e+09,
        4.9307e+09, 3.2260e+10, 1.2391e+09, 1.8722e+09, 3.8159e+09, 2.8924e+09,
        1.3250e+10, 9.0203e+08])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.7442e+09, 1.7817e+10, 1.8603e+09, 2.3550e+10, 3.8930e+09, 8.5960e+09,
        4.0611e+09, 5.3770e+09, 2.5121e+09, 1.2748e+10, 4.4603e+09, 8.3415e+09,
        5.4596e+09, 2.0480e+10, 1.5530e+10, 9.3761e+09, 7.3532e+09, 5.2622e+09,
        9.5002e+09, 9.2264e+09, 1.0354e+10, 1.4786e+10, 8.1823e+09, 3.0852e+09,
        7.0758e+09, 5.3523e+09, 6.1356e+09, 5.3959e+09, 2.4482e+09, 1.9349e+10,
        1.6749e+10, 5.7506e+09, 2.2809e+10, 1.3733e+10, 9.9938e+09, 7.5052e+09,
        6.0105e+09, 7.2093e+09, 2.4017e+10, 9.7701e+09, 5.0264e+09, 2.7958e+09,
        5.4242e+09, 3.4290e+09, 1.9551e+10, 1.4243e+10, 1.3667e+10, 1.5860e+09,
        2.1316e+09, 3.2691e+09, 3.9086e+09, 1.3117e+10, 4.9456e+09, 7.1826e+09,
        1.5116e+10, 1.1515e+10, 1.1435e+10, 7.7108e+09, 1.2661e+10, 2.0099e+10,
        3.9167e+09, 1.0251e+10, 8.1124e+09, 2.6919e+10, 6.7992e+09, 1.4383e+10,
        5.0011e+09, 1.5649e+10, 2.2082e+10, 3.8719e+09, 2.0439e+09, 6.0647e+09,
        5.6852e+09, 3.6169e+09, 1.4620e+09, 1.1445e+10, 1.3302e+10, 7.6646e+09,
        1.1971e+10, 6.8997e+09, 6.2478e+09, 1.2730e+10, 5.5585e+09, 1.7741e+10,
        1.0934e+10, 1.0491e+10, 1.1793e+10, 6.9024e+09, 5.0897e+09, 3.3512e+09,
        3.4348e+09, 2.5285e+09, 1.6557e+10, 2.7335e+09, 7.2815e+09, 5.3533e+09,
        6.8525e+09, 1.3591e+10, 1.0359e+10, 4.3032e+09, 4.1098e+09, 3.1094e+09,
        4.2408e+09, 4.6509e+09, 8.7981e+09, 1.6636e+10, 2.3590e+10, 1.6367e+10,
        4.6783e+09, 1.2684e+10, 4.4543e+09, 3.8485e+09, 5.3179e+09, 3.6550e+09,
        1.6547e+10, 2.9236e+09, 9.4013e+09, 1.2935e+10, 6.6535e+09, 1.4597e+10,
        5.2737e+09, 2.0422e+09, 7.2209e+09, 6.9515e+09, 1.6381e+10, 1.6958e+10,
        1.4599e+10, 1.6781e+10, 1.0299e+10, 1.7433e+10, 1.7535e+10, 9.2400e+09,
        1.9234e+10, 4.5005e+09, 3.2828e+09, 1.9897e+10, 1.4256e+10, 7.7594e+09,
        9.5184e+09, 1.5981e+10, 4.1517e+09, 1.3431e+10, 4.1616e+09, 5.6567e+09,
        2.3513e+09, 1.5490e+10, 1.4604e+10, 9.1337e+09, 1.3753e+10, 9.7072e+09,
        7.7915e+09, 6.0476e+09, 2.8563e+09, 6.0095e+09, 6.6627e+09, 2.1643e+10,
        3.0621e+10, 3.3107e+09, 6.5941e+09, 2.8898e+09, 1.8204e+10, 3.4627e+09,
        1.6920e+10, 1.0188e+10, 1.0830e+10, 7.4423e+09, 1.7636e+10, 7.7470e+09,
        9.0907e+09, 1.0295e+10, 8.7734e+09, 1.0237e+10, 1.2599e+10, 8.2660e+09,
        2.1008e+10, 6.3539e+09, 8.3826e+09, 2.1376e+10, 6.1604e+09, 5.5339e+09,
        1.7187e+10, 1.7151e+10, 6.6329e+09, 2.3654e+10, 1.0683e+10, 1.8012e+09,
        7.4611e+09, 1.0214e+10, 5.3420e+09, 9.4455e+09, 1.3724e+10, 2.4175e+09,
        1.2793e+09, 1.1164e+10, 1.2681e+10, 1.0430e+10, 1.2337e+10, 4.6862e+09,
        6.6917e+09, 4.7404e+09, 2.0977e+09, 1.2649e+10, 2.9854e+09, 1.2022e+10,
        2.6215e+09, 1.4094e+10, 1.4422e+10, 2.1346e+09, 5.7265e+09, 5.6732e+09,
        8.2506e+09, 2.8352e+09, 4.0578e+09, 4.2731e+09, 1.5412e+10, 3.8514e+09,
        5.0201e+09, 1.2326e+10, 2.6250e+10, 3.1889e+09, 8.0321e+09, 1.6121e+10,
        1.8907e+09, 1.0310e+10, 4.2385e+09, 1.0745e+09, 1.2105e+10, 9.9892e+09,
        1.0131e+10, 8.5661e+09, 7.3205e+09, 9.4326e+09, 1.2856e+10, 1.3267e+10,
        4.8886e+09, 2.0886e+10, 1.8753e+10, 1.1504e+09, 3.2428e+09, 9.9887e+09,
        4.5997e+09, 8.1562e+09, 7.7351e+09, 1.7822e+10, 2.9414e+09, 2.2865e+09,
        7.0039e+09, 2.0298e+10, 9.5358e+09, 6.7687e+09, 3.6603e+09, 6.1353e+09,
        6.6147e+09, 4.8275e+09, 1.4948e+10, 1.2435e+10])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.5647e+10, 1.5908e+10, 1.6170e+10, 1.5548e+10, 1.1780e+10, 1.0887e+10,
        8.1589e+09, 2.0568e+10, 2.7561e+10, 1.9077e+10, 1.4678e+10, 4.9084e+09,
        1.2618e+10, 1.1114e+10, 6.5842e+09, 3.5590e+09, 3.7869e+10, 3.2157e+09,
        1.2458e+10, 5.1543e+09, 3.4675e+10, 5.3451e+09, 2.1271e+10, 1.0078e+10,
        8.4215e+09, 8.4731e+09, 1.8498e+10, 1.5888e+10, 1.6987e+10, 6.4787e+09,
        1.9143e+10, 1.0955e+10, 1.2589e+10, 3.0251e+10, 2.1699e+10, 3.1230e+09,
        6.7863e+09, 2.4676e+10, 5.9392e+09, 1.2707e+10, 5.5757e+09, 1.9878e+10,
        1.3430e+10, 1.1816e+10, 9.3189e+09, 1.0701e+10, 3.6324e+09, 1.6114e+10,
        2.7408e+10, 1.4001e+10, 1.3991e+10, 3.7560e+10, 1.7972e+10, 1.9009e+10,
        1.3580e+10, 1.8421e+10, 4.1600e+09, 1.9090e+10, 1.2538e+10, 5.2121e+09,
        1.1693e+10, 1.1658e+10, 1.9531e+10, 2.5139e+10, 8.5730e+09, 2.2509e+10,
        3.5347e+10, 5.2169e+09, 4.1510e+10, 3.0276e+10, 3.9812e+10, 5.8091e+09,
        4.4859e+09, 7.7405e+09, 1.5580e+10, 1.3288e+10, 1.4040e+10, 1.0536e+10,
        3.3502e+09, 3.4297e+09, 2.0058e+10, 2.6488e+10, 3.5381e+10, 1.0469e+10,
        1.0042e+10, 1.2430e+10, 1.9289e+10, 2.3552e+10, 1.5837e+10, 1.8056e+10,
        4.3992e+09, 1.5002e+10, 6.3535e+09, 7.4395e+09, 5.3637e+09, 2.3938e+10,
        6.8598e+09, 1.2501e+10, 9.1757e+09, 8.4526e+09, 1.5879e+10, 1.0139e+10,
        2.0121e+10, 1.3432e+10, 1.6000e+10, 1.0581e+10, 4.5320e+09, 1.4560e+10,
        7.8952e+09, 1.2087e+10, 1.2915e+10, 1.8533e+10, 2.2412e+10, 9.7543e+09,
        1.8158e+10, 1.1840e+10, 1.5641e+10, 1.2484e+10, 8.6099e+09, 2.4354e+10,
        2.1867e+10, 1.1045e+10, 2.7840e+10, 1.1627e+10, 1.4314e+10, 1.6465e+10,
        5.5983e+09, 2.4279e+10, 3.3292e+09, 1.8565e+10, 1.5683e+10, 1.3373e+10,
        4.1361e+10, 3.2691e+10, 1.0243e+10, 1.5781e+10, 7.8313e+09, 2.9235e+10,
        1.8591e+09, 2.1587e+10, 3.3851e+10, 1.6225e+10, 1.5745e+10, 9.5995e+09,
        1.1339e+10, 6.5591e+09, 2.1620e+10, 7.4422e+09, 4.5019e+09, 9.6717e+09,
        1.6383e+10, 7.4789e+09, 3.3673e+10, 7.5758e+09, 8.7613e+09, 1.7562e+10,
        2.3769e+09, 1.7078e+10, 1.5132e+10, 6.6646e+09, 2.4759e+10, 6.0597e+09,
        1.0222e+10, 4.1104e+09, 7.8779e+09, 4.6077e+10, 1.5268e+10, 1.6840e+10,
        3.0219e+10, 7.3950e+09, 1.3005e+10, 1.3458e+10, 1.2547e+10, 1.3828e+10,
        5.6139e+09, 7.5810e+09, 1.5223e+10, 7.1111e+09, 8.4269e+09, 8.7839e+09,
        8.7675e+09, 1.7615e+10, 3.3615e+10, 3.9209e+10, 6.0101e+09, 1.5000e+10,
        5.8829e+09, 1.1489e+10, 1.3673e+10, 1.1630e+10, 2.2738e+10, 5.1742e+09,
        1.3168e+10, 1.0727e+10, 2.0972e+10, 3.1163e+10, 2.3363e+10, 7.0321e+09,
        1.4032e+10, 7.4219e+09, 1.8426e+10, 2.2439e+10, 1.1241e+10, 1.4151e+10,
        3.8995e+10, 2.2204e+10, 5.9830e+09, 2.2284e+10, 1.6767e+10, 1.8389e+10,
        1.0806e+10, 3.6189e+10, 6.5360e+09, 1.1191e+10, 1.5184e+10, 2.9185e+10,
        1.1850e+10, 5.8354e+09, 3.5817e+09, 8.4091e+09, 2.0926e+10, 3.7266e+09,
        2.4088e+10, 2.1643e+10, 2.4624e+10, 1.5623e+10, 3.8502e+10, 6.4178e+09,
        5.3954e+10, 1.5676e+10, 2.5701e+10, 1.0781e+10, 4.2867e+10, 2.3617e+10,
        6.4262e+09, 6.3291e+09, 2.2230e+10, 4.3125e+10, 1.7271e+10, 6.9763e+09,
        6.2105e+09, 1.6986e+10, 6.9139e+09, 1.3929e+10, 6.5159e+09, 1.2944e+10,
        1.2820e+10, 1.0448e+10, 1.2533e+10, 4.1849e+09, 1.7152e+10, 1.2368e+10,
        3.9134e+10, 1.2709e+10, 2.5025e+10, 1.9287e+10])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.9634e+07, 3.7422e+07, 4.3639e+08,  ..., 9.8253e+07, 3.3843e+07,
        1.1345e+07])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.9683e+07, 6.5885e+07, 2.3609e+08,  ..., 3.8622e+08, 9.2630e+08,
        8.5680e+07])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1493e+09, 2.0863e+09, 5.0901e+08, 8.2955e+08, 1.5191e+09, 6.1642e+08,
        7.6596e+08, 1.5558e+09, 1.2468e+09, 7.5021e+07, 3.8384e+08, 2.4673e+08,
        1.4989e+09, 7.1440e+08, 1.5597e+08, 9.4168e+08, 7.0912e+08, 1.6326e+08,
        1.1564e+09, 8.7366e+07, 2.1145e+08, 4.1209e+08, 4.6154e+08, 1.2333e+09,
        1.3086e+08, 1.4356e+09, 3.8191e+08, 1.1875e+08, 3.9730e+08, 9.5169e+08,
        5.6038e+08, 1.4325e+09, 1.3752e+09, 1.8336e+08, 1.0536e+09, 9.1507e+08,
        7.7100e+08, 3.2961e+08, 8.6883e+08, 4.7106e+08, 7.1296e+08, 2.8098e+09,
        1.2475e+09, 6.4053e+07, 8.8752e+08, 1.1834e+09, 9.2208e+08, 3.0321e+08,
        4.2536e+08, 1.2584e+09, 1.4490e+09, 5.5637e+08, 4.9301e+08, 1.6740e+08,
        6.6652e+08, 1.9138e+09, 1.7093e+09, 3.2780e+08, 3.1231e+08, 2.8180e+08,
        6.6076e+08, 6.9866e+08, 3.0333e+08, 1.2049e+08, 9.0018e+08, 1.5218e+09,
        1.9006e+09, 4.5137e+08, 2.6159e+08, 2.0639e+09, 1.3122e+09, 5.3513e+08,
        4.7418e+08, 7.7618e+08, 1.4666e+08, 4.1769e+08, 5.3504e+08, 5.6000e+08,
        7.2644e+08, 7.3843e+08, 1.3486e+09, 9.7957e+08, 6.1114e+08, 4.4392e+08,
        1.1932e+09, 1.2758e+09, 2.6258e+08, 5.9030e+08, 4.3846e+08, 2.4569e+09,
        4.9754e+08, 4.2611e+08, 6.7727e+08, 1.3762e+08, 3.7167e+08, 6.8743e+08,
        1.9783e+09, 6.3131e+08, 1.0267e+09, 2.5819e+09, 4.8427e+08, 2.3118e+08,
        1.5538e+08, 9.3294e+08, 7.6500e+08, 4.2658e+08, 9.3155e+08, 7.5964e+08,
        6.3563e+08, 4.5892e+08, 8.4865e+08, 6.7340e+07, 1.0553e+08, 3.1570e+08,
        4.3131e+08, 1.4759e+09, 1.1971e+09, 1.4845e+09, 4.2918e+08, 3.1552e+08,
        4.4798e+08, 3.0170e+08, 1.2626e+09, 2.4350e+08, 1.9996e+08, 6.2835e+08,
        6.3772e+08, 7.6573e+08, 1.5203e+09, 8.1277e+08, 7.0866e+08, 1.4624e+09,
        1.3156e+09, 1.1137e+08, 7.2823e+08, 5.7457e+08, 9.4149e+08, 1.8923e+08,
        4.5368e+08, 6.6606e+08, 2.7205e+08, 1.7726e+09, 1.8138e+08, 7.0053e+08,
        9.0071e+08, 1.5730e+09, 4.9340e+08, 8.7839e+08, 3.5758e+08, 1.4868e+08,
        8.1293e+08, 5.9534e+08, 3.4717e+08, 1.8737e+08, 8.7630e+08, 4.6943e+08,
        1.5936e+09, 8.3822e+08, 1.1981e+09, 1.2547e+09, 6.0278e+08, 3.1140e+08,
        7.0564e+08, 1.3256e+09, 2.5627e+08, 6.8901e+07, 4.0381e+08, 5.8867e+08,
        3.3910e+08, 1.3038e+09, 6.4709e+08, 8.8294e+08, 1.5065e+09, 1.6883e+09,
        3.0394e+08, 1.0234e+09, 6.2121e+08, 1.4699e+08, 5.9556e+08, 3.8405e+08,
        7.2498e+08, 1.7494e+09, 1.2174e+09, 7.4318e+08, 4.9052e+08, 2.2347e+08,
        4.5947e+08, 2.5318e+08, 1.7379e+09, 3.8935e+08, 6.0969e+08, 8.4808e+08,
        9.1497e+08, 2.2439e+08, 4.8243e+08, 3.7348e+08, 4.8386e+08, 1.5001e+09,
        1.4083e+09, 1.5487e+09, 3.4436e+08, 7.9776e+08, 3.5948e+08, 1.7856e+09,
        1.4219e+08, 6.3817e+08, 1.0383e+08, 1.0963e+09, 1.7658e+09, 3.7898e+08,
        1.4743e+09, 3.2881e+08, 1.3567e+09, 6.1489e+08, 3.3481e+08, 3.5791e+08,
        1.6899e+08, 4.2432e+08, 1.5312e+09, 5.2622e+08, 9.8836e+08, 2.3058e+08,
        1.0921e+09, 1.1710e+09, 5.0384e+08, 1.8016e+09, 1.1066e+09, 5.5018e+08,
        8.4124e+07, 7.0669e+08, 3.4564e+08, 1.9894e+08, 6.1802e+08, 4.4603e+08,
        6.6393e+08, 6.7849e+08, 7.2611e+08, 2.7799e+08, 1.3955e+09, 1.3081e+09,
        7.5148e+08, 4.1579e+08, 1.0490e+09, 8.2880e+08, 1.1361e+08, 1.3100e+09,
        7.7341e+07, 6.8079e+08, 2.9853e+08, 1.5331e+08, 1.5812e+09, 5.5681e+08,
        9.3080e+08, 1.2952e+09, 1.4122e+09, 5.4083e+08])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.8947e+09, 3.2423e+09, 3.9822e+09, 9.5211e+08, 2.9446e+09, 2.6656e+09,
        2.2522e+09, 4.3133e+08, 2.2830e+09, 4.7042e+09, 1.5906e+09, 4.1649e+09,
        6.1692e+08, 6.3530e+08, 8.6800e+08, 3.0365e+08, 2.4629e+09, 7.6868e+08,
        1.6466e+09, 2.0762e+09, 9.5716e+08, 1.3093e+09, 3.1401e+09, 8.2924e+08,
        1.8452e+09, 2.6111e+09, 1.0071e+09, 8.7840e+08, 3.1032e+08, 5.9963e+08,
        6.9603e+08, 2.8423e+09, 1.1543e+09, 1.8772e+09, 2.1134e+09, 1.7823e+09,
        1.4351e+09, 4.8112e+09, 1.3511e+08, 9.2052e+08, 4.0968e+08, 2.7163e+09,
        3.3159e+09, 4.9888e+09, 1.3813e+09, 6.2495e+08, 1.8928e+09, 9.7818e+08,
        1.1463e+09, 1.3764e+09, 3.9044e+09, 3.0443e+09, 1.0832e+09, 3.5493e+09,
        2.1438e+09, 1.0008e+09, 9.3838e+08, 2.5306e+09, 1.6328e+09, 6.1556e+08,
        1.0460e+09, 5.8950e+08, 9.2513e+08, 9.7114e+08, 2.8779e+09, 1.5981e+09,
        1.1376e+09, 2.9808e+08, 1.4719e+09, 2.2171e+09, 1.9368e+09, 4.5809e+08,
        4.9000e+09, 1.4252e+09, 3.2320e+09, 2.7644e+09, 1.1663e+09, 5.5379e+08,
        1.2704e+09, 5.6571e+08, 2.1802e+08, 1.9549e+09, 1.6217e+09, 2.0372e+09,
        1.0763e+09, 3.4585e+08, 9.5437e+08, 1.4083e+09, 4.0087e+08, 4.0067e+09,
        5.8348e+08, 2.6149e+08, 2.7018e+09, 3.2085e+09, 7.1406e+08, 3.3982e+09,
        3.5466e+09, 2.9164e+09, 2.3343e+08, 1.6164e+09, 1.2682e+09, 4.1161e+08,
        2.9160e+08, 9.0510e+08, 1.9183e+08, 4.8051e+08, 1.7954e+09, 2.0328e+09,
        1.8678e+09, 1.4003e+09, 6.0928e+08, 1.6298e+08, 4.8578e+08, 2.4769e+09,
        3.0063e+09, 2.0699e+09, 1.3130e+09, 1.4716e+09, 6.2601e+08, 3.7869e+09,
        5.9457e+09, 3.5175e+09, 1.0012e+09, 1.3722e+09, 2.9397e+08, 4.9754e+08,
        4.8011e+09, 1.5357e+09, 1.0434e+09, 6.7207e+08, 3.9236e+08, 7.3799e+08,
        1.2685e+09, 1.0452e+09, 2.0427e+09, 1.9673e+09, 4.4936e+08, 2.8198e+09,
        1.7589e+09, 1.5327e+09, 2.5368e+09, 3.0952e+09, 2.3174e+09, 1.8197e+09,
        6.8330e+08, 2.7210e+09, 8.9432e+08, 1.0275e+09, 1.2937e+09, 1.3059e+09,
        1.2634e+09, 4.3047e+09, 1.2442e+09, 2.5815e+09, 1.8164e+09, 2.4130e+08,
        8.8007e+08, 2.5970e+08, 2.2690e+09, 1.4694e+09, 1.5456e+08, 3.8771e+09,
        1.1761e+09, 1.4593e+09, 3.4908e+09, 8.0071e+08, 1.9746e+09, 6.6473e+08,
        1.7170e+09, 9.8856e+08, 1.3986e+09, 2.5885e+09, 1.7078e+09, 2.3304e+09,
        3.9226e+09, 1.8361e+09, 7.9293e+08, 1.1326e+09, 1.0152e+09, 5.5485e+08,
        2.7571e+09, 1.2429e+09, 3.5571e+09, 4.4583e+08, 1.2819e+09, 4.5770e+09,
        2.8700e+09, 1.8382e+09, 3.1830e+08, 2.6037e+09, 3.6790e+09, 7.8636e+08,
        1.6122e+09, 1.8304e+09, 3.7298e+08, 3.2207e+09, 3.3521e+09, 1.7881e+09,
        1.5715e+09, 4.2512e+09, 1.3366e+09, 1.2073e+09, 3.3012e+09, 5.9818e+08,
        2.3613e+09, 1.4076e+09, 3.0363e+09, 2.4720e+09, 1.1716e+09, 5.4364e+08,
        1.1354e+09, 4.4753e+08, 7.9083e+08, 6.5705e+08, 1.3212e+09, 1.0703e+09,
        7.3969e+08, 6.4746e+08, 1.9611e+08, 1.3000e+08, 1.8678e+09, 7.3507e+08,
        1.8384e+09, 2.9205e+08, 2.9999e+09, 1.1814e+09, 1.5806e+09, 2.1297e+09,
        2.0746e+08, 1.2041e+09, 3.0541e+09, 3.9654e+09, 1.7113e+09, 1.7480e+09,
        1.1573e+09, 1.4549e+09, 7.0498e+08, 5.9962e+08, 2.8692e+08, 2.9429e+08,
        2.4754e+09, 4.2023e+08, 4.0006e+08, 9.5685e+08, 2.1978e+09, 4.7305e+09,
        2.5898e+09, 2.2019e+09, 4.2953e+09, 1.0813e+09, 3.0615e+08, 1.3106e+08,
        1.5582e+09, 1.6240e+09, 3.8081e+08, 2.3252e+09])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6576852.5000, 4484106.0000, 4148861.2500,  ..., 7000342.5000,
        4341533.5000, 4537876.0000])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([16034983.0000, 17466978.0000,  8954526.0000, 20185046.0000,
         9779728.0000, 13677611.0000, 12726552.0000, 10138397.0000,
        15536695.0000, 13145756.0000,  9247293.0000, 13511160.0000,
        11173430.0000, 10402017.0000, 12657198.0000, 14677674.0000,
        17068536.0000,  9870653.0000, 14191294.0000, 14044079.0000,
        12375866.0000, 13344132.0000, 15413450.0000, 12053161.0000,
        14461882.0000, 11654406.0000, 15290078.0000, 15560062.0000,
        13996884.0000,  9470907.0000, 16960488.0000, 16119774.0000,
        12956908.0000, 10385204.0000, 11304752.0000, 12169575.0000,
        15117727.0000, 16492781.0000, 15361164.0000, 19182536.0000,
        13491005.0000, 11051763.0000, 17715330.0000, 11401442.0000,
        15156190.0000, 15803184.0000, 10403684.0000, 14311471.0000,
        12223766.0000, 16268574.0000, 13703420.0000, 16252360.0000,
        14286931.0000,  9585887.0000, 17006782.0000, 15875756.0000,
        11006932.0000, 12362345.0000, 14097531.0000, 12822163.0000,
        14657388.0000, 14597866.0000, 16788778.0000, 14718688.0000,
        11843918.0000, 15360248.0000, 14116257.0000, 12253472.0000,
         9612027.0000, 10711397.0000, 13793478.0000, 15461882.0000,
        13745394.0000, 14491338.0000, 13084292.0000, 11922440.0000,
        12611041.0000, 12558973.0000,  8251409.5000, 15895433.0000,
        12284799.0000, 10571374.0000, 15833554.0000, 10098401.0000,
        10965605.0000, 13812383.0000, 15310079.0000, 20132980.0000,
        15030361.0000, 12514733.0000, 14874084.0000, 16579041.0000,
         9630587.0000, 16161885.0000, 16681842.0000, 14233644.0000,
        14792244.0000, 12447513.0000, 10561272.0000,  9085677.0000,
        13409584.0000, 10373280.0000, 11015327.0000, 18997352.0000,
        10780228.0000, 11931023.0000, 13654949.0000,  9189343.0000,
        14181781.0000, 12918659.0000, 13475446.0000, 14582415.0000,
        12736896.0000, 16651327.0000, 23073244.0000,  9700119.0000,
        12321986.0000, 20609736.0000, 13603391.0000,  9942572.0000,
        15928700.0000, 15726529.0000, 14166188.0000, 13425070.0000,
        11466977.0000, 10927670.0000, 12833569.0000, 10757558.0000,
         9926945.0000, 12558852.0000, 12586395.0000,  8621323.0000,
        15613087.0000, 12634149.0000, 16881994.0000, 10776885.0000,
        19002924.0000, 13239182.0000, 16529834.0000, 14037869.0000,
        17444506.0000, 11921283.0000, 11350355.0000, 11701372.0000,
        11887858.0000, 12031944.0000, 14175424.0000,  9977756.0000,
        13091203.0000, 10677050.0000, 13424499.0000, 17409938.0000,
         9877431.0000, 11459860.0000, 16468564.0000, 11736231.0000,
        10667581.0000, 13513334.0000, 15252026.0000, 10688618.0000,
        15280753.0000, 20063850.0000, 13780631.0000, 11231963.0000,
        12879951.0000, 15191597.0000, 20388228.0000,  9085836.0000,
        10341576.0000, 17506116.0000, 14652919.0000, 13201158.0000,
        13273901.0000, 16515153.0000, 11263305.0000, 12061863.0000,
        15670809.0000,  9789337.0000, 13603661.0000, 16184452.0000,
        13483880.0000, 12373401.0000, 16920040.0000, 11645254.0000,
        13559797.0000, 12739156.0000,  9668785.0000,  9717434.0000,
        12131615.0000, 12974957.0000, 12912737.0000, 12615417.0000,
        14200044.0000, 18061802.0000, 12620651.0000, 11282124.0000,
        18797260.0000, 16637924.0000, 12841802.0000, 10137545.0000,
        12633707.0000, 17257564.0000, 16114136.0000, 10053126.0000,
        12017656.0000, 16735362.0000, 16647038.0000, 14538823.0000,
        21336814.0000, 12375369.0000, 12596741.0000, 21804822.0000,
        14401662.0000, 12090341.0000,  9108432.0000, 17083870.0000,
        16049677.0000, 14239127.0000, 13971164.0000, 15495866.0000,
        16604687.0000, 10785565.0000, 14901345.0000, 15767218.0000,
        13480599.0000, 10280738.0000, 15627968.0000, 17595614.0000,
        12937991.0000, 12298456.0000, 15186230.0000, 12981942.0000,
        18235782.0000, 12966354.0000, 12473576.0000, 12895194.0000,
        10738116.0000, 12587555.0000, 16849700.0000, 13297841.0000,
         9605243.0000, 10873312.0000, 12914798.0000, 17042980.0000,
        11884018.0000,  9978182.0000, 12288698.0000, 11299398.0000,
        12910069.0000, 12853382.0000, 11742000.0000, 10946547.0000,
        19127406.0000, 11425301.0000, 12131465.0000, 10076581.0000])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([43097484., 32518168., 45709232., 48172628., 38756892., 33447968.,
        25666314., 38421804., 32758270., 32486800., 31800002., 28964140.,
        35487732., 37393780., 33635548., 29996342., 46584544., 31159522.,
        39329032., 56237028., 34513180., 32331626., 36283732., 50380904.,
        31976258., 34637896., 37664072., 32260974., 41236352., 38467032.,
        39369320., 46408652., 42533500., 33910956., 28462582., 41026136.,
        51794728., 36994300., 38352160., 37448176., 30021464., 37014728.,
        38915932., 50755408., 29680188., 34156920., 42727332., 41471968.,
        40985376., 50200632., 39949172., 34468216., 29219426., 44161568.,
        33045156., 35876432., 59282080., 25672920., 35583132., 28479682.,
        41091080., 31908740., 43672736., 41508124., 36722680., 38695680.,
        29772946., 48341908., 36898352., 44089720., 40927928., 48439428.,
        32500596., 39294312., 39435432., 39934568., 33053010., 36767920.,
        47389316., 52242648., 44349728., 39135672., 32675292., 30010384.,
        42807704., 34244892., 42597328., 42586320., 50541016., 28592330.,
        38313336., 58130760., 26327820., 33500858., 35596212., 51562740.,
        42409124., 37335264., 39673280., 29188460., 35813336., 37275384.,
        31389288., 26698582., 44404344., 43979616., 38719876., 31338552.,
        34284712., 34940564., 27833752., 53935300., 30491264., 43381952.,
        39581264., 50002156., 41343492., 28177064., 29777342., 40494848.,
        38322288., 34514840., 34290156., 33048224., 44011264., 43178708.,
        35577716., 35111744., 28702216., 37571132., 29886626., 32259074.,
        43219524., 40344380., 40090740., 24906424., 32190012., 37449868.,
        34119236., 44491712., 38591584., 44426436., 34580740., 28607718.,
        31425968., 35935684., 35658104., 37251428., 51782712., 51306164.,
        45650344., 35722668., 37364332., 31587046., 46184268., 33312690.,
        30471842., 37711264., 38830640., 29863144., 48728132., 31709346.,
        33031884., 26758464., 39142512., 40679324., 37588420., 45563916.,
        46836588., 26507520., 41299232., 33260224., 33989464., 44509500.,
        36736028., 29935218., 42081584., 44117656., 42874036., 29133106.,
        31096176., 49200296., 31541124., 31267556., 40107800., 44357168.,
        28064322., 34430060., 56026596., 33953060., 35118092., 45244396.,
        58721584., 34487988., 53605864., 60021368., 49494348., 32221914.,
        33162240., 44007044., 58626784., 33786264., 47648188., 41665228.,
        48691624., 38947000., 33056516., 35324628., 35934468., 42166108.,
        46341456., 34007584., 38814220., 29224436., 53026732., 34641416.,
        31599988., 48099156., 38022700., 25343080., 33695864., 30678574.,
        50038052., 44454588., 30098444., 37811232., 41741788., 41216996.,
        33711960., 31634438., 48752616., 46517412., 55454512., 41125072.,
        31581444., 32694318., 34429388., 38358320., 46835400., 32665420.,
        33408878., 29441326., 34219960., 33365474., 40315296., 33801064.,
        32442494., 30601074., 29186192., 37928800., 36820016., 45922964.,
        36863200., 51166004., 43271636., 34902256.])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1209428.7500, 1381443.0000, 4952838.0000,  ..., 3394313.5000,
        1397687.6250, 1361260.1250])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3971064.7500, 4468497.0000, 3676265.2500, 3940946.5000, 4877441.5000,
        4522447.5000, 4962284.0000, 5094618.5000, 4772664.0000, 4286001.5000,
        3760692.0000, 3333885.0000, 4181930.0000, 5952282.0000, 4253936.5000,
        4680566.0000, 3613356.0000, 3701026.7500, 4344196.5000, 3817836.5000,
        4501802.5000, 4246140.0000, 4157496.2500, 5190362.0000, 4111049.2500,
        4504762.0000, 4729843.5000, 4074086.7500, 3581297.7500, 4564016.5000,
        6031070.0000, 4558910.0000, 4518987.0000, 3366809.7500, 4678944.5000,
        3201760.0000, 4573583.5000, 3611900.5000, 3853512.2500, 5595534.5000,
        4254706.0000, 4652733.5000, 4077691.2500, 4112029.5000, 3172284.0000,
        4962919.0000, 3448157.7500, 3485871.5000, 4206112.0000, 5081678.0000,
        4369018.5000, 5440866.0000, 3498206.7500, 3538311.0000, 5013048.0000,
        4385097.5000, 5303411.5000, 3571709.2500, 4316236.0000, 3968200.7500,
        5709151.5000, 4523501.5000, 5644565.5000, 3444648.5000, 6084855.0000,
        3725908.2500, 3445882.2500, 4764470.5000, 2606761.7500, 5085527.0000,
        4687264.0000, 4993967.0000, 5209314.0000, 3705668.5000, 5997120.5000,
        3644106.0000, 4050830.7500, 5847458.5000, 3417141.5000, 5869400.0000,
        3980118.7500, 4906476.5000, 3530744.0000, 4855831.0000, 4120405.5000,
        2897526.5000, 3567385.0000, 6137230.5000, 5033586.5000, 5321277.5000,
        5183380.5000, 4728276.0000, 4221904.5000, 4457618.0000, 3773782.7500,
        3582552.0000, 5610588.0000, 3450714.0000, 3887617.5000, 4064712.0000,
        4902105.5000, 3516400.7500, 2756477.2500, 4107942.2500, 4242045.0000,
        3022283.2500, 5225498.0000, 4307456.5000, 3970201.5000, 3593228.2500,
        3252602.7500, 3726390.2500, 5517416.5000, 3279538.7500, 4600018.0000,
        4739872.5000, 5132569.0000, 4899987.5000, 4955883.5000, 3888285.7500,
        4642009.0000, 3991861.7500, 4207356.5000, 4838128.5000, 5123875.0000,
        5396079.5000, 3948676.0000, 3607908.5000, 4498898.5000, 5266343.0000,
        4213636.0000, 3528545.2500, 4265262.5000, 3431015.5000, 3431515.7500,
        4598482.0000, 3640344.7500, 4822781.0000, 4355051.0000, 4857417.0000,
        4021120.7500, 5390102.5000, 3618568.0000, 3179225.0000, 3807188.2500,
        3891174.2500, 4266960.5000, 3739810.0000, 4023668.7500, 5643302.0000,
        3765207.5000, 4378982.0000, 3458029.2500, 3888157.2500, 3867762.5000,
        5848378.0000, 3364339.2500, 3635421.0000, 3453473.2500, 3362144.5000,
        5704477.0000, 4607823.0000, 3650948.0000, 2520414.7500, 3441583.5000,
        6835705.0000, 3510500.7500, 3210810.5000, 4057869.5000, 3371792.0000,
        3272890.7500, 3206162.7500, 4455048.0000, 4216246.0000, 3985424.2500,
        5570997.5000, 3775086.0000, 2882427.0000, 3832209.7500, 4039889.5000,
        3708720.2500, 4359528.5000, 5477215.0000, 5398447.0000, 5265635.5000,
        6034566.5000, 3543876.5000, 3307330.2500, 3850627.7500, 5150853.5000,
        4009608.0000, 3577115.0000, 4267275.5000, 5786770.5000, 3607305.7500,
        4546738.5000, 4843032.5000, 4498635.0000, 4474961.0000, 3837243.7500,
        4882427.5000, 6230549.5000, 4747198.0000, 4303058.5000, 4846588.5000,
        3319490.0000, 4122082.5000, 4649022.0000, 3529509.2500, 5729097.5000,
        4292119.0000, 4714960.5000, 4803493.0000, 5229099.0000, 4154451.7500,
        5338475.5000, 3397444.7500, 3225982.7500, 4581270.5000, 4302931.0000,
        5591036.5000, 4023640.7500, 3756557.2500, 3777130.7500, 3723978.2500,
        4281230.0000, 5300099.5000, 3279253.0000, 5050521.5000, 4437344.0000,
        2623742.5000, 3188526.2500, 4760866.5000, 5447783.0000, 2812902.7500,
        4015063.7500, 5040710.5000, 3792291.7500, 4076508.7500, 4403602.5000,
        4515993.5000, 4332424.0000, 6724047.0000, 3810956.0000, 5488517.0000,
        3939308.2500, 4547008.5000, 6384878.5000, 4081959.7500, 6246894.5000,
        3273745.2500, 3827639.5000, 4888879.0000, 4058867.5000, 4155508.7500,
        5296749.0000])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([10384628.0000, 13950858.0000, 13333130.0000, 15803794.0000,
        10191700.0000, 13189224.0000, 17095904.0000, 13892946.0000,
        14350061.0000,  9877559.0000, 10244448.0000, 14235225.0000,
         7917004.0000, 11159336.0000,  8463193.0000, 11223273.0000,
        17421362.0000, 10055262.0000, 10349312.0000, 13724027.0000,
        12236170.0000,  8117153.5000, 16033557.0000, 13512655.0000,
        12046718.0000,  8975969.0000, 16805468.0000, 13641424.0000,
        10096193.0000, 12328220.0000,  9244685.0000, 10627621.0000,
        16336675.0000, 11311012.0000, 14724199.0000, 15253160.0000,
         9592411.0000, 12583128.0000, 12246521.0000, 12320104.0000,
        10905229.0000,  9661470.0000,  9542931.0000, 10345108.0000,
        14299214.0000, 14757575.0000, 10627689.0000, 10543660.0000,
         8868158.0000, 11729881.0000, 15278023.0000, 14306917.0000,
        14584041.0000, 16778894.0000, 16209965.0000, 11389805.0000,
        14394116.0000, 13911823.0000,  9475587.0000, 14011152.0000,
        11163605.0000, 11635596.0000, 13402760.0000, 12004292.0000,
        10422283.0000,  9621683.0000, 13654422.0000, 10675917.0000,
        15529079.0000, 14314840.0000, 10962501.0000, 10403392.0000,
        11601868.0000, 13334440.0000, 12690966.0000, 10990079.0000,
        16081028.0000, 12164005.0000, 12980181.0000, 14342251.0000,
        10200883.0000, 14979808.0000, 11882480.0000, 12174685.0000,
        14001602.0000,  9674670.0000, 15940091.0000, 18781376.0000,
        11892981.0000,  9383199.0000, 11908602.0000, 10724849.0000,
        15711982.0000, 12775835.0000, 16106164.0000,  9469479.0000,
        14326072.0000, 11496406.0000, 16204261.0000, 10173150.0000,
        10032593.0000, 12514332.0000, 11079364.0000, 12361027.0000,
        10062480.0000,  9649341.0000, 14377362.0000, 12710515.0000,
        15670145.0000, 11799980.0000, 13534230.0000,  9429743.0000,
        13876621.0000, 14288215.0000, 11537241.0000, 13180520.0000,
        11351577.0000, 11089935.0000, 13195386.0000, 12760881.0000,
        12933412.0000, 10310806.0000, 11661104.0000, 12985647.0000,
        12990158.0000, 15107404.0000, 20467310.0000, 13169778.0000,
        13628400.0000, 13452023.0000, 10129608.0000,  8818999.0000,
        18623602.0000, 14460474.0000, 13372310.0000, 10646827.0000,
        13490127.0000, 13597144.0000, 10045164.0000, 14398667.0000,
        13692948.0000, 11310677.0000, 11968500.0000, 10235546.0000,
        11242630.0000, 13836010.0000, 10277214.0000, 17233068.0000,
        11763683.0000, 10478709.0000,  9589669.0000, 10214503.0000,
        13642758.0000, 12292394.0000, 12895533.0000, 13233902.0000,
        13353284.0000, 14477313.0000,  9043986.0000, 16327320.0000,
        11802600.0000, 14587898.0000, 14116680.0000, 10898913.0000,
        14974615.0000, 13864890.0000, 15715467.0000, 11383974.0000,
        13702064.0000,  9236946.0000, 11181813.0000, 15678118.0000,
        13177989.0000, 12216984.0000, 16106510.0000, 11430039.0000,
        12933130.0000, 12759274.0000, 11471806.0000, 11840283.0000,
         9365838.0000,  9190781.0000, 10398311.0000, 10859581.0000,
        13424901.0000, 11055075.0000, 11545704.0000, 15950583.0000,
        15576649.0000, 14913642.0000, 17140326.0000,  9916505.0000,
        14646266.0000, 13914433.0000, 13422191.0000, 15705424.0000,
        15136594.0000,  9901729.0000, 12218194.0000, 10950108.0000,
        13343834.0000,  9646286.0000,  9194894.0000, 13620777.0000,
        15238528.0000, 12737259.0000, 11125509.0000, 12459798.0000,
        10288843.0000, 11497538.0000, 12307613.0000, 13306403.0000,
        10574108.0000, 13349044.0000, 12855272.0000,  8606782.0000,
        11423737.0000, 13249576.0000, 14355378.0000, 14071487.0000,
        14491252.0000, 11328369.0000,  8874083.0000, 13934141.0000,
        12160727.0000, 13589491.0000, 12149332.0000, 14930968.0000,
        13589871.0000, 13633677.0000, 14872609.0000, 14758946.0000,
        13019967.0000, 15804031.0000, 13426083.0000, 10274653.0000,
        15560305.0000,  9869239.0000, 11482879.0000, 10039226.0000,
        13186163.0000, 12216402.0000, 11207439.0000, 16079979.0000,
        13852296.0000, 14600708.0000, 12162676.0000, 13432835.0000,
        19888066.0000, 15675658.0000, 15442942.0000, 11234613.0000,
        15091604.0000, 19659828.0000, 12911475.0000, 12280010.0000])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1773430.6250,  188454.0156,  274021.3438,  ...,  114462.2266,
         185877.0781,  264852.9688])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([678277.5625, 632371.1250, 638760.8750, 649987.6875, 744903.8125,
        806921.9375, 437209.7500, 685021.3125, 607453.7500, 441861.3125,
        561522.2500, 542851.0000, 716490.5000, 657024.6875, 546201.8125,
        618621.9375, 437582.7500, 432087.4688, 672427.1250, 716100.3750,
        568582.8125, 723239.6875, 703324.3750, 572868.9375, 640316.1875,
        462919.1562, 638403.7500, 555439.2500, 634862.8125, 848906.8125,
        622572.5000, 483947.0000, 795076.8750, 677109.5625, 458424.7188,
        612599.8125, 497816.2500, 603920.0625, 445335.6875, 586140.0000,
        516224.3438, 584429.4375, 483982.5625, 588645.6875, 629560.1250,
        680741.5625, 585264.1875, 643206.3750, 453565.6562, 544948.8125,
        603218.6875, 602697.6875, 845974.5625, 653176.2500, 603825.3750,
        586098.3750, 730670.1875, 461473.0000, 471521.2500, 493712.1875,
        817101.4375, 704040.6875, 637851.0000, 746621.0000, 611561.9375,
        595536.4375, 705793.1875, 712759.1250, 585553.3125, 629605.3125,
        698424.6875, 556966.7500, 715350.5000, 674072.3750, 699312.3750,
        546873.8750, 712350.5625, 542729.7500, 712407.8750, 625303.6250,
        726660.0625, 560005.7500, 615880.9375, 483107.5625, 667684.5000,
        507071.1562, 669788.6250, 471543.9688, 589793.2500, 653362.4375,
        451589.2188, 674611.2500, 730634.8750, 685678.0000, 600030.5625,
        621950.0625, 613333.2500, 521962.9688, 644050.1875, 510063.9375,
        482472.3438, 520795.7188, 709837.6250, 626334.6250, 580262.4375,
        608414.0000, 566534.2500, 568905.6250, 519568.5625, 766018.3125,
        526494.6250, 522467.2188, 536328.0000, 518527.1250, 688072.0000,
        544536.5625, 606844.3125, 501516.2812, 424104.8438, 695026.8125,
        830279.4375, 572729.5625, 647833.5625, 694126.5000, 554009.2500,
        570564.7500, 427201.3125, 676825.6875, 585269.5625, 640817.3125,
        687495.1875, 681824.2500, 745215.0625, 616085.9375, 719096.8750,
        528009.2500, 475098.1250, 529598.9375, 732961.0625, 702943.7500,
        606952.1875, 664456.7500, 581070.8125, 749830.5000, 709803.6250,
        548839.3125, 536119.8125, 697825.0625, 716646.8125, 539145.8750,
        555781.7500, 686529.3125, 841988.9375, 567051.8125, 646836.6875,
        685318.4375, 527472.3750, 430525.6562, 671262.4375, 711157.1875,
        566694.0625, 559949.8125, 510394.8125, 530682.7500, 621325.9375,
        512494.4688, 513592.6875, 638217.6875, 685292.1250, 456244.6562,
        595127.0000, 525858.1875, 620159.9375, 638200.0000, 649985.4375,
        525427.0000, 628625.1250, 648708.7500, 623428.8125, 564148.7500,
        485405.0625, 594150.3750, 717141.8125, 678323.7500, 690292.6250,
        678385.4375, 717539.1875, 636843.8750, 579408.5000, 659313.8125,
        591997.0000, 747625.5000, 564180.5000, 587140.8750, 629122.9375,
        591448.1875, 544664.9375, 643624.8125, 708432.8750, 601961.4375,
        576695.3125, 575253.0000, 699015.6250, 653925.4375, 636261.6250,
        620961.4375, 570823.6250, 636616.3125, 710538.6875, 476677.3125,
        558753.5625, 556356.8750, 653178.9375, 604735.3750, 685594.0000,
        486857.8438, 539265.8125, 610522.1250, 550267.6250, 688076.2500,
        456151.0938, 643877.6250, 801524.6875, 660476.5000, 808399.6875,
        666723.3125, 607493.6250, 536697.8125, 651546.8750, 536228.1875,
        587150.8750, 653043.0625, 568202.6875, 614966.0000, 449807.0000,
        615536.8750, 549872.9375, 501087.2500, 481977.8125, 622708.9375,
        601055.7500, 571084.3750, 559163.7500, 777917.2500, 659626.2500,
        565952.4375, 535235.2500, 623945.5000, 471929.5000, 659170.1875,
        516912.4688, 506598.7500, 754259.8750, 657355.8125, 539076.5625,
        769702.1250])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1724959.1250, 2154120.5000, 1812856.6250, 2157347.5000, 2078927.6250,
        1690896.5000, 1565810.6250, 1909294.1250, 1910942.6250, 2400947.2500,
        2123220.2500, 2253922.2500, 2177828.2500, 2114336.5000, 1887848.3750,
        2157702.0000, 2467214.7500, 2144216.0000, 1872148.6250, 2256657.2500,
        2079467.1250, 2030529.7500, 2113560.0000, 1867925.6250, 1983018.7500,
        1731732.3750, 2095912.3750, 2236351.5000, 1782474.1250, 1915556.2500,
        1765452.2500, 2205814.0000, 2287637.2500, 1865555.7500, 1697205.7500,
        1819083.1250, 1892735.8750, 1932732.1250, 1525804.7500, 1722959.2500,
        1953353.8750, 1967510.8750, 2277216.5000, 1926679.2500, 2338754.2500,
        2310649.5000, 2147620.0000, 2022852.2500, 2012100.5000, 2228592.2500,
        1875627.2500, 2328702.7500, 2066729.1250, 1954988.2500, 1792386.6250,
        1822110.6250, 2023777.0000, 2211199.5000, 1777397.2500, 2156881.7500,
        1890028.6250, 1824838.2500, 1698025.7500, 1875794.5000, 1812782.8750,
        2280137.5000, 1648534.3750, 2167870.0000, 2676481.5000, 1891136.5000,
        2135185.0000, 2169591.0000, 1670396.5000, 2465544.7500, 1666111.1250,
        2529834.0000, 1920796.0000, 1828109.0000, 2313707.2500, 2205996.0000,
        2805885.0000, 1851161.5000, 2116262.0000, 1770374.7500, 1533810.3750,
        2070074.2500, 1615375.2500, 2249060.7500, 2149689.5000, 1728095.5000,
        2057075.1250, 1696659.3750, 1832054.5000, 2002667.8750, 1489763.8750,
        1934227.7500, 2576150.5000, 2060827.1250, 1637236.2500, 1968458.7500,
        1896786.6250, 2092755.5000, 1778727.7500, 1734690.1250, 2439842.2500,
        1571050.0000, 1775902.7500, 1946461.3750, 1844720.8750, 2545907.2500,
        2101624.5000, 1611822.5000, 1450881.6250, 2094225.3750, 1954089.7500,
        1908191.3750, 1519513.5000, 2340293.5000, 2254580.5000, 1742860.2500,
        1819869.1250, 2171092.2500, 1879431.6250, 1806325.8750, 1704749.8750,
        1892894.3750, 1763482.3750, 1849955.1250, 2143037.7500, 2278327.7500,
        1796546.8750, 1471529.5000, 1561792.1250, 1728197.7500, 1509039.0000,
        2036446.0000, 2097340.0000, 1863962.1250, 2066410.5000, 2450372.2500,
        1829888.1250, 2660029.2500, 2062761.0000, 1629662.1250, 1929364.3750,
        1801495.0000, 2141515.2500, 2418375.5000, 1693721.0000, 1828545.2500,
        1691972.5000, 2586082.7500, 1925564.3750, 1997459.1250, 2345314.0000,
        2079325.3750, 2174061.2500, 1801081.3750, 2164295.5000, 1861916.0000,
        1795924.2500, 2051496.1250, 2228724.7500, 2338543.0000, 2341405.5000,
        2059332.0000, 1981059.5000, 1863487.2500, 1869137.6250, 1933849.3750,
        1997471.5000, 2190860.5000, 1733682.6250, 2042423.5000, 2032226.6250,
        2198551.7500, 2092019.6250, 2191547.5000, 2334906.5000, 2311188.0000,
        2674322.7500, 1912975.1250, 2067588.2500, 1623235.0000, 2207087.7500,
        2289676.7500, 1677193.1250, 1849809.8750, 1651349.8750, 1637253.7500,
        2308704.2500, 2187616.2500, 2004966.8750, 2092686.7500, 3141229.5000,
        1917233.3750, 1789922.1250, 2421317.5000, 2188132.5000, 1779249.5000,
        2108380.5000, 1755933.1250, 2133260.7500, 2376096.2500, 2225073.0000,
        1944690.7500, 2010750.0000, 2207980.0000, 1977879.1250, 1786930.3750,
        1756020.8750, 1857836.8750, 1989413.7500, 2265903.5000, 2081900.1250,
        2021482.5000, 1742683.1250, 2055327.3750, 1828450.0000, 1851200.3750,
        2517524.7500, 2452698.2500, 1606698.1250, 1737861.3750, 1972093.6250,
        2382618.2500, 1908250.7500, 1480238.1250, 2051182.3750, 2095225.5000,
        2094198.2500, 2285775.7500, 2138614.5000, 2274299.0000, 1474161.8750,
        2139893.2500, 2265250.0000, 2182317.5000, 1663641.0000, 2029277.3750,
        2136082.0000, 1743522.6250, 2326053.0000, 1742231.8750, 2224848.0000,
        1965228.3750, 1718707.6250, 1835519.6250, 2050049.6250, 1720164.0000,
        2400897.2500, 1705400.2500, 1819666.1250, 2008096.3750, 1951592.1250,
        1991103.8750])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([599017.0625, 112349.6719,  88660.8750,  ...,  14260.8154,
         33764.6562, 212406.7031])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([127749.4375, 164336.6719, 126991.4766, 170542.8594, 151272.1562,
        172194.3594, 155869.0312, 160201.6406, 119490.3750, 110774.9141,
        151416.8438, 155306.1250, 165569.0000, 168515.9375, 205580.0781,
        187604.2500, 101061.4688,  90461.0547, 172456.3906, 102777.7266,
        188631.8906,  96355.6641, 185628.9219, 156411.8281, 160075.9531,
        146178.2656, 156993.4844, 174975.4688, 194274.0781, 134218.2031,
        133042.0625, 180177.0938, 103259.1094, 190090.6875, 149969.6250,
        169887.1562, 187982.4375, 165907.6719, 131100.5625, 149545.4219,
        152851.8594, 136907.6875, 135920.0625,  86916.6328, 164121.3438,
        161590.0938, 177236.3438, 145334.3281, 192864.7031, 163298.3750,
        197838.0000, 173343.7656, 169780.2500, 137541.2031, 161588.0312,
        181548.9844, 181239.2031, 153893.0469, 123159.4922, 160776.0469,
        164142.2656, 129270.2188, 158771.3281, 155139.2500, 170378.9844,
        128691.9219, 156985.3906, 136969.6562, 140543.2656, 128017.9609,
        161043.4688, 176889.9062, 133198.7812, 126947.4453, 142850.0781,
        189991.8906, 197180.9688, 136959.7344, 155342.1406, 172268.8281,
        172213.5781, 128618.1250, 154554.6406, 126818.5781, 138880.7031,
        154968.7500, 158877.0781, 170575.0000, 155060.8281, 124341.2500,
        170039.3281, 143008.0000, 139448.3438, 208438.3594, 136722.8906,
        136233.1250, 106770.8594, 141940.6719, 124877.1250, 185697.1719,
         94883.3594, 142744.7812, 116134.0156, 116572.1328, 159311.6562,
        251583.8438, 229469.5469, 123315.9453, 139116.1875, 176230.8125,
        120187.5000, 238495.8125, 164217.4844, 110615.2891, 150640.6719,
        156523.5625, 142381.5000, 116112.5156, 189776.2812, 124316.6562,
        149972.3906, 183489.3125, 139694.7344, 103427.8047, 158563.2031,
        175249.8438, 188727.1094, 170525.5000, 123917.9375, 132708.2969,
         99373.3203, 147593.7500, 123227.9609, 189273.4844, 143556.2500,
        200451.6875, 114695.1484, 137331.4375, 141150.1719, 156038.0938,
        118296.2109, 243798.4219, 167210.6875, 145923.0469, 148708.8125,
        202544.3906, 179335.0312, 135865.9844, 146840.5000, 126858.2578,
        138342.1562, 155346.2656, 130894.0234, 157298.9688, 213349.9531,
        146473.7344, 144486.9844, 176297.7188, 123144.2578, 140419.4062,
        162997.7500, 215429.5000, 150827.5938, 160450.3594, 102616.7891,
        132846.1094, 137056.1406, 128593.1016, 195303.9688, 146607.7500,
        150598.5781, 109663.3828, 176170.0938, 295239.4375, 174314.1406,
        216416.5938, 123867.8594, 127721.8438, 200220.7031, 149376.9375,
        155688.5312, 165786.6406, 156966.5469, 133149.4844, 174307.7812,
        160428.7344, 136101.1719, 111979.6719, 152196.5156, 125495.6250,
        159872.4375, 182667.4844, 186103.2969, 198835.8906, 153063.9844,
        149361.9844, 165642.8438, 120547.8203, 152635.6094, 188482.0938,
        103814.2578, 190287.1562, 150043.5938, 136919.1875, 165731.9531,
        105857.9297, 123924.6172, 159166.7812, 146012.2344, 128846.5859,
        174424.5312, 135297.2656, 176190.6250, 190342.0000, 172093.4531,
        156396.3906, 127909.0234, 175823.8438, 170266.0625, 126102.2578,
        157418.1875, 144761.3906, 165989.1406, 162799.0156, 123984.9453,
        108108.7891, 135633.3594, 178462.8125, 162097.6875, 170752.8438,
        181396.8594, 124411.4766, 166558.9531, 156834.2812, 139288.5781,
        123885.0312, 158840.4688, 144722.2500, 112015.9375, 135427.8281,
        170315.3125, 154694.6875, 131630.0781, 150578.8750, 150618.6094,
        217008.2656, 144180.9219, 141110.2031, 135755.2500, 157458.0938,
        164454.6719, 217434.2656, 155021.8438, 154063.2344, 152432.2656,
        171414.7656])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([749708.3750, 790442.0625, 620372.5625, 594553.7500, 674432.0000,
        764868.6875, 876941.7500, 438640.6562, 491704.2500, 438095.7812,
        540858.5000, 392073.0000, 585023.7500, 582763.4375, 601679.5625,
        681541.8750, 596667.0625, 547488.4375, 556849.3750, 542599.7500,
        441998.0625, 613939.0000, 681294.3750, 824880.8750, 621768.1875,
        458895.8750, 606008.0000, 687744.1875, 634388.5000, 502180.7500,
        786274.9375, 399004.0312, 494544.2812, 644777.3125, 585001.2500,
        608246.3750, 481299.7188, 544420.3750, 565070.9375, 694269.2500,
        639362.7500, 556622.0625, 623299.3750, 758970.7500, 724636.8750,
        591496.3125, 543391.7500, 575739.8125, 668113.6875, 334629.0000,
        661044.1250, 518288.6250, 618042.6250, 588755.6250, 671621.8125,
        611356.5000, 662689.1875, 439219.3125, 538074.1250, 471727.5312,
        572825.3125, 632458.0000, 558902.4375, 549156.4375, 531424.7500,
        601455.3125, 643325.1250, 491818.7812, 591414.0000, 509209.6250,
        607311.6875, 485329.2812, 760098.2500, 630677.8125, 571293.6250,
        498585.7812, 610812.9375, 436098.2500, 744239.6250, 525581.8750,
        607650.7500, 533765.1875, 689389.9375, 657305.8125, 505226.6875,
        550842.1250, 483068.5312, 356129.7500, 582901.3125, 438361.0312,
        708981.1250, 691055.4375, 527223.8125, 733337.5000, 760557.3125,
        663347.0625, 469918.2500, 621873.6250, 575424.8125, 656436.6875,
        511988.3125, 665403.3125, 695486.3750, 781874.5000, 644973.3125,
        688926.1875, 592390.1250, 541352.7500, 599109.2500, 621864.6250,
        592606.4375, 473769.1250, 585138.0000, 513880.2188, 866854.4375,
        820670.9375, 415388.4062, 420375.2812, 660666.0625, 554049.0625,
        770466.9375, 653748.6250, 583318.8750, 719030.5000, 412015.4688,
        546339.9375, 669566.7500, 582726.3125, 538941.1875, 517172.7188,
        521557.4375, 728805.1875, 650634.2500, 752518.3125, 718683.5000,
        511120.3125, 479438.0000, 548525.3125, 633866.8750, 639664.6875,
        626166.0625, 768820.0625, 611949.6875, 664917.8750, 563607.6875,
        629615.2500, 553654.5000, 604873.8125, 771578.6875, 519040.0938,
        493751.4688, 600686.1250, 511096.8125, 544219.0625, 921946.1875,
        495491.7812, 621126.3750, 654010.6250, 579777.9375, 505139.7812,
        666741.5000, 548747.1875, 589373.1250, 592843.7500, 689343.9375,
        641358.7500, 988638.5000, 912118.6875, 682064.3750, 460309.7500,
        628354.3125, 818467.6250, 531210.5000, 717274.8750, 676372.6250,
        559878.5625, 463994.9062, 604149.0625, 588365.6250, 633934.3125,
        486434.1250, 623025.8750, 499698.5312, 456730.2188, 580970.3750,
        545950.9375, 594555.8125, 583283.6875, 494242.3750, 516207.1250,
        726033.4375, 781917.0625, 580319.0625, 509865.7188, 542085.1875,
        601176.0625, 606581.6875, 773637.4375, 581835.9375, 551350.8750,
        646411.1875, 518177.2812, 540595.1250, 781444.1875, 438094.3750,
        619209.3125, 597624.7500, 507933.0000, 611958.5625, 716457.1875,
        400556.9062, 683610.6250, 453439.5625, 420470.9375, 580964.0625,
        511244.4688, 489511.5938, 586080.7500, 639285.6875, 603081.3125,
        531960.9375, 610026.3750, 604644.5625, 633984.3125, 573372.6250,
        533391.8750, 699106.8125, 581806.6875, 743584.9375, 772617.7500,
        698537.3125, 404105.6250, 707499.8750, 671512.3125, 646847.6875,
        405592.5000, 611755.0000, 588972.1875, 518215.8438, 767526.2500,
        751222.0000, 583112.9375, 578919.1250, 498694.1250, 520101.5000,
        550799.5625, 626423.9375, 490721.5625, 555856.6250, 566920.0000,
        524793.9375, 900176.5000, 583336.1875, 655350.8125, 607252.7500,
        511235.3750])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0183e+06, 9.6288e+04, 9.0334e+03,  ..., 9.0190e+02, 2.3890e+03,
        1.0350e+04])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 9150.8438, 15218.6240,  9198.0879, 13229.0684,  7001.5410,  8680.9355,
        12502.3594, 12720.2061, 11391.0049, 16153.2861, 11466.7725, 10384.6514,
        11243.1074, 14734.3945, 11181.8672, 13162.9229, 11440.3252, 13150.4014,
         9279.4131, 12145.1602, 11447.4258, 12255.7559, 10344.9053, 14480.7344,
        13081.3555, 11008.2236, 12328.8750, 13646.4004, 12328.9277, 13514.6113,
        10593.7129, 13250.5371,  9762.6367, 11098.0762, 15406.0947, 16727.3223,
        11355.8301,  8952.9385, 10156.9492, 12332.7500,  7600.3447, 10459.6230,
        17134.2461, 11626.6025, 15225.3066, 13922.0645, 13654.8896, 23176.3086,
        20305.3281, 15492.9580, 18179.1621, 10828.8740, 10713.0107, 10289.6924,
        10742.8838,  8116.6816, 20502.5234, 13822.4346, 15580.2549, 10272.2061,
        11455.9873, 14903.6572, 14003.8223, 11347.9189, 12988.8994, 14031.9346,
        13912.1768, 13427.4014, 10023.7783, 13383.1387, 14766.5000, 12987.0098,
        13079.3408, 13964.1943, 10616.7041, 13270.1484, 10725.9199,  8655.7402,
        15360.1562, 11999.8164, 13526.8545, 14439.9834, 14866.9424, 16143.5449,
         9915.9775, 14471.1416, 12637.2520, 10284.6572, 14573.4961, 13387.8232,
        13496.3506,  9799.4561, 16140.5811, 12255.6885, 10804.2393, 12369.3428,
        13526.2178, 14452.7168, 14287.0908,  8281.7100, 13793.3750, 17979.0938,
        11955.1660, 12128.3271,  9947.1914, 23679.8184,  9866.2305, 17762.3535,
        14349.7178, 14251.7832,  7471.7007,  9872.4922, 14186.0811, 13854.5596,
        15995.0537, 11365.2930, 13121.1904,  9274.6914, 14736.0518, 20259.2012,
        14867.0996,  9380.5615, 10105.2637,  9687.2207, 15906.0264,  8639.2285,
        11341.3555, 15411.5107, 14866.4395, 19023.3145, 16114.8086,  9224.1885,
         7301.9536, 13149.6309,  8292.9453, 12930.3223, 11870.2363, 16673.8672,
        22986.1211,  7752.5205, 17559.2461, 10370.4600, 13764.3564,  9593.5332,
        19507.4199, 10689.7080, 12937.3418, 14013.7080, 10290.3789, 13988.7275,
        16382.9180, 15392.3008, 11769.0215, 12818.9258,  9446.3721, 14582.8701,
        12599.9473,  7754.9824, 12569.1807,  9357.1426, 13102.7520, 10910.4785,
        12521.0107, 10177.0508, 15184.8838, 11036.2715, 15675.7549, 13481.6455,
         9657.2803, 23042.9238, 16473.5840, 16284.1826, 14055.9775, 19631.3184,
        14846.2656,  9043.0352, 10536.8271, 11576.8018, 16941.7188, 12663.6113,
        14479.6523, 15665.0059, 11702.7275,  9823.9170, 12231.0410, 16893.4824,
        10029.0732, 16122.5400, 11090.8672, 14066.7539, 11096.6943, 10950.8203,
         8784.3115,  9323.5957,  8204.6562, 13835.3096, 10020.4551,  9139.0273,
         9124.8027, 13472.0137, 10552.1895,  9850.1084, 15030.2422, 15704.8721,
        13013.0410, 16120.6895, 15591.6807, 11766.6191,  9050.8154, 13249.5312,
         7695.6514, 10135.9668, 10484.6230, 10635.0898, 10640.5947, 13341.6963,
        14241.9414, 15381.4541, 14555.8916, 15153.7783, 14600.4570, 13018.7578,
         9990.2080,  8452.3633, 14253.1064, 11974.8760, 10833.3486, 17152.8125,
        10598.8926, 16985.0977, 10589.2529, 13607.5762, 11239.0859,  9752.1025,
        17916.5254,  8155.2056, 10466.6201,  9763.8975, 11316.6680, 11263.7998,
        17497.6816, 13125.7500, 11851.1094, 15718.2871, 11146.3564, 16377.4727,
        18193.3379, 14807.1748, 11657.9248, 17874.4707, 11766.8818, 10326.3691,
         8852.2461, 11633.0146, 11606.4209, 15055.7510, 12268.9746, 14726.7646,
         9335.0928, 13773.7529, 14013.7998, 10427.3750, 11891.4834, 22129.3965,
        12885.8623,  9286.0469, 13764.9678, 12391.6494, 12667.9326, 12481.6299,
        10696.1035, 12031.6973, 11248.0205, 11080.4307, 10836.1309,  8700.5098,
        12463.4131, 16156.2422, 11383.6240,  9571.2129,  9783.3350,  6757.4229,
        15724.7842, 13471.7490, 14535.8643, 12288.4629, 10087.9053, 11726.9268,
        13396.6719, 10118.1855, 12349.6455, 12101.6299, 18232.6309, 15537.3779,
        10126.6865, 15286.5752, 10478.0127, 11450.8281, 11857.3252, 12164.9512,
        15706.0557,  7523.2256, 10132.7051, 11689.8662, 14439.3389, 14384.6211,
         6645.3350, 25108.0762, 12287.5635, 16853.2285,  9358.0547, 12728.1338,
        12606.1133, 10234.7334, 18253.0488,  9704.1777, 17663.4629, 12559.9180,
         9214.1582,  9039.3027, 18442.7305, 15252.9541, 17424.7773,  8436.4512,
        12114.4404, 15913.4697,  7263.3745, 10685.3027, 12789.0586, 16920.1680,
        15540.2002,  8651.5996, 10985.5400, 10552.4580, 18402.9023,  8560.1729,
        10061.0781, 14584.5342, 11637.1660,  9775.3799, 17835.6816, 12958.7090,
        17531.1172, 13689.3809, 13515.0410, 15581.4590, 15919.0127, 19290.7656,
        15070.3516, 10998.3574, 16580.2578,  8478.5693, 10602.6094, 14619.2754,
        15698.0400, 12672.7168, 16596.3105, 10601.8486,  9614.1934, 15722.2598,
        14839.0488, 15249.8350, 12920.6289, 10778.7256, 13072.8223, 10977.0410,
        10354.6318, 17111.4590, 12191.9590,  9717.3926, 14819.3301, 14618.8232,
        13639.2246, 16746.9238, 13159.5068, 11230.9453, 15709.1465, 12056.3877,
        14688.3604, 11408.1680, 16635.4453, 14529.1953, 14073.3262, 12145.6055,
        14537.4277, 13968.7041, 21463.8496, 13052.9980,  9433.5967,  8840.1768,
        12494.6992, 13774.0771,  9350.4561,  9757.0312, 16258.3916, 10589.3496,
        10185.4629,  9272.6191, 11361.0039, 12414.7725, 11004.3516, 13110.5469,
        11101.9512,  9382.5312, 10432.7910, 14393.8379, 11754.7617, 13172.6016,
        15384.4756, 12961.6348, 11016.0205, 14777.8701, 15905.8535,  6983.9790,
        15500.7705,  9835.3936, 17591.4707, 10635.7812, 16073.9697, 13087.0137,
        17011.9629, 16650.4746, 19330.1582, 12970.7373, 18257.7617, 14753.6201,
        12570.7422, 13856.3936, 15552.2822, 11311.4170, 11839.8223, 11212.1406,
        18078.3027, 11073.6992, 10383.8408, 12508.8457, 15736.1504, 10997.5273,
        15371.7852,  9827.3379, 11197.8496, 19547.9043, 11736.2979, 10616.4590,
        11000.3848, 16303.9033, 13922.6436, 13205.7559, 18784.7617, 10120.5957,
        12555.6572,  9527.1338, 13321.4258, 14439.8350, 12552.2480, 18406.2090,
        13506.7295, 14103.0342, 17231.8281,  9653.9385, 10208.0273, 14179.2100,
        11633.5752,  7840.8711, 10430.2432, 13179.7158,  7562.8281, 14414.3105,
        16724.7324, 12184.9326, 11655.8184, 10992.2139, 10514.2031, 18780.3633,
        15332.0010, 16993.4570, 15476.5547, 10393.3760, 13914.6660, 15549.1152,
        10521.2295, 14409.1748, 13740.8281, 12678.2295,  9872.1514, 11411.9590,
        17043.9238, 13664.4922, 10837.8174, 12873.7363, 10490.8770, 13687.9971,
        14377.8223, 13960.1387,  7705.2812, 14822.2773,  9349.5488, 17428.5820,
        11054.8555, 13506.6904, 16397.5918, 20126.1113,  9062.3027, 13152.4111,
        11762.2549, 10394.0635, 16556.5859, 13951.9697, 12596.4219, 19002.9180,
        12433.5029, 10409.5801])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([121551.2812,  96714.3203,  81713.2500,  65935.3359,  76275.3906,
         87096.7344,  68474.7891,  75338.4531, 104842.6172, 122747.6250,
        105417.0000,  84695.5234,  85668.9141,  94991.0547,  83713.0312,
         74674.9453, 119345.5078, 106133.2734,  83692.8203, 105759.3594,
         76667.5938,  74954.9375, 141094.9531,  62733.3906,  86613.8359,
        101169.7969, 111390.8047, 120255.2812,  59494.8867,  47009.0391,
         79127.8281,  98201.8906,  90402.9609,  97359.4062,  87418.8906,
        116999.3125,  53158.1914,  67777.7109, 102068.9531, 143238.0625,
         85147.6641, 123558.8281,  97762.0547, 125250.1328,  96872.1797,
        104319.6094,  94168.2891, 109239.3828,  78999.3594,  79780.9453,
         93417.9766, 120446.4844,  90027.4922,  84230.6875,  99328.0859,
        108043.2578,  77188.9141, 125330.5703,  73633.1719, 118457.3672,
         64123.0664,  94096.8125,  98428.6094,  66055.5781,  85531.2578,
         53208.7461, 143108.8594, 134414.5312, 104471.0625,  73103.8516,
         95837.1406,  95519.3672,  98010.2656, 125780.4844,  99985.5781,
         87333.5000,  95480.4375, 104499.2578, 106603.8516,  97980.7969,
        109224.7109,  75298.6250,  94226.4688, 128678.1562,  95396.0625,
         73072.6484,  74749.9922,  93285.6484,  73636.9531, 103347.4609,
         90814.9297,  97165.5859, 124680.5547, 105565.9609,  71765.7188,
         59188.8828, 113856.3125,  91179.7734, 112473.0078, 111176.4141,
         74061.1797,  68361.0156, 103085.9297, 131684.2188,  75842.5391,
        100414.0703,  51380.9023,  75575.4297,  71349.4375, 115502.6484,
         75304.2734,  79980.6328,  94210.6719, 107256.6328,  76078.0625,
         82971.6484,  81849.1875,  59999.3320, 109102.3047,  79297.1875,
        122900.3594, 116743.3125,  82646.1328, 108578.5938,  62409.6562,
         84718.8125,  70441.0547, 155454.8125,  94797.7109,  87281.8125,
         94930.8672, 109188.1719, 128633.0938,  80790.4375,  86063.7812,
         90271.3516,  56183.1719,  96708.0625,  95276.5781, 104238.2422,
         84952.2656,  83212.0625,  67661.7344,  99564.1719,  91450.9766,
         96561.0000, 108116.4922,  79136.4141, 128277.7031,  67587.4453,
        145029.4688, 113204.8828,  90674.4062,  88559.4844,  92145.0938,
         94977.5312,  85898.9922, 119426.4453,  72715.4453,  67282.6875,
        113494.4219, 101603.8594, 112230.9844,  95393.1953,  74082.5469,
        102871.6797, 106932.9766,  95563.9219,  71053.8047, 112464.3750,
        105244.4922,  80532.9922, 108904.7188,  75105.1875,  79637.2031,
        119917.2578,  59895.4141,  79952.5859, 121903.5547,  77057.1172,
         93555.6016,  99404.4297, 115952.9062,  67186.4844,  74054.8125,
         93391.8203,  84852.5859, 154232.2812,  60991.2266, 126089.2812,
         59965.1875,  63784.2266,  86347.5859, 111873.1562,  93139.1094,
        108001.2969,  85087.5938,  97023.9297,  98269.6406,  96258.3594,
         97582.6250,  79774.8828,  73330.1797, 112660.1875,  87305.5156,
         64163.6953, 149734.9219, 136339.0938,  81369.3516,  83378.7891,
         96044.1406,  78450.8125, 114452.4609,  84336.2422, 100488.5234,
         83115.5312, 103325.1406, 111556.2578, 113914.2891, 120993.9609,
         75460.8203,  93806.3516, 128025.7891, 125640.1797,  74741.7578,
        136530.6406,  86530.0781,  59568.5977,  97084.4844, 128371.7656,
         83203.8359,  78833.1484,  76059.6094,  77345.5625, 106607.0078,
         90545.8672,  97935.9531,  86569.4609, 136849.4375, 101466.4922,
        115677.0156, 101606.2109, 106514.7344, 111006.1562,  73351.6406,
         68661.2969,  80271.7969,  87372.0781, 106617.7031,  75560.7422,
         82831.3438,  90203.3125,  81222.9062, 125430.1875, 114826.8594,
        113340.6172,  96508.0703,  65725.4453, 122624.7344,  88996.8828,
         69411.8047,  96994.2266,  73987.1172, 120384.7109,  80276.7891,
         85623.1953,  80450.3594, 101306.9688,  81100.0156,  83828.3594,
         92255.1953,  93595.0234, 119352.3281,  78639.8438,  76816.8828,
        109051.3281,  86179.7422,  84846.3438,  71807.6172,  70507.8125,
         87515.6641,  67368.3594,  93369.8984,  76814.9141, 131546.1094,
        110771.1250, 100156.3359, 105944.3203,  57325.9570, 113366.3359,
         72090.6875,  84207.8516, 114631.7969,  64675.5469,  93853.8750,
         77916.9062,  67769.6406,  70938.5625, 106644.3359,  98673.2656,
         89977.1094,  55711.8945,  81917.5000,  97887.0781, 102290.5156,
         75481.1094, 116751.9609,  80754.9297,  87058.9531,  83950.4297,
         91260.0156, 107949.2188, 150479.9062, 118145.8438, 123872.1484,
         87610.2344,  90230.0156, 110740.9141,  86969.9766,  86774.2500,
         89132.8594, 102813.2344, 100930.7891,  57013.4570,  61069.2539,
         91528.1719, 123696.8203,  97503.9219, 108459.3906,  87746.4844,
        129253.6172, 127792.9453,  63676.7734,  84599.3047,  87590.6406,
        106362.8203,  91875.7266,  84779.5078, 128574.6250, 146898.9375,
        159120.8438,  82741.5547,  63158.8789,  99033.4609,  91050.7266,
        105507.1875, 118460.5312,  85958.9688,  69748.5703, 116092.0312,
        101146.5234,  70102.5625,  96892.7422, 100640.7031,  95548.6484,
         88550.1797,  75917.7891, 105934.7812,  83048.8281,  85493.8438,
         69152.6250,  79394.6641,  72332.2188,  66733.3438,  72216.0703,
         76552.2266,  52251.0273,  69058.3281,  79504.1641,  68276.5078,
        138592.2031,  61643.9336,  72714.0547,  79824.7969, 112023.9531,
        111658.5312, 110308.0000, 106453.2109,  71610.3281, 102078.6016,
         89688.6406, 155355.9062,  68100.4062,  75622.0391,  76352.0391,
         88376.6875, 101404.0156,  62580.3516,  98483.9531,  92411.3438,
        134871.5625, 108033.1016,  67592.7422,  85561.7031,  78285.0625,
         70529.3828, 123006.0469,  89108.6562,  84168.0938, 138099.1719,
         89163.0703,  82604.1641,  81078.3047, 114014.6875,  78514.7969,
        104867.8594,  87242.7031,  71010.8125,  71552.6094,  63764.7148,
        127322.6172,  96273.3281, 112083.4453,  82934.1484,  87524.1797,
        105351.4844,  85004.8203,  85667.2812, 135293.3438,  98028.8594,
        132268.0156,  63162.6055,  85508.1484,  96148.0156,  79978.8672,
        127524.5391, 104438.7188,  71377.8047,  75841.8281,  65247.6562,
         79615.1953,  97026.5000,  97394.6172, 119070.6797,  88877.8125,
         70280.2266,  87832.4062, 113252.8438,  92616.9141,  72111.1562,
         68485.5000,  69027.6953, 112961.7734, 136189.5156, 102238.6953,
         89890.5156,  80871.3672,  91852.6875,  84488.5234,  65405.1758,
         92371.7109,  98997.2578,  95342.3672,  68867.2969,  67322.7891,
         65788.3516,  61145.9141,  92010.2031,  99442.6406, 101015.4609,
         79215.2812,  85681.8594, 138492.1250,  86183.7031,  60259.4141,
        113249.3359,  87593.0312, 106926.3203,  71349.1953, 119687.3125,
         99949.3281,  84468.1484,  86218.9844,  57187.6992,  85345.1719,
         97552.4453,  98846.7812, 122323.0781, 114098.8516, 106558.0859,
         92605.7109,  78530.9922,  82811.6641, 138491.0312,  85438.2109,
         99893.1328, 110605.1953,  84051.8906, 116132.0547, 106259.8281,
         61965.7500,  62104.6523, 107790.6953, 116008.4375, 113900.5000,
         78512.7188, 142708.7500,  92352.9141,  50164.8281,  71225.7656,
         99167.1875, 118730.2891,  95283.7109,  86095.9766,  83779.3594,
         73683.0625,  81930.3438,  77207.3516, 109723.8594,  94143.0000,
         76453.0781,  78764.1172])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2643.1755,  227.5357, 4500.2861,  ...,  180.9651,  668.9054,
         536.1365])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2387.0881, 1008.4975, 3920.9746,  ...,  832.0339, 1337.5762,
        1244.3019])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 5636.5503,  4238.7749,  3602.3176,  4106.3027,  5557.5786,  4196.8286,
         3908.3074,  1580.0211,  4736.1064,  2710.4526,  1560.3495,  1742.9128,
         3372.6848,  4758.9990,  2051.2693,  2073.8901,  8817.0303,  1688.6205,
         2650.6018,  2382.2468,  5044.4736,  1773.3074,  8493.4561,  2024.1598,
         4274.7261,  4697.1304,  4721.6187,  2253.2031,  2847.8389,  3470.2058,
         6415.4868,  2444.0879,  5260.1875,  1981.2445,  2858.6621,  4661.0835,
         2792.5293,  2467.8040,  3008.0081,  4266.4717,  4167.7378,  1865.7822,
         1652.2537,  2349.0498,  3856.6274,  3936.4263,  2067.8064,  3697.8174,
         2702.0527,  1994.8722,  4872.9561,  2686.0322,  2140.3550,  4647.6836,
         2151.8889,  3730.0281,  2216.8428,  1636.7418,  7817.5220,  3180.9614,
        11906.8662,  2092.3901,  4872.1812,  2404.1895,  3833.8333,  3001.7043,
         7519.8291,  7926.5015,  2255.8535,  3818.1870,  2973.6770,  2965.8748,
         6924.4463,  4540.3818,  4088.5750,  3492.1160,  6265.7026,  6502.8882,
         3220.6284,  3227.3713,  3107.7368,  2115.3872,  3545.2024,  6433.1089,
         2811.5793,  2240.0000,  6644.3179,  1831.5011,  4587.4824,  2340.6421,
         6610.0820,  5123.0698,  3941.6482,  2901.7903,  2403.5649,  2987.3901,
         5979.2246,  2096.0232,  4736.4888,  4979.9570,  5106.4292,  1874.7662,
         3589.9238,  4605.2998,  4125.6948,  2334.5303,  2332.3237,  6933.2051,
         5702.0688,  2219.8296,  3688.6980,  2582.5999,  3188.5591,  3136.0520,
         2784.7705,  5954.5547,  3112.0352,  5727.2720,  2502.1528,  3553.7371,
         3356.0178,  2720.6831,  3983.5281,  4150.0381,  5203.5752,  3981.8794,
         4792.4751,  1891.6798,  5025.6494,  1847.5530,  2227.2461,  6215.0986,
         4493.0586,  3992.4258,  3013.5693,  2662.9194,  4923.7910,  5765.7012,
         1613.8489,  1681.2737,  8226.4932,  1958.7522,  6317.5938,  1860.4271,
         5598.5366,  2488.2063,  5857.7397,  5929.7910,  1711.5636,  3071.3228,
         4476.7974,  4560.9839,  3965.1345,  2407.2227,  3898.6721,  3090.0146,
         2539.2021,  1577.6576,  1637.8231,  4785.4941,  6365.8955, 10879.5938,
         8239.0996,  1769.0859,  3249.0867,  2475.1619,  3554.5479,  4975.4688,
         3277.2817,  5649.2178,  4976.8022,  6400.0776,  9402.5625,  5603.0522,
         2102.8882,  2095.4553,  1953.2385,  6101.1958,  1691.6101,  1957.7531,
         1909.8706, 11224.7275,  4119.5156,  5032.7520,  4515.7456,  2355.6011,
         3490.4009,  2929.7881,  4127.8745,  2205.9280,  2910.2046,  4635.2793,
         2247.8870,  3067.6389,  1644.4956,  8991.9912,  6115.0879,  2284.9009,
         2975.8552,  8487.6758,  2655.7573,  5410.4868,  5042.7734,  6338.7524,
         2817.2900,  8729.1377,  8407.5420,  6184.8511,  5298.8765,  3078.4812,
         3664.8159,  3180.2781,  2359.9180,  4554.2671,  4077.8706,  2390.6965,
         3630.3171,  6066.9604,  7813.8276,  1715.7477,  1678.2961,  2412.0715,
         6859.4731,  3440.6013,  5513.1440,  1673.6681,  5636.1362,  3777.5396,
         9576.5410,  2574.7151,  4849.4155,  3105.8020,  4696.9043,  2165.0037,
         3075.2510,  3067.5161,  6124.8047,  6063.4448,  4079.0371,  1801.6010,
         4072.6758,  2259.0342,  8488.7432,  7327.3311,  2146.8511,  4267.6309,
         3827.0054,  2010.0753,  3048.1821,  2737.8013,  6688.2920,  4246.5972,
         7434.7729,  3943.3728,  4897.5215,  4493.3877,  2695.0337,  3082.1379,
         6714.8716,  1952.8448,  4385.5806,  4623.4653,  3322.6943,  4113.6392,
         6621.5649,  2201.8813,  8817.8301,  5000.5469,  2181.4109,  1638.3285,
         2901.4014,  3495.8635,  4661.2437,  7090.1787,  6455.2593,  1555.3684,
         2997.1838,  3796.5059,  7089.0845,  2518.4802,  7260.6670,  7937.0093,
         2219.0713,  3896.5530,  8055.3057,  2939.0278,  3195.0442,  7046.3652,
         3978.4216,  2582.3911,  2993.3738,  4736.9263,  5428.1162,  5055.8154,
         2017.2577,  2721.2375,  1398.0188,  5387.7002,  3594.5481,  6860.5376,
         3506.8213,  4835.5640,  2718.0305,  5182.4497,  7519.8896,  9184.2061,
         4647.9458,  3889.3870,  6957.8159,  7649.7178,  6260.4150,  3379.9661,
         5406.7153,  2192.0825,  3075.0583,  7636.6299,  1860.4019,  6906.1021,
         9805.9092,  5226.7974,  6583.3340,  5398.1118,  4293.7896,  1879.7367,
         6106.0112,  5501.7349,  1938.2727,  3535.3308,  2178.0332,  2900.7905,
         1910.7220,  3922.2485,  7361.5518,  3937.1277,  1813.0524,  3576.5334,
         5581.5088,  3235.6799,  2767.5674,  3813.6531,  4214.1680,  6050.4775,
         2904.8086,  7445.7310,  2366.1165,  5689.2529,  3908.7769,  6242.1738,
         4523.7407,  2503.6106,  3144.6677,  8181.0801,  2283.6641, 10126.5166,
         2958.4045,  7271.8804,  3931.1755,  2718.9915,  4017.8640,  3341.6506,
         3399.6560,  2111.3220,  7202.9629,  1658.0393,  5478.8062,  2749.6631,
         6952.8892,  3275.1367,  4836.3794,  2339.6328,  3391.6875,  7983.4551,
         1992.2980,  5351.8721,  4208.6650,  3426.5955,  8568.2578,  1563.0728,
         5565.1426,  3915.9138,  3875.6094,  4710.3428,  7510.5513,  3245.6343,
         3921.8354,  2386.7468,  2636.0227,  5876.1040,  5123.4492,  1946.8906,
         2122.0439,  3221.1301,  3805.2454,  3403.8623,  3064.3369,  5015.5679,
         3148.6248,  3938.5779,  3941.2456,  6729.9756,  3783.8352,  7816.6089,
         2263.4382,  1672.3000,  2136.6287,  2518.9622,  1447.9270,  3681.4421,
         1612.0524,  7731.7632,  2093.8479,  1880.2148,  5662.8071,  4324.0381,
         4726.3589,  7830.9307,  1869.8328,  5740.9951,  6919.4585,  7456.4746,
         2553.1599,  4923.8262,  4191.1147,  4032.3701,  6018.9995,  2040.0817,
         4641.0288,  3245.9817,  2624.7229,  5338.9976,  2604.9250,  5210.2383,
         7212.7642,  3786.2502,  2741.5396,  4377.8687,  4864.1807,  2647.9783,
         1598.4061,  4101.7671,  3009.8828,  2643.0945,  5034.9238,  5953.0957,
         6120.3311,  2383.5249,  7185.0542,  5842.8716,  2392.7139,  6089.1040,
         9350.9482,  1739.9183,  1705.2183,  6541.2969,  2366.5864,  8346.0586,
         2643.0300,  1851.0438,  6360.8223,  5772.1460,  5327.6387,  6165.7949,
         6482.1084,  5262.6172,  5549.0054,  4945.7583,  7197.1694,  4387.9629,
         2862.9744,  3396.1829,  1883.9109,  4586.9878,  3401.5645,  9810.7832,
         3629.1670,  4098.5137,  3679.4944,  1992.0582,  4750.0806,  2037.0291,
         2508.3792,  3455.3010,  2105.1628,  4542.3481,  4419.0371,  3930.2659,
         3975.0210,  3008.1350,  3918.7542,  4148.0366,  3097.5369,  1751.8494,
         3383.7551,  4149.2485,  2400.9995,  3304.9773,  5769.7446,  2440.3606,
         5513.1333,  3115.4519,  5986.1489,  5058.2314,  4977.8149,  6264.3359,
         2157.3362,  2107.6023,  4376.5718,  2850.5994,  5825.4731,  5748.7354,
         6432.0625,  2323.0583])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([39332.7500, 21413.4473, 16820.6289, 19164.7734, 24274.3164, 30159.1445,
        49060.2891, 29287.4316, 28144.9668, 12862.6299, 21372.6777, 19115.9258,
        25240.2207, 84337.2812, 26962.8926, 32083.9590, 42583.4375, 41668.7695,
        43505.1445, 14943.4932, 38535.4141, 31673.3457, 34976.1367, 35579.7031,
        24965.5469, 46226.2656, 14576.9102, 36368.8203, 31761.6445, 11877.6543,
        26021.0605, 14932.3184, 18761.5215, 38309.8477, 15835.9902, 43874.1641,
        40852.8359, 54442.8867, 20408.6641, 28335.1777, 11145.4531, 15591.5537,
        27729.2090, 53394.7188, 23987.3359, 30163.6426, 39974.5781, 52971.6289,
        30236.9883, 16735.6270, 25241.9590, 17604.5781, 33660.5859, 26010.8789,
        31018.4453, 42373.9844, 14319.3281, 16951.1992, 27154.8145, 19833.9590,
        17584.5762, 35420.7930, 39923.0859, 23625.3301, 16749.4355, 76760.0859,
        19398.6074, 20742.6562, 25979.0801, 13254.9980, 16876.1367, 21374.2598,
        37305.8242, 43723.9844, 38881.8047, 12706.3574, 18875.2988, 16041.1387,
        22326.6230, 44811.3594, 13908.9580, 16945.1406, 17562.7207, 44433.4531,
        18897.4219, 29282.4238, 28709.1797, 21162.6523, 18808.0234, 54193.6055,
        16349.2295, 33362.7812, 21412.0430, 33322.1797, 30036.9336, 27465.5195,
        27448.1172, 19029.6992, 17739.8125, 27551.7012, 11789.5479, 23831.6875,
        58615.4062, 16801.5996, 28732.5586, 37213.6328, 49873.8633, 23764.8730,
        39483.8008, 25991.6055, 33122.8164, 28062.7891, 11437.5654, 38332.0977,
        12482.8809, 18517.4512, 29755.7812, 19961.5625, 29287.2793, 12364.0059,
        15323.9873, 44999.7031, 36900.4141, 22119.0742, 18159.9258, 51248.9531,
        51789.2695, 26472.1973, 38779.0195, 30993.1797, 48940.7969, 19700.2676,
        54136.1992, 52086.2734, 61664.6836, 41650.3750, 48170.3438, 22633.5293,
        14636.3438, 22436.6445, 43713.4727, 21205.7930, 46311.9766, 35793.5820,
        50039.1445, 43995.4688, 16407.2363, 18709.7480, 30574.1250, 29484.6699,
        37106.1055, 43452.5625, 46674.7266, 25667.1895, 28050.0215, 43669.3828,
        15272.9688, 29436.8867, 38464.9336, 16558.2051, 32597.6172, 20273.5352,
        53192.4805, 14822.9658, 16132.4258, 17295.9531, 35675.2617, 20511.3242,
        14660.4502, 14595.4170, 39187.1953, 39541.0469, 23201.0684, 17467.4902,
        22922.7852, 67069.6250, 20120.4844, 36559.8398, 23589.9844, 34510.9141,
        34833.2695, 36082.7969, 14830.8184, 37355.7930, 38458.2852, 19784.0508,
        43782.6797, 44765.2500, 18555.3145, 21838.5625, 12563.4229, 13078.0762,
        40599.2695, 11386.1885, 35885.5859, 24233.2520, 19880.3242, 22962.8945,
        37523.0742, 15177.9326, 20367.9844, 45660.3008, 18703.7090, 21068.7402,
        25844.9902, 17408.7285, 13745.1309, 32553.2695, 33563.8945, 19006.6426,
        28476.9395, 25749.8027, 22674.8301, 36842.3594, 22986.5625, 30488.7988,
        73134.3125, 26865.5273, 49696.4258, 20881.1543, 24088.1895, 16653.8770,
        17993.8301, 55211.2031, 15993.7793, 40322.7852, 63423.1406, 52426.8047,
        17555.1582, 17158.5156, 17458.3926, 20772.2188, 41681.1719, 15989.5156,
        45335.5508, 34471.6562, 35309.9297, 18140.0723, 12615.2656, 19914.2188,
        16789.0684, 21019.0566, 31291.8809, 22102.7383, 13565.3115, 35964.0156,
        18793.9961, 38002.7617, 28772.7520, 15589.7920, 35633.3203, 24308.7734,
        13610.6719, 57026.6367, 13208.7607, 32529.1758, 41700.2930, 18191.8105,
        33984.5703, 10474.4355, 26523.4375, 41081.6641, 25184.2559, 32251.8906,
        31790.5078, 29843.8516, 20777.1309, 10460.9346, 54343.1562, 18685.5312,
        37562.3516, 17134.6602, 48731.0938, 15468.1934, 22553.9473, 20316.6348,
        47304.2422, 19225.5664, 28094.4883, 13363.6973, 42042.7500, 24890.5098,
        16127.0117, 13859.2178, 18720.9844, 42167.3477, 16387.3301, 14079.0098,
        13574.9033, 38930.2812, 24312.4570, 23994.5156, 49601.2500, 27439.2402,
        34003.7969, 38055.5742, 15481.0400, 40020.8945, 21155.7227, 18378.5176,
        20962.0332, 24430.2520, 15843.3545, 26888.5508, 29524.2793, 29684.6680,
        32378.8184, 25854.4375, 45531.3945, 40583.5039, 14292.1123, 61260.3086,
        11713.6299, 17928.8496, 38699.1484, 23786.2832, 28037.1094, 16235.1396,
        21976.3672, 53537.1367, 37710.4766, 49568.0273, 47847.4609, 26622.2012,
        57326.3008, 39016.5078, 22552.0625, 24859.7168, 24980.9375, 21902.7773,
        29745.2637, 27105.2227, 16024.0537, 15325.5547, 29736.7969, 59406.1523,
        24584.3867, 20946.9121, 34022.3789, 20549.2988, 93955.2578, 58908.9648,
        55508.7891, 11804.5400, 22499.4727, 33780.4336, 16637.7012, 68113.7656,
        16569.1855, 37864.4102, 38229.4062, 39769.4492, 73104.1094, 16453.3887,
        24642.2344, 44930.3164, 30190.0605, 18341.7773, 17422.5781, 21352.6602,
        47914.0117, 24611.4922, 40431.7891, 51657.7852, 44457.7031, 25033.9395,
        13520.6709, 13955.7852, 48160.5781, 22913.3555, 61716.2109, 14183.8711,
        20024.4355, 35113.7305, 54069.0859, 25994.0020, 39225.1367, 10561.8730,
        14346.3906, 41588.8359, 15334.4912, 18376.8262, 27548.8945, 15378.7080,
        48300.3242, 19077.5801, 11786.7344, 27178.8867, 25456.3105, 12652.0322,
        22144.3633, 53709.9180, 14632.5508, 22087.5996, 19066.1465, 19705.0996,
        28143.0312, 15990.8379, 48663.9336, 15531.2783, 40252.1367, 51569.9336,
        18789.3184, 73962.6719, 17028.3242, 18515.8359, 17906.9980, 29776.3398,
        35851.0664, 14296.8955, 15807.3994, 62715.1250, 25015.5000, 21800.0254,
        20526.6348, 27930.5684, 19289.1133, 20711.8848, 25915.3242, 47643.9141,
        21920.7090, 16862.4785, 22975.3438, 27166.4961, 34829.3828, 30988.2520,
        43751.5781, 27298.5215, 50109.5586, 18621.8262, 15456.1250, 50672.2539,
        43839.2969, 19183.6113, 32290.0859, 25910.8809, 23347.7344, 32104.5742,
        42600.9844, 18403.9258, 12193.7275, 33952.3672, 12520.8516, 23151.3711,
        50193.7617, 17190.3457, 36426.1484, 32065.1055, 30487.1406, 21543.8945,
        11316.0918, 20889.2559, 34044.3438, 13337.5703, 19000.6992, 32375.7676,
        19545.3418, 24192.8730, 18014.3262, 30607.1836, 10254.2803, 36973.6367,
        21546.2598, 22972.3301, 20615.2520, 27431.5195, 22460.9434, 27644.0938,
        28456.9531, 27643.6055, 25794.2012, 11781.8418, 67436.3281, 20234.7539,
        27799.9102, 27063.0645, 35062.3477, 20228.4805, 40691.1484, 13183.3057,
        21858.8809, 44373.2891, 12770.3105, 30217.6992, 15533.6436, 25098.7363,
        36980.8984, 14883.9062, 52452.1953, 48159.9453, 16821.7363, 15000.1494,
        29150.7266, 33066.5977, 36803.4531, 30912.6465, 25379.6738, 33047.4609,
        23674.8555, 45434.0391, 14079.6152, 26368.5410, 29304.8848, 17423.2344,
        30345.8027, 22671.3594, 34940.2344, 32783.4180, 21930.5000, 30288.0957,
        26996.4473, 32486.6699])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([673.7173, 330.7315, 555.3526,  ..., 307.8333, 428.1603,  71.3900])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 908.9851, 1052.9326, 1137.4417, 1168.7910, 1249.3496, 1074.3599,
        1067.6840, 1069.4735, 1165.2957, 1243.1046, 1240.7616, 1020.3049,
        1029.7075, 1028.4705, 1178.0962, 1275.3163,  996.3589, 1081.0974,
        1316.4407, 1315.4647,  885.1060, 1434.1099, 1381.6210, 1643.9188,
        1121.5206, 1055.5017,  974.8655, 1095.1807, 1080.5449, 1281.2683,
        1045.8352,  969.4578, 1049.6537, 1145.7970, 1212.2424,  934.4459,
         937.6204, 1168.2743, 1077.4398, 1052.1229, 1020.9143,  860.0512,
        1460.3142, 1048.3595, 1358.3960,  939.4860,  996.2874,  980.3635,
        1139.2852,  938.1548, 1072.8966,  924.1608, 1129.5580,  835.5394,
        1176.9166,  891.6208, 1134.7677,  821.2092, 1018.0504, 1174.7546,
        1083.0834, 1077.0841, 1015.8795, 1131.7958,  894.2665,  852.9818,
         923.8540, 1131.4125,  908.9639,  817.8337, 1046.8521, 1107.1854,
        1350.8521, 1182.8474, 1233.7158,  825.4125, 1198.6967, 1195.6278,
        1052.4106, 1277.7810, 1024.5947, 1041.3444,  944.4081, 1106.8914,
        1093.7311,  987.2054, 1201.9264, 1328.0490, 1010.3322,  997.5281,
        1030.7684, 1198.5907,  944.0950, 1187.4901, 1114.4207, 1051.9215,
        1066.2738,  923.2122,  888.6346,  935.8571, 1240.9025,  990.7483,
        1104.1398, 1123.0286,  960.6469, 1083.4734,  910.1321, 1351.8135,
         797.9078, 1127.6995,  996.7054, 1030.9343, 1236.6495,  986.6932,
        1109.3965, 1429.2866, 1078.5074,  953.7487, 1561.4496, 1278.2568,
        1046.4492, 1355.7799, 1320.0770,  985.6058,  942.4161, 1178.9561,
        1220.8925, 1221.7516, 1116.2976, 1009.0317, 1288.2882,  971.6391,
        1121.4258, 1295.4935,  971.7899,  910.3958, 1374.9751, 1088.5726,
         923.5143, 1014.4186, 1065.4583,  929.4498, 1037.0037, 1032.6731,
         981.7714,  897.1117,  978.7474, 1216.2911, 1266.5278, 1011.6924,
        1060.2416,  903.7209, 1541.6053,  873.1550, 1108.9919,  927.0458,
        1359.7830, 1028.7168, 1178.5151, 1165.6255, 1229.5690, 1009.8839,
        1159.4318,  988.2964, 1115.3518,  993.1502, 1088.1798, 1419.2273,
        1322.5995, 1036.7970, 1059.5084, 1122.5037,  912.7463,  966.8884,
         797.9628, 1059.7209,  908.2231, 1295.4955,  980.5077, 1008.2115,
        1243.9139, 1134.0575, 1302.7003,  979.2341, 1185.3158, 1471.5542,
        1327.1075, 1216.2343,  928.7143, 1347.7938, 1062.0089,  920.5989,
        1043.0380, 1373.5114, 1206.6866,  914.1388, 1076.2878, 1284.4213,
         862.0456,  945.1613, 1017.5190, 1089.5751, 1161.6105,  997.3276,
         915.9681, 1178.1539, 1117.7198, 1051.9485, 1270.1606,  959.9137,
        1168.2157,  957.7725, 1205.3217, 1155.5690, 1202.6198, 1141.9890,
        1091.9695, 1151.3217,  996.9715, 1033.6085,  939.2624, 1103.9471,
        1156.2532,  908.8936, 1063.1267, 1188.9517, 1094.1716, 1281.0601,
        1036.1891,  903.9162, 1021.7873, 1385.1836, 1000.7253, 1205.4323,
        1060.3752,  935.2252, 1333.1216, 1293.0934, 1170.7952, 1266.1014,
        1058.4303, 1016.0644, 1362.4700, 1195.8655,  970.6550,  981.1481,
        1080.9558, 1304.3009, 1108.8722,  869.1024, 1214.9492, 1147.1561,
        1058.5759, 1251.9913,  971.9529, 1158.9137,  972.9745, 1048.2935,
        1264.5245, 1217.6328, 1078.5142, 1191.5985,  932.6845, 1043.0505,
         976.1418, 1028.4966,  859.7867, 1249.4532,  974.1320,  873.0170,
        1079.4442, 1139.9216,  962.6364, 1041.7305, 1454.0935,  796.9671,
        1221.2693, 1278.1556,  913.3233, 1264.5476, 1038.7268,  821.9335,
        1258.5609, 1063.4922, 1103.8512, 1002.6389,  843.0253, 1238.8253,
        1194.6986, 1158.0376, 1319.1047, 1145.3607, 1027.6199, 1127.4730,
        1126.2811, 1090.4244,  955.4818, 1521.7876, 1039.0787,  882.5187,
        1320.9250, 1181.9993, 1174.7623, 1160.1781, 1290.2255,  857.0258,
         928.9918,  857.5051, 1176.6934,  926.4363,  956.2319, 1129.6216,
         909.2811, 1092.7406, 1184.4242, 1130.3094, 1337.8643, 1120.7062,
        1056.5814,  986.8859, 1108.3774, 1310.1027, 1120.7299, 1127.1631,
        1135.1703,  926.0480, 1130.5535, 1239.1975, 1229.1278,  907.9861,
        1098.5509, 1087.7889, 1131.2402, 1009.8328, 1193.1707, 1005.3529,
        1257.1346, 1117.6594,  995.6501, 1002.5248, 1310.7576, 1099.9990,
        1020.0032, 1157.9390,  946.2336,  959.3562, 1064.3628, 1237.3262,
        1324.7208, 1083.3888, 1049.1615,  909.6954, 1232.9316, 1429.6925,
        1277.5150, 1111.6943, 1329.8901, 1112.7433, 1173.1801,  832.7074,
         896.1268, 1013.6624, 1117.5308, 1090.6685, 1137.1160, 1175.1047,
         922.2974, 1015.7467, 1020.7711, 1170.8131, 1496.7585, 1182.3112,
        1138.8925, 1073.4604,  978.3097, 1142.3528, 1060.1356, 1149.4731,
         954.3833,  842.4988, 1066.4982, 1183.9055,  901.0702, 1032.3599,
        1071.6194,  914.6154, 1013.5911, 1040.7545, 1150.8757, 1140.5216,
        1015.6245, 1228.8312,  943.4739, 1084.7600,  846.6843, 1054.6147,
        1026.3138, 1006.7280, 1070.1393, 1092.9215, 1199.4233, 1126.5934,
         913.5775, 1184.3049, 1101.3103,  913.6838, 1148.7036, 1107.8142,
        1142.9026,  938.3917, 1213.5172, 1112.1818, 1312.0530, 1036.5078,
        1154.7021, 1201.2712, 1271.8055, 1347.6204, 1151.7701, 1246.2349,
        1148.0302, 1269.2209, 1576.4780,  851.9376, 1309.7461, 1014.0394,
         972.9189, 1292.3452, 1086.5315, 1269.7926,  998.7484, 1106.8536,
        1283.7135,  971.8422, 1086.6499, 1075.0929, 1230.3881, 1170.8159,
        1132.0586, 1134.8328, 1026.9181, 1200.2761,  756.5006, 1140.1648,
        1065.0612, 1266.9827, 1021.4365,  978.7738, 1061.2494, 1147.5758,
        1096.9810, 1145.8601, 1263.3035, 1057.6881,  963.7272, 1135.7123,
         963.4730, 1130.9871, 1180.7163, 1169.5665, 1184.4734, 1307.3961,
        1008.3345,  939.7734, 1024.2158,  965.8685, 1162.0087, 1079.9702,
        1153.5060,  907.7572, 1298.9363,  957.5343,  831.0596,  942.4391,
        1043.7086, 1157.4459, 1010.8845, 1193.8887, 1150.7479, 1118.6826,
        1171.6864, 1185.2327, 1159.4927,  962.6597, 1075.1299, 1297.3069,
        1287.0920, 1052.2905, 1181.6036, 1030.4131, 1124.6188,  848.4858,
        1091.0729, 1289.9686,  970.7209, 1088.2662,  909.2570, 1041.9601,
         923.5124, 1197.2502, 1207.5752, 1068.8209, 1068.5085, 1002.0996,
        1158.2416, 1150.0956,  880.9117,  994.8238, 1180.0548,  982.5902,
         799.1051, 1023.6804])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 8128.7363,  8490.5381,  8587.9473,  8881.3203,  6770.2354, 10531.9014,
         8724.7529,  8447.0752,  8711.1357,  8640.6055,  8549.1426, 10570.6074,
        10530.3467,  9737.4482,  9120.4941,  9330.5293,  9414.3604, 12121.7373,
         8361.3008,  8028.9604,  8949.2139,  9816.2061,  7872.4409, 10916.8457,
         9383.5400,  9257.8115, 11589.3604,  9659.0527,  8883.5967,  9144.1621,
        10250.2246,  8704.1094,  9679.3438,  8412.9775,  7936.8794,  8405.4072,
         8632.3330,  8296.3916,  8906.4912, 10327.2227,  8786.8643, 11031.5713,
        10474.2266,  9950.4775,  9950.6338,  8923.1387,  8895.1992,  8631.5918,
         9046.0977,  7862.2568, 10273.1797,  8923.8486,  9570.3711, 10054.8760,
        10039.4863,  8677.0146,  9481.5312,  9742.0732,  9605.2500, 10467.2363,
        10150.0576,  9555.1904, 10222.9648,  8350.8711,  7418.4707,  9761.1562,
         8667.4307,  7789.3291,  7724.5728,  7477.6973,  9060.0303,  8977.0977,
         9752.3926, 10570.9307,  8867.1357,  8829.4023,  9325.4414,  9634.8037,
        11556.6699,  9356.3750,  8248.8574,  8344.2861,  8490.7383, 10071.7637,
         7450.9590,  9863.5986,  8285.0127,  9505.2852,  7981.2144, 10251.1113,
         9150.8018,  8721.2734,  7603.8169,  7080.5415, 10903.5195,  9844.3008,
         9663.5332,  8955.7314, 10080.2881,  9477.5264,  9484.1240,  8474.5869,
         8033.0610,  8993.3711,  9241.0938,  7651.9810, 10033.7119,  8760.6982,
         9032.4980,  7426.6929,  7790.8354,  8805.2266, 10403.3613,  9938.7646,
        11190.4346, 10665.1074,  8143.4771,  8042.8730, 11098.1416,  9766.6514,
         8854.8857, 10849.2217,  8616.8926, 10523.4893,  9667.7383,  8796.8936,
         9485.3945, 10933.5332, 10757.5107,  9759.9346,  9696.9082,  8615.1309,
         9980.3682,  8711.8350, 10364.5117,  8880.4805,  8303.6377,  9498.4072,
         9960.1006,  9344.1826, 10224.9170,  9329.8008,  9744.1504,  9274.0371,
        11091.8887,  9354.0420, 10000.7637, 12988.3457,  8360.5869, 10488.6719,
         8125.9888,  9192.0469, 10031.5596,  8017.1904,  8112.6768,  8107.3745,
        10530.1602,  9309.3105,  6804.5103,  9236.6602,  8344.6299, 10500.2119,
         8681.3643,  7813.6025,  9941.2842,  7741.4551,  9833.1973,  9067.5439,
        11417.1680,  7365.7466,  7657.6343,  8302.2461,  9584.8418,  8526.6182,
         9234.1904,  9110.7725,  6729.0562,  8525.6895,  9895.3691,  7808.1948,
        10091.6748,  8486.0439,  9930.4902, 10795.2207, 10030.9932,  9019.0918,
         8658.6426,  8606.0918,  9650.5361,  8150.2832,  8348.0674, 10011.4346,
         9566.7979,  9043.7002,  9117.1484, 11909.7314,  7938.6929,  9185.6582,
         8212.9521,  9019.4014,  8519.0859, 12525.2568,  9110.6562,  8609.7012,
         9991.1553, 10516.9717,  8802.0322, 10222.1504,  7983.2490,  8529.8428,
         7640.4263,  9668.8037,  8215.2246,  9519.1416, 13146.6006, 10114.9170,
        11158.1260,  9952.8936, 10532.4570, 10549.1113,  8505.2012,  8783.1592,
         8063.1016, 10089.5820,  8355.1846,  9273.6475, 11310.6924,  8293.3232,
        10857.3838,  8928.5381,  9399.5273, 10637.7793,  7404.2290,  8966.1357,
         7198.0801,  8490.0469,  9509.1982, 10904.7500,  8944.5039,  9599.9590,
         8623.7998,  8240.4873,  7541.1128,  9049.9785,  9618.9492, 10661.3574,
         9901.7568,  8039.8848,  8355.9932,  8196.7969,  9191.7080,  9800.8975,
         9390.5312,  8581.1748, 10493.4121,  8697.8047,  8106.4624, 10398.5947,
        12875.3496,  7727.4727,  8275.8252,  8738.3193,  6614.7065, 11685.8359,
        10271.2314,  8493.9346,  9697.0098, 10037.3223,  8821.1035,  8724.0908,
         9192.3896,  8400.6465,  7236.6123,  9683.7305,  8998.9229, 10047.2832,
         8963.8623,  9966.2246, 12241.5674,  8444.0420, 12052.3906,  8855.2051,
         7393.5601,  9654.6553, 12008.8066,  8838.7959, 12400.3369, 11037.5098,
         9048.6152,  9078.6650, 10168.1660,  8924.8691,  9555.7158,  9340.5703,
         9552.8369,  9332.8604,  9947.0830,  9414.0605,  8062.8423, 12282.5283,
         8942.8682,  7932.3301,  9921.1064,  8852.7070,  9465.4893,  9611.6016,
         7851.6777,  7449.7793, 10060.1875, 11292.7725,  9400.6875, 12388.0771,
         8328.9385,  9873.5078,  9635.6992,  8952.2432,  8937.2725,  8279.1426,
        10476.0420,  9031.8027, 10502.9053,  9110.4980,  9842.2705, 10155.2471,
         8934.0830, 10063.9033,  9340.2246,  8525.0938,  9785.6250, 10928.1953,
         9083.3975, 10471.5869, 11105.7041, 11377.2178,  7903.1704,  9172.9678,
        10010.9238, 11782.2090,  8769.8438,  8368.2559,  7443.1196,  8736.3418,
         8676.5000,  9712.7012, 10046.7764,  8761.0625,  7628.1162,  8478.8535,
         9048.4082,  9652.9619, 10345.8623,  7518.1108,  7922.7876, 13100.4922,
         7793.6064,  9032.7637,  8141.1372,  8815.0908,  8116.7524,  7962.3369,
         8363.1797,  8169.0508, 11912.3945,  9038.6270, 12083.6182,  8911.3086,
         8870.6113,  8683.4453, 10076.9658, 10478.3818,  9759.0908, 10243.6406,
         8702.9072,  8584.5557,  8640.5361, 11841.1758,  8193.5889,  7570.8564,
         9156.5430,  7957.0337,  9699.7100,  9486.0850, 12188.1885,  9832.2109,
         7797.3975, 10473.7314,  9536.7354, 11018.7734,  9681.0771, 10672.4121,
         8631.6113, 11318.4746, 10699.9922,  8050.2690, 10977.9766,  9833.9990,
        11249.3525,  9107.6504,  8862.3066,  8976.1201,  8431.7646,  9170.5029,
         7465.8994,  8906.1807,  7967.2593, 10219.1631,  9768.9521, 10834.1348,
         9216.0674,  9108.1182,  9446.0703,  7710.5815,  7308.9976, 10012.2168,
         9234.3027,  9714.2148,  8790.5049,  9320.5439,  9026.2852,  9372.7900,
         9028.1914,  6837.5063,  8562.7422, 12051.0225, 10044.3564,  9188.2031,
         8766.9307,  8102.9844,  8876.4365, 10235.2666, 10051.9229, 13505.9121,
         8522.2197, 11080.9785,  8632.0469,  9151.3105,  8507.8164,  9626.1943,
        11717.3477,  8984.3877,  7699.1807, 10846.1260,  9458.5615,  8848.9141,
        10356.7070, 11327.1904,  9265.5146,  8450.8271,  8689.5557,  8158.0806,
         9913.7109, 10745.2012, 10197.4775, 10031.5723, 10084.5195,  8782.1338,
        10923.2666, 10223.1504, 10098.6357,  8984.9707, 10447.0508, 10423.1719,
        12301.2803,  8509.6963,  7539.7837,  9575.7715,  7287.3726,  9671.7822,
         8434.6533, 11266.4854,  9369.0215,  7549.7969,  9351.5850, 10147.7324,
        11299.8857,  9169.0137,  9926.1240,  9967.0645,  8764.0068,  9369.3809,
        10410.7529,  8536.3906,  9190.8604,  9915.6172,  8743.0605, 12431.0000,
         8311.7383,  7938.3799,  9228.6680,  9093.0908,  9259.1240, 12857.0957,
         8989.4473,  6715.4243, 10952.9414,  7873.0552, 12208.5635, 12549.2773,
         9184.1865, 10135.6426,  8140.5645, 11060.5938, 11640.0996,  7682.9268,
        11027.5010, 10421.6289,  9022.8730,  9438.1709,  7671.7974,  9448.7930,
         8752.3623,  8891.0459])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([433.6976,  59.8468, 205.6327,  ...,   7.7433,  63.7847, 221.5422])
Globale Pruning-Maske generiert: 53 Layer
Global concept maps: OrderedDict([('conv1', {'Conv2d': {'weight': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        [[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        [[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        ...,


        [[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0.], device='cuda:0')}}), ('encoder.4.0.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1.], device='cuda:0')}}), ('encoder.4.0.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1.], device='cuda:0')}}), ('encoder.4.0.conv3', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 1., 1.], device='cuda:0')}}), ('encoder.4.0.downsample.0', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1.], device='cuda:0')}}), ('encoder.4.1.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 1.], device='cuda:0')}}), ('encoder.4.1.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0.], device='cuda:0')}}), ('encoder.4.1.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1.], device='cuda:0')}}), ('encoder.4.2.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0.], device='cuda:0')}}), ('encoder.4.2.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]]], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0')}}), ('encoder.4.2.conv3', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.5.0.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 1.], device='cuda:0')}}), ('encoder.5.0.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        ...,


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1.], device='cuda:0')}}), ('encoder.5.0.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 0.], device='cuda:0')}}), ('encoder.5.0.downsample.0', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0.], device='cuda:0')}}), ('encoder.5.1.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}}), ('encoder.5.1.conv2', {'Conv2d': {'weight': tensor([[[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1.], device='cuda:0')}}), ('encoder.5.1.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1.], device='cuda:0')}}), ('encoder.5.2.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,
        1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,
        1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,
        1., 0.], device='cuda:0')}}), ('encoder.5.2.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        ...,


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.2.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0')}}), ('encoder.5.3.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1.], device='cuda:0')}}), ('encoder.5.3.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1.], device='cuda:0')}}), ('encoder.5.3.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0.], device='cuda:0')}}), ('encoder.6.0.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.0.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.0.conv3', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([0., 0., 1.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1.,  ..., 1., 0., 0.], device='cuda:0')}}), ('encoder.6.0.downsample.0', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([0., 0., 1.,  ..., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1.,  ..., 1., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1.,  ..., 1., 1., 0.], device='cuda:0')}}), ('encoder.6.1.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.1.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1.], device='cuda:0')}}), ('encoder.6.1.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.2.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 0.], device='cuda:0')}}), ('encoder.6.2.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.2.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.3.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.3.conv2', {'Conv2d': {'weight': tensor([[[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.3.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.4.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 1.], device='cuda:0')}}), ('encoder.6.4.conv2', {'Conv2d': {'weight': tensor([[[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.4.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.5.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.5.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0.], device='cuda:0')}}), ('encoder.6.5.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}}), ('encoder.7.0.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0.], device='cuda:0')}}), ('encoder.7.0.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0.], device='cuda:0')}}), ('encoder.7.0.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 0., 1.,  ..., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1.,  ..., 0., 1., 1.], device='cuda:0')}}), ('encoder.7.0.downsample.0', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}}), ('encoder.7.1.conv1', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0.], device='cuda:0')}}), ('encoder.7.1.conv2', {'Conv2d': {'weight': tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}}), ('encoder.7.1.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0')}}), ('encoder.7.2.conv1', {'Conv2d': {'weight': tensor([[[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 1.], device='cuda:0')}}), ('encoder.7.2.conv2', {'Conv2d': {'weight': tensor([[[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        [[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         ...,

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],

         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]],


        ...,


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         ...,

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],

         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}}), ('encoder.7.2.conv3', {'Conv2d': {'weight': tensor([[[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        ...,


        [[[0.]],

         [[0.]],

         [[0.]],

         ...,

         [[0.]],

         [[0.]],

         [[0.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]],


        [[[1.]],

         [[1.]],

         [[1.]],

         ...,

         [[1.]],

         [[1.]],

         [[1.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 1., 1.], device='cuda:0')}})])
Layer: conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        [[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        [[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        ...,


        [[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0.], device='cuda:0')}}
Failed to compute pruning mask: 'dict' object has no attribute 'numel'
