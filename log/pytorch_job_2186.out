Lines that potentially need to be canonized 309
Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    7150 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    4263 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    7180 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    3248 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
[INFO] Collecting local mean and std from clients...
[INFO] Calculating data mean and standard deviation...
[DEBUG] Batch 0: Data Min: 1.0, Max: 16032.0, Mean: 1092.764892578125, Std: 1094.674072265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 1: Data Min: 1.0, Max: 16608.0, Mean: 1060.4288330078125, Std: 1088.5697021484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 2: Data Min: 1.0, Max: 10960.0, Mean: 1026.5419921875, Std: 1102.1942138671875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 3: Data Min: 1.0, Max: 8048.0, Mean: 1023.8707885742188, Std: 1067.3106689453125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 4: Data Min: 1.0, Max: 8496.0, Mean: 1044.4490966796875, Std: 1090.1982421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 5: Data Min: 1.0, Max: 17136.0, Mean: 1024.67138671875, Std: 1056.4482421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 6: Data Min: 1.0, Max: 17376.0, Mean: 1078.60986328125, Std: 1071.08642578125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 7: Data Min: 1.0, Max: 8208.0, Mean: 1084.62353515625, Std: 1072.8319091796875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 8: Data Min: 1.0, Max: 8280.0, Mean: 1063.4312744140625, Std: 1096.598876953125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 9: Data Min: 1.0, Max: 15440.0, Mean: 1078.97216796875, Std: 1089.73291015625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 10: Data Min: 1.0, Max: 7924.0, Mean: 1025.6075439453125, Std: 1090.5980224609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 11: Data Min: 1.0, Max: 8440.0, Mean: 1033.6759033203125, Std: 1082.536865234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 12: Data Min: 1.0, Max: 8580.0, Mean: 1017.6873168945312, Std: 1059.8895263671875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 13: Data Min: 1.0, Max: 7480.0, Mean: 1036.7105712890625, Std: 1091.1876220703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 14: Data Min: 1.0, Max: 7396.0, Mean: 1101.8038330078125, Std: 1070.9842529296875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 15: Data Min: 1.0, Max: 9448.0, Mean: 1012.4028930664062, Std: 1079.4068603515625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 16: Data Min: 1.0, Max: 17504.0, Mean: 1057.5784912109375, Std: 1083.364013671875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 17: Data Min: 1.0, Max: 10312.0, Mean: 1059.9542236328125, Std: 1093.1845703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 18: Data Min: 1.0, Max: 10448.0, Mean: 1059.1878662109375, Std: 1061.552734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 19: Data Min: 1.0, Max: 9368.0, Mean: 1073.831787109375, Std: 1080.9810791015625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 20: Data Min: 1.0, Max: 8184.0, Mean: 1037.9713134765625, Std: 1072.1102294921875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 21: Data Min: 1.0, Max: 14242.0, Mean: 1034.98583984375, Std: 1073.330078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 22: Data Min: 1.0, Max: 16032.0, Mean: 1074.0780029296875, Std: 1110.2415771484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 23: Data Min: 1.0, Max: 7788.0, Mean: 1049.2777099609375, Std: 1087.3331298828125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 24: Data Min: 1.0, Max: 17456.0, Mean: 1010.0674438476562, Std: 1083.5107421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 25: Data Min: 1.0, Max: 10880.0, Mean: 1083.6190185546875, Std: 1078.2503662109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 26: Data Min: 1.0, Max: 8880.0, Mean: 1093.891845703125, Std: 1092.368408203125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 27: Data Min: 1.0, Max: 14456.0, Mean: 1099.7880859375, Std: 1070.259765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 28: Data Min: 1.0, Max: 15976.0, Mean: 1071.2794189453125, Std: 1072.76123046875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 29: Data Min: 1.0, Max: 7852.0, Mean: 1086.4586181640625, Std: 1100.2093505859375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 30: Data Min: 1.0, Max: 17488.0, Mean: 1071.7730712890625, Std: 1083.9178466796875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 31: Data Min: 1.0, Max: 10568.0, Mean: 1027.971435546875, Std: 1067.701904296875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 32: Data Min: 1.0, Max: 12040.0, Mean: 1127.752685546875, Std: 1110.6375732421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 33: Data Min: 1.0, Max: 7764.0, Mean: 1029.388427734375, Std: 1065.556640625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 34: Data Min: 1.0, Max: 8304.0, Mean: 1072.9034423828125, Std: 1096.893798828125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 35: Data Min: 1.0, Max: 10256.0, Mean: 1048.8759765625, Std: 1082.1741943359375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 36: Data Min: 1.0, Max: 17456.0, Mean: 1023.8973388671875, Std: 1075.1312255859375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 37: Data Min: 1.0, Max: 9496.0, Mean: 1019.146240234375, Std: 1040.5289306640625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 38: Data Min: 1.0, Max: 17488.0, Mean: 1014.7125854492188, Std: 1061.6021728515625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 39: Data Min: 1.0, Max: 8824.0, Mean: 1065.48828125, Std: 1062.265869140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 40: Data Min: 1.0, Max: 8180.0, Mean: 1053.145263671875, Std: 1077.8057861328125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 41: Data Min: 1.0, Max: 17504.0, Mean: 1044.9100341796875, Std: 1102.8846435546875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 42: Data Min: 1.0, Max: 10232.0, Mean: 1039.8082275390625, Std: 1076.67138671875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 43: Data Min: 1.0, Max: 17504.0, Mean: 1105.236083984375, Std: 1102.9775390625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 44: Data Min: 1.0, Max: 8792.0, Mean: 1076.86474609375, Std: 1104.837890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 45: Data Min: 1.0, Max: 10368.0, Mean: 1026.234130859375, Std: 1069.5814208984375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 46: Data Min: 1.0, Max: 7772.0, Mean: 973.7395629882812, Std: 1067.3370361328125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 47: Data Min: 1.0, Max: 17536.0, Mean: 1090.6502685546875, Std: 1114.1094970703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 48: Data Min: 1.0, Max: 9432.0, Mean: 1039.4217529296875, Std: 1075.3790283203125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 49: Data Min: 1.0, Max: 8584.0, Mean: 1024.975830078125, Std: 1082.9088134765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 50: Data Min: 1.0, Max: 7320.0, Mean: 1083.8199462890625, Std: 1092.091796875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 51: Data Min: 1.0, Max: 15888.0, Mean: 1027.124267578125, Std: 1059.7908935546875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 52: Data Min: 1.0, Max: 7304.0, Mean: 1083.9471435546875, Std: 1096.4384765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 53: Data Min: 1.0, Max: 14640.0, Mean: 1030.7515869140625, Std: 1069.6639404296875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 54: Data Min: 1.0, Max: 17536.0, Mean: 1028.5179443359375, Std: 1097.1221923828125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 55: Data Min: 1.0, Max: 7832.0, Mean: 1038.0797119140625, Std: 1123.69677734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 56: Data Min: 1.0, Max: 7080.0, Mean: 1057.760009765625, Std: 1073.958251953125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 57: Data Min: 1.0, Max: 8616.0, Mean: 1052.177001953125, Std: 1085.111328125
[WARNING] Data out of normal range. Applying pre-normalization.
[INFO] Final Data Mean: 0.10128556488986454, Final Data Std: 0.1128994832542204
[INFO] Calculating data mean and standard deviation...
[DEBUG] Batch 0: Data Min: 1.0, Max: 20592.0, Mean: 1480.662841796875, Std: 1663.687744140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 1: Data Min: 1.0, Max: 18512.0, Mean: 1334.740234375, Std: 1624.98779296875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 2: Data Min: 1.0, Max: 12266.0, Mean: 1338.9276123046875, Std: 1618.196044921875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 3: Data Min: 1.0, Max: 20688.0, Mean: 1423.6649169921875, Std: 1643.7076416015625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 4: Data Min: 1.0, Max: 18448.0, Mean: 1437.2357177734375, Std: 1649.7642822265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 5: Data Min: 1.0, Max: 18416.0, Mean: 1330.2177734375, Std: 1619.062255859375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 6: Data Min: 1.0, Max: 18384.0, Mean: 1514.505615234375, Std: 1658.4132080078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 7: Data Min: 1.0, Max: 11814.0, Mean: 1446.492919921875, Std: 1651.13037109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 8: Data Min: 1.0, Max: 19056.0, Mean: 1381.612548828125, Std: 1637.1197509765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 9: Data Min: 1.0, Max: 17280.0, Mean: 1371.452880859375, Std: 1651.683837890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 10: Data Min: 1.0, Max: 10208.0, Mean: 1454.392822265625, Std: 1640.948974609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 11: Data Min: 1.0, Max: 17888.0, Mean: 1521.3519287109375, Std: 1654.3673095703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 12: Data Min: 1.0, Max: 18352.0, Mean: 1448.8848876953125, Std: 1654.5872802734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 13: Data Min: 1.0, Max: 20592.0, Mean: 1433.9642333984375, Std: 1648.8763427734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 14: Data Min: 1.0, Max: 16024.0, Mean: 1457.943115234375, Std: 1653.677490234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 15: Data Min: 1.0, Max: 20688.0, Mean: 1404.3511962890625, Std: 1626.0704345703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 16: Data Min: 1.0, Max: 20672.0, Mean: 1383.9671630859375, Std: 1624.423828125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 17: Data Min: 1.0, Max: 19056.0, Mean: 1483.7042236328125, Std: 1651.930419921875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 18: Data Min: 1.0, Max: 18352.0, Mean: 1391.616943359375, Std: 1630.8218994140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 19: Data Min: 1.0, Max: 20544.0, Mean: 1530.5416259765625, Std: 1681.60400390625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 20: Data Min: 1.0, Max: 17888.0, Mean: 1460.7891845703125, Std: 1669.2337646484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 21: Data Min: 1.0, Max: 18368.0, Mean: 1470.040283203125, Std: 1631.5924072265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 22: Data Min: 1.0, Max: 16274.0, Mean: 1455.3662109375, Std: 1628.970947265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 23: Data Min: 1.0, Max: 20608.0, Mean: 1380.77294921875, Std: 1627.2965087890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 24: Data Min: 1.0, Max: 18416.0, Mean: 1404.5142822265625, Std: 1630.7855224609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 25: Data Min: 1.0, Max: 19088.0, Mean: 1392.7667236328125, Std: 1635.438720703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 26: Data Min: 1.0, Max: 20624.0, Mean: 1493.817626953125, Std: 1658.853271484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 27: Data Min: 1.0, Max: 18384.0, Mean: 1385.091064453125, Std: 1619.4005126953125
[WARNING] Data out of normal range. Applying pre-normalization.
[INFO] Final Data Mean: 0.08117907002719503, Final Data Std: 0.0967750505209424
[INFO] Calculating data mean and standard deviation...
[DEBUG] Batch 0: Data Min: 1.0, Max: 18800.0, Mean: 1793.69140625, Std: 1156.1904296875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 1: Data Min: 1.0, Max: 15528.0, Mean: 1805.5579833984375, Std: 1187.060302734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 2: Data Min: 1.0, Max: 18752.0, Mean: 1783.5947265625, Std: 1151.7860107421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 3: Data Min: 1.0, Max: 16784.0, Mean: 1823.90966796875, Std: 1163.4615478515625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 4: Data Min: 1.0, Max: 18832.0, Mean: 1809.126220703125, Std: 1164.5091552734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 5: Data Min: 1.0, Max: 16752.0, Mean: 1785.2646484375, Std: 1147.4180908203125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 6: Data Min: 1.0, Max: 15171.0, Mean: 1759.035400390625, Std: 1127.1209716796875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 7: Data Min: 1.0, Max: 18784.0, Mean: 1805.7808837890625, Std: 1151.0330810546875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 8: Data Min: 1.0, Max: 18832.0, Mean: 1796.1123046875, Std: 1146.3636474609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 9: Data Min: 1.0, Max: 17520.0, Mean: 1791.564697265625, Std: 1121.2445068359375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 10: Data Min: 1.0, Max: 15480.0, Mean: 1792.6629638671875, Std: 1115.7891845703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 11: Data Min: 1.0, Max: 15139.0, Mean: 1794.2869873046875, Std: 1139.4844970703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 12: Data Min: 1.0, Max: 15896.0, Mean: 1781.7095947265625, Std: 1146.965087890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 13: Data Min: 1.0, Max: 18784.0, Mean: 1798.351318359375, Std: 1140.867431640625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 14: Data Min: 1.0, Max: 17632.0, Mean: 1798.51318359375, Std: 1141.18115234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 15: Data Min: 1.0, Max: 15536.0, Mean: 1775.023193359375, Std: 1135.3323974609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 16: Data Min: 1.0, Max: 11200.0, Mean: 1781.658447265625, Std: 1131.73828125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 17: Data Min: 1.0, Max: 18768.0, Mean: 1799.6058349609375, Std: 1144.6099853515625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 18: Data Min: 1.0, Max: 17616.0, Mean: 1804.106689453125, Std: 1167.3206787109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 19: Data Min: 1.0, Max: 17552.0, Mean: 1779.849853515625, Std: 1148.6690673828125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 20: Data Min: 1.0, Max: 15528.0, Mean: 1773.8934326171875, Std: 1139.787109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 21: Data Min: 1.0, Max: 15223.0, Mean: 1807.6845703125, Std: 1135.9163818359375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 22: Data Min: 1.0, Max: 15080.0, Mean: 1790.872314453125, Std: 1148.8431396484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 23: Data Min: 1.0, Max: 16832.0, Mean: 1824.38671875, Std: 1159.9656982421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 24: Data Min: 1.0, Max: 16784.0, Mean: 1822.989501953125, Std: 1172.81884765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 25: Data Min: 1.0, Max: 16104.0, Mean: 1810.2557373046875, Std: 1189.5484619140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 26: Data Min: 1.0, Max: 17456.0, Mean: 1786.6932373046875, Std: 1153.9949951171875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 27: Data Min: 1.0, Max: 16800.0, Mean: 1810.8878173828125, Std: 1146.55810546875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 28: Data Min: 12.0, Max: 13864.0, Mean: 1787.383544921875, Std: 1049.5035400390625
[WARNING] Data out of normal range. Applying pre-normalization.
[INFO] Final Data Mean: 0.10841895682813951, Final Data Std: 0.07099286839826516
[INFO] Global data mean: 0.0981091856956482, Global data std: 0.1000843197107315
Initializing LRP Pruning...
LRP initialized successfully.
=== Round 1/4 ===
Training and communication for Round 1...
[INFO] Calculating data mean and standard deviation...
[DEBUG] Batch 0: Data Min: 1.0, Max: 7540.0, Mean: 1079.0799560546875, Std: 1101.9744873046875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 1: Data Min: 1.0, Max: 8824.0, Mean: 1040.1334228515625, Std: 1075.3111572265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 2: Data Min: 1.0, Max: 11992.0, Mean: 1060.8948974609375, Std: 1076.932861328125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 3: Data Min: 1.0, Max: 17376.0, Mean: 1020.0524291992188, Std: 1074.493408203125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 4: Data Min: 1.0, Max: 10256.0, Mean: 1081.4217529296875, Std: 1059.59716796875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 5: Data Min: 1.0, Max: 9368.0, Mean: 1122.9783935546875, Std: 1106.4234619140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 6: Data Min: 1.0, Max: 15000.0, Mean: 993.2321166992188, Std: 1073.6656494140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 7: Data Min: 1.0, Max: 17488.0, Mean: 1068.2535400390625, Std: 1101.299072265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 8: Data Min: 1.0, Max: 8124.0, Mean: 1027.8392333984375, Std: 1063.6966552734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 9: Data Min: 1.0, Max: 9496.0, Mean: 1017.3179931640625, Std: 1087.2359619140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 10: Data Min: 1.0, Max: 8208.0, Mean: 1071.837158203125, Std: 1107.697998046875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 11: Data Min: 1.0, Max: 7276.0, Mean: 1036.5960693359375, Std: 1095.00830078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 12: Data Min: 1.0, Max: 17536.0, Mean: 1016.7298583984375, Std: 1069.6265869140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 13: Data Min: 1.0, Max: 7320.0, Mean: 1080.1988525390625, Std: 1091.9918212890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 14: Data Min: 1.0, Max: 17456.0, Mean: 1000.4191284179688, Std: 1047.1318359375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 15: Data Min: 1.0, Max: 13416.0, Mean: 1062.769287109375, Std: 1068.9306640625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 16: Data Min: 1.0, Max: 15976.0, Mean: 1058.7093505859375, Std: 1098.2633056640625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 17: Data Min: 1.0, Max: 8520.0, Mean: 1018.663330078125, Std: 1101.48486328125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 18: Data Min: 1.0, Max: 9432.0, Mean: 1086.97265625, Std: 1096.571533203125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 19: Data Min: 1.0, Max: 16032.0, Mean: 1090.5224609375, Std: 1094.5694580078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 20: Data Min: 1.0, Max: 9992.0, Mean: 1073.8055419921875, Std: 1089.56787109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 21: Data Min: 1.0, Max: 17536.0, Mean: 1094.4539794921875, Std: 1110.6058349609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 22: Data Min: 1.0, Max: 10408.0, Mean: 1060.6416015625, Std: 1092.4910888671875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 23: Data Min: 1.0, Max: 17456.0, Mean: 1027.3404541015625, Std: 1077.8182373046875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 24: Data Min: 1.0, Max: 14640.0, Mean: 1068.677734375, Std: 1084.792724609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 25: Data Min: 1.0, Max: 12040.0, Mean: 1095.40673828125, Std: 1074.70947265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 26: Data Min: 1.0, Max: 7248.0, Mean: 998.78466796875, Std: 1074.1676025390625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 27: Data Min: 1.0, Max: 17136.0, Mean: 1049.9405517578125, Std: 1058.9849853515625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 28: Data Min: 1.0, Max: 15045.0, Mean: 1055.4437255859375, Std: 1090.4239501953125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 29: Data Min: 1.0, Max: 11552.0, Mean: 966.4650268554688, Std: 1067.3214111328125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 30: Data Min: 1.0, Max: 17504.0, Mean: 1014.4381103515625, Std: 1051.1790771484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 31: Data Min: 1.0, Max: 14456.0, Mean: 1043.9959716796875, Std: 1078.260498046875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 32: Data Min: 1.0, Max: 10232.0, Mean: 1077.3966064453125, Std: 1091.728271484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 33: Data Min: 1.0, Max: 11112.0, Mean: 1079.5718994140625, Std: 1089.1683349609375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 34: Data Min: 1.0, Max: 7812.0, Mean: 1029.6318359375, Std: 1071.0576171875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 35: Data Min: 1.0, Max: 15888.0, Mean: 1062.5687255859375, Std: 1080.7388916015625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 36: Data Min: 1.0, Max: 7964.0, Mean: 1058.0731201171875, Std: 1094.5772705078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 37: Data Min: 1.0, Max: 8048.0, Mean: 1102.9012451171875, Std: 1073.54736328125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 38: Data Min: 1.0, Max: 8320.0, Mean: 1056.9898681640625, Std: 1100.4517822265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 39: Data Min: 1.0, Max: 17504.0, Mean: 1057.6527099609375, Std: 1095.6873779296875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 40: Data Min: 1.0, Max: 8136.0, Mean: 1168.9541015625, Std: 1096.5672607421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 41: Data Min: 1.0, Max: 9728.0, Mean: 1088.36181640625, Std: 1079.70703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 42: Data Min: 1.0, Max: 10928.0, Mean: 1019.2693481445312, Std: 1054.1119384765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 43: Data Min: 1.0, Max: 7220.0, Mean: 996.5592041015625, Std: 1053.5137939453125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 44: Data Min: 1.0, Max: 17504.0, Mean: 1073.1781005859375, Std: 1095.6097412109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 45: Data Min: 1.0, Max: 17488.0, Mean: 1046.8914794921875, Std: 1078.2864990234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 46: Data Min: 1.0, Max: 16032.0, Mean: 979.290771484375, Std: 1070.771240234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 47: Data Min: 1.0, Max: 9448.0, Mean: 1040.7178955078125, Std: 1064.054931640625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 48: Data Min: 1.0, Max: 8156.0, Mean: 1041.2149658203125, Std: 1082.092041015625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 49: Data Min: 1.0, Max: 10448.0, Mean: 1066.65087890625, Std: 1078.39013671875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 50: Data Min: 1.0, Max: 17456.0, Mean: 1064.8719482421875, Std: 1093.5096435546875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 51: Data Min: 1.0, Max: 15440.0, Mean: 1023.3208618164062, Std: 1074.10693359375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 52: Data Min: 1.0, Max: 9968.0, Mean: 1091.3792724609375, Std: 1100.65185546875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 53: Data Min: 1.0, Max: 9528.0, Mean: 1011.5635375976562, Std: 1072.9739990234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 54: Data Min: 1.0, Max: 16608.0, Mean: 1068.066650390625, Std: 1087.8797607421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 55: Data Min: 1.0, Max: 7524.0, Mean: 1112.6070556640625, Std: 1105.4564208984375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 56: Data Min: 1.0, Max: 14242.0, Mean: 1056.889892578125, Std: 1075.109619140625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 57: Data Min: 1.0, Max: 7684.0, Mean: 1011.9458618164062, Std: 1064.61572265625
[WARNING] Data out of normal range. Applying pre-normalization.
[INFO] Final Data Mean: 0.09610633208563173, Final Data Std: 0.10783882131170364
Epoch 1/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8432.0, Mean: 1010.3322143554688, Std: 1072.5413818359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 78189.8671875, Mean: 9368.017578125, Std: 9945.78125
[DEBUG] Loss for batch 0: 0.9724168544858395
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 1019.3283081054688, Std: 1077.0255126953125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 9451.4404296875, Std: 9987.3623046875
[DEBUG] Loss for batch 1: 0.5731799396344034
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11112.0, Mean: 1089.8607177734375, Std: 1094.51025390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 103041.7734375, Mean: 10105.4931640625, Std: 10149.5
[DEBUG] Loss for batch 2: 0.288949468157312
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1074.1280517578125, Std: 1065.4056396484375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 9959.6025390625, Std: 9879.609375
[DEBUG] Loss for batch 3: 0.24356478000972512
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10408.0, Mean: 1067.33740234375, Std: 1112.90673828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96513.5078125, Mean: 9896.630859375, Std: 10320.0927734375
[DEBUG] Loss for batch 4: 0.24530390569531874
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8440.0, Mean: 963.921875, Std: 1069.807373046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 78264.0546875, Mean: 8937.650390625, Std: 9920.427734375
[DEBUG] Loss for batch 5: 0.2505380985124661
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12040.0, Mean: 1069.9241943359375, Std: 1074.2374267578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 111647.203125, Mean: 9920.6220703125, Std: 9961.5087890625
[DEBUG] Loss for batch 6: 0.23774977713674927
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12245.0, Mean: 1052.4461669921875, Std: 1094.7764892578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 113548.1875, Mean: 9758.5439453125, Std: 10151.96875
[DEBUG] Loss for batch 7: 0.20088640213816125
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10448.0, Mean: 1044.1636962890625, Std: 1108.6180419921875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96884.4375, Mean: 9681.7392578125, Std: 10280.322265625
[DEBUG] Loss for batch 8: 0.21023370692458299
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7360.0, Mean: 992.0983276367188, Std: 1054.3397216796875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68249.109375, Mean: 9198.93359375, Std: 9776.9951171875
[DEBUG] Loss for batch 9: 0.18850293553441588
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10232.0, Mean: 1095.8905029296875, Std: 1078.4779052734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 94881.4453125, Mean: 10161.41015625, Std: 10000.8310546875
[DEBUG] Loss for batch 10: 0.21684071141254435
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10536.0, Mean: 1034.415283203125, Std: 1079.2105712890625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 97700.46875, Mean: 9591.34375, Std: 10007.6240234375
[DEBUG] Loss for batch 11: 0.20537524627747908
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7768.0, Mean: 1062.76318359375, Std: 1098.0
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72032.53125, Mean: 9854.21484375, Std: 10181.861328125
[DEBUG] Loss for batch 12: 0.18245774921820157
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8584.0, Mean: 1039.30859375, Std: 1053.3455810546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 79599.3828125, Mean: 9636.7177734375, Std: 9767.7763671875
[DEBUG] Loss for batch 13: 0.2021206187599474
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8136.0, Mean: 1024.8350830078125, Std: 1055.12255859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75445.03125, Mean: 9502.50390625, Std: 9784.2548828125
[DEBUG] Loss for batch 14: 0.18526161523086937
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15888.0, Mean: 1070.5203857421875, Std: 1120.2294921875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 147330.078125, Mean: 9926.1474609375, Std: 10387.9970703125
[DEBUG] Loss for batch 15: 0.22086018987658398
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8016.0, Mean: 1066.0396728515625, Std: 1064.3135986328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74332.2578125, Mean: 9884.5986328125, Std: 9869.484375
[DEBUG] Loss for batch 16: 0.21821857968905162
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12816.0, Mean: 1090.8704833984375, Std: 1087.9261474609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 118843.1328125, Mean: 10114.8564453125, Std: 10088.4453125
[DEBUG] Loss for batch 17: 0.19930188630560486
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15045.0, Mean: 1060.0137939453125, Std: 1074.9384765625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 139512.859375, Mean: 9828.7197265625, Std: 9968.0087890625
[DEBUG] Loss for batch 18: 0.20899149741137443
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16032.0, Mean: 1044.309814453125, Std: 1077.5311279296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148665.40625, Mean: 9683.09375, Std: 9992.05078125
[DEBUG] Loss for batch 19: 0.18391808480210958
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7788.0, Mean: 1110.7213134765625, Std: 1095.4898681640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72217.9921875, Mean: 10298.935546875, Std: 10158.583984375
[DEBUG] Loss for batch 20: 0.18943537933248117
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1102.168212890625, Std: 1102.246826171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 10219.62109375, Std: 10221.2421875
[DEBUG] Loss for batch 21: 0.16820994740125852
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14242.0, Mean: 1074.475830078125, Std: 1100.1951904296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 132066.5625, Mean: 9962.830078125, Std: 10202.2177734375
[DEBUG] Loss for batch 22: 0.1947943741930815
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9472.0, Mean: 1023.471435546875, Std: 1069.237060546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 87833.890625, Mean: 9489.859375, Std: 9915.1396484375
[DEBUG] Loss for batch 23: 0.16003813600981245
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17376.0, Mean: 1041.829833984375, Std: 1071.3221435546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161128.453125, Mean: 9660.099609375, Std: 9934.474609375
[DEBUG] Loss for batch 24: 0.18924667459678995
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8144.0, Mean: 1079.1337890625, Std: 1078.964111328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75519.2109375, Mean: 10006.021484375, Std: 10005.3388671875
[DEBUG] Loss for batch 25: 0.1654941176873156
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8824.0, Mean: 1110.416015625, Std: 1094.6619873046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 81824.921875, Mean: 10296.103515625, Std: 10150.90625
[DEBUG] Loss for batch 26: 0.17599651831130186
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7684.0, Mean: 1044.4188232421875, Std: 1083.9769287109375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 71253.5859375, Mean: 9684.10546875, Std: 10051.8232421875
[DEBUG] Loss for batch 27: 0.16461265153854748
[DEBUG] Batch 28: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10568.0, Mean: 1043.1697998046875, Std: 1063.1148681640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 97997.203125, Mean: 9672.5224609375, Std: 9858.3671875
[DEBUG] Loss for batch 28: 0.16782726806971504
[DEBUG] Batch 29: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7284.0, Mean: 1027.9405517578125, Std: 1070.3721923828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67544.3515625, Mean: 9531.302734375, Std: 9925.666015625
[DEBUG] Loss for batch 29: 0.18432226983282235
[DEBUG] Batch 30: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1068.14111328125, Std: 1075.0062255859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9904.083984375, Std: 9968.6376953125
[DEBUG] Loss for batch 30: 0.17149564283246774
[DEBUG] Batch 31: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9992.0, Mean: 1036.1444091796875, Std: 1068.973388671875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 92655.90625, Mean: 9607.376953125, Std: 9912.6943359375
[DEBUG] Loss for batch 31: 0.17292238202272525
[DEBUG] Batch 32: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11552.0, Mean: 1050.88623046875, Std: 1098.478515625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 107121.9375, Mean: 9744.0791015625, Std: 10186.2978515625
[DEBUG] Loss for batch 32: 0.1575063152167792
[DEBUG] Batch 33: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7508.0, Mean: 1016.3580932617188, Std: 1082.7467041015625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69621.5234375, Mean: 9423.896484375, Std: 10040.4150390625
[DEBUG] Loss for batch 33: 0.1533070773510453
[DEBUG] Batch 34: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7636.0, Mean: 1011.3623046875, Std: 1083.12451171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70808.4765625, Mean: 9377.5693359375, Std: 10043.919921875
[DEBUG] Loss for batch 34: 0.16348960422069062
[DEBUG] Batch 35: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7480.0, Mean: 1069.3302001953125, Std: 1099.188232421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69361.875, Mean: 9915.111328125, Std: 10192.8798828125
[DEBUG] Loss for batch 35: 0.16530716030709652
[DEBUG] Batch 36: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1054.84765625, Std: 1103.4425048828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9780.8154296875, Std: 10232.330078125
[DEBUG] Loss for batch 36: 0.16215778625877753
[DEBUG] Batch 37: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8280.0, Mean: 1056.7716064453125, Std: 1096.728515625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 76780.359375, Mean: 9798.65625, Std: 10170.0703125
[DEBUG] Loss for batch 37: 0.1474788168271863
[DEBUG] Batch 38: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9088.0, Mean: 1031.5556640625, Std: 1092.07275390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 84273.0234375, Mean: 9564.8251953125, Std: 10126.896484375
[DEBUG] Loss for batch 38: 0.14468850957787013
[DEBUG] Batch 39: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17136.0, Mean: 1132.37646484375, Std: 1111.9144287109375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 158902.90625, Mean: 10499.74609375, Std: 10310.890625
[DEBUG] Loss for batch 39: 0.16580019650417385
[DEBUG] Batch 40: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7684.0, Mean: 1097.9398193359375, Std: 1071.2852783203125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 71253.5859375, Mean: 10180.412109375, Std: 9934.1328125
[DEBUG] Loss for batch 40: 0.15811270631940164
[DEBUG] Batch 41: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7668.0, Mean: 1059.5526123046875, Std: 1069.390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 71105.21875, Mean: 9824.4443359375, Std: 9916.5625
[DEBUG] Loss for batch 41: 0.16111646692193413
[DEBUG] Batch 42: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7272.0, Mean: 1024.532470703125, Std: 1069.8924560546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67433.0703125, Mean: 9499.6982421875, Std: 9921.216796875
[DEBUG] Loss for batch 42: 0.13151546546568574
[DEBUG] Batch 43: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 1040.5040283203125, Std: 1078.406982421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 9647.802734375, Std: 10000.1728515625
[DEBUG] Loss for batch 43: 0.14719957735493405
[DEBUG] Batch 44: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1040.028564453125, Std: 1092.7464599609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 9643.3955078125, Std: 10133.1435546875
[DEBUG] Loss for batch 44: 0.1494854392539342
[DEBUG] Batch 45: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14456.0, Mean: 1058.8348388671875, Std: 1075.3045654296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 134051.015625, Mean: 9817.787109375, Std: 9971.4033203125
[DEBUG] Loss for batch 45: 0.17776884596831533
[DEBUG] Batch 46: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1084.36376953125, Std: 1097.63525390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 10054.51953125, Std: 10178.478515625
[DEBUG] Loss for batch 46: 0.1538385637946505
[DEBUG] Batch 47: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10312.0, Mean: 1008.3885498046875, Std: 1072.784423828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 95623.296875, Mean: 9349.9951171875, Std: 9948.0341796875
[DEBUG] Loss for batch 47: 0.1513450328282396
[DEBUG] Batch 48: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7484.0, Mean: 1020.6616821289062, Std: 1065.2445068359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69398.96875, Mean: 9463.8046875, Std: 9878.1162109375
[DEBUG] Loss for batch 48: 0.1415596398075676
[DEBUG] Batch 49: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1085.2083740234375, Std: 1091.17919921875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 10062.3505859375, Std: 10118.6103515625
[DEBUG] Loss for batch 49: 0.15186148665551927
[DEBUG] Batch 50: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8156.0, Mean: 1089.870849609375, Std: 1097.826171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75630.4921875, Mean: 10105.587890625, Std: 10180.2490234375
[DEBUG] Loss for batch 50: 0.13667992075587224
[DEBUG] Batch 51: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7612.0, Mean: 1012.2030029296875, Std: 1062.3389892578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70585.9296875, Mean: 9385.3671875, Std: 9851.1728515625
[DEBUG] Loss for batch 51: 0.13733570394298467
[DEBUG] Batch 52: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7272.0, Mean: 1018.4693603515625, Std: 1077.4293212890625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67433.0703125, Mean: 9443.474609375, Std: 9991.107421875
[DEBUG] Loss for batch 52: 0.15829313232455952
[DEBUG] Batch 53: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16032.0, Mean: 1021.6595458984375, Std: 1061.9871826171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148665.40625, Mean: 9473.056640625, Std: 9847.9111328125
[DEBUG] Loss for batch 53: 0.13854084951192722
[DEBUG] Batch 54: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10880.0, Mean: 1059.3939208984375, Std: 1069.05810546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 100890.4140625, Mean: 9822.9716796875, Std: 9913.4794921875
[DEBUG] Loss for batch 54: 0.13364916900339469
[DEBUG] Batch 55: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1123.2808837890625, Std: 1106.7288818359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 10415.4013671875, Std: 10262.8046875
[DEBUG] Loss for batch 55: 0.14065860817769957
[DEBUG] Batch 56: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10368.0, Mean: 1038.5550537109375, Std: 1094.829833984375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96142.5859375, Mean: 9629.7314453125, Std: 10152.4638671875
[DEBUG] Loss for batch 56: 0.14218577022889892
[DEBUG] Batch 57: Data shape: torch.Size([213, 10, 120, 120]), Labels shape: torch.Size([213, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7236.0, Mean: 1032.516357421875, Std: 1064.388916015625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67099.2421875, Mean: 9573.7333984375, Std: 9870.181640625
[DEBUG] Loss for batch 57: 0.13628641079753262
[INFO] Training epoch completed successfully.
Epoch 2/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10312.0, Mean: 1042.55908203125, Std: 1063.9842529296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 95623.296875, Mean: 9666.8603515625, Std: 9866.4296875
[DEBUG] Loss for batch 0: 0.13592270161467215
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 988.9982299804688, Std: 1076.5390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 9170.1845703125, Std: 9982.8515625
[DEBUG] Loss for batch 1: 0.1347254994328217
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7788.0, Mean: 971.683349609375, Std: 1059.8563232421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72217.9921875, Mean: 9009.6240234375, Std: 9828.150390625
[DEBUG] Loss for batch 2: 0.12788923136022423
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8180.0, Mean: 1101.404541015625, Std: 1089.64013671875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75853.046875, Mean: 10212.5400390625, Std: 10104.3388671875
[DEBUG] Loss for batch 3: 0.16309664950436722
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9728.0, Mean: 1045.012939453125, Std: 1083.0667724609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 90207.8046875, Mean: 9689.6142578125, Std: 10043.3837890625
[DEBUG] Loss for batch 4: 0.14382066459490403
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8136.0, Mean: 1062.940673828125, Std: 1095.6785888671875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75445.03125, Mean: 9855.861328125, Std: 10160.333984375
[DEBUG] Loss for batch 5: 0.14111810188402266
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15000.0, Mean: 1102.9697265625, Std: 1081.0322265625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 139095.578125, Mean: 10227.0546875, Std: 10024.517578125
[DEBUG] Loss for batch 6: 0.13678034407933867
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 1061.8785400390625, Std: 1075.302734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 9846.0126953125, Std: 9971.38671875
[DEBUG] Loss for batch 7: 0.1309209951376184
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9496.0, Mean: 1065.1241455078125, Std: 1071.97314453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 88056.4453125, Mean: 9876.1083984375, Std: 9940.5107421875
[DEBUG] Loss for batch 8: 0.12991686029663596
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8496.0, Mean: 1063.2691650390625, Std: 1104.056884765625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 78783.3515625, Mean: 9858.9052734375, Std: 10238.02734375
[DEBUG] Loss for batch 9: 0.13710747233546564
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8068.0, Mean: 1027.020263671875, Std: 1072.3338623046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74814.4609375, Mean: 9522.767578125, Std: 9943.85546875
[DEBUG] Loss for batch 10: 0.14855374708447874
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16032.0, Mean: 1047.3260498046875, Std: 1076.14453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148665.40625, Mean: 9711.064453125, Std: 9979.1923828125
[DEBUG] Loss for batch 11: 0.127405622942125
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8432.0, Mean: 1075.1240234375, Std: 1107.7620849609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 78189.8671875, Mean: 9968.837890625, Std: 10272.3857421875
[DEBUG] Loss for batch 12: 0.1366531987454538
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8304.0, Mean: 1056.8785400390625, Std: 1077.7471923828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 77002.9140625, Mean: 9799.646484375, Std: 9994.0546875
[DEBUG] Loss for batch 13: 0.14372054682437047
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8148.0, Mean: 1016.27978515625, Std: 1078.4295654296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75556.3046875, Mean: 9423.169921875, Std: 10000.3818359375
[DEBUG] Loss for batch 14: 0.1374312431821151
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13416.0, Mean: 1027.87060546875, Std: 1063.7550048828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 124406.9921875, Mean: 9530.6533203125, Std: 9864.302734375
[DEBUG] Loss for batch 15: 0.12751580840586885
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17136.0, Mean: 1016.044677734375, Std: 1053.695556640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 158902.90625, Mean: 9420.9892578125, Std: 9771.021484375
[DEBUG] Loss for batch 16: 0.1244494929180632
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7232.0, Mean: 1078.921875, Std: 1088.708251953125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67062.1484375, Mean: 10004.0556640625, Std: 10095.697265625
[DEBUG] Loss for batch 17: 0.12046472222945272
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8520.0, Mean: 1035.9515380859375, Std: 1078.7816162109375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 79005.90625, Mean: 9605.587890625, Std: 10003.646484375
[DEBUG] Loss for batch 18: 0.12849227413640033
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10568.0, Mean: 1091.1689453125, Std: 1097.3857421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 97997.203125, Mean: 10117.625, Std: 10176.1640625
[DEBUG] Loss for batch 19: 0.1523799401609125
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1081.9163818359375, Std: 1084.04150390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 10031.826171875, Std: 10052.421875
[DEBUG] Loss for batch 20: 0.12495898508986036
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8124.0, Mean: 1086.0404052734375, Std: 1109.421142578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75333.75, Mean: 10070.0673828125, Std: 10287.7705078125
[DEBUG] Loss for batch 21: 0.14076187817368704
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7832.0, Mean: 1079.6986083984375, Std: 1088.0379638671875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72626.0078125, Mean: 10011.2587890625, Std: 10089.482421875
[DEBUG] Loss for batch 22: 0.14707986121711544
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16608.0, Mean: 1035.2666015625, Std: 1062.6805419921875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 154006.71875, Mean: 9599.2353515625, Std: 9854.33984375
[DEBUG] Loss for batch 23: 0.12726490910759475
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7612.0, Mean: 1052.892822265625, Std: 1083.3389892578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70585.9296875, Mean: 9762.6865234375, Std: 10045.908203125
[DEBUG] Loss for batch 24: 0.12894236138186618
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8580.0, Mean: 1062.107666015625, Std: 1098.5570068359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 79562.2890625, Mean: 9848.13671875, Std: 10187.025390625
[DEBUG] Loss for batch 25: 0.12651040814252792
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1132.5657958984375, Std: 1094.3109130859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 10501.501953125, Std: 10147.6513671875
[DEBUG] Loss for batch 26: 0.14470022378936304
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1060.7452392578125, Std: 1094.4779052734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 9835.5029296875, Std: 10149.19921875
[DEBUG] Loss for batch 27: 0.13544942167793805
[DEBUG] Batch 28: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8208.0, Mean: 1075.8001708984375, Std: 1096.579833984375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 76112.6953125, Mean: 9975.109375, Std: 10168.69140625
[DEBUG] Loss for batch 28: 0.1488480594741858
[DEBUG] Batch 29: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9528.0, Mean: 1064.94921875, Std: 1096.7425537109375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 88353.1875, Mean: 9874.4873046875, Std: 10170.201171875
[DEBUG] Loss for batch 29: 0.13047740767363641
[DEBUG] Batch 30: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7500.0, Mean: 1041.2703857421875, Std: 1097.664794921875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69547.3359375, Mean: 9654.912109375, Std: 10178.7529296875
[DEBUG] Loss for batch 30: 0.13203487406952033
[DEBUG] Batch 31: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7416.0, Mean: 1093.60986328125, Std: 1084.826904296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68768.3984375, Mean: 10140.259765625, Std: 10059.7060546875
[DEBUG] Loss for batch 31: 0.12282336227855171
[DEBUG] Batch 32: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1049.2716064453125, Std: 1097.745361328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9729.107421875, Std: 10179.4990234375
[DEBUG] Loss for batch 32: 0.12543921619205195
[DEBUG] Batch 33: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11552.0, Mean: 1051.1322021484375, Std: 1076.15625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 107121.9375, Mean: 9746.3603515625, Std: 9979.30078125
[DEBUG] Loss for batch 33: 0.12214188220293648
[DEBUG] Batch 34: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7524.0, Mean: 1025.4000244140625, Std: 1067.1307373046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69769.890625, Mean: 9507.7421875, Std: 9895.607421875
[DEBUG] Loss for batch 34: 0.13292318789767787
[DEBUG] Batch 35: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7344.0, Mean: 1055.631591796875, Std: 1109.24658203125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68100.734375, Mean: 9788.08203125, Std: 10286.1513671875
[DEBUG] Loss for batch 35: 0.13657475153211904
[DEBUG] Batch 36: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10408.0, Mean: 1105.3138427734375, Std: 1094.0928955078125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96513.5078125, Mean: 10248.79296875, Std: 10145.6298828125
[DEBUG] Loss for batch 36: 0.1397445566322319
[DEBUG] Batch 37: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8792.0, Mean: 1030.4237060546875, Std: 1093.5108642578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 81528.1875, Mean: 9554.3291015625, Std: 10140.232421875
[DEBUG] Loss for batch 37: 0.1321899906902545
[DEBUG] Batch 38: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14456.0, Mean: 1097.1649169921875, Std: 1107.511474609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 134051.015625, Mean: 10173.2265625, Std: 10270.0615234375
[DEBUG] Loss for batch 38: 0.13747297293990152
[DEBUG] Batch 39: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1053.0711669921875, Std: 1088.873046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9764.341796875, Std: 10097.2265625
[DEBUG] Loss for batch 39: 0.13006800225018747
[DEBUG] Batch 40: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1099.951416015625, Std: 1088.8487548828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 10199.0654296875, Std: 10097.0009765625
[DEBUG] Loss for batch 40: 0.144331133370817
[DEBUG] Batch 41: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 995.2728881835938, Std: 1072.5299072265625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9228.3701171875, Std: 9945.6748046875
[DEBUG] Loss for batch 41: 0.1280841857758381
[DEBUG] Batch 42: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17376.0, Mean: 1072.10107421875, Std: 1082.031982421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161128.453125, Mean: 9940.806640625, Std: 10033.7880859375
[DEBUG] Loss for batch 42: 0.1411766546058688
[DEBUG] Batch 43: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8032.0, Mean: 1120.697998046875, Std: 1080.7440185546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74480.625, Mean: 10391.451171875, Std: 10021.8447265625
[DEBUG] Loss for batch 43: 0.13494730773905797
[DEBUG] Batch 44: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7636.0, Mean: 1002.9132080078125, Std: 1055.1517333984375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70808.4765625, Mean: 9299.22265625, Std: 9784.5244140625
[DEBUG] Loss for batch 44: 0.13280537875604576
[DEBUG] Batch 45: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15888.0, Mean: 1051.8428955078125, Std: 1081.7906494140625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 147330.078125, Mean: 9752.9501953125, Std: 10031.5498046875
[DEBUG] Loss for batch 45: 0.13307832295330158
[DEBUG] Batch 46: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7304.0, Mean: 967.0765380859375, Std: 1057.849365234375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67729.8125, Mean: 8966.904296875, Std: 9809.5400390625
[DEBUG] Loss for batch 46: 0.12425147641164991
[DEBUG] Batch 47: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8024.0, Mean: 1019.6361083984375, Std: 1094.6156005859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74406.4453125, Mean: 9454.29296875, Std: 10150.4775390625
[DEBUG] Loss for batch 47: 0.12891758087703895
[DEBUG] Batch 48: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1005.4400024414062, Std: 1064.038818359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9322.6513671875, Std: 9866.935546875
[DEBUG] Loss for batch 48: 0.14498866106069147
[DEBUG] Batch 49: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10928.0, Mean: 1055.6446533203125, Std: 1072.338134765625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 101335.5234375, Mean: 9788.203125, Std: 9943.896484375
[DEBUG] Loss for batch 49: 0.13722889628737828
[DEBUG] Batch 50: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10880.0, Mean: 1070.5142822265625, Std: 1092.2099609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 100890.4140625, Mean: 9926.091796875, Std: 10128.1689453125
[DEBUG] Loss for batch 50: 0.13976722075547568
[DEBUG] Batch 51: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14242.0, Mean: 1062.8106689453125, Std: 1055.1383056640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 132066.5625, Mean: 9854.6552734375, Std: 9784.400390625
[DEBUG] Loss for batch 51: 0.1374638193218612
[DEBUG] Batch 52: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7772.0, Mean: 986.5425415039062, Std: 1054.9970703125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72069.625, Mean: 9147.4130859375, Std: 9783.0908203125
[DEBUG] Loss for batch 52: 0.11762200566426066
[DEBUG] Batch 53: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15976.0, Mean: 999.48779296875, Std: 1078.8236083984375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148146.125, Mean: 9267.45703125, Std: 10004.037109375
[DEBUG] Loss for batch 53: 0.1257601698973459
[DEBUG] Batch 54: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10448.0, Mean: 1088.7088623046875, Std: 1071.3768310546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96884.4375, Mean: 10094.8134765625, Std: 9934.982421875
[DEBUG] Loss for batch 54: 0.14080590052296726
[DEBUG] Batch 55: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14640.0, Mean: 1098.1561279296875, Std: 1095.431640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 135757.265625, Mean: 10182.416015625, Std: 10158.0439453125
[DEBUG] Loss for batch 55: 0.13562368579034417
[DEBUG] Batch 56: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7572.0, Mean: 1070.312255859375, Std: 1086.2513427734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70215.0, Mean: 9924.2177734375, Std: 10072.9140625
[DEBUG] Loss for batch 56: 0.1339052165280191
[DEBUG] Batch 57: Data shape: torch.Size([213, 10, 120, 120]), Labels shape: torch.Size([213, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12816.0, Mean: 986.4451904296875, Std: 1061.064697265625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 118843.1328125, Mean: 9146.5126953125, Std: 9839.3564453125
[DEBUG] Loss for batch 57: 0.1136576826064898
[INFO] Training epoch completed successfully.
[INFO] Calculating data mean and standard deviation...
[DEBUG] Batch 0: Data Min: 1.0, Max: 20608.0, Mean: 1424.058837890625, Std: 1608.755126953125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 1: Data Min: 1.0, Max: 16274.0, Mean: 1417.8494873046875, Std: 1617.936767578125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 2: Data Min: 1.0, Max: 16448.0, Mean: 1515.841796875, Std: 1671.30908203125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 3: Data Min: 1.0, Max: 20592.0, Mean: 1425.1826171875, Std: 1642.1307373046875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 4: Data Min: 1.0, Max: 19088.0, Mean: 1398.955078125, Std: 1623.8070068359375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 5: Data Min: 1.0, Max: 8480.0, Mean: 1443.3602294921875, Std: 1627.9229736328125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 6: Data Min: 1.0, Max: 11814.0, Mean: 1425.7528076171875, Std: 1609.7449951171875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 7: Data Min: 1.0, Max: 20688.0, Mean: 1439.7313232421875, Std: 1650.6259765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 8: Data Min: 1.0, Max: 17824.0, Mean: 1440.095458984375, Std: 1663.1134033203125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 9: Data Min: 1.0, Max: 8344.0, Mean: 1331.6878662109375, Std: 1612.3984375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 10: Data Min: 1.0, Max: 13264.0, Mean: 1414.01708984375, Std: 1647.2451171875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 11: Data Min: 1.0, Max: 10088.0, Mean: 1354.55908203125, Std: 1626.8204345703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 12: Data Min: 1.0, Max: 20688.0, Mean: 1429.4195556640625, Std: 1654.8055419921875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 13: Data Min: 1.0, Max: 15397.0, Mean: 1483.8021240234375, Std: 1661.35595703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 14: Data Min: 1.0, Max: 8128.0, Mean: 1338.96484375, Std: 1625.44873046875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 15: Data Min: 1.0, Max: 20672.0, Mean: 1468.43994140625, Std: 1671.7742919921875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 16: Data Min: 1.0, Max: 18448.0, Mean: 1298.22119140625, Std: 1615.575439453125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 17: Data Min: 1.0, Max: 15259.0, Mean: 1512.5281982421875, Std: 1666.949462890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 18: Data Min: 1.0, Max: 13397.0, Mean: 1437.38525390625, Std: 1666.5037841796875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 19: Data Min: 1.0, Max: 17888.0, Mean: 1464.1876220703125, Std: 1650.26708984375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 20: Data Min: 1.0, Max: 20624.0, Mean: 1505.5986328125, Std: 1657.987548828125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 21: Data Min: 1.0, Max: 17280.0, Mean: 1433.7979736328125, Std: 1652.36865234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 22: Data Min: 1.0, Max: 18720.0, Mean: 1500.33935546875, Std: 1667.444580078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 23: Data Min: 1.0, Max: 19056.0, Mean: 1383.8865966796875, Std: 1618.658447265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 24: Data Min: 1.0, Max: 20592.0, Mean: 1484.01318359375, Std: 1675.2518310546875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 25: Data Min: 1.0, Max: 20544.0, Mean: 1441.42333984375, Std: 1644.626953125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 26: Data Min: 1.0, Max: 16846.0, Mean: 1356.634765625, Std: 1612.116455078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 27: Data Min: 1.0, Max: 19056.0, Mean: 1448.08447265625, Std: 1643.6715087890625
[WARNING] Data out of normal range. Applying pre-normalization.
[INFO] Final Data Mean: 0.0928414911130536, Final Data Std: 0.11696092480904897
Epoch 1/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18512.0, Mean: 1357.0396728515625, Std: 1603.2099609375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 158274.265625, Mean: 11601.7099609375, Std: 13707.2255859375
[DEBUG] Loss for batch 0: 0.9694389453020639
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18992.0, Mean: 1345.2159423828125, Std: 1620.629150390625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162378.203125, Mean: 11500.619140625, Std: 13856.1572265625
[DEBUG] Loss for batch 1: 0.46568806184582473
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 1350.9293212890625, Std: 1624.13427734375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176057.984375, Mean: 11549.4697265625, Std: 13886.1259765625
[DEBUG] Loss for batch 2: 0.24437376149532902
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18288.0, Mean: 1327.1046142578125, Std: 1599.4149169921875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156359.09375, Mean: 11345.7685546875, Std: 13674.779296875
[DEBUG] Loss for batch 3: 0.26906750728883183
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19056.0, Mean: 1379.6024169921875, Std: 1614.3970947265625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162925.390625, Mean: 11794.619140625, Std: 13802.875
[DEBUG] Loss for batch 4: 0.2288699945022113
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15920.0, Mean: 1304.3387451171875, Std: 1611.361083984375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 136113.03125, Mean: 11151.1240234375, Std: 13776.9169921875
[DEBUG] Loss for batch 5: 0.2426846715271578
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1451.5498046875, Std: 1639.46630859375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 12409.7578125, Std: 14017.212890625
[DEBUG] Loss for batch 6: 0.23176093318133542
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20608.0, Mean: 1444.601806640625, Std: 1665.4945068359375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176194.78125, Mean: 12350.353515625, Std: 14239.7509765625
[DEBUG] Loss for batch 7: 0.19458093867301404
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17888.0, Mean: 1550.210693359375, Std: 1702.5999755859375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 152939.15625, Mean: 13253.294921875, Std: 14556.9970703125
[DEBUG] Loss for batch 8: 0.20133670058578224
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1417.1343994140625, Std: 1641.844970703125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 12115.5126953125, Std: 14037.5498046875
[DEBUG] Loss for batch 9: 0.1858175659193938
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17936.0, Mean: 1399.322265625, Std: 1655.9163818359375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 153349.546875, Mean: 11963.2216796875, Std: 14157.859375
[DEBUG] Loss for batch 10: 0.17617994663765718
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1443.694580078125, Std: 1647.15234375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 12342.59765625, Std: 14082.927734375
[DEBUG] Loss for batch 11: 0.18129706145292776
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18384.0, Mean: 1535.939208984375, Std: 1660.4161376953125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157179.890625, Mean: 13131.2783203125, Std: 14196.3310546875
[DEBUG] Loss for batch 12: 0.17309859495520888
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9864.0, Mean: 1506.2391357421875, Std: 1657.4952392578125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 84335.0546875, Mean: 12877.345703125, Std: 14171.3583984375
[DEBUG] Loss for batch 13: 0.17814302338120946
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20640.0, Mean: 1503.6900634765625, Std: 1659.3226318359375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176468.375, Mean: 12855.5498046875, Std: 14186.9814453125
[DEBUG] Loss for batch 14: 0.20658764537217444
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19056.0, Mean: 1447.1026611328125, Std: 1657.9425048828125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162925.390625, Mean: 12371.7373046875, Std: 14175.1826171875
[DEBUG] Loss for batch 15: 0.16474644700841382
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20672.0, Mean: 1440.3350830078125, Std: 1646.3253173828125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176741.96875, Mean: 12313.875, Std: 14075.8564453125
[DEBUG] Loss for batch 16: 0.17415126780004933
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20624.0, Mean: 1480.8353271484375, Std: 1663.1400146484375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176331.578125, Mean: 12660.1455078125, Std: 14219.6201171875
[DEBUG] Loss for batch 17: 0.15140312511305645
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 1472.859130859375, Std: 1643.5115966796875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176057.984375, Mean: 12591.94921875, Std: 14051.7998046875
[DEBUG] Loss for batch 18: 0.18658190578383946
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14160.0, Mean: 1393.3326416015625, Std: 1652.2413330078125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 121065.2734375, Mean: 11912.0107421875, Std: 14126.4375
[DEBUG] Loss for batch 19: 0.13881876447419803
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16024.0, Mean: 1409.98974609375, Std: 1617.48828125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 137002.21875, Mean: 12054.4248046875, Std: 13829.3037109375
[DEBUG] Loss for batch 20: 0.16293305530575983
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17888.0, Mean: 1423.6268310546875, Std: 1635.5191650390625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 152939.15625, Mean: 12171.0224609375, Std: 13983.46484375
[DEBUG] Loss for batch 21: 0.1681860940149217
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15607.0, Mean: 1495.627197265625, Std: 1648.028076171875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 133436.921875, Mean: 12786.6142578125, Std: 14090.4150390625
[DEBUG] Loss for batch 22: 0.1725854481594609
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1381.385498046875, Std: 1628.200439453125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 11809.86328125, Std: 13920.890625
[DEBUG] Loss for batch 23: 0.16390789548606075
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13726.0, Mean: 1410.869384765625, Std: 1638.586181640625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 117354.6328125, Mean: 12061.9482421875, Std: 14009.6884765625
[DEBUG] Loss for batch 24: 0.15706638581315918
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15411.0, Mean: 1422.6097412109375, Std: 1619.2784423828125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 131761.15625, Mean: 12162.326171875, Std: 13844.609375
[DEBUG] Loss for batch 25: 0.16696287336499574
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15397.0, Mean: 1520.2015380859375, Std: 1690.8475341796875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 131641.453125, Mean: 12996.7216796875, Std: 14456.515625
[DEBUG] Loss for batch 26: 0.14785754966817496
[DEBUG] Batch 27: Data shape: torch.Size([238, 10, 120, 120]), Labels shape: torch.Size([238, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18720.0, Mean: 1398.9774169921875, Std: 1633.423828125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 160052.640625, Mean: 11960.271484375, Std: 13965.55078125
[DEBUG] Loss for batch 27: 0.1378446013899618
[INFO] Training epoch completed successfully.
Epoch 2/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20608.0, Mean: 1525.594970703125, Std: 1629.798583984375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176194.78125, Mean: 13042.8359375, Std: 13934.5556640625
[DEBUG] Loss for batch 0: 0.1701387023262025
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1378.5340576171875, Std: 1651.84375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 11785.484375, Std: 14123.0390625
[DEBUG] Loss for batch 1: 0.13136522907101844
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15384.0, Mean: 1326.5377197265625, Std: 1617.8724365234375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 131530.296875, Mean: 11340.9228515625, Std: 13832.587890625
[DEBUG] Loss for batch 2: 0.135257996628335
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20672.0, Mean: 1503.729736328125, Std: 1675.0517578125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176741.96875, Mean: 12855.8876953125, Std: 14321.4638671875
[DEBUG] Loss for batch 3: 0.1373230054754182
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19056.0, Mean: 1393.35595703125, Std: 1617.8502197265625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162925.390625, Mean: 11912.2099609375, Std: 13832.3984375
[DEBUG] Loss for batch 4: 0.1316479089465605
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19056.0, Mean: 1511.92626953125, Std: 1643.5748291015625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162925.390625, Mean: 12925.96875, Std: 14052.3408203125
[DEBUG] Loss for batch 5: 0.15460577974513218
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1261.68994140625, Std: 1599.2978515625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 10786.484375, Std: 13673.7783203125
[DEBUG] Loss for batch 6: 0.11249089152881639
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18384.0, Mean: 1469.2445068359375, Std: 1662.4920654296875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157179.890625, Mean: 12561.044921875, Std: 14214.080078125
[DEBUG] Loss for batch 7: 0.1460428800032254
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15607.0, Mean: 1531.859619140625, Std: 1687.347412109375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 133436.921875, Mean: 13096.3984375, Std: 14426.58984375
[DEBUG] Loss for batch 8: 0.13625694270583535
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8188.0, Mean: 1454.7154541015625, Std: 1673.359375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 70005.484375, Mean: 12436.82421875, Std: 14306.994140625
[DEBUG] Loss for batch 9: 0.18014511906867306
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18416.0, Mean: 1381.3277587890625, Std: 1636.9306640625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157453.484375, Mean: 11809.37109375, Std: 13995.5341796875
[DEBUG] Loss for batch 10: 0.16682560193997586
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8211.0, Mean: 1347.361083984375, Std: 1588.677001953125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 70202.1328125, Mean: 11518.9599609375, Std: 13582.9716796875
[DEBUG] Loss for batch 11: 0.13565543733993563
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20640.0, Mean: 1419.02734375, Std: 1615.49560546875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176468.375, Mean: 12131.697265625, Std: 13812.2666015625
[DEBUG] Loss for batch 12: 0.16546617222447413
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18512.0, Mean: 1448.5216064453125, Std: 1649.2730712890625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 158274.265625, Mean: 12383.8671875, Std: 14101.0595703125
[DEBUG] Loss for batch 13: 0.15612776647717216
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18368.0, Mean: 1368.908935546875, Std: 1608.330322265625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157043.09375, Mean: 11703.189453125, Std: 13751.00390625
[DEBUG] Loss for batch 14: 0.15351449195720662
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1401.476806640625, Std: 1640.826171875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 11981.6396484375, Std: 14028.83984375
[DEBUG] Loss for batch 15: 0.14538874194248785
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18448.0, Mean: 1405.836181640625, Std: 1636.7467041015625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157727.078125, Mean: 12018.9140625, Std: 13993.9609375
[DEBUG] Loss for batch 16: 0.1471513633341299
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17280.0, Mean: 1500.5574951171875, Std: 1670.817138671875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 147740.828125, Mean: 12828.767578125, Std: 14285.2587890625
[DEBUG] Loss for batch 17: 0.17972683639666726
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18992.0, Mean: 1443.2540283203125, Std: 1646.3040771484375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162378.203125, Mean: 12338.83203125, Std: 14075.6748046875
[DEBUG] Loss for batch 18: 0.1436955907971597
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12266.0, Mean: 1465.68408203125, Std: 1647.64892578125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 104871.828125, Mean: 12530.6044921875, Std: 14087.1728515625
[DEBUG] Loss for batch 19: 0.14446612547432355
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 1373.4049072265625, Std: 1620.3529052734375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176057.984375, Mean: 11741.6298828125, Std: 13853.7958984375
[DEBUG] Loss for batch 20: 0.16613073283907423
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20624.0, Mean: 1504.9876708984375, Std: 1681.336669921875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176331.578125, Mean: 12866.64453125, Std: 14375.1982421875
[DEBUG] Loss for batch 21: 0.15104674358694126
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1517.3240966796875, Std: 1679.1080322265625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 12972.1201171875, Std: 14356.14453125
[DEBUG] Loss for batch 22: 0.25650883495108806
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8688.0, Mean: 1475.59765625, Std: 1640.5606689453125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 74280.421875, Mean: 12615.361328125, Std: 14026.5693359375
[DEBUG] Loss for batch 23: 0.2151468613227149
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17824.0, Mean: 1440.1583251953125, Std: 1634.26708984375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 152391.953125, Mean: 12312.36328125, Std: 13972.759765625
[DEBUG] Loss for batch 24: 0.18708720023229292
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18416.0, Mean: 1320.426025390625, Std: 1630.1435546875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157453.484375, Mean: 11288.66796875, Std: 13937.50390625
[DEBUG] Loss for batch 25: 0.15153394526022756
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 1384.1934814453125, Std: 1632.0401611328125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176057.984375, Mean: 11833.8701171875, Std: 13953.720703125
[DEBUG] Loss for batch 26: 0.1478090167704279
[DEBUG] Batch 27: Data shape: torch.Size([238, 10, 120, 120]), Labels shape: torch.Size([238, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15220.0, Mean: 1463.6793212890625, Std: 1654.180908203125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 130128.125, Mean: 12513.4638671875, Std: 14143.0205078125
[DEBUG] Loss for batch 27: 0.14663886074329505
[INFO] Training epoch completed successfully.
[INFO] Calculating data mean and standard deviation...
[DEBUG] Batch 0: Data Min: 1.0, Max: 17632.0, Mean: 1777.4051513671875, Std: 1148.863037109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 1: Data Min: 1.0, Max: 15416.0, Mean: 1795.81982421875, Std: 1157.8447265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 2: Data Min: 1.0, Max: 15480.0, Mean: 1786.5699462890625, Std: 1126.318359375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 3: Data Min: 1.0, Max: 17456.0, Mean: 1787.507568359375, Std: 1154.90380859375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 4: Data Min: 1.0, Max: 18832.0, Mean: 1804.237548828125, Std: 1143.5921630859375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 5: Data Min: 1.0, Max: 18768.0, Mean: 1774.22119140625, Std: 1118.8880615234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 6: Data Min: 1.0, Max: 15400.0, Mean: 1797.43359375, Std: 1134.39306640625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 7: Data Min: 1.0, Max: 15928.0, Mean: 1769.2183837890625, Std: 1142.080078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 8: Data Min: 1.0, Max: 15408.0, Mean: 1806.8160400390625, Std: 1143.5382080078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 9: Data Min: 1.0, Max: 16104.0, Mean: 1779.71484375, Std: 1145.9656982421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 10: Data Min: 1.0, Max: 15896.0, Mean: 1815.7042236328125, Std: 1176.61572265625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 11: Data Min: 1.0, Max: 17616.0, Mean: 1789.748779296875, Std: 1119.390625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 12: Data Min: 1.0, Max: 15968.0, Mean: 1776.6746826171875, Std: 1152.73095703125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 13: Data Min: 1.0, Max: 7500.0, Mean: 1798.8731689453125, Std: 1167.030517578125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 14: Data Min: 1.0, Max: 15824.0, Mean: 1818.4610595703125, Std: 1147.99267578125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 15: Data Min: 1.0, Max: 17584.0, Mean: 1811.0850830078125, Std: 1170.2052001953125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 16: Data Min: 1.0, Max: 16752.0, Mean: 1827.7525634765625, Std: 1156.3746337890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 17: Data Min: 1.0, Max: 18656.0, Mean: 1790.2928466796875, Std: 1135.06396484375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 18: Data Min: 1.0, Max: 18656.0, Mean: 1808.447998046875, Std: 1149.9638671875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 19: Data Min: 1.0, Max: 15912.0, Mean: 1790.762939453125, Std: 1176.8621826171875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 20: Data Min: 1.0, Max: 16800.0, Mean: 1807.17431640625, Std: 1150.6488037109375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 21: Data Min: 1.0, Max: 17552.0, Mean: 1802.988525390625, Std: 1164.212890625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 22: Data Min: 1.0, Max: 18656.0, Mean: 1788.8828125, Std: 1145.3330078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 23: Data Min: 1.0, Max: 18784.0, Mean: 1789.112060546875, Std: 1138.360107421875
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 24: Data Min: 1.0, Max: 16752.0, Mean: 1792.11181640625, Std: 1137.6771240234375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 25: Data Min: 1.0, Max: 15146.0, Mean: 1798.8922119140625, Std: 1149.6455078125
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 26: Data Min: 1.0, Max: 18832.0, Mean: 1812.984619140625, Std: 1155.7669677734375
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 27: Data Min: 1.0, Max: 18784.0, Mean: 1792.133056640625, Std: 1158.5572509765625
[WARNING] Data out of normal range. Applying pre-normalization.
[DEBUG] Batch 28: Data Min: 1.0, Max: 6644.0, Mean: 1702.9520263671875, Std: 1222.1640625
[WARNING] Data out of normal range. Applying pre-normalization.
[INFO] Final Data Mean: 0.11087724107643725, Final Data Std: 0.07796243813231303
Epoch 1/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9700.0, Mean: 1794.980224609375, Std: 1161.1658935546875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 124417.453125, Mean: 23022.228515625, Std: 14893.9140625
[DEBUG] Loss for batch 0: 0.9834815857946714
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17616.0, Mean: 1791.8785400390625, Std: 1154.2249755859375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225953.515625, Mean: 22982.447265625, Std: 14804.884765625
[DEBUG] Loss for batch 1: 0.6203142756267321
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17408.0, Mean: 1789.0909423828125, Std: 1144.133544921875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223285.578125, Mean: 22946.69140625, Std: 14675.4453125
[DEBUG] Loss for batch 2: 0.3834284381227858
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14965.0, Mean: 1810.842041015625, Std: 1153.575439453125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 191949.96875, Mean: 23225.6875, Std: 14796.5537109375
[DEBUG] Loss for batch 3: 0.31452444046135625
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17584.0, Mean: 1789.710693359375, Std: 1160.4671630859375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225543.0625, Mean: 22954.638671875, Std: 14884.951171875
[DEBUG] Loss for batch 4: 0.3038072360577297
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15035.0, Mean: 1783.85595703125, Std: 1170.61279296875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 192847.84375, Mean: 22879.544921875, Std: 15015.0869140625
[DEBUG] Loss for batch 5: 0.2736646745550651
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16800.0, Mean: 1788.3173828125, Std: 1181.5010986328125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215486.9375, Mean: 22936.765625, Std: 15154.7470703125
[DEBUG] Loss for batch 6: 0.2352230875924964
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15480.0, Mean: 1790.5587158203125, Std: 1140.6729736328125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 198555.71875, Mean: 22965.517578125, Std: 14631.056640625
[DEBUG] Loss for batch 7: 0.24156863491813088
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8056.0, Mean: 1815.9761962890625, Std: 1153.4859619140625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 103330.3828125, Mean: 23291.541015625, Std: 14795.4052734375
[DEBUG] Loss for batch 8: 0.2122473185039189
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16736.0, Mean: 1803.7740478515625, Std: 1128.2022705078125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 214666.03125, Mean: 23135.0234375, Std: 14471.099609375
[DEBUG] Loss for batch 9: 0.21520513420717732
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1808.9605712890625, Std: 1145.3114013671875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 23201.5546875, Std: 14690.552734375
[DEBUG] Loss for batch 10: 0.2170382439041622
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1806.6895751953125, Std: 1133.9581298828125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223901.25, Mean: 23172.423828125, Std: 14544.927734375
[DEBUG] Loss for batch 11: 0.22059896575637786
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17424.0, Mean: 1818.8880615234375, Std: 1152.682373046875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223490.796875, Mean: 23328.888671875, Std: 14785.09765625
[DEBUG] Loss for batch 12: 0.20908159383306207
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13976.0, Mean: 1782.06103515625, Std: 1112.9324951171875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 179264.375, Mean: 22856.5234375, Std: 14275.2392578125
[DEBUG] Loss for batch 13: 0.19056841398822763
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16832.0, Mean: 1791.4822998046875, Std: 1128.309326171875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215897.40625, Mean: 22977.361328125, Std: 14472.4736328125
[DEBUG] Loss for batch 14: 0.1924359815377943
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15129.0, Mean: 1789.822509765625, Std: 1147.3709716796875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 194053.546875, Mean: 22956.07421875, Std: 14716.9716796875
[DEBUG] Loss for batch 15: 0.20944673088938745
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17552.0, Mean: 1779.4832763671875, Std: 1140.710205078125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225132.609375, Mean: 22823.45703125, Std: 14631.5341796875
[DEBUG] Loss for batch 16: 0.16838778553264497
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18784.0, Mean: 1807.3868408203125, Std: 1169.3394775390625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240935.09375, Mean: 23181.365234375, Std: 14998.75390625
[DEBUG] Loss for batch 17: 0.1922578129967112
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15233.0, Mean: 1786.1578369140625, Std: 1130.8677978515625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 195387.515625, Mean: 22909.0703125, Std: 14505.2900390625
[DEBUG] Loss for batch 18: 0.19403512138340548
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1784.8348388671875, Std: 1154.8109130859375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 22892.1015625, Std: 14812.4013671875
[DEBUG] Loss for batch 19: 0.17575524148854152
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16752.0, Mean: 1799.67333984375, Std: 1149.017822265625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 214871.265625, Mean: 23082.427734375, Std: 14738.0947265625
[DEBUG] Loss for batch 20: 0.17710947263597482
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1805.6455078125, Std: 1151.71875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 23159.029296875, Std: 14772.73828125
[DEBUG] Loss for batch 21: 0.1819995212805362
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17520.0, Mean: 1789.78369140625, Std: 1176.1988525390625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 224722.15625, Mean: 22955.576171875, Std: 15086.736328125
[DEBUG] Loss for batch 22: 0.1849067368417656
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15872.0, Mean: 1784.6143798828125, Std: 1134.918701171875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 203583.78125, Mean: 22889.26953125, Std: 14557.25
[DEBUG] Loss for batch 23: 0.2136396961861321
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18800.0, Mean: 1800.7391357421875, Std: 1149.687255859375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241140.328125, Mean: 23096.099609375, Std: 14746.6806640625
[DEBUG] Loss for batch 24: 0.18377700383375906
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16800.0, Mean: 1783.0408935546875, Std: 1139.7445068359375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215486.9375, Mean: 22869.08984375, Std: 14619.1484375
[DEBUG] Loss for batch 25: 0.19687734061872825
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18752.0, Mean: 1806.5736083984375, Std: 1153.5980224609375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240524.640625, Mean: 23170.935546875, Std: 14796.84375
[DEBUG] Loss for batch 26: 0.1650805453260799
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18768.0, Mean: 1800.945556640625, Std: 1156.992431640625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240729.875, Mean: 23098.744140625, Std: 14840.3818359375
[DEBUG] Loss for batch 27: 0.1796128697374145
[DEBUG] Batch 28: Data shape: torch.Size([12, 10, 120, 120]), Labels shape: torch.Size([12, 19])
[DEBUG] Data before standardization: Min: 69.0, Max: 7728.0, Mean: 1815.147705078125, Std: 1085.446533203125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 883.6193237304688, Max: 99123.2265625, Mean: 23280.912109375, Std: 13922.685546875
[DEBUG] Loss for batch 28: 0.38366254563007124
[INFO] Training epoch completed successfully.
Epoch 2/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15920.0, Mean: 1813.5418701171875, Std: 1145.1431884765625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 204199.453125, Mean: 23260.314453125, Std: 14688.3955078125
[DEBUG] Loss for batch 0: 0.20008021076880908
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17584.0, Mean: 1795.3074951171875, Std: 1156.510498046875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225543.0625, Mean: 23026.4296875, Std: 14834.2001953125
[DEBUG] Loss for batch 1: 0.21452343917006916
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18800.0, Mean: 1803.4571533203125, Std: 1145.95849609375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241140.328125, Mean: 23130.9609375, Std: 14698.853515625
[DEBUG] Loss for batch 2: 0.1712309693575879
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15059.0, Mean: 1794.3521728515625, Std: 1132.4083251953125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 193155.671875, Mean: 23014.171875, Std: 14525.048828125
[DEBUG] Loss for batch 3: 0.1564218232234418
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13976.0, Mean: 1794.1273193359375, Std: 1113.170654296875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 179264.375, Mean: 23011.291015625, Std: 14278.2939453125
[DEBUG] Loss for batch 4: 0.19196120915110923
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16784.0, Mean: 1791.3900146484375, Std: 1155.5379638671875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215281.71875, Mean: 22976.1796875, Std: 14821.7255859375
[DEBUG] Loss for batch 5: 0.18129504867167157
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15416.0, Mean: 1802.6199951171875, Std: 1144.1292724609375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 197734.8125, Mean: 23120.22265625, Std: 14675.3896484375
[DEBUG] Loss for batch 6: 0.1946510003928007
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17424.0, Mean: 1829.5711669921875, Std: 1168.589599609375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223490.796875, Mean: 23465.919921875, Std: 14989.1357421875
[DEBUG] Loss for batch 7: 0.17699744375231055
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15223.0, Mean: 1809.423828125, Std: 1161.173583984375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 195259.25, Mean: 23207.494140625, Std: 14894.0126953125
[DEBUG] Loss for batch 8: 0.181280100350293
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18784.0, Mean: 1807.2291259765625, Std: 1168.565673828125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240935.09375, Mean: 23179.34375, Std: 14988.8291015625
[DEBUG] Loss for batch 9: 0.1780038614635968
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18768.0, Mean: 1797.383056640625, Std: 1132.6524658203125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240729.875, Mean: 23053.0546875, Std: 14528.1806640625
[DEBUG] Loss for batch 10: 0.17270741757484206
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1802.616943359375, Std: 1149.6793212890625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 23120.18359375, Std: 14746.5791015625
[DEBUG] Loss for batch 11: 0.1660602382928571
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17520.0, Mean: 1784.2401123046875, Std: 1151.5919189453125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 224722.15625, Mean: 22884.470703125, Std: 14771.1123046875
[DEBUG] Loss for batch 12: 0.166278218433917
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15872.0, Mean: 1779.6697998046875, Std: 1141.4666748046875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 203583.78125, Mean: 22825.84765625, Std: 14641.2373046875
[DEBUG] Loss for batch 13: 0.16737009601050815
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16736.0, Mean: 1790.0550537109375, Std: 1145.9365234375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 214666.03125, Mean: 22959.05859375, Std: 14698.572265625
[DEBUG] Loss for batch 14: 0.16234833356576242
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15536.0, Mean: 1791.18798828125, Std: 1117.0848388671875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 199274.015625, Mean: 22973.591796875, Std: 14328.5
[DEBUG] Loss for batch 15: 0.18401854003130874
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15400.0, Mean: 1764.98291015625, Std: 1143.171142578125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 197529.578125, Mean: 22637.46484375, Std: 14663.1005859375
[DEBUG] Loss for batch 16: 0.1721059413386953
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18784.0, Mean: 1802.0596923828125, Std: 1157.6312255859375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240935.09375, Mean: 23113.037109375, Std: 14848.5751953125
[DEBUG] Loss for batch 17: 0.16545884580449022
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18752.0, Mean: 1796.0379638671875, Std: 1153.3314208984375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240524.640625, Mean: 23035.80078125, Std: 14793.4228515625
[DEBUG] Loss for batch 18: 0.1573431708605966
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1808.41064453125, Std: 1172.8912353515625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 23194.49609375, Std: 15044.310546875
[DEBUG] Loss for batch 19: 0.1587503069502874
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16800.0, Mean: 1793.77490234375, Std: 1148.090087890625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215486.9375, Mean: 23006.76953125, Std: 14726.1943359375
[DEBUG] Loss for batch 20: 0.17079914863955387
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1801.513916015625, Std: 1160.9117431640625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 23106.037109375, Std: 14890.6533203125
[DEBUG] Loss for batch 21: 0.18469459585760795
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17616.0, Mean: 1780.62109375, Std: 1128.2431640625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225953.515625, Mean: 22838.05078125, Std: 14471.625
[DEBUG] Loss for batch 22: 0.1689337451298565
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1812.24853515625, Std: 1140.231201171875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 23243.7265625, Std: 14625.3916015625
[DEBUG] Loss for batch 23: 0.1701600275927251
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16832.0, Mean: 1772.2147216796875, Std: 1139.627685546875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215897.40625, Mean: 22730.228515625, Std: 14617.650390625
[DEBUG] Loss for batch 24: 0.15866486522914536
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15480.0, Mean: 1786.3330078125, Std: 1158.6904296875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 198555.71875, Mean: 22911.314453125, Std: 14862.1611328125
[DEBUG] Loss for batch 25: 0.17340738704789496
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17408.0, Mean: 1786.8834228515625, Std: 1154.14306640625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223285.578125, Mean: 22918.375, Std: 14803.8330078125
[DEBUG] Loss for batch 26: 0.1668565189466354
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15896.0, Mean: 1794.8165283203125, Std: 1186.898193359375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 203891.625, Mean: 23020.130859375, Std: 15223.9736328125
[DEBUG] Loss for batch 27: 0.14910134297981856
[DEBUG] Batch 28: Data shape: torch.Size([12, 10, 120, 120]), Labels shape: torch.Size([12, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16800.0, Mean: 1808.68994140625, Std: 1125.9649658203125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215486.9375, Mean: 23198.080078125, Std: 14442.40234375
[DEBUG] Loss for batch 28: 0.14840058216769217
[INFO] Training epoch completed successfully.
[INFO] Starting validation for Round 1...
[DEBUG] Batch 0: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 4038.0, Mean: 208.4311065673828, Std: 119.26319122314453
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 40344.99609375, Mean: 2081.574951171875, Std: 1191.626953125
[DEBUG] Top-3 class probabilities:
tensor([[0.6946, 0.2849, 0.2237],
        [0.6948, 0.2843, 0.2232],
        [0.6940, 0.2840, 0.2235],
        [0.6938, 0.2844, 0.2237],
        [0.6936, 0.2850, 0.2237],
        [0.6935, 0.2846, 0.2235],
        [0.6937, 0.2840, 0.2239],
        [0.6929, 0.2852, 0.2238],
        [0.6934, 0.2838, 0.2230],
        [0.6938, 0.2833, 0.2226],
        [0.6927, 0.2832, 0.2232],
        [0.6934, 0.2826, 0.2232],
        [0.6933, 0.2829, 0.2232],
        [0.6936, 0.2827, 0.2226],
        [0.6930, 0.2829, 0.2233],
        [0.6921, 0.2826, 0.2236],
        [0.6925, 0.2834, 0.2234],
        [0.6944, 0.2818, 0.2220],
        [0.6953, 0.2802, 0.2215],
        [0.6950, 0.2810, 0.2214],
        [0.6953, 0.2808, 0.2213],
        [0.6946, 0.2804, 0.2216],
        [0.6963, 0.2798, 0.2210],
        [0.6947, 0.2797, 0.2208],
        [0.6964, 0.2789, 0.2205],
        [0.6964, 0.2776, 0.2198],
        [0.6993, 0.2751, 0.2179],
        [0.7108, 0.2669, 0.2169],
        [0.6982, 0.2704, 0.2229],
        [0.6996, 0.2692, 0.2159],
        [0.6911, 0.2663, 0.2242],
        [0.6976, 0.2730, 0.2288],
        [0.6982, 0.2771, 0.2193],
        [0.6969, 0.2733, 0.2185],
        [0.6976, 0.2716, 0.2179],
        [0.6995, 0.2688, 0.2169],
        [0.6945, 0.2853, 0.2238],
        [0.6944, 0.2847, 0.2237],
        [0.6945, 0.2835, 0.2231],
        [0.6937, 0.2847, 0.2238],
        [0.6931, 0.2854, 0.2242],
        [0.6944, 0.2853, 0.2237],
        [0.6941, 0.2851, 0.2236],
        [0.6939, 0.2845, 0.2235],
        [0.6923, 0.2846, 0.2242],
        [0.6930, 0.2841, 0.2231],
        [0.6938, 0.2828, 0.2230],
        [0.6930, 0.2825, 0.2231],
        [0.6926, 0.2830, 0.2232],
        [0.6924, 0.2833, 0.2234],
        [0.6927, 0.2826, 0.2231],
        [0.6931, 0.2822, 0.2233],
        [0.6929, 0.2822, 0.2233],
        [0.6933, 0.2832, 0.2230],
        [0.6938, 0.2815, 0.2227],
        [0.6934, 0.2813, 0.2229],
        [0.6947, 0.2802, 0.2215],
        [0.6947, 0.2808, 0.2213],
        [0.6952, 0.2801, 0.2211],
        [0.6961, 0.2796, 0.2206],
        [0.6952, 0.2810, 0.2215],
        [0.6951, 0.2792, 0.2212],
        [0.6963, 0.2786, 0.2208],
        [0.6980, 0.2740, 0.2178],
        [0.7046, 0.2667, 0.2142],
        [0.6969, 0.2720, 0.2282],
        [0.6988, 0.2706, 0.2171],
        [0.7034, 0.2733, 0.2223],
        [0.6918, 0.2700, 0.2396],
        [0.7110, 0.2618, 0.2210],
        [0.7034, 0.2696, 0.2158],
        [0.6986, 0.2701, 0.2171],
        [0.6891, 0.2705, 0.2323],
        [0.6933, 0.2845, 0.2237],
        [0.6942, 0.2846, 0.2236],
        [0.6939, 0.2844, 0.2235],
        [0.6935, 0.2845, 0.2238],
        [0.6933, 0.2849, 0.2239],
        [0.6938, 0.2853, 0.2235],
        [0.6935, 0.2854, 0.2239],
        [0.6934, 0.2838, 0.2238],
        [0.6938, 0.2830, 0.2235],
        [0.6939, 0.2835, 0.2229],
        [0.6931, 0.2825, 0.2226],
        [0.6931, 0.2816, 0.2228],
        [0.6937, 0.2832, 0.2230],
        [0.6930, 0.2828, 0.2226],
        [0.6939, 0.2818, 0.2226],
        [0.6923, 0.2821, 0.2233],
        [0.6933, 0.2821, 0.2226],
        [0.6943, 0.2817, 0.2224],
        [0.6956, 0.2814, 0.2220],
        [0.6939, 0.2811, 0.2222],
        [0.6932, 0.2802, 0.2216],
        [0.6949, 0.2801, 0.2215],
        [0.6947, 0.2805, 0.2213],
        [0.6948, 0.2810, 0.2215],
        [0.6953, 0.2814, 0.2215],
        [0.6951, 0.2795, 0.2212],
        [0.6967, 0.2784, 0.2201],
        [0.7004, 0.2738, 0.2169],
        [0.7006, 0.2711, 0.2168],
        [0.7007, 0.2710, 0.2195],
        [0.6993, 0.2717, 0.2173],
        [0.6985, 0.2693, 0.2203],
        [0.7023, 0.2686, 0.2181],
        [0.7002, 0.2769, 0.2225],
        [0.7005, 0.2771, 0.2195],
        [0.7030, 0.2816, 0.2183],
        [0.6997, 0.2830, 0.2222],
        [0.6947, 0.2835, 0.2234],
        [0.6936, 0.2834, 0.2235],
        [0.6934, 0.2838, 0.2238],
        [0.6940, 0.2834, 0.2232],
        [0.6933, 0.2854, 0.2239],
        [0.6936, 0.2858, 0.2242],
        [0.6935, 0.2843, 0.2238],
        [0.6930, 0.2838, 0.2236],
        [0.6931, 0.2837, 0.2234],
        [0.6937, 0.2822, 0.2228],
        [0.6935, 0.2823, 0.2224],
        [0.6927, 0.2826, 0.2230],
        [0.6939, 0.2824, 0.2222],
        [0.6940, 0.2816, 0.2224],
        [0.6940, 0.2813, 0.2221],
        [0.6925, 0.2823, 0.2229],
        [0.6934, 0.2821, 0.2225],
        [0.6941, 0.2813, 0.2225]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 1: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 5296.0, Mean: 301.6671142578125, Std: 399.45068359375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 52914.3984375, Mean: 3013.1494140625, Std: 3991.14111328125
[DEBUG] Top-3 class probabilities:
tensor([[0.6938, 0.2817, 0.2220],
        [0.6948, 0.2802, 0.2216],
        [0.6953, 0.2794, 0.2206],
        [0.6961, 0.2802, 0.2207],
        [0.6954, 0.2806, 0.2213],
        [0.6951, 0.2809, 0.2217],
        [0.6969, 0.2779, 0.2199],
        [0.6970, 0.2772, 0.2191],
        [0.6936, 0.2764, 0.2229],
        [0.6909, 0.2739, 0.2296],
        [0.7014, 0.2739, 0.2167],
        [0.7020, 0.2733, 0.2193],
        [0.7014, 0.2802, 0.2280],
        [0.6988, 0.2821, 0.2280],
        [0.7032, 0.2717, 0.2164],
        [0.6993, 0.2760, 0.2207],
        [0.6973, 0.3085, 0.2454],
        [0.6835, 0.2793, 0.2608],
        [0.6524, 0.2713, 0.2602],
        [0.6928, 0.2831, 0.2234],
        [0.6934, 0.2836, 0.2233],
        [0.6938, 0.2840, 0.2234],
        [0.6930, 0.2846, 0.2241],
        [0.6940, 0.2855, 0.2234],
        [0.6938, 0.2864, 0.2239],
        [0.6946, 0.2841, 0.2230],
        [0.6931, 0.2839, 0.2233],
        [0.6940, 0.2830, 0.2228],
        [0.6934, 0.2822, 0.2227],
        [0.6939, 0.2818, 0.2225],
        [0.6940, 0.2820, 0.2226],
        [0.6941, 0.2814, 0.2221],
        [0.6945, 0.2813, 0.2217],
        [0.6936, 0.2812, 0.2222],
        [0.6942, 0.2812, 0.2220],
        [0.6948, 0.2809, 0.2215],
        [0.6953, 0.2819, 0.2213],
        [0.6943, 0.2813, 0.2220],
        [0.6952, 0.2792, 0.2211],
        [0.6972, 0.2785, 0.2201],
        [0.6968, 0.2779, 0.2198],
        [0.6962, 0.2808, 0.2207],
        [0.6988, 0.2769, 0.2192],
        [0.6978, 0.2724, 0.2230],
        [0.6907, 0.2789, 0.2252],
        [0.6529, 0.2835, 0.2471],
        [0.6494, 0.2854, 0.2643],
        [0.6999, 0.2890, 0.2229],
        [0.7028, 0.2749, 0.2167],
        [0.7032, 0.2758, 0.2156],
        [0.6988, 0.2705, 0.2228],
        [0.7090, 0.2764, 0.2232],
        [0.6806, 0.2612, 0.2608],
        [0.5718, 0.3226, 0.2749],
        [0.5047, 0.3573, 0.3279],
        [0.5246, 0.3539, 0.3091],
        [0.6942, 0.2827, 0.2230],
        [0.6934, 0.2832, 0.2233],
        [0.6930, 0.2839, 0.2235],
        [0.6931, 0.2844, 0.2233],
        [0.6947, 0.2847, 0.2229],
        [0.6944, 0.2844, 0.2231],
        [0.6939, 0.2839, 0.2231],
        [0.6936, 0.2841, 0.2231],
        [0.6942, 0.2816, 0.2223],
        [0.6936, 0.2819, 0.2223],
        [0.6943, 0.2814, 0.2218],
        [0.6944, 0.2803, 0.2220],
        [0.6932, 0.2808, 0.2221],
        [0.6943, 0.2817, 0.2219],
        [0.6943, 0.2804, 0.2217],
        [0.6942, 0.2820, 0.2219],
        [0.6942, 0.2824, 0.2222],
        [0.6956, 0.2814, 0.2213],
        [0.6955, 0.2805, 0.2211],
        [0.6956, 0.2785, 0.2204],
        [0.6968, 0.2785, 0.2196],
        [0.6970, 0.2793, 0.2198],
        [0.6973, 0.2791, 0.2197],
        [0.7004, 0.2747, 0.2189],
        [0.7107, 0.2585, 0.2399],
        [0.7140, 0.2852, 0.2339],
        [0.6888, 0.2625, 0.2588],
        [0.6377, 0.2839, 0.2425],
        [0.7063, 0.2770, 0.2169],
        [0.7022, 0.2716, 0.2159],
        [0.7036, 0.2720, 0.2144],
        [0.7001, 0.2817, 0.2339],
        [0.6985, 0.2936, 0.2401],
        [0.6103, 0.3097, 0.2695],
        [0.5592, 0.3451, 0.2971],
        [0.5892, 0.3232, 0.2636],
        [0.6765, 0.2706, 0.2482],
        [0.6934, 0.2828, 0.2230],
        [0.6938, 0.2837, 0.2227],
        [0.6943, 0.2839, 0.2232],
        [0.6949, 0.2844, 0.2231],
        [0.6952, 0.2846, 0.2226],
        [0.6942, 0.2840, 0.2232],
        [0.6936, 0.2826, 0.2227],
        [0.6944, 0.2824, 0.2224],
        [0.6947, 0.2812, 0.2223],
        [0.6942, 0.2822, 0.2227],
        [0.6942, 0.2814, 0.2219],
        [0.6946, 0.2805, 0.2214],
        [0.6945, 0.2798, 0.2211],
        [0.6941, 0.2800, 0.2218],
        [0.6942, 0.2800, 0.2215],
        [0.6942, 0.2814, 0.2218],
        [0.6949, 0.2821, 0.2215],
        [0.6968, 0.2808, 0.2208],
        [0.6975, 0.2791, 0.2196],
        [0.7001, 0.2753, 0.2176],
        [0.7011, 0.2731, 0.2238],
        [0.6945, 0.2667, 0.2457],
        [0.6810, 0.2794, 0.2475],
        [0.6964, 0.2831, 0.2271],
        [0.7036, 0.2629, 0.2374],
        [0.6944, 0.2751, 0.2275],
        [0.7063, 0.2807, 0.2218],
        [0.7074, 0.2815, 0.2199],
        [0.7013, 0.2727, 0.2162],
        [0.6995, 0.2765, 0.2258],
        [0.7056, 0.2729, 0.2305],
        [0.6942, 0.2931, 0.2466],
        [0.6810, 0.2670, 0.2515],
        [0.6601, 0.2892, 0.2469],
        [0.6521, 0.2744, 0.2728]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 2: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 4772.0, Mean: 297.2980041503906, Std: 390.6563415527344
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 47678.8125, Mean: 2969.49462890625, Std: 3903.271728515625
[DEBUG] Top-3 class probabilities:
tensor([[0.6908, 0.2714, 0.2387],
        [0.6767, 0.2650, 0.2549],
        [0.6937, 0.2824, 0.2228],
        [0.6935, 0.2828, 0.2230],
        [0.6940, 0.2835, 0.2231],
        [0.6939, 0.2840, 0.2228],
        [0.6949, 0.2831, 0.2227],
        [0.6935, 0.2842, 0.2228],
        [0.6949, 0.2827, 0.2227],
        [0.6932, 0.2829, 0.2231],
        [0.6946, 0.2817, 0.2220],
        [0.6938, 0.2821, 0.2223],
        [0.6952, 0.2818, 0.2217],
        [0.6950, 0.2809, 0.2218],
        [0.6943, 0.2803, 0.2215],
        [0.6942, 0.2801, 0.2217],
        [0.6938, 0.2795, 0.2213],
        [0.6950, 0.2810, 0.2218],
        [0.6954, 0.2816, 0.2214],
        [0.6949, 0.2819, 0.2216],
        [0.6974, 0.2799, 0.2208],
        [0.6979, 0.2782, 0.2189],
        [0.7059, 0.2597, 0.2219],
        [0.6959, 0.2763, 0.2337],
        [0.6479, 0.2904, 0.2462],
        [0.5973, 0.3148, 0.2675],
        [0.6880, 0.2758, 0.2544],
        [0.6982, 0.2720, 0.2329],
        [0.6906, 0.2826, 0.2285],
        [0.7039, 0.2769, 0.2166],
        [0.6798, 0.2661, 0.2433],
        [0.7004, 0.2725, 0.2212],
        [0.6943, 0.2734, 0.2242],
        [0.7053, 0.2689, 0.2146],
        [0.7029, 0.2762, 0.2178],
        [0.7047, 0.2735, 0.2165],
        [0.7129, 0.2807, 0.2219],
        [0.5565, 0.3307, 0.3020],
        [0.5067, 0.3727, 0.3341],
        [0.5559, 0.3312, 0.2799],
        [0.6937, 0.2830, 0.2224],
        [0.6935, 0.2827, 0.2227],
        [0.6941, 0.2830, 0.2225],
        [0.6938, 0.2826, 0.2231],
        [0.6941, 0.2831, 0.2227],
        [0.6938, 0.2826, 0.2223],
        [0.6941, 0.2830, 0.2222],
        [0.6940, 0.2818, 0.2222],
        [0.6949, 0.2822, 0.2216],
        [0.6944, 0.2827, 0.2223],
        [0.6950, 0.2812, 0.2218],
        [0.6946, 0.2805, 0.2218],
        [0.6950, 0.2797, 0.2211],
        [0.6945, 0.2802, 0.2212],
        [0.6951, 0.2792, 0.2208],
        [0.6965, 0.2802, 0.2206],
        [0.6955, 0.2831, 0.2215],
        [0.6966, 0.2827, 0.2215],
        [0.6985, 0.2816, 0.2206],
        [0.7029, 0.2798, 0.2183],
        [0.7059, 0.2740, 0.2158],
        [0.7068, 0.2796, 0.2193],
        [0.6969, 0.2853, 0.2478],
        [0.6825, 0.2776, 0.2394],
        [0.7114, 0.2712, 0.2337],
        [0.7075, 0.2775, 0.2310],
        [0.7020, 0.2750, 0.2190],
        [0.6855, 0.2716, 0.2318],
        [0.7008, 0.2780, 0.2179],
        [0.7044, 0.2693, 0.2153],
        [0.6953, 0.2558, 0.2345],
        [0.7053, 0.2701, 0.2140],
        [0.7084, 0.2664, 0.2120],
        [0.7055, 0.2694, 0.2209],
        [0.7094, 0.2731, 0.2211],
        [0.6306, 0.3014, 0.2572],
        [0.5782, 0.3325, 0.2724],
        [0.5330, 0.3421, 0.3127],
        [0.6944, 0.2825, 0.2222],
        [0.6940, 0.2818, 0.2223],
        [0.6941, 0.2818, 0.2225],
        [0.6948, 0.2823, 0.2219],
        [0.6946, 0.2819, 0.2223],
        [0.6947, 0.2825, 0.2224],
        [0.6944, 0.2826, 0.2223],
        [0.6946, 0.2812, 0.2218],
        [0.6951, 0.2818, 0.2217],
        [0.6947, 0.2812, 0.2216],
        [0.6950, 0.2811, 0.2219],
        [0.6951, 0.2802, 0.2207],
        [0.6950, 0.2798, 0.2214],
        [0.6950, 0.2797, 0.2207],
        [0.6950, 0.2805, 0.2213],
        [0.6971, 0.2840, 0.2220],
        [0.6999, 0.2840, 0.2212],
        [0.6999, 0.2824, 0.2205],
        [0.6999, 0.2807, 0.2197],
        [0.7010, 0.2796, 0.2194],
        [0.7033, 0.2780, 0.2171],
        [0.6759, 0.2810, 0.2623],
        [0.6684, 0.2757, 0.2653],
        [0.6989, 0.3002, 0.2468],
        [0.6770, 0.2705, 0.2544],
        [0.6973, 0.2868, 0.2393],
        [0.6769, 0.2627, 0.2445],
        [0.6743, 0.2688, 0.2421],
        [0.7038, 0.2724, 0.2156],
        [0.7023, 0.2714, 0.2160],
        [0.7026, 0.2707, 0.2156],
        [0.7018, 0.2707, 0.2158],
        [0.7083, 0.2664, 0.2118],
        [0.7072, 0.2630, 0.2132],
        [0.7023, 0.2597, 0.2163],
        [0.6966, 0.2540, 0.2451],
        [0.7021, 0.2702, 0.2214],
        [0.6211, 0.3072, 0.2443],
        [0.6946, 0.2818, 0.2221],
        [0.6950, 0.2816, 0.2215],
        [0.6948, 0.2806, 0.2217],
        [0.6949, 0.2820, 0.2214],
        [0.6951, 0.2817, 0.2217],
        [0.6938, 0.2818, 0.2223],
        [0.6938, 0.2821, 0.2220],
        [0.6952, 0.2819, 0.2221],
        [0.6955, 0.2824, 0.2214],
        [0.6956, 0.2820, 0.2218],
        [0.6957, 0.2805, 0.2209],
        [0.6952, 0.2794, 0.2208]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 3: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 5384.0, Mean: 240.40733337402344, Std: 244.81922912597656
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 53793.65625, Mean: 2401.067138671875, Std: 2446.12939453125
[DEBUG] Top-3 class probabilities:
tensor([[0.6954, 0.2815, 0.2210],
        [0.6983, 0.2822, 0.2212],
        [0.6990, 0.2828, 0.2207],
        [0.6999, 0.2837, 0.2211],
        [0.6986, 0.2838, 0.2216],
        [0.6989, 0.2823, 0.2209],
        [0.7001, 0.2817, 0.2201],
        [0.7015, 0.2791, 0.2191],
        [0.7016, 0.2783, 0.2185],
        [0.7000, 0.2611, 0.2298],
        [0.5949, 0.3201, 0.2767],
        [0.5721, 0.3295, 0.2869],
        [0.6569, 0.2728, 0.2571],
        [0.6555, 0.2661, 0.2493],
        [0.7118, 0.2882, 0.2247],
        [0.7057, 0.2714, 0.2224],
        [0.7046, 0.2723, 0.2192],
        [0.7000, 0.2726, 0.2168],
        [0.6998, 0.2727, 0.2171],
        [0.6995, 0.2721, 0.2171],
        [0.7046, 0.2696, 0.2139],
        [0.7066, 0.2649, 0.2169],
        [0.7044, 0.2650, 0.2170],
        [0.7083, 0.2484, 0.2183],
        [0.6524, 0.2780, 0.2401],
        [0.7043, 0.2929, 0.2319],
        [0.6947, 0.2809, 0.2215],
        [0.6956, 0.2805, 0.2215],
        [0.6954, 0.2813, 0.2213],
        [0.6953, 0.2814, 0.2218],
        [0.6949, 0.2816, 0.2217],
        [0.6945, 0.2819, 0.2217],
        [0.6956, 0.2820, 0.2213],
        [0.6964, 0.2809, 0.2206],
        [0.6966, 0.2822, 0.2211],
        [0.6964, 0.2830, 0.2212],
        [0.6982, 0.2815, 0.2208],
        [0.6980, 0.2817, 0.2208],
        [0.6984, 0.2825, 0.2207],
        [0.6975, 0.2844, 0.2220],
        [0.6988, 0.2844, 0.2214],
        [0.6994, 0.2827, 0.2209],
        [0.6980, 0.2814, 0.2206],
        [0.6984, 0.2818, 0.2209],
        [0.7008, 0.2834, 0.2194],
        [0.7082, 0.2738, 0.2152],
        [0.7005, 0.2713, 0.2181],
        [0.7051, 0.2678, 0.2208],
        [0.6974, 0.2513, 0.2457],
        [0.6896, 0.2659, 0.2296],
        [0.7084, 0.2680, 0.2147],
        [0.7085, 0.2681, 0.2180],
        [0.7010, 0.2723, 0.2163],
        [0.6990, 0.2741, 0.2180],
        [0.6999, 0.2726, 0.2173],
        [0.6973, 0.2745, 0.2188],
        [0.6981, 0.2743, 0.2186],
        [0.6999, 0.2728, 0.2174],
        [0.7030, 0.2700, 0.2150],
        [0.7012, 0.2706, 0.2161],
        [0.7041, 0.2693, 0.2153],
        [0.6670, 0.2651, 0.2507],
        [0.6837, 0.2772, 0.2427],
        [0.6947, 0.2812, 0.2219],
        [0.6953, 0.2806, 0.2213],
        [0.6956, 0.2808, 0.2215],
        [0.6954, 0.2812, 0.2217],
        [0.6958, 0.2812, 0.2212],
        [0.6949, 0.2828, 0.2216],
        [0.6968, 0.2849, 0.2219],
        [0.6976, 0.2835, 0.2217],
        [0.6982, 0.2835, 0.2210],
        [0.6983, 0.2827, 0.2210],
        [0.6979, 0.2825, 0.2209],
        [0.6981, 0.2816, 0.2212],
        [0.6982, 0.2828, 0.2211],
        [0.6981, 0.2831, 0.2215],
        [0.6977, 0.2842, 0.2222],
        [0.6990, 0.2824, 0.2208],
        [0.6982, 0.2814, 0.2205],
        [0.6987, 0.2813, 0.2204],
        [0.6996, 0.2810, 0.2204],
        [0.7044, 0.2772, 0.2144],
        [0.7018, 0.2752, 0.2181],
        [0.7032, 0.2755, 0.2167],
        [0.7002, 0.2751, 0.2176],
        [0.7001, 0.2736, 0.2173],
        [0.6992, 0.2748, 0.2181],
        [0.7003, 0.2744, 0.2175],
        [0.6989, 0.2741, 0.2177],
        [0.6992, 0.2736, 0.2175],
        [0.6971, 0.2756, 0.2193],
        [0.6974, 0.2763, 0.2194],
        [0.6974, 0.2766, 0.2195],
        [0.6976, 0.2762, 0.2193],
        [0.6991, 0.2752, 0.2183],
        [0.7006, 0.2729, 0.2171],
        [0.6998, 0.2722, 0.2174],
        [0.7021, 0.2698, 0.2162],
        [0.7008, 0.2692, 0.2162],
        [0.6940, 0.2812, 0.2224],
        [0.6940, 0.2811, 0.2218],
        [0.6945, 0.2816, 0.2214],
        [0.6959, 0.2815, 0.2213],
        [0.6969, 0.2826, 0.2212],
        [0.6982, 0.2837, 0.2212],
        [0.6993, 0.2831, 0.2210],
        [0.6987, 0.2829, 0.2211],
        [0.6986, 0.2832, 0.2211],
        [0.6982, 0.2819, 0.2209],
        [0.6981, 0.2819, 0.2210],
        [0.6981, 0.2821, 0.2211],
        [0.6980, 0.2827, 0.2211],
        [0.6977, 0.2819, 0.2214],
        [0.6979, 0.2831, 0.2211],
        [0.6990, 0.2831, 0.2213],
        [0.6992, 0.2818, 0.2208],
        [0.6987, 0.2823, 0.2213],
        [0.6985, 0.2828, 0.2214],
        [0.6976, 0.2809, 0.2206],
        [0.6982, 0.2818, 0.2207],
        [0.6995, 0.2786, 0.2205],
        [0.7053, 0.2729, 0.2187],
        [0.7030, 0.2767, 0.2178],
        [0.6990, 0.2780, 0.2189],
        [0.7008, 0.2766, 0.2184],
        [0.7000, 0.2751, 0.2180],
        [0.6993, 0.2747, 0.2181]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 4: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 4006.0, Mean: 205.72503662109375, Std: 125.69835662841797
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 40025.265625, Mean: 2054.53662109375, Std: 1255.9244384765625
[DEBUG] Top-3 class probabilities:
tensor([[0.6983, 0.2759, 0.2188],
        [0.6983, 0.2773, 0.2194],
        [0.6987, 0.2765, 0.2189],
        [0.6986, 0.2765, 0.2190],
        [0.6985, 0.2755, 0.2184],
        [0.7000, 0.2740, 0.2175],
        [0.6987, 0.2747, 0.2183],
        [0.7038, 0.2761, 0.2175],
        [0.6753, 0.2601, 0.2535],
        [0.6678, 0.2652, 0.2501],
        [0.6948, 0.2825, 0.2221],
        [0.6951, 0.2803, 0.2214],
        [0.6945, 0.2811, 0.2215],
        [0.6955, 0.2831, 0.2213],
        [0.6969, 0.2844, 0.2218],
        [0.6995, 0.2831, 0.2212],
        [0.6993, 0.2823, 0.2212],
        [0.6989, 0.2830, 0.2209],
        [0.6989, 0.2831, 0.2205],
        [0.6978, 0.2831, 0.2215],
        [0.6976, 0.2834, 0.2214],
        [0.6980, 0.2817, 0.2211],
        [0.6979, 0.2823, 0.2215],
        [0.6980, 0.2824, 0.2216],
        [0.6982, 0.2838, 0.2218],
        [0.6983, 0.2823, 0.2213],
        [0.6985, 0.2826, 0.2210],
        [0.6973, 0.2821, 0.2213],
        [0.6980, 0.2828, 0.2210],
        [0.6991, 0.2835, 0.2213],
        [0.6988, 0.2823, 0.2206],
        [0.6990, 0.2814, 0.2208],
        [0.6983, 0.2801, 0.2206],
        [0.6996, 0.2801, 0.2196],
        [0.6978, 0.2803, 0.2203],
        [0.7010, 0.2779, 0.2184],
        [0.6998, 0.2781, 0.2192],
        [0.6988, 0.2784, 0.2193],
        [0.6988, 0.2774, 0.2191],
        [0.6983, 0.2763, 0.2192],
        [0.6985, 0.2770, 0.2190],
        [0.6986, 0.2774, 0.2188],
        [0.6997, 0.2762, 0.2185],
        [0.6988, 0.2765, 0.2188],
        [0.6992, 0.2756, 0.2182],
        [0.6986, 0.2764, 0.2190],
        [0.6990, 0.2759, 0.2185],
        [0.6990, 0.2760, 0.2190],
        [0.6730, 0.2568, 0.2532],
        [0.6684, 0.2525, 0.2497],
        [0.6949, 0.2813, 0.2217],
        [0.6940, 0.2820, 0.2223],
        [0.6945, 0.2831, 0.2223],
        [0.6944, 0.2835, 0.2224],
        [0.6963, 0.2831, 0.2220],
        [0.6974, 0.2832, 0.2216],
        [0.6985, 0.2833, 0.2214],
        [0.6986, 0.2840, 0.2218],
        [0.6995, 0.2831, 0.2208],
        [0.6983, 0.2829, 0.2207],
        [0.6988, 0.2827, 0.2212],
        [0.6979, 0.2829, 0.2214],
        [0.6994, 0.2817, 0.2209],
        [0.6977, 0.2821, 0.2211],
        [0.6981, 0.2825, 0.2211],
        [0.6986, 0.2834, 0.2215],
        [0.6984, 0.2827, 0.2216],
        [0.6989, 0.2825, 0.2211],
        [0.6995, 0.2830, 0.2212],
        [0.6995, 0.2815, 0.2206],
        [0.6988, 0.2816, 0.2206],
        [0.6986, 0.2839, 0.2214],
        [0.6992, 0.2814, 0.2206],
        [0.6992, 0.2812, 0.2201],
        [0.6997, 0.2799, 0.2198],
        [0.6990, 0.2799, 0.2198],
        [0.6990, 0.2792, 0.2199],
        [0.6990, 0.2790, 0.2198],
        [0.6988, 0.2788, 0.2192],
        [0.6981, 0.2787, 0.2199],
        [0.6986, 0.2781, 0.2194],
        [0.6987, 0.2771, 0.2190],
        [0.6979, 0.2774, 0.2191],
        [0.6977, 0.2781, 0.2200],
        [0.7013, 0.2773, 0.2182],
        [0.6982, 0.2712, 0.2193],
        [0.6970, 0.2762, 0.2195],
        [0.6982, 0.2766, 0.2193],
        [0.6984, 0.2764, 0.2192],
        [0.6981, 0.2747, 0.2192],
        [0.6991, 0.2735, 0.2185],
        [0.7000, 0.2732, 0.2188],
        [0.6945, 0.2772, 0.2238],
        [0.6948, 0.2804, 0.2217],
        [0.6946, 0.2820, 0.2219],
        [0.6964, 0.2847, 0.2223],
        [0.6975, 0.2833, 0.2218],
        [0.6976, 0.2829, 0.2216],
        [0.6975, 0.2829, 0.2215],
        [0.6979, 0.2832, 0.2216],
        [0.6983, 0.2835, 0.2216],
        [0.6971, 0.2836, 0.2221],
        [0.6978, 0.2830, 0.2217],
        [0.6983, 0.2823, 0.2213],
        [0.6984, 0.2828, 0.2214],
        [0.6991, 0.2823, 0.2209],
        [0.6987, 0.2821, 0.2207],
        [0.6978, 0.2823, 0.2213],
        [0.6985, 0.2822, 0.2211],
        [0.6989, 0.2836, 0.2214],
        [0.6985, 0.2826, 0.2211],
        [0.6975, 0.2830, 0.2215],
        [0.6982, 0.2825, 0.2210],
        [0.6987, 0.2821, 0.2205],
        [0.6986, 0.2827, 0.2212],
        [0.6990, 0.2828, 0.2213],
        [0.6986, 0.2821, 0.2210],
        [0.7019, 0.2795, 0.2186],
        [0.7011, 0.2788, 0.2183],
        [0.7004, 0.2782, 0.2184],
        [0.7007, 0.2768, 0.2185],
        [0.6996, 0.2765, 0.2185],
        [0.6990, 0.2773, 0.2188],
        [0.6978, 0.2774, 0.2196],
        [0.6981, 0.2774, 0.2195],
        [0.6979, 0.2775, 0.2197],
        [0.6971, 0.2774, 0.2197],
        [0.6963, 0.2785, 0.2206]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 5: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 5624.0, Mean: 306.1050109863281, Std: 433.63006591796875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 56191.6328125, Mean: 3057.490478515625, Std: 4332.64697265625
[DEBUG] Top-3 class probabilities:
tensor([[0.6986, 0.2751, 0.2195],
        [0.6991, 0.2761, 0.2181],
        [0.6919, 0.2687, 0.2294],
        [0.6630, 0.2729, 0.2477],
        [0.6991, 0.2900, 0.2347],
        [0.7027, 0.2715, 0.2154],
        [0.7011, 0.2709, 0.2184],
        [0.5496, 0.3309, 0.2956],
        [0.5232, 0.3441, 0.3152],
        [0.6948, 0.2823, 0.2221],
        [0.6953, 0.2827, 0.2221],
        [0.6969, 0.2834, 0.2219],
        [0.6985, 0.2843, 0.2217],
        [0.6988, 0.2841, 0.2217],
        [0.6980, 0.2834, 0.2213],
        [0.6975, 0.2828, 0.2211],
        [0.6979, 0.2824, 0.2214],
        [0.6987, 0.2862, 0.2222],
        [0.6979, 0.2826, 0.2216],
        [0.6980, 0.2829, 0.2218],
        [0.6974, 0.2826, 0.2215],
        [0.6975, 0.2826, 0.2215],
        [0.6977, 0.2826, 0.2213],
        [0.6976, 0.2829, 0.2213],
        [0.6983, 0.2818, 0.2209],
        [0.6975, 0.2828, 0.2214],
        [0.6968, 0.2851, 0.2225],
        [0.6987, 0.2837, 0.2213],
        [0.6984, 0.2835, 0.2216],
        [0.6982, 0.2832, 0.2215],
        [0.6989, 0.2815, 0.2204],
        [0.6972, 0.2823, 0.2212],
        [0.6993, 0.2828, 0.2212],
        [0.6980, 0.2824, 0.2210],
        [0.6995, 0.2809, 0.2199],
        [0.7024, 0.2779, 0.2174],
        [0.7002, 0.2776, 0.2184],
        [0.7000, 0.2790, 0.2183],
        [0.7129, 0.2718, 0.2166],
        [0.6992, 0.2753, 0.2183],
        [0.6987, 0.2756, 0.2184],
        [0.6982, 0.2759, 0.2186],
        [0.6973, 0.2775, 0.2198],
        [0.6968, 0.2772, 0.2200],
        [0.6968, 0.2783, 0.2204],
        [0.6966, 0.2784, 0.2203],
        [0.6873, 0.2705, 0.2307],
        [0.5828, 0.3035, 0.2477],
        [0.5159, 0.3504, 0.3330],
        [0.5676, 0.3180, 0.2840],
        [0.6976, 0.2866, 0.2228],
        [0.6814, 0.2548, 0.2493],
        [0.6697, 0.2608, 0.2426],
        [0.6258, 0.2796, 0.2646],
        [0.6251, 0.2910, 0.2399],
        [0.6976, 0.2837, 0.2216],
        [0.6983, 0.2840, 0.2218],
        [0.6980, 0.2842, 0.2216],
        [0.6990, 0.2842, 0.2215],
        [0.6981, 0.2824, 0.2215],
        [0.6979, 0.2824, 0.2211],
        [0.6972, 0.2828, 0.2218],
        [0.6977, 0.2829, 0.2215],
        [0.6975, 0.2835, 0.2218],
        [0.6977, 0.2829, 0.2215],
        [0.6970, 0.2836, 0.2223],
        [0.6967, 0.2829, 0.2220],
        [0.6976, 0.2830, 0.2217],
        [0.6982, 0.2826, 0.2214],
        [0.6981, 0.2829, 0.2210],
        [0.6975, 0.2820, 0.2209],
        [0.6976, 0.2841, 0.2215],
        [0.6991, 0.2844, 0.2217],
        [0.6984, 0.2847, 0.2213],
        [0.6988, 0.2833, 0.2215],
        [0.6982, 0.2826, 0.2210],
        [0.6983, 0.2817, 0.2206],
        [0.6987, 0.2825, 0.2213],
        [0.6990, 0.2825, 0.2206],
        [0.6981, 0.2823, 0.2209],
        [0.6983, 0.2807, 0.2209],
        [0.7010, 0.2788, 0.2192],
        [0.6995, 0.2792, 0.2194],
        [0.6996, 0.2794, 0.2190],
        [0.6996, 0.2782, 0.2187],
        [0.7009, 0.2768, 0.2183],
        [0.7050, 0.2727, 0.2163],
        [0.6977, 0.2758, 0.2193],
        [0.6969, 0.2775, 0.2202],
        [0.6971, 0.2777, 0.2197],
        [0.6937, 0.2696, 0.2321],
        [0.5957, 0.2933, 0.2350],
        [0.5203, 0.3492, 0.3190],
        [0.5316, 0.3432, 0.3082],
        [0.6046, 0.3222, 0.2578],
        [0.6659, 0.2645, 0.2568],
        [0.6608, 0.2756, 0.2730],
        [0.6390, 0.2751, 0.2461],
        [0.5967, 0.3133, 0.2614],
        [0.5924, 0.3189, 0.2725],
        [0.5337, 0.3464, 0.3096],
        [0.6977, 0.2841, 0.2218],
        [0.6981, 0.2842, 0.2220],
        [0.6979, 0.2861, 0.2224],
        [0.6989, 0.2842, 0.2220],
        [0.6973, 0.2824, 0.2217],
        [0.6982, 0.2827, 0.2215],
        [0.6973, 0.2826, 0.2217],
        [0.6970, 0.2831, 0.2217],
        [0.6973, 0.2832, 0.2217],
        [0.6971, 0.2835, 0.2219],
        [0.6969, 0.2832, 0.2221],
        [0.6969, 0.2829, 0.2218],
        [0.6975, 0.2827, 0.2214],
        [0.6985, 0.2823, 0.2214],
        [0.6979, 0.2817, 0.2212],
        [0.6988, 0.2823, 0.2212],
        [0.6986, 0.2843, 0.2217],
        [0.6988, 0.2835, 0.2213],
        [0.6994, 0.2839, 0.2213],
        [0.6982, 0.2828, 0.2219],
        [0.6976, 0.2820, 0.2214],
        [0.6979, 0.2821, 0.2212],
        [0.6977, 0.2825, 0.2211],
        [0.6991, 0.2830, 0.2209],
        [0.6991, 0.2817, 0.2206],
        [0.6975, 0.2816, 0.2213],
        [0.6983, 0.2805, 0.2205]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 6: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 367.2724609375, Std: 544.1629028320312
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 175211.265625, Mean: 3668.649658203125, Std: 5437.0439453125
[DEBUG] Top-3 class probabilities:
tensor([[0.7001, 0.2810, 0.2200],
        [0.7001, 0.2796, 0.2191],
        [0.6997, 0.2795, 0.2191],
        [0.6971, 0.2796, 0.2203],
        [0.6969, 0.2792, 0.2203],
        [0.6984, 0.2775, 0.2198],
        [0.6988, 0.2775, 0.2190],
        [0.6974, 0.2778, 0.2198],
        [0.6747, 0.2548, 0.2440],
        [0.5760, 0.3218, 0.2696],
        [0.5844, 0.3222, 0.2646],
        [0.6643, 0.2681, 0.2631],
        [0.5267, 0.3453, 0.3132],
        [0.6598, 0.2868, 0.2582],
        [0.6041, 0.3012, 0.2501],
        [0.6382, 0.2796, 0.2619],
        [0.5654, 0.3354, 0.2913],
        [0.5463, 0.3388, 0.3033],
        [0.5285, 0.3537, 0.3215],
        [0.6973, 0.2843, 0.2221],
        [0.6980, 0.2853, 0.2224],
        [0.6991, 0.2853, 0.2220],
        [0.6981, 0.2834, 0.2216],
        [0.6982, 0.2838, 0.2216],
        [0.6974, 0.2828, 0.2217],
        [0.6984, 0.2828, 0.2217],
        [0.6973, 0.2835, 0.2225],
        [0.6969, 0.2842, 0.2225],
        [0.6976, 0.2827, 0.2216],
        [0.6975, 0.2824, 0.2219],
        [0.6975, 0.2822, 0.2217],
        [0.6974, 0.2835, 0.2217],
        [0.6970, 0.2817, 0.2217],
        [0.6983, 0.2812, 0.2207],
        [0.6981, 0.2827, 0.2215],
        [0.6996, 0.2841, 0.2215],
        [0.6999, 0.2838, 0.2212],
        [0.6984, 0.2821, 0.2212],
        [0.6971, 0.2823, 0.2214],
        [0.6977, 0.2821, 0.2213],
        [0.6982, 0.2815, 0.2207],
        [0.6982, 0.2823, 0.2212],
        [0.6996, 0.2822, 0.2206],
        [0.6980, 0.2820, 0.2208],
        [0.6982, 0.2815, 0.2206],
        [0.6984, 0.2812, 0.2203],
        [0.6982, 0.2805, 0.2204],
        [0.7004, 0.2786, 0.2191],
        [0.7008, 0.2791, 0.2188],
        [0.6979, 0.2799, 0.2204],
        [0.6980, 0.2798, 0.2200],
        [0.6987, 0.2777, 0.2195],
        [0.6986, 0.2786, 0.2195],
        [0.7005, 0.2762, 0.2182],
        [0.6703, 0.2557, 0.2555],
        [0.5083, 0.3498, 0.3298],
        [0.4669, 0.3577, 0.3567],
        [0.6104, 0.2991, 0.2582],
        [0.5585, 0.3373, 0.2935],
        [0.6352, 0.2877, 0.2650],
        [0.6272, 0.2969, 0.2445],
        [0.6129, 0.3108, 0.2487],
        [0.4675, 0.3703, 0.3555],
        [0.4601, 0.3695, 0.3651],
        [0.5496, 0.3429, 0.3069],
        [0.6973, 0.2858, 0.2228],
        [0.6989, 0.2858, 0.2226],
        [0.6983, 0.2844, 0.2218],
        [0.6965, 0.2836, 0.2221],
        [0.6969, 0.2835, 0.2219],
        [0.6971, 0.2837, 0.2220],
        [0.6974, 0.2834, 0.2216],
        [0.6979, 0.2835, 0.2218],
        [0.6982, 0.2831, 0.2217],
        [0.6973, 0.2832, 0.2214],
        [0.6965, 0.2829, 0.2219],
        [0.6970, 0.2826, 0.2222],
        [0.6975, 0.2812, 0.2212],
        [0.6977, 0.2813, 0.2208],
        [0.6978, 0.2818, 0.2209],
        [0.7005, 0.2835, 0.2212],
        [0.6988, 0.2841, 0.2216],
        [0.6984, 0.2835, 0.2218],
        [0.6986, 0.2832, 0.2218],
        [0.6970, 0.2816, 0.2214],
        [0.6974, 0.2829, 0.2216],
        [0.6981, 0.2819, 0.2210],
        [0.6988, 0.2816, 0.2207],
        [0.6974, 0.2822, 0.2213],
        [0.6977, 0.2830, 0.2214],
        [0.6991, 0.2812, 0.2203],
        [0.6982, 0.2816, 0.2206],
        [0.6986, 0.2804, 0.2204],
        [0.7013, 0.2766, 0.2186],
        [0.7013, 0.2770, 0.2183],
        [0.6975, 0.2789, 0.2201],
        [0.6952, 0.2812, 0.2230],
        [0.7000, 0.2829, 0.2203],
        [0.6986, 0.2768, 0.2194],
        [0.7001, 0.2749, 0.2190],
        [0.6996, 0.2637, 0.2320],
        [0.5650, 0.3313, 0.2888],
        [0.4850, 0.3576, 0.3484],
        [0.5465, 0.3428, 0.3114],
        [0.6967, 0.2756, 0.2255],
        [0.6987, 0.2694, 0.2183],
        [0.6832, 0.2622, 0.2371],
        [0.6369, 0.2731, 0.2527],
        [0.6703, 0.2618, 0.2470],
        [0.6453, 0.2699, 0.2491],
        [0.5812, 0.3139, 0.2694],
        [0.6986, 0.2861, 0.2227],
        [0.6975, 0.2852, 0.2227],
        [0.6969, 0.2847, 0.2223],
        [0.6972, 0.2841, 0.2222],
        [0.6969, 0.2841, 0.2225],
        [0.6972, 0.2843, 0.2221],
        [0.6972, 0.2840, 0.2221],
        [0.6978, 0.2836, 0.2222],
        [0.6975, 0.2839, 0.2217],
        [0.6965, 0.2828, 0.2217],
        [0.6980, 0.2826, 0.2216],
        [0.6980, 0.2812, 0.2210],
        [0.6969, 0.2823, 0.2213],
        [0.6984, 0.2813, 0.2205],
        [0.6988, 0.2840, 0.2216],
        [0.7000, 0.2841, 0.2212],
        [0.6992, 0.2835, 0.2213]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 7: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6644.0, Mean: 334.3930358886719, Std: 467.485107421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 66383.0390625, Mean: 3340.13232421875, Std: 4670.912109375
[DEBUG] Top-3 class probabilities:
tensor([[0.6986, 0.2829, 0.2214],
        [0.6977, 0.2822, 0.2210],
        [0.6978, 0.2810, 0.2212],
        [0.6965, 0.2830, 0.2213],
        [0.6975, 0.2819, 0.2215],
        [0.6992, 0.2805, 0.2203],
        [0.6991, 0.2810, 0.2205],
        [0.6988, 0.2820, 0.2214],
        [0.6997, 0.2797, 0.2197],
        [0.6983, 0.2804, 0.2202],
        [0.6995, 0.2798, 0.2194],
        [0.6990, 0.2788, 0.2196],
        [0.6991, 0.2785, 0.2191],
        [0.7012, 0.2755, 0.2185],
        [0.6116, 0.2913, 0.2312],
        [0.6865, 0.2597, 0.2444],
        [0.6987, 0.2751, 0.2185],
        [0.6965, 0.2748, 0.2194],
        [0.7009, 0.2723, 0.2180],
        [0.6359, 0.2905, 0.2336],
        [0.6414, 0.2822, 0.2673],
        [0.6763, 0.2605, 0.2474],
        [0.6992, 0.2682, 0.2160],
        [0.7046, 0.2752, 0.2215],
        [0.5995, 0.3053, 0.2430],
        [0.5236, 0.3415, 0.3179],
        [0.5369, 0.3455, 0.3045],
        [0.6200, 0.2928, 0.2584],
        [0.5200, 0.3485, 0.3278],
        [0.6976, 0.2868, 0.2228],
        [0.6971, 0.2852, 0.2226],
        [0.6966, 0.2844, 0.2226],
        [0.6968, 0.2847, 0.2225],
        [0.6977, 0.2845, 0.2225],
        [0.6969, 0.2839, 0.2223],
        [0.6967, 0.2840, 0.2225],
        [0.6984, 0.2845, 0.2218],
        [0.6966, 0.2834, 0.2220],
        [0.6972, 0.2832, 0.2222],
        [0.6973, 0.2826, 0.2212],
        [0.6978, 0.2817, 0.2212],
        [0.6974, 0.2817, 0.2209],
        [0.6978, 0.2823, 0.2214],
        [0.6985, 0.2844, 0.2218],
        [0.6988, 0.2836, 0.2212],
        [0.6978, 0.2830, 0.2216],
        [0.6979, 0.2830, 0.2211],
        [0.6980, 0.2820, 0.2210],
        [0.6974, 0.2817, 0.2210],
        [0.6965, 0.2814, 0.2216],
        [0.6979, 0.2815, 0.2209],
        [0.6985, 0.2814, 0.2204],
        [0.6991, 0.2804, 0.2204],
        [0.6987, 0.2819, 0.2205],
        [0.7002, 0.2791, 0.2196],
        [0.7001, 0.2783, 0.2191],
        [0.6992, 0.2786, 0.2193],
        [0.6972, 0.2698, 0.2233],
        [0.6901, 0.2775, 0.2324],
        [0.7014, 0.2712, 0.2221],
        [0.6980, 0.2749, 0.2225],
        [0.6956, 0.2735, 0.2212],
        [0.6973, 0.2735, 0.2190],
        [0.6970, 0.2743, 0.2192],
        [0.6918, 0.2673, 0.2246],
        [0.7036, 0.2718, 0.2230],
        [0.7008, 0.2785, 0.2279],
        [0.6996, 0.2685, 0.2164],
        [0.6988, 0.2666, 0.2161],
        [0.7023, 0.2640, 0.2278],
        [0.5455, 0.3504, 0.2880],
        [0.4808, 0.3787, 0.3429],
        [0.4784, 0.3712, 0.3479],
        [0.5126, 0.3676, 0.3211],
        [0.5407, 0.3547, 0.3057],
        [0.6979, 0.2857, 0.2225],
        [0.6975, 0.2851, 0.2223],
        [0.6966, 0.2851, 0.2225],
        [0.6976, 0.2854, 0.2228],
        [0.6972, 0.2852, 0.2224],
        [0.6973, 0.2842, 0.2224],
        [0.6975, 0.2849, 0.2222],
        [0.6971, 0.2832, 0.2220],
        [0.6974, 0.2836, 0.2222],
        [0.6973, 0.2832, 0.2217],
        [0.6967, 0.2828, 0.2216],
        [0.6971, 0.2818, 0.2213],
        [0.6977, 0.2815, 0.2209],
        [0.6988, 0.2830, 0.2212],
        [0.6997, 0.2828, 0.2210],
        [0.6980, 0.2826, 0.2213],
        [0.6984, 0.2820, 0.2211],
        [0.6978, 0.2818, 0.2208],
        [0.6989, 0.2806, 0.2204],
        [0.6971, 0.2811, 0.2211],
        [0.6977, 0.2811, 0.2211],
        [0.6975, 0.2810, 0.2210],
        [0.7014, 0.2797, 0.2191],
        [0.6952, 0.2802, 0.2243],
        [0.7003, 0.2764, 0.2183],
        [0.6998, 0.2760, 0.2181],
        [0.7003, 0.2757, 0.2180],
        [0.6999, 0.2769, 0.2185],
        [0.6749, 0.2646, 0.2462],
        [0.6896, 0.2531, 0.2475],
        [0.7013, 0.2752, 0.2172],
        [0.6983, 0.2741, 0.2186],
        [0.6980, 0.2739, 0.2185],
        [0.6967, 0.2722, 0.2191],
        [0.6974, 0.2714, 0.2182],
        [0.6956, 0.2688, 0.2232],
        [0.7029, 0.2694, 0.2273],
        [0.6592, 0.2607, 0.2596],
        [0.7008, 0.2769, 0.2237],
        [0.6995, 0.2654, 0.2156],
        [0.6999, 0.2670, 0.2192],
        [0.6671, 0.2767, 0.2531],
        [0.5875, 0.3291, 0.2737],
        [0.5021, 0.3659, 0.3310],
        [0.5121, 0.3614, 0.3342],
        [0.5853, 0.3168, 0.2706],
        [0.6981, 0.2849, 0.2220],
        [0.6973, 0.2842, 0.2221],
        [0.6965, 0.2850, 0.2227],
        [0.6974, 0.2858, 0.2223],
        [0.6976, 0.2845, 0.2222],
        [0.6968, 0.2848, 0.2226],
        [0.6976, 0.2838, 0.2217]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 8: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 5068.0, Mean: 314.5447692871094, Std: 422.6250305175781
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 50636.3203125, Mean: 3141.8173828125, Std: 4222.689453125
[DEBUG] Top-3 class probabilities:
tensor([[0.6972, 0.2833, 0.2221],
        [0.6976, 0.2837, 0.2217],
        [0.6976, 0.2824, 0.2214],
        [0.6966, 0.2817, 0.2213],
        [0.6968, 0.2820, 0.2215],
        [0.6990, 0.2825, 0.2209],
        [0.6995, 0.2823, 0.2210],
        [0.6988, 0.2824, 0.2207],
        [0.6977, 0.2816, 0.2213],
        [0.6984, 0.2810, 0.2207],
        [0.6972, 0.2822, 0.2212],
        [0.6983, 0.2826, 0.2211],
        [0.6977, 0.2827, 0.2212],
        [0.6974, 0.2818, 0.2214],
        [0.6974, 0.2818, 0.2213],
        [0.7048, 0.2762, 0.2172],
        [0.6726, 0.2754, 0.2472],
        [0.7030, 0.2764, 0.2172],
        [0.6996, 0.2769, 0.2184],
        [0.7018, 0.2758, 0.2171],
        [0.7029, 0.2766, 0.2170],
        [0.6997, 0.2781, 0.2184],
        [0.6995, 0.2776, 0.2195],
        [0.6976, 0.2748, 0.2190],
        [0.6921, 0.2693, 0.2249],
        [0.7002, 0.2730, 0.2301],
        [0.6960, 0.2727, 0.2219],
        [0.6985, 0.2685, 0.2170],
        [0.6995, 0.2692, 0.2168],
        [0.6846, 0.2677, 0.2527],
        [0.7071, 0.2740, 0.2276],
        [0.7017, 0.2640, 0.2157],
        [0.6980, 0.2621, 0.2225],
        [0.6984, 0.2613, 0.2173],
        [0.6988, 0.2714, 0.2264],
        [0.6111, 0.3060, 0.2512],
        [0.4888, 0.3628, 0.3385],
        [0.5435, 0.3584, 0.3112],
        [0.6069, 0.3018, 0.2521],
        [0.6976, 0.2842, 0.2222],
        [0.6973, 0.2839, 0.2223],
        [0.6971, 0.2848, 0.2224],
        [0.6975, 0.2847, 0.2221],
        [0.6969, 0.2845, 0.2226],
        [0.6975, 0.2842, 0.2222],
        [0.6976, 0.2838, 0.2221],
        [0.6979, 0.2835, 0.2220],
        [0.6967, 0.2838, 0.2221],
        [0.6975, 0.2815, 0.2214],
        [0.6975, 0.2820, 0.2215],
        [0.6974, 0.2825, 0.2211],
        [0.6980, 0.2825, 0.2214],
        [0.6989, 0.2821, 0.2208],
        [0.6980, 0.2817, 0.2207],
        [0.6983, 0.2821, 0.2208],
        [0.6979, 0.2826, 0.2214],
        [0.6973, 0.2826, 0.2220],
        [0.6979, 0.2828, 0.2215],
        [0.6978, 0.2829, 0.2217],
        [0.6979, 0.2821, 0.2211],
        [0.6975, 0.2827, 0.2210],
        [0.6993, 0.2819, 0.2207],
        [0.7009, 0.2815, 0.2198],
        [0.7013, 0.2794, 0.2191],
        [0.7036, 0.2785, 0.2175],
        [0.7033, 0.2634, 0.2294],
        [0.7079, 0.2771, 0.2324],
        [0.7046, 0.2825, 0.2223],
        [0.6986, 0.2767, 0.2191],
        [0.6926, 0.2603, 0.2376],
        [0.6810, 0.2674, 0.2387],
        [0.6865, 0.2608, 0.2369],
        [0.6806, 0.2762, 0.2469],
        [0.7003, 0.2716, 0.2246],
        [0.6987, 0.2672, 0.2161],
        [0.6957, 0.2580, 0.2252],
        [0.6851, 0.2667, 0.2437],
        [0.6983, 0.2620, 0.2150],
        [0.6979, 0.2618, 0.2173],
        [0.6984, 0.2661, 0.2187],
        [0.7009, 0.2702, 0.2299],
        [0.6172, 0.2892, 0.2400],
        [0.5056, 0.3651, 0.3387],
        [0.4813, 0.3738, 0.3518],
        [0.5063, 0.3642, 0.3406],
        [0.6969, 0.2839, 0.2220],
        [0.6969, 0.2841, 0.2222],
        [0.6971, 0.2852, 0.2226],
        [0.6974, 0.2837, 0.2220],
        [0.6970, 0.2842, 0.2222],
        [0.6975, 0.2838, 0.2219],
        [0.6979, 0.2837, 0.2216],
        [0.6980, 0.2835, 0.2219],
        [0.6973, 0.2821, 0.2212],
        [0.6974, 0.2819, 0.2215],
        [0.6969, 0.2823, 0.2214],
        [0.6976, 0.2812, 0.2214],
        [0.6978, 0.2830, 0.2214],
        [0.6983, 0.2822, 0.2213],
        [0.6973, 0.2826, 0.2213],
        [0.6972, 0.2824, 0.2215],
        [0.6978, 0.2821, 0.2215],
        [0.6977, 0.2819, 0.2215],
        [0.6986, 0.2828, 0.2215],
        [0.6986, 0.2824, 0.2216],
        [0.6989, 0.2811, 0.2203],
        [0.7006, 0.2815, 0.2201],
        [0.7011, 0.2819, 0.2200],
        [0.7029, 0.2814, 0.2196],
        [0.7049, 0.2801, 0.2181],
        [0.7079, 0.2775, 0.2161],
        [0.6986, 0.2620, 0.2314],
        [0.6078, 0.2968, 0.2262],
        [0.5304, 0.3349, 0.2903],
        [0.5480, 0.3230, 0.2922],
        [0.6572, 0.2633, 0.2579],
        [0.6998, 0.2735, 0.2172],
        [0.6991, 0.2656, 0.2156],
        [0.6993, 0.2628, 0.2167],
        [0.6981, 0.2611, 0.2148],
        [0.7006, 0.2660, 0.2175],
        [0.6965, 0.2624, 0.2179],
        [0.6972, 0.2612, 0.2156],
        [0.6976, 0.2571, 0.2209],
        [0.6920, 0.2676, 0.2264],
        [0.6549, 0.2561, 0.2549],
        [0.6177, 0.2802, 0.2400],
        [0.5938, 0.2929, 0.2430]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 9: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6368.0, Mean: 426.47314453125, Std: 583.0779418945312
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 63625.36328125, Mean: 4260.158203125, Std: 5825.8662109375
[DEBUG] Top-3 class probabilities:
tensor([[0.4976, 0.3611, 0.3502],
        [0.5160, 0.3624, 0.3301],
        [0.5242, 0.3626, 0.3049],
        [0.6968, 0.2838, 0.2221],
        [0.6971, 0.2854, 0.2222],
        [0.6975, 0.2852, 0.2220],
        [0.6975, 0.2835, 0.2216],
        [0.6978, 0.2843, 0.2218],
        [0.6984, 0.2834, 0.2214],
        [0.6980, 0.2830, 0.2215],
        [0.6983, 0.2827, 0.2215],
        [0.6957, 0.2820, 0.2219],
        [0.6977, 0.2828, 0.2213],
        [0.6973, 0.2829, 0.2219],
        [0.6968, 0.2822, 0.2219],
        [0.6969, 0.2822, 0.2214],
        [0.6971, 0.2825, 0.2216],
        [0.6967, 0.2822, 0.2214],
        [0.6969, 0.2833, 0.2222],
        [0.6974, 0.2832, 0.2215],
        [0.6981, 0.2832, 0.2217],
        [0.6985, 0.2836, 0.2217],
        [0.6989, 0.2827, 0.2216],
        [0.7008, 0.2810, 0.2201],
        [0.7004, 0.2818, 0.2196],
        [0.7015, 0.2829, 0.2206],
        [0.7017, 0.2824, 0.2203],
        [0.7024, 0.2808, 0.2195],
        [0.7067, 0.2738, 0.2153],
        [0.7110, 0.2638, 0.2098],
        [0.5394, 0.3374, 0.2857],
        [0.4392, 0.3716, 0.3502],
        [0.4625, 0.3666, 0.3592],
        [0.5153, 0.3616, 0.3342],
        [0.5300, 0.3287, 0.2808],
        [0.6790, 0.2756, 0.2335],
        [0.6906, 0.2650, 0.2247],
        [0.6975, 0.2608, 0.2162],
        [0.6917, 0.2648, 0.2259],
        [0.6975, 0.2622, 0.2170],
        [0.6949, 0.2604, 0.2170],
        [0.6963, 0.2627, 0.2166],
        [0.6831, 0.2498, 0.2313],
        [0.6528, 0.2744, 0.2438],
        [0.5791, 0.3054, 0.2658],
        [0.5417, 0.3279, 0.3028],
        [0.5024, 0.3427, 0.3282],
        [0.6155, 0.3085, 0.2399],
        [0.6415, 0.2827, 0.2638],
        [0.6978, 0.2839, 0.2223],
        [0.6984, 0.2848, 0.2223],
        [0.6978, 0.2844, 0.2220],
        [0.6969, 0.2836, 0.2217],
        [0.6978, 0.2836, 0.2213],
        [0.6982, 0.2834, 0.2214],
        [0.6972, 0.2823, 0.2214],
        [0.6980, 0.2803, 0.2209],
        [0.6973, 0.2810, 0.2209],
        [0.6964, 0.2807, 0.2215],
        [0.6965, 0.2817, 0.2215],
        [0.6974, 0.2817, 0.2212],
        [0.6977, 0.2817, 0.2210],
        [0.6968, 0.2832, 0.2220],
        [0.6974, 0.2816, 0.2211],
        [0.6978, 0.2817, 0.2215],
        [0.6986, 0.2828, 0.2215],
        [0.6984, 0.2838, 0.2214],
        [0.7002, 0.2830, 0.2208],
        [0.7006, 0.2818, 0.2203],
        [0.6994, 0.2814, 0.2208],
        [0.7007, 0.2818, 0.2204],
        [0.7024, 0.2810, 0.2201],
        [0.7024, 0.2775, 0.2184],
        [0.6998, 0.2745, 0.2178],
        [0.7028, 0.2700, 0.2156],
        [0.7101, 0.2596, 0.2230],
        [0.5749, 0.3266, 0.2589],
        [0.4764, 0.3711, 0.3456],
        [0.4489, 0.3827, 0.3526],
        [0.5094, 0.3557, 0.3420],
        [0.4920, 0.3758, 0.3390],
        [0.5455, 0.3341, 0.2696],
        [0.5951, 0.3070, 0.2410],
        [0.6613, 0.2674, 0.2425],
        [0.6966, 0.2644, 0.2171],
        [0.6958, 0.2630, 0.2170],
        [0.6985, 0.2622, 0.2203],
        [0.6964, 0.2613, 0.2162],
        [0.6968, 0.2605, 0.2159],
        [0.6982, 0.2595, 0.2146],
        [0.6757, 0.2663, 0.2495],
        [0.6761, 0.2598, 0.2594],
        [0.6908, 0.2684, 0.2251],
        [0.6517, 0.2688, 0.2545],
        [0.5614, 0.3136, 0.2895],
        [0.6984, 0.2844, 0.2219],
        [0.6987, 0.2846, 0.2217],
        [0.6980, 0.2831, 0.2214],
        [0.6977, 0.2832, 0.2217],
        [0.6973, 0.2831, 0.2215],
        [0.6982, 0.2815, 0.2209],
        [0.6967, 0.2820, 0.2213],
        [0.6972, 0.2799, 0.2205],
        [0.6975, 0.2803, 0.2207],
        [0.6966, 0.2814, 0.2215],
        [0.6968, 0.2827, 0.2216],
        [0.6966, 0.2817, 0.2215],
        [0.6977, 0.2820, 0.2214],
        [0.6970, 0.2816, 0.2215],
        [0.6974, 0.2816, 0.2212],
        [0.6988, 0.2819, 0.2211],
        [0.6998, 0.2826, 0.2212],
        [0.6994, 0.2827, 0.2208],
        [0.7010, 0.2805, 0.2199],
        [0.7000, 0.2794, 0.2195],
        [0.7010, 0.2788, 0.2187],
        [0.7019, 0.2785, 0.2180],
        [0.7000, 0.2656, 0.2190],
        [0.6429, 0.2660, 0.2517],
        [0.6911, 0.2656, 0.2362],
        [0.6970, 0.2705, 0.2191],
        [0.6949, 0.2455, 0.2246],
        [0.5443, 0.3403, 0.2852],
        [0.4758, 0.3824, 0.3410],
        [0.5145, 0.3584, 0.3160],
        [0.5140, 0.3767, 0.3301],
        [0.4993, 0.3764, 0.3203],
        [0.5383, 0.3577, 0.2880]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 10: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6128.0, Mean: 587.0831909179688, Std: 728.6054077148438
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 61227.38671875, Mean: 5864.90478515625, Std: 7279.91455078125
[DEBUG] Top-3 class probabilities:
tensor([[0.5919, 0.3231, 0.2419],
        [0.6977, 0.2651, 0.2206],
        [0.6961, 0.2633, 0.2157],
        [0.7043, 0.2590, 0.2221],
        [0.6963, 0.2613, 0.2161],
        [0.6960, 0.2606, 0.2163],
        [0.6959, 0.2659, 0.2240],
        [0.6969, 0.2616, 0.2159],
        [0.6949, 0.2605, 0.2259],
        [0.7002, 0.2584, 0.2234],
        [0.6365, 0.2792, 0.2460],
        [0.5374, 0.3671, 0.3157],
        [0.5125, 0.3672, 0.3333],
        [0.6976, 0.2848, 0.2217],
        [0.6979, 0.2829, 0.2215],
        [0.6969, 0.2817, 0.2218],
        [0.6974, 0.2827, 0.2217],
        [0.6985, 0.2827, 0.2211],
        [0.6972, 0.2815, 0.2212],
        [0.6975, 0.2801, 0.2207],
        [0.6978, 0.2804, 0.2212],
        [0.6979, 0.2804, 0.2206],
        [0.6971, 0.2807, 0.2212],
        [0.6976, 0.2824, 0.2213],
        [0.6974, 0.2812, 0.2211],
        [0.6986, 0.2811, 0.2208],
        [0.6978, 0.2812, 0.2212],
        [0.6993, 0.2815, 0.2204],
        [0.6995, 0.2814, 0.2208],
        [0.7043, 0.2796, 0.2180],
        [0.7039, 0.2761, 0.2249],
        [0.6997, 0.2806, 0.2197],
        [0.7004, 0.2748, 0.2195],
        [0.6884, 0.2605, 0.2309],
        [0.5474, 0.3093, 0.2523],
        [0.5143, 0.3528, 0.3059],
        [0.4716, 0.3653, 0.3436],
        [0.4549, 0.3803, 0.3477],
        [0.5476, 0.3406, 0.2870],
        [0.6661, 0.2579, 0.2531],
        [0.6019, 0.3135, 0.2527],
        [0.4805, 0.3841, 0.3385],
        [0.4755, 0.3766, 0.3457],
        [0.4727, 0.3763, 0.3550],
        [0.4959, 0.3719, 0.3294],
        [0.4995, 0.3751, 0.2987],
        [0.5196, 0.3663, 0.3193],
        [0.5013, 0.3449, 0.3057],
        [0.5607, 0.3279, 0.2815],
        [0.6976, 0.2695, 0.2193],
        [0.6960, 0.2610, 0.2164],
        [0.6963, 0.2597, 0.2165],
        [0.6966, 0.2602, 0.2156],
        [0.6958, 0.2643, 0.2222],
        [0.6982, 0.2610, 0.2150],
        [0.6966, 0.2591, 0.2244],
        [0.5386, 0.3369, 0.3093],
        [0.5074, 0.3558, 0.3370],
        [0.5184, 0.3568, 0.3203],
        [0.6981, 0.2830, 0.2216],
        [0.6960, 0.2828, 0.2218],
        [0.6972, 0.2819, 0.2210],
        [0.6977, 0.2833, 0.2215],
        [0.6977, 0.2824, 0.2213],
        [0.6976, 0.2817, 0.2213],
        [0.6978, 0.2802, 0.2203],
        [0.6979, 0.2805, 0.2203],
        [0.6983, 0.2804, 0.2206],
        [0.6976, 0.2810, 0.2208],
        [0.6980, 0.2806, 0.2208],
        [0.6971, 0.2815, 0.2213],
        [0.6989, 0.2807, 0.2205],
        [0.7006, 0.2805, 0.2200],
        [0.7023, 0.2812, 0.2194],
        [0.6998, 0.2816, 0.2194],
        [0.6968, 0.2735, 0.2305],
        [0.7022, 0.2670, 0.2410],
        [0.5919, 0.3029, 0.2394],
        [0.5431, 0.3438, 0.3024],
        [0.5753, 0.3336, 0.2629],
        [0.5464, 0.3461, 0.3047],
        [0.5035, 0.3626, 0.3352],
        [0.5144, 0.3595, 0.3223],
        [0.4792, 0.3672, 0.3451],
        [0.4824, 0.3688, 0.3440],
        [0.4979, 0.3648, 0.3102],
        [0.6198, 0.3054, 0.2442],
        [0.6453, 0.2787, 0.2410],
        [0.5453, 0.3595, 0.2839],
        [0.4945, 0.3818, 0.3427],
        [0.5087, 0.3502, 0.3283],
        [0.4805, 0.3700, 0.3321],
        [0.5180, 0.3616, 0.3083],
        [0.4805, 0.3753, 0.3535],
        [0.5007, 0.3662, 0.3266],
        [0.6294, 0.2777, 0.2510],
        [0.6975, 0.2652, 0.2169],
        [0.6962, 0.2597, 0.2163],
        [0.6965, 0.2591, 0.2159],
        [0.6978, 0.2558, 0.2167],
        [0.6983, 0.2646, 0.2200],
        [0.7015, 0.2573, 0.2151],
        [0.6945, 0.2503, 0.2261],
        [0.5768, 0.3259, 0.2970],
        [0.5110, 0.3619, 0.3374],
        [0.6966, 0.2832, 0.2220],
        [0.6970, 0.2819, 0.2216],
        [0.6966, 0.2837, 0.2219],
        [0.6984, 0.2836, 0.2219],
        [0.6980, 0.2818, 0.2209],
        [0.6980, 0.2805, 0.2209],
        [0.6973, 0.2804, 0.2206],
        [0.6978, 0.2799, 0.2205],
        [0.6984, 0.2802, 0.2206],
        [0.6985, 0.2806, 0.2204],
        [0.6980, 0.2803, 0.2208],
        [0.6983, 0.2805, 0.2204],
        [0.7003, 0.2786, 0.2185],
        [0.6694, 0.2508, 0.2505],
        [0.5695, 0.3093, 0.2441],
        [0.5837, 0.3131, 0.2439],
        [0.6924, 0.2643, 0.2418],
        [0.6803, 0.2654, 0.2544],
        [0.6024, 0.3159, 0.2564],
        [0.5211, 0.3665, 0.3261],
        [0.5992, 0.3202, 0.2491],
        [0.6019, 0.3267, 0.2447],
        [0.5060, 0.3689, 0.3334]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 11: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 739.0349731445312, Std: 843.2438354492188
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 174891.546875, Mean: 7383.14306640625, Std: 8425.3330078125
[DEBUG] Top-3 class probabilities:
tensor([[0.4908, 0.3671, 0.3265],
        [0.5780, 0.3220, 0.2622],
        [0.4972, 0.3877, 0.3231],
        [0.5156, 0.3672, 0.3005],
        [0.4834, 0.3809, 0.3327],
        [0.6112, 0.3023, 0.2362],
        [0.5661, 0.3288, 0.2539],
        [0.5064, 0.3617, 0.3265],
        [0.5119, 0.3666, 0.3341],
        [0.5207, 0.3628, 0.3160],
        [0.5414, 0.3529, 0.2927],
        [0.5158, 0.3736, 0.2969],
        [0.5335, 0.3670, 0.2952],
        [0.5074, 0.3681, 0.2933],
        [0.6017, 0.2964, 0.2443],
        [0.6916, 0.2697, 0.2212],
        [0.6962, 0.2601, 0.2163],
        [0.6919, 0.2545, 0.2323],
        [0.7070, 0.2507, 0.2235],
        [0.6512, 0.2658, 0.2499],
        [0.7061, 0.2534, 0.2142],
        [0.6944, 0.2327, 0.2216],
        [0.5145, 0.3761, 0.3225],
        [0.6964, 0.2828, 0.2220],
        [0.6967, 0.2829, 0.2221],
        [0.6978, 0.2836, 0.2223],
        [0.6987, 0.2819, 0.2214],
        [0.6987, 0.2812, 0.2207],
        [0.6979, 0.2803, 0.2206],
        [0.6977, 0.2803, 0.2202],
        [0.6965, 0.2801, 0.2210],
        [0.6981, 0.2809, 0.2210],
        [0.6994, 0.2790, 0.2196],
        [0.6983, 0.2807, 0.2203],
        [0.6988, 0.2799, 0.2202],
        [0.7065, 0.2717, 0.2175],
        [0.6909, 0.2608, 0.2349],
        [0.6084, 0.3070, 0.2365],
        [0.5605, 0.3408, 0.2810],
        [0.6309, 0.2758, 0.2519],
        [0.6836, 0.2549, 0.2383],
        [0.6905, 0.2640, 0.2359],
        [0.6888, 0.2595, 0.2224],
        [0.7056, 0.2603, 0.2238],
        [0.6825, 0.2598, 0.2459],
        [0.6591, 0.2610, 0.2537],
        [0.6440, 0.2780, 0.2413],
        [0.6153, 0.3107, 0.2365],
        [0.4845, 0.3767, 0.3476],
        [0.5256, 0.3642, 0.3089],
        [0.5147, 0.3670, 0.3212],
        [0.4944, 0.3776, 0.3235],
        [0.5983, 0.3044, 0.2436],
        [0.6439, 0.2840, 0.2257],
        [0.5039, 0.3693, 0.3388],
        [0.5217, 0.3545, 0.3198],
        [0.5116, 0.3643, 0.3310],
        [0.5367, 0.3618, 0.3075],
        [0.4780, 0.3804, 0.3384],
        [0.4949, 0.3861, 0.3364],
        [0.4672, 0.3901, 0.3482],
        [0.5225, 0.3600, 0.3134],
        [0.6258, 0.2745, 0.2418],
        [0.6964, 0.2644, 0.2177],
        [0.6376, 0.2836, 0.2285],
        [0.5127, 0.3708, 0.3232],
        [0.5466, 0.3586, 0.2762],
        [0.5666, 0.3439, 0.2686],
        [0.5046, 0.3828, 0.3196],
        [0.6957, 0.2827, 0.2224],
        [0.6973, 0.2837, 0.2220],
        [0.6983, 0.2830, 0.2219],
        [0.6989, 0.2827, 0.2214],
        [0.6975, 0.2815, 0.2212],
        [0.6990, 0.2793, 0.2202],
        [0.6986, 0.2806, 0.2201],
        [0.6985, 0.2816, 0.2207],
        [0.6991, 0.2810, 0.2206],
        [0.6992, 0.2790, 0.2198],
        [0.6975, 0.2783, 0.2201],
        [0.6969, 0.2707, 0.2209],
        [0.6823, 0.2632, 0.2358],
        [0.6980, 0.2660, 0.2162],
        [0.6957, 0.2648, 0.2175],
        [0.6961, 0.2681, 0.2172],
        [0.6985, 0.2613, 0.2148],
        [0.6902, 0.2596, 0.2296],
        [0.6852, 0.2647, 0.2225],
        [0.6966, 0.2655, 0.2255],
        [0.6989, 0.2680, 0.2238],
        [0.6991, 0.2630, 0.2151],
        [0.6842, 0.2540, 0.2295],
        [0.6930, 0.2567, 0.2240],
        [0.6344, 0.2911, 0.2456],
        [0.6218, 0.2997, 0.2546],
        [0.6234, 0.2965, 0.2403],
        [0.5509, 0.3497, 0.2890],
        [0.5332, 0.3633, 0.3051],
        [0.4686, 0.3919, 0.3446],
        [0.5346, 0.3494, 0.2899],
        [0.5014, 0.3753, 0.3329],
        [0.5069, 0.3584, 0.3272],
        [0.5092, 0.3575, 0.3236],
        [0.4955, 0.3631, 0.3419],
        [0.5013, 0.3786, 0.3287],
        [0.5242, 0.3571, 0.3185],
        [0.4903, 0.3696, 0.3352],
        [0.4652, 0.3697, 0.3483],
        [0.5350, 0.3530, 0.3103],
        [0.5352, 0.3593, 0.2887],
        [0.5350, 0.3694, 0.2905],
        [0.5510, 0.3534, 0.2690],
        [0.4999, 0.3747, 0.3227],
        [0.5095, 0.3661, 0.3221],
        [0.4881, 0.3648, 0.3404],
        [0.6959, 0.2824, 0.2219],
        [0.6973, 0.2833, 0.2219],
        [0.6978, 0.2833, 0.2218],
        [0.6988, 0.2825, 0.2214],
        [0.6985, 0.2821, 0.2213],
        [0.6992, 0.2815, 0.2211],
        [0.6989, 0.2825, 0.2210],
        [0.6995, 0.2785, 0.2197],
        [0.6976, 0.2772, 0.2199],
        [0.6952, 0.2746, 0.2202],
        [0.6948, 0.2742, 0.2200],
        [0.6933, 0.2725, 0.2199],
        [0.6942, 0.2699, 0.2189]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 12: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 881.2821044921875, Std: 883.324462890625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 174891.546875, Mean: 8804.4150390625, Std: 8825.8017578125
[DEBUG] Top-3 class probabilities:
tensor([[0.6972, 0.2659, 0.2163],
        [0.7050, 0.2601, 0.2172],
        [0.6474, 0.2868, 0.2311],
        [0.6170, 0.2976, 0.2299],
        [0.6395, 0.2760, 0.2559],
        [0.5359, 0.3504, 0.3036],
        [0.5916, 0.3048, 0.2455],
        [0.6347, 0.2754, 0.2592],
        [0.5906, 0.3018, 0.2563],
        [0.6245, 0.2913, 0.2370],
        [0.6761, 0.2610, 0.2493],
        [0.6977, 0.2568, 0.2156],
        [0.7051, 0.2464, 0.2244],
        [0.6522, 0.2801, 0.2404],
        [0.5640, 0.3485, 0.2753],
        [0.5212, 0.3673, 0.3046],
        [0.5577, 0.3584, 0.2867],
        [0.4942, 0.3809, 0.3283],
        [0.4991, 0.3740, 0.3064],
        [0.4963, 0.3789, 0.3433],
        [0.5266, 0.3520, 0.3064],
        [0.5223, 0.3633, 0.3220],
        [0.5074, 0.3559, 0.3297],
        [0.5027, 0.3755, 0.3379],
        [0.5138, 0.3627, 0.3173],
        [0.5136, 0.3670, 0.3250],
        [0.5000, 0.3699, 0.3378],
        [0.4833, 0.3801, 0.3097],
        [0.5422, 0.3626, 0.2926],
        [0.5113, 0.3760, 0.3294],
        [0.5021, 0.3685, 0.3292],
        [0.5356, 0.3580, 0.3096],
        [0.5012, 0.3627, 0.3237],
        [0.6966, 0.2824, 0.2219],
        [0.6977, 0.2829, 0.2219],
        [0.6983, 0.2844, 0.2219],
        [0.6990, 0.2839, 0.2215],
        [0.6984, 0.2825, 0.2213],
        [0.6978, 0.2809, 0.2214],
        [0.6961, 0.2800, 0.2215],
        [0.6962, 0.2756, 0.2197],
        [0.6955, 0.2734, 0.2192],
        [0.6956, 0.2706, 0.2180],
        [0.6952, 0.2690, 0.2181],
        [0.6945, 0.2690, 0.2190],
        [0.6939, 0.2698, 0.2197],
        [0.6946, 0.2676, 0.2184],
        [0.6995, 0.2607, 0.2147],
        [0.6337, 0.2688, 0.2371],
        [0.5562, 0.3204, 0.2663],
        [0.6421, 0.2623, 0.2577],
        [0.7044, 0.2526, 0.2199],
        [0.6860, 0.2554, 0.2487],
        [0.5976, 0.3141, 0.2505],
        [0.6466, 0.2791, 0.2428],
        [0.6912, 0.2599, 0.2402],
        [0.6689, 0.2571, 0.2568],
        [0.5937, 0.3112, 0.2418],
        [0.5711, 0.3443, 0.2578],
        [0.5236, 0.3636, 0.2960],
        [0.5523, 0.3546, 0.2782],
        [0.5276, 0.3744, 0.3035],
        [0.5251, 0.3730, 0.3082],
        [0.5279, 0.3633, 0.3031],
        [0.5486, 0.3541, 0.2749],
        [0.5410, 0.3515, 0.2628],
        [0.5534, 0.3587, 0.2795],
        [0.5304, 0.3577, 0.2921],
        [0.4625, 0.3929, 0.3703],
        [0.4890, 0.3644, 0.3458],
        [0.5165, 0.3499, 0.3274],
        [0.5015, 0.3654, 0.3336],
        [0.4957, 0.3631, 0.3367],
        [0.4884, 0.3658, 0.3442],
        [0.5297, 0.3619, 0.3008],
        [0.4553, 0.3808, 0.3594],
        [0.4605, 0.3805, 0.3689],
        [0.4965, 0.3685, 0.3355],
        [0.4950, 0.3784, 0.3224],
        [0.6974, 0.2843, 0.2221],
        [0.6992, 0.2837, 0.2214],
        [0.6973, 0.2825, 0.2224],
        [0.6956, 0.2806, 0.2221],
        [0.6934, 0.2802, 0.2223],
        [0.6939, 0.2785, 0.2212],
        [0.6941, 0.2741, 0.2199],
        [0.6931, 0.2724, 0.2200],
        [0.6938, 0.2707, 0.2194],
        [0.6948, 0.2695, 0.2186],
        [0.6955, 0.2689, 0.2181],
        [0.6972, 0.2668, 0.2164],
        [0.6964, 0.2699, 0.2175],
        [0.6979, 0.2658, 0.2161],
        [0.6862, 0.2672, 0.2426],
        [0.5578, 0.3199, 0.2776],
        [0.5339, 0.3444, 0.2952],
        [0.6639, 0.2491, 0.2391],
        [0.6376, 0.2981, 0.2547],
        [0.5424, 0.3479, 0.3024],
        [0.5442, 0.3403, 0.2772],
        [0.5762, 0.3090, 0.2453],
        [0.5034, 0.3655, 0.3179],
        [0.4850, 0.3715, 0.3331],
        [0.5126, 0.3642, 0.3213],
        [0.5127, 0.3647, 0.3218],
        [0.4868, 0.3814, 0.3417],
        [0.4990, 0.3610, 0.3335],
        [0.4933, 0.3685, 0.3175],
        [0.4991, 0.3747, 0.3268],
        [0.4743, 0.3786, 0.3438],
        [0.4615, 0.3742, 0.3507],
        [0.5174, 0.3786, 0.3141],
        [0.5542, 0.3483, 0.2596],
        [0.5112, 0.3746, 0.2977],
        [0.5020, 0.3786, 0.3310],
        [0.4835, 0.3741, 0.3396],
        [0.4809, 0.3679, 0.3255],
        [0.5396, 0.3477, 0.2864],
        [0.5079, 0.3644, 0.3259],
        [0.5358, 0.3535, 0.2984],
        [0.5124, 0.3682, 0.2847],
        [0.4791, 0.3834, 0.3399],
        [0.4889, 0.3597, 0.3432],
        [0.4910, 0.3652, 0.3418],
        [0.5122, 0.3573, 0.3192],
        [0.6973, 0.2822, 0.2223],
        [0.6994, 0.2798, 0.2208],
        [0.6949, 0.2780, 0.2216]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 13: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1038.0306396484375, Std: 907.979736328125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 174891.546875, Mean: 10370.5791015625, Std: 9072.146484375
[DEBUG] Top-3 class probabilities:
tensor([[0.6935, 0.2756, 0.2217],
        [0.6932, 0.2743, 0.2211],
        [0.6942, 0.2719, 0.2198],
        [0.6933, 0.2727, 0.2207],
        [0.6937, 0.2708, 0.2199],
        [0.6937, 0.2707, 0.2195],
        [0.6966, 0.2692, 0.2178],
        [0.6961, 0.2632, 0.2180],
        [0.6262, 0.2642, 0.2481],
        [0.6083, 0.2845, 0.2417],
        [0.6544, 0.2544, 0.2469],
        [0.5759, 0.3334, 0.2800],
        [0.5171, 0.3596, 0.3128],
        [0.6527, 0.2555, 0.2503],
        [0.5720, 0.3172, 0.2655],
        [0.5599, 0.3475, 0.2842],
        [0.5624, 0.3385, 0.2905],
        [0.5007, 0.3805, 0.3361],
        [0.4960, 0.3667, 0.3376],
        [0.4958, 0.3701, 0.3306],
        [0.5562, 0.3544, 0.2909],
        [0.5379, 0.3603, 0.3022],
        [0.5141, 0.3726, 0.3187],
        [0.4935, 0.3713, 0.3353],
        [0.4752, 0.3694, 0.3431],
        [0.5002, 0.3563, 0.3345],
        [0.5069, 0.3648, 0.3275],
        [0.4816, 0.3816, 0.3356],
        [0.4912, 0.3710, 0.3278],
        [0.4701, 0.3750, 0.3520],
        [0.4952, 0.3786, 0.3401],
        [0.5369, 0.3593, 0.2846],
        [0.5584, 0.3572, 0.2671],
        [0.5470, 0.3536, 0.2798],
        [0.5033, 0.3621, 0.2988],
        [0.5192, 0.3633, 0.3103],
        [0.4851, 0.3627, 0.3365],
        [0.5318, 0.3546, 0.3029],
        [0.5227, 0.3650, 0.2894],
        [0.5115, 0.3696, 0.3084],
        [0.4991, 0.3658, 0.3337],
        [0.4904, 0.3516, 0.3310],
        [0.4866, 0.3702, 0.3538],
        [0.6833, 0.2758, 0.2353],
        [0.6360, 0.2686, 0.2591],
        [0.6922, 0.2757, 0.2224],
        [0.6932, 0.2721, 0.2213],
        [0.6936, 0.2709, 0.2204],
        [0.6933, 0.2685, 0.2196],
        [0.6936, 0.2667, 0.2194],
        [0.6934, 0.2679, 0.2198],
        [0.6934, 0.2669, 0.2188],
        [0.6979, 0.2577, 0.2226],
        [0.5938, 0.2990, 0.2226],
        [0.4909, 0.3523, 0.3321],
        [0.5146, 0.3664, 0.3274],
        [0.4943, 0.3656, 0.3305],
        [0.4983, 0.3675, 0.3334],
        [0.5139, 0.3640, 0.3209],
        [0.5303, 0.3629, 0.3151],
        [0.5423, 0.3631, 0.2918],
        [0.5652, 0.3554, 0.2735],
        [0.5091, 0.3788, 0.3180],
        [0.5387, 0.3566, 0.2975],
        [0.5713, 0.3491, 0.2766],
        [0.5996, 0.3330, 0.2457],
        [0.5996, 0.3452, 0.2560],
        [0.5753, 0.3464, 0.2678],
        [0.5834, 0.3468, 0.2630],
        [0.5414, 0.3633, 0.2859],
        [0.5553, 0.3531, 0.2870],
        [0.5278, 0.3665, 0.3032],
        [0.4960, 0.3689, 0.3351],
        [0.4813, 0.3764, 0.3408],
        [0.5279, 0.3572, 0.2891],
        [0.5035, 0.3707, 0.3372],
        [0.4909, 0.3777, 0.3322],
        [0.5247, 0.3609, 0.3157],
        [0.4809, 0.3773, 0.3417],
        [0.5116, 0.3696, 0.3182],
        [0.4956, 0.3817, 0.3098],
        [0.5124, 0.3665, 0.3114],
        [0.5569, 0.3448, 0.2798],
        [0.5298, 0.3561, 0.2908],
        [0.5073, 0.3787, 0.2702],
        [0.5652, 0.3513, 0.2660],
        [0.5229, 0.3438, 0.2941],
        [0.5393, 0.3423, 0.3018],
        [0.4770, 0.3762, 0.3481],
        [0.6513, 0.2627, 0.2582],
        [0.6977, 0.2697, 0.2399],
        [0.6955, 0.2769, 0.2229],
        [0.6939, 0.2692, 0.2226],
        [0.6867, 0.2723, 0.2266],
        [0.6824, 0.2631, 0.2437],
        [0.6971, 0.2668, 0.2223],
        [0.6955, 0.2632, 0.2181],
        [0.7108, 0.2446, 0.2102],
        [0.5254, 0.3499, 0.3001],
        [0.4884, 0.3598, 0.3347],
        [0.4756, 0.3656, 0.3429],
        [0.4726, 0.3551, 0.3400],
        [0.4787, 0.3618, 0.3426],
        [0.4327, 0.3884, 0.3666],
        [0.4408, 0.3820, 0.3642],
        [0.5141, 0.3730, 0.3248],
        [0.5333, 0.3587, 0.2994],
        [0.5485, 0.3452, 0.2988],
        [0.5565, 0.3585, 0.2750],
        [0.4995, 0.3791, 0.3150],
        [0.5074, 0.3591, 0.3219],
        [0.5403, 0.3558, 0.3042],
        [0.5388, 0.3530, 0.2860],
        [0.5566, 0.3521, 0.2848],
        [0.5592, 0.3590, 0.2788],
        [0.5396, 0.3561, 0.3026],
        [0.5122, 0.3647, 0.3226],
        [0.5313, 0.3641, 0.2858],
        [0.5671, 0.3552, 0.2735],
        [0.5223, 0.3638, 0.3067],
        [0.5007, 0.3830, 0.3263],
        [0.4854, 0.3805, 0.3230],
        [0.5050, 0.3669, 0.3291],
        [0.4899, 0.3736, 0.3305],
        [0.4779, 0.3640, 0.3340],
        [0.5101, 0.3590, 0.3229],
        [0.5101, 0.3641, 0.3157],
        [0.5143, 0.3754, 0.3040]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 14: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13832.0, Mean: 1135.717529296875, Std: 909.3834838867188
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 138202.484375, Mean: 11346.6279296875, Std: 9086.1728515625
[DEBUG] Top-3 class probabilities:
tensor([[0.4938, 0.3820, 0.3015],
        [0.5086, 0.3753, 0.2869],
        [0.5056, 0.3762, 0.3061],
        [0.5213, 0.3597, 0.2987],
        [0.4939, 0.3699, 0.3306],
        [0.5466, 0.3319, 0.2924],
        [0.4627, 0.3786, 0.3434],
        [0.5144, 0.3349, 0.3019],
        [0.5954, 0.3091, 0.2464],
        [0.6851, 0.2683, 0.2369],
        [0.6924, 0.2622, 0.2241],
        [0.6766, 0.2630, 0.2378],
        [0.6946, 0.2622, 0.2222],
        [0.6944, 0.2663, 0.2205],
        [0.6960, 0.2620, 0.2205],
        [0.7039, 0.2600, 0.2219],
        [0.6028, 0.2947, 0.2409],
        [0.4647, 0.3873, 0.3512],
        [0.4404, 0.3712, 0.3532],
        [0.4532, 0.3768, 0.3472],
        [0.4667, 0.3747, 0.3361],
        [0.5047, 0.3585, 0.3205],
        [0.4737, 0.3703, 0.3402],
        [0.4650, 0.3916, 0.3290],
        [0.5046, 0.3743, 0.3172],
        [0.5172, 0.3840, 0.3131],
        [0.4752, 0.3918, 0.3470],
        [0.4895, 0.3702, 0.3281],
        [0.4794, 0.3766, 0.3345],
        [0.4690, 0.3640, 0.3378],
        [0.4569, 0.3667, 0.3425],
        [0.4886, 0.3604, 0.3289],
        [0.5045, 0.3728, 0.3200],
        [0.5360, 0.3551, 0.2974],
        [0.5148, 0.3665, 0.3065],
        [0.5603, 0.3603, 0.2726],
        [0.5118, 0.3829, 0.2931],
        [0.5279, 0.3770, 0.2930],
        [0.5338, 0.3519, 0.2906],
        [0.5171, 0.3569, 0.3006],
        [0.5217, 0.3596, 0.3046],
        [0.5029, 0.3596, 0.3180],
        [0.4943, 0.3734, 0.3370],
        [0.4887, 0.3622, 0.3377],
        [0.4848, 0.3633, 0.3308],
        [0.5304, 0.3574, 0.2953],
        [0.5313, 0.3805, 0.2975],
        [0.4837, 0.3899, 0.3214],
        [0.5090, 0.3653, 0.3140],
        [0.4565, 0.3724, 0.3545],
        [0.4653, 0.3720, 0.3457],
        [0.5043, 0.3564, 0.3435],
        [0.4536, 0.3741, 0.3508],
        [0.5854, 0.3080, 0.2444],
        [0.5904, 0.3167, 0.2613],
        [0.6669, 0.2499, 0.2495],
        [0.6780, 0.2697, 0.2341],
        [0.6413, 0.2682, 0.2430],
        [0.5861, 0.2947, 0.2422],
        [0.5701, 0.2885, 0.2411],
        [0.5817, 0.3220, 0.2544],
        [0.5375, 0.3409, 0.3020],
        [0.4597, 0.3788, 0.3587],
        [0.4717, 0.3739, 0.3565],
        [0.5124, 0.3644, 0.3181],
        [0.4651, 0.3806, 0.3482],
        [0.4965, 0.3623, 0.3244],
        [0.4927, 0.3670, 0.3384],
        [0.4650, 0.3906, 0.3435],
        [0.4850, 0.3876, 0.3502],
        [0.5297, 0.3703, 0.3027],
        [0.5454, 0.3542, 0.2948],
        [0.5211, 0.3599, 0.3129],
        [0.4823, 0.3689, 0.3355],
        [0.5112, 0.3702, 0.3133],
        [0.5163, 0.3536, 0.3038],
        [0.5199, 0.3535, 0.2996],
        [0.4754, 0.3736, 0.3237],
        [0.4897, 0.3691, 0.3387],
        [0.5047, 0.3595, 0.3270],
        [0.4805, 0.3744, 0.3262],
        [0.4810, 0.3825, 0.3388],
        [0.5375, 0.3665, 0.3003],
        [0.5314, 0.3670, 0.2911],
        [0.5722, 0.3581, 0.2651],
        [0.5367, 0.3746, 0.2952],
        [0.5449, 0.3520, 0.2823],
        [0.5212, 0.3665, 0.2989],
        [0.5056, 0.3675, 0.3170],
        [0.4824, 0.3767, 0.3317],
        [0.4514, 0.3951, 0.3580],
        [0.5057, 0.3700, 0.3226],
        [0.5439, 0.3629, 0.2769],
        [0.4874, 0.3825, 0.3189],
        [0.4750, 0.3596, 0.3275],
        [0.5133, 0.3437, 0.3088],
        [0.4850, 0.3706, 0.3333],
        [0.4563, 0.3805, 0.3498],
        [0.4195, 0.4034, 0.3672],
        [0.6902, 0.2739, 0.2355],
        [0.6898, 0.2672, 0.2456],
        [0.6552, 0.2603, 0.2591],
        [0.6893, 0.2694, 0.2358],
        [0.6870, 0.2657, 0.2342],
        [0.5992, 0.3037, 0.2492],
        [0.6656, 0.2695, 0.2486],
        [0.5190, 0.3632, 0.3023],
        [0.4839, 0.3752, 0.3362],
        [0.4809, 0.3738, 0.3410],
        [0.5202, 0.3532, 0.3145],
        [0.4768, 0.3702, 0.3368],
        [0.4917, 0.3691, 0.3351],
        [0.4936, 0.3714, 0.3279],
        [0.4745, 0.3758, 0.3355],
        [0.4800, 0.3835, 0.3411],
        [0.5006, 0.3803, 0.3181],
        [0.5236, 0.3776, 0.3115],
        [0.4963, 0.3725, 0.3250],
        [0.4694, 0.3770, 0.3267],
        [0.4852, 0.3674, 0.3357],
        [0.5298, 0.3532, 0.2998],
        [0.5229, 0.3601, 0.3159],
        [0.4978, 0.3675, 0.3250],
        [0.4807, 0.3557, 0.3286],
        [0.4775, 0.3630, 0.3316],
        [0.4773, 0.3776, 0.3339],
        [0.4238, 0.3967, 0.3747],
        [0.4379, 0.3934, 0.3649]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 15: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13946.0, Mean: 1730.210693359375, Std: 1494.169921875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 139341.515625, Mean: 17286.548828125, Std: 14929.1103515625
[DEBUG] Top-3 class probabilities:
tensor([[0.5026, 0.3694, 0.3236],
        [0.5108, 0.3644, 0.3338],
        [0.4994, 0.3677, 0.3289],
        [0.5432, 0.3568, 0.2726],
        [0.5463, 0.3679, 0.2929],
        [0.5088, 0.3834, 0.3069],
        [0.4995, 0.3743, 0.3085],
        [0.4980, 0.3789, 0.3288],
        [0.4822, 0.3725, 0.3389],
        [0.4898, 0.3450, 0.3187],
        [0.5046, 0.3746, 0.3076],
        [0.5017, 0.3643, 0.3351],
        [0.5144, 0.3569, 0.3200],
        [0.5688, 0.3285, 0.2798],
        [0.4416, 0.3953, 0.3472],
        [0.4695, 0.3637, 0.3436],
        [0.4217, 0.4039, 0.3821],
        [0.6295, 0.2878, 0.2586],
        [0.6574, 0.2648, 0.2602],
        [0.6146, 0.2885, 0.2438],
        [0.6852, 0.2646, 0.2302],
        [0.6968, 0.2687, 0.2206],
        [0.6942, 0.2580, 0.2293],
        [0.5356, 0.3649, 0.2940],
        [0.5278, 0.3746, 0.3059],
        [0.5002, 0.3681, 0.3289],
        [0.4745, 0.3961, 0.3415],
        [0.4652, 0.3843, 0.3512],
        [0.4956, 0.3649, 0.3197],
        [0.4976, 0.3573, 0.3273],
        [0.5091, 0.3616, 0.3049],
        [0.5319, 0.3574, 0.2926],
        [0.4962, 0.3748, 0.2998],
        [0.5094, 0.3710, 0.3092],
        [0.4929, 0.3660, 0.3182],
        [0.4609, 0.3614, 0.3500],
        [0.4732, 0.3739, 0.3250],
        [0.4954, 0.3571, 0.3163],
        [0.5074, 0.3673, 0.3211],
        [0.5228, 0.3690, 0.3085],
        [0.5092, 0.3750, 0.3109],
        [0.4669, 0.3713, 0.3136],
        [0.4860, 0.3719, 0.3167],
        [0.5084, 0.3597, 0.3154],
        [0.4737, 0.3769, 0.3350],
        [0.4769, 0.3796, 0.3350],
        [0.4721, 0.3668, 0.3410],
        [0.4704, 0.3827, 0.3556],
        [0.5105, 0.3732, 0.3017],
        [0.5980, 0.3415, 0.2453],
        [0.5491, 0.3700, 0.2702],
        [0.5285, 0.3723, 0.2921],
        [0.5091, 0.3682, 0.3214],
        [0.4964, 0.3721, 0.3174],
        [0.4950, 0.3702, 0.3295],
        [0.4930, 0.3608, 0.3264],
        [0.4832, 0.3631, 0.3301],
        [0.4563, 0.3678, 0.3426],
        [0.4585, 0.3652, 0.3402],
        [0.4847, 0.3584, 0.3298],
        [0.4453, 0.3774, 0.3518],
        [0.4942, 0.3621, 0.3202],
        [0.5009, 0.4103, 0.2887],
        [0.4808, 0.3823, 0.3173],
        [0.5116, 0.4164, 0.2606],
        [0.5035, 0.4012, 0.2776],
        [0.4975, 0.4009, 0.2973],
        [0.5019, 0.4098, 0.2859],
        [0.5185, 0.4137, 0.2618],
        [0.4880, 0.3848, 0.3113],
        [0.4620, 0.3703, 0.3611],
        [0.4854, 0.3975, 0.3064],
        [0.4769, 0.3824, 0.3343],
        [0.4867, 0.3851, 0.3212],
        [0.4900, 0.4015, 0.3012],
        [0.4866, 0.4099, 0.3067],
        [0.4166, 0.4093, 0.3562],
        [0.4501, 0.4070, 0.3272],
        [0.4447, 0.3942, 0.3773],
        [0.4545, 0.3978, 0.3589],
        [0.4705, 0.3870, 0.3353],
        [0.4284, 0.4105, 0.3082],
        [0.4262, 0.4042, 0.3630],
        [0.4479, 0.3876, 0.3500],
        [0.4364, 0.4085, 0.3479],
        [0.4401, 0.3962, 0.3771],
        [0.4856, 0.3948, 0.3231],
        [0.4442, 0.3686, 0.3538],
        [0.4710, 0.3684, 0.3523],
        [0.4954, 0.3941, 0.2872],
        [0.5093, 0.4052, 0.2802],
        [0.5132, 0.4135, 0.2764],
        [0.4802, 0.3863, 0.3316],
        [0.4936, 0.4015, 0.3095],
        [0.4681, 0.3665, 0.3444],
        [0.4931, 0.4002, 0.3143],
        [0.4967, 0.3864, 0.3125],
        [0.4961, 0.3902, 0.2617],
        [0.4493, 0.3771, 0.3759],
        [0.4488, 0.3610, 0.3509],
        [0.4788, 0.3958, 0.3363],
        [0.4927, 0.4022, 0.3154],
        [0.4909, 0.3871, 0.2959],
        [0.4543, 0.3716, 0.3522],
        [0.4718, 0.3748, 0.3444],
        [0.4209, 0.4004, 0.3637],
        [0.4705, 0.3808, 0.3600],
        [0.4838, 0.3950, 0.3280],
        [0.4534, 0.3637, 0.3617],
        [0.4823, 0.3910, 0.3296],
        [0.5112, 0.3944, 0.2775],
        [0.5099, 0.4198, 0.2514],
        [0.4917, 0.4133, 0.2987],
        [0.5169, 0.4153, 0.2524],
        [0.5331, 0.4102, 0.2505],
        [0.5172, 0.4178, 0.2680],
        [0.4923, 0.4041, 0.3111],
        [0.4768, 0.3961, 0.3310],
        [0.4999, 0.4090, 0.2884],
        [0.4808, 0.3955, 0.3198],
        [0.4602, 0.3759, 0.3509],
        [0.4808, 0.4079, 0.3212],
        [0.4808, 0.4007, 0.3201],
        [0.4415, 0.4043, 0.3450],
        [0.4473, 0.3799, 0.3745],
        [0.4325, 0.3847, 0.3711],
        [0.4789, 0.4019, 0.3269],
        [0.4663, 0.3779, 0.3554]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 16: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9543.0, Mean: 2246.700927734375, Std: 1754.857666015625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 95348.6171875, Mean: 22447.099609375, Std: 17533.791015625
[DEBUG] Top-3 class probabilities:
tensor([[0.4771, 0.3864, 0.3186],
        [0.4510, 0.3795, 0.3628],
        [0.4571, 0.3707, 0.3611],
        [0.4573, 0.3938, 0.3494],
        [0.4258, 0.4006, 0.3612],
        [0.4907, 0.3959, 0.3218],
        [0.4903, 0.3935, 0.3001],
        [0.4865, 0.3885, 0.3109],
        [0.4882, 0.4005, 0.2935],
        [0.4833, 0.3939, 0.3226],
        [0.4831, 0.3905, 0.3127],
        [0.4865, 0.3948, 0.2906],
        [0.4953, 0.3948, 0.2834],
        [0.4913, 0.3856, 0.3003],
        [0.4955, 0.3907, 0.3044],
        [0.4415, 0.3853, 0.3431],
        [0.4854, 0.3958, 0.3103],
        [0.4874, 0.3969, 0.3025],
        [0.4883, 0.3829, 0.3217],
        [0.4776, 0.3876, 0.3176],
        [0.4239, 0.4196, 0.3157],
        [0.4962, 0.4092, 0.2935],
        [0.4285, 0.4083, 0.3523],
        [0.4171, 0.4076, 0.3789],
        [0.4974, 0.4066, 0.2911],
        [0.4551, 0.3701, 0.3607],
        [0.4770, 0.3843, 0.3350],
        [0.5067, 0.4124, 0.2771],
        [0.5255, 0.4083, 0.2613],
        [0.4559, 0.3700, 0.3598],
        [0.4301, 0.3983, 0.3330],
        [0.4886, 0.3793, 0.3156],
        [0.5062, 0.4079, 0.2778],
        [0.4928, 0.3885, 0.3006],
        [0.4876, 0.3819, 0.3299],
        [0.4986, 0.4064, 0.3033],
        [0.5230, 0.4224, 0.2400],
        [0.4981, 0.3944, 0.3042],
        [0.5074, 0.4216, 0.2677],
        [0.4838, 0.3839, 0.3339],
        [0.4272, 0.4065, 0.3508],
        [0.4202, 0.3971, 0.3789],
        [0.4729, 0.3987, 0.3233],
        [0.4973, 0.4218, 0.2846],
        [0.4468, 0.4048, 0.3396],
        [0.4419, 0.3769, 0.3677],
        [0.4171, 0.4099, 0.3492],
        [0.4772, 0.3961, 0.3315],
        [0.4539, 0.3856, 0.3637],
        [0.5006, 0.4105, 0.2947],
        [0.4229, 0.4075, 0.3646],
        [0.4481, 0.3876, 0.3652],
        [0.4802, 0.3893, 0.3290],
        [0.4671, 0.3829, 0.3268],
        [0.4809, 0.3845, 0.3112],
        [0.4879, 0.3844, 0.3293],
        [0.5234, 0.4061, 0.2699],
        [0.4897, 0.3909, 0.3157],
        [0.4883, 0.4067, 0.2914],
        [0.4434, 0.3918, 0.3468],
        [0.4268, 0.4174, 0.3248],
        [0.5177, 0.4223, 0.2819],
        [0.4538, 0.3667, 0.3580],
        [0.4409, 0.3896, 0.3742],
        [0.4209, 0.4006, 0.3744],
        [0.4728, 0.3842, 0.3118],
        [0.4531, 0.3838, 0.3757],
        [0.4841, 0.3956, 0.3415],
        [0.4335, 0.3937, 0.3634],
        [0.4745, 0.3807, 0.3310],
        [0.4822, 0.4014, 0.3116],
        [0.4816, 0.4034, 0.3207],
        [0.4604, 0.3663, 0.3579],
        [0.4757, 0.3675, 0.3228],
        [0.4630, 0.3685, 0.3621],
        [0.4970, 0.4005, 0.3139],
        [0.5212, 0.4240, 0.2227],
        [0.4960, 0.3970, 0.2935],
        [0.4908, 0.3893, 0.2951],
        [0.5070, 0.4095, 0.2809],
        [0.4937, 0.4026, 0.3021],
        [0.5045, 0.4080, 0.2673],
        [0.4904, 0.3876, 0.2897],
        [0.4822, 0.4019, 0.3181],
        [0.4819, 0.3978, 0.3069],
        [0.4095, 0.3832, 0.3832],
        [0.4444, 0.3915, 0.3448],
        [0.4841, 0.4112, 0.3231],
        [0.4352, 0.3840, 0.3778],
        [0.4271, 0.4057, 0.3861],
        [0.4338, 0.4014, 0.3673],
        [0.4399, 0.3872, 0.3812],
        [0.4958, 0.3983, 0.2933],
        [0.4992, 0.3944, 0.3031],
        [0.4850, 0.3897, 0.3176],
        [0.5009, 0.3982, 0.2896],
        [0.5085, 0.3973, 0.2840],
        [0.4770, 0.3903, 0.3350],
        [0.4425, 0.4120, 0.3236],
        [0.4769, 0.3841, 0.3315],
        [0.4706, 0.3671, 0.3461],
        [0.4986, 0.3951, 0.2694],
        [0.4949, 0.3887, 0.3011],
        [0.4872, 0.3977, 0.3159],
        [0.5013, 0.3895, 0.2809],
        [0.4906, 0.3967, 0.3094],
        [0.5112, 0.4114, 0.2718],
        [0.5151, 0.4088, 0.2768],
        [0.4764, 0.3802, 0.3244],
        [0.4889, 0.4087, 0.3103],
        [0.4982, 0.3953, 0.2755],
        [0.4341, 0.3697, 0.3460],
        [0.4919, 0.4046, 0.3024],
        [0.4835, 0.3924, 0.2951],
        [0.4869, 0.3955, 0.3088],
        [0.4759, 0.3928, 0.3393],
        [0.4765, 0.3793, 0.3233],
        [0.4562, 0.3912, 0.3426],
        [0.5040, 0.4123, 0.2834],
        [0.4887, 0.3825, 0.3143],
        [0.4806, 0.3992, 0.3335],
        [0.4659, 0.3814, 0.3344],
        [0.5114, 0.3968, 0.2688],
        [0.5084, 0.4037, 0.2759],
        [0.5181, 0.4051, 0.2648],
        [0.5058, 0.4129, 0.2721],
        [0.5029, 0.4018, 0.2871],
        [0.5128, 0.4117, 0.2669]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 17: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 23.0, Max: 7947.0, Mean: 2283.306640625, Std: 1762.7548828125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 228.82595825195312, Max: 79402.0625, Mean: 22812.845703125, Std: 17612.697265625
[DEBUG] Top-3 class probabilities:
tensor([[0.4838, 0.3810, 0.3308],
        [0.4850, 0.3967, 0.3122],
        [0.5011, 0.4113, 0.2809],
        [0.4533, 0.3890, 0.3388],
        [0.4734, 0.3818, 0.3497],
        [0.4382, 0.3807, 0.3734],
        [0.4503, 0.3967, 0.3440],
        [0.4672, 0.4128, 0.3395],
        [0.5113, 0.4254, 0.2700],
        [0.5051, 0.4059, 0.2681],
        [0.5034, 0.4023, 0.2914],
        [0.4954, 0.3842, 0.2923],
        [0.4514, 0.3658, 0.3572],
        [0.5100, 0.4153, 0.2857],
        [0.4933, 0.3969, 0.2980],
        [0.5194, 0.4111, 0.2726],
        [0.4656, 0.3671, 0.3400],
        [0.4657, 0.3695, 0.3639],
        [0.4810, 0.3836, 0.3332],
        [0.4926, 0.3941, 0.2916],
        [0.4861, 0.3962, 0.3128],
        [0.4201, 0.4144, 0.3278],
        [0.4642, 0.3807, 0.3464],
        [0.5101, 0.4166, 0.2761],
        [0.4727, 0.3586, 0.3534],
        [0.4759, 0.3746, 0.3173],
        [0.4721, 0.3732, 0.3331],
        [0.4394, 0.3719, 0.3496],
        [0.4977, 0.4089, 0.3057],
        [0.4971, 0.3981, 0.2811],
        [0.4727, 0.4110, 0.3238],
        [0.4769, 0.3936, 0.3224],
        [0.5074, 0.4037, 0.2751],
        [0.4934, 0.4004, 0.3122],
        [0.4901, 0.3820, 0.3080],
        [0.4628, 0.3738, 0.3553],
        [0.4551, 0.3677, 0.3633],
        [0.4309, 0.4110, 0.3506],
        [0.4772, 0.3889, 0.3570],
        [0.4719, 0.3687, 0.3242],
        [0.5013, 0.3944, 0.2939],
        [0.4986, 0.4125, 0.2913],
        [0.5203, 0.4323, 0.2559],
        [0.5395, 0.4445, 0.2252],
        [0.5183, 0.4167, 0.2574],
        [0.4940, 0.4073, 0.2821],
        [0.5105, 0.4135, 0.2669],
        [0.5073, 0.3874, 0.2749],
        [0.4757, 0.3838, 0.3446],
        [0.4766, 0.4011, 0.3365],
        [0.4504, 0.3672, 0.3652],
        [0.4295, 0.4283, 0.3448],
        [0.4739, 0.3910, 0.3364],
        [0.4801, 0.3880, 0.3143],
        [0.4797, 0.3872, 0.3157],
        [0.5035, 0.4195, 0.2874],
        [0.4926, 0.3880, 0.3049],
        [0.4757, 0.3905, 0.3211],
        [0.4936, 0.3957, 0.2888],
        [0.4662, 0.3720, 0.3477],
        [0.4885, 0.3880, 0.3108],
        [0.4698, 0.3812, 0.3334],
        [0.4365, 0.3921, 0.3517],
        [0.4844, 0.3962, 0.3133],
        [0.5067, 0.4157, 0.2624],
        [0.4902, 0.3938, 0.2954],
        [0.5188, 0.4050, 0.2531],
        [0.4766, 0.3825, 0.3292],
        [0.5027, 0.4098, 0.2936],
        [0.4925, 0.4034, 0.3115],
        [0.5040, 0.4069, 0.2700],
        [0.4704, 0.3671, 0.3327],
        [0.4788, 0.4000, 0.3128],
        [0.4514, 0.3723, 0.3506],
        [0.4316, 0.4054, 0.3704],
        [0.5227, 0.4106, 0.2569],
        [0.4898, 0.3895, 0.2961],
        [0.4978, 0.3986, 0.2970],
        [0.4879, 0.3864, 0.3075],
        [0.4212, 0.4124, 0.3408],
        [0.4775, 0.3993, 0.3351],
        [0.4360, 0.4065, 0.3429],
        [0.4521, 0.3787, 0.3537],
        [0.4741, 0.3779, 0.3269],
        [0.4956, 0.3933, 0.3224],
        [0.4913, 0.3935, 0.2997],
        [0.4874, 0.3902, 0.3131],
        [0.5181, 0.4070, 0.2587],
        [0.5153, 0.4252, 0.2534],
        [0.5223, 0.4190, 0.2426],
        [0.4983, 0.3946, 0.2930],
        [0.4271, 0.4116, 0.3073],
        [0.4891, 0.4044, 0.2931],
        [0.4787, 0.3755, 0.3335],
        [0.5013, 0.4009, 0.2932],
        [0.4936, 0.4015, 0.3039],
        [0.4860, 0.3891, 0.3093],
        [0.4368, 0.4001, 0.3402],
        [0.4648, 0.3650, 0.3479],
        [0.4846, 0.3911, 0.3169],
        [0.4838, 0.4057, 0.3201],
        [0.4841, 0.3799, 0.3075],
        [0.5005, 0.3921, 0.2904],
        [0.4861, 0.3950, 0.3147],
        [0.4744, 0.3658, 0.3623],
        [0.4913, 0.4032, 0.2899],
        [0.5009, 0.3991, 0.2825],
        [0.4537, 0.3709, 0.3495],
        [0.4616, 0.3771, 0.3532],
        [0.4843, 0.3726, 0.3087],
        [0.4966, 0.4059, 0.2776],
        [0.4897, 0.3922, 0.3002],
        [0.4825, 0.3905, 0.3127],
        [0.4803, 0.3735, 0.3200],
        [0.4518, 0.3678, 0.3631],
        [0.4979, 0.4026, 0.2960],
        [0.4976, 0.3925, 0.3054],
        [0.5175, 0.4156, 0.2766],
        [0.5016, 0.4091, 0.2814],
        [0.5011, 0.3925, 0.2793],
        [0.5068, 0.4094, 0.2613],
        [0.4948, 0.4106, 0.2842],
        [0.4555, 0.3794, 0.3457],
        [0.4805, 0.3924, 0.3144],
        [0.4605, 0.3979, 0.3423],
        [0.4188, 0.4054, 0.3974],
        [0.4148, 0.4086, 0.3588],
        [0.4520, 0.4138, 0.3216]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 18: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11776.0, Mean: 2270.577880859375, Std: 1789.537109375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 117659.8046875, Mean: 22685.669921875, Std: 17880.29296875
[DEBUG] Top-3 class probabilities:
tensor([[0.4771, 0.3712, 0.3448],
        [0.4702, 0.3765, 0.3549],
        [0.4541, 0.3798, 0.3642],
        [0.5102, 0.4196, 0.2853],
        [0.5113, 0.4064, 0.2959],
        [0.4992, 0.3959, 0.2871],
        [0.4915, 0.3946, 0.3116],
        [0.4646, 0.3663, 0.3597],
        [0.5023, 0.3955, 0.2968],
        [0.4398, 0.3842, 0.3471],
        [0.5126, 0.4141, 0.2756],
        [0.4846, 0.3807, 0.3090],
        [0.4601, 0.3697, 0.3541],
        [0.4773, 0.3756, 0.3367],
        [0.4915, 0.3965, 0.3084],
        [0.4980, 0.4078, 0.3024],
        [0.4761, 0.3577, 0.3386],
        [0.4246, 0.4088, 0.3227],
        [0.4750, 0.4028, 0.3200],
        [0.5069, 0.3996, 0.2722],
        [0.5103, 0.4001, 0.2893],
        [0.4325, 0.4093, 0.3321],
        [0.4311, 0.4258, 0.3297],
        [0.4915, 0.3909, 0.2965],
        [0.5021, 0.3994, 0.2796],
        [0.4742, 0.3634, 0.3578],
        [0.4466, 0.4061, 0.3347],
        [0.4748, 0.3734, 0.3486],
        [0.4700, 0.3854, 0.3583],
        [0.4796, 0.3813, 0.3332],
        [0.4468, 0.3920, 0.3469],
        [0.4376, 0.4100, 0.3499],
        [0.4367, 0.3972, 0.3463],
        [0.5087, 0.3981, 0.2928],
        [0.5146, 0.4029, 0.2748],
        [0.4833, 0.3924, 0.3067],
        [0.5155, 0.4225, 0.2650],
        [0.4896, 0.3784, 0.3028],
        [0.5135, 0.4065, 0.2741],
        [0.4929, 0.4044, 0.2896],
        [0.5186, 0.4099, 0.2673],
        [0.4284, 0.4020, 0.3556],
        [0.4186, 0.4042, 0.3634],
        [0.4178, 0.4106, 0.3699],
        [0.4483, 0.3948, 0.3488],
        [0.4568, 0.3839, 0.3575],
        [0.4559, 0.3842, 0.3614],
        [0.4414, 0.4148, 0.3373],
        [0.4521, 0.3727, 0.3540],
        [0.4884, 0.3946, 0.3200],
        [0.5089, 0.4229, 0.2639],
        [0.5075, 0.3993, 0.3032],
        [0.4975, 0.4056, 0.2943],
        [0.5099, 0.4037, 0.2673],
        [0.4770, 0.3704, 0.3136],
        [0.4794, 0.3721, 0.3295],
        [0.4322, 0.4186, 0.3222],
        [0.4436, 0.4005, 0.3322],
        [0.4509, 0.3745, 0.3516],
        [0.4421, 0.3561, 0.3442],
        [0.4752, 0.3877, 0.3272],
        [0.4758, 0.3851, 0.3325],
        [0.4530, 0.3824, 0.3561],
        [0.4584, 0.3793, 0.3538],
        [0.4476, 0.3847, 0.3571],
        [0.4324, 0.3870, 0.3435],
        [0.4431, 0.3767, 0.3588],
        [0.5109, 0.4279, 0.2742],
        [0.4468, 0.3836, 0.3639],
        [0.5117, 0.3902, 0.2834],
        [0.4860, 0.3865, 0.3390],
        [0.4757, 0.3696, 0.3429],
        [0.4852, 0.3892, 0.3307],
        [0.5241, 0.4209, 0.2580],
        [0.5028, 0.3836, 0.2935],
        [0.4491, 0.3720, 0.3477],
        [0.4925, 0.3840, 0.2996],
        [0.5010, 0.4023, 0.3094],
        [0.4755, 0.3666, 0.3438],
        [0.5076, 0.4086, 0.2864],
        [0.4948, 0.4133, 0.2984],
        [0.4907, 0.3990, 0.2908],
        [0.5135, 0.4109, 0.2754],
        [0.4799, 0.3803, 0.3271],
        [0.4544, 0.3839, 0.3646],
        [0.4742, 0.3769, 0.3124],
        [0.4107, 0.3975, 0.3806],
        [0.4086, 0.4061, 0.3696],
        [0.4523, 0.3868, 0.3557],
        [0.4926, 0.4002, 0.3092],
        [0.4838, 0.3745, 0.3208],
        [0.4686, 0.3611, 0.3575],
        [0.4636, 0.3700, 0.3637],
        [0.4800, 0.3918, 0.3076],
        [0.4802, 0.3821, 0.3226],
        [0.4230, 0.4194, 0.3433],
        [0.4612, 0.3785, 0.3474],
        [0.4753, 0.3857, 0.3211],
        [0.4791, 0.3719, 0.3284],
        [0.4761, 0.3720, 0.3355],
        [0.4411, 0.4169, 0.3468],
        [0.4267, 0.4055, 0.3265],
        [0.5746, 0.3628, 0.2484],
        [0.4994, 0.3905, 0.3004],
        [0.4396, 0.4128, 0.3375],
        [0.4467, 0.3831, 0.3558],
        [0.4509, 0.4156, 0.3142],
        [0.4476, 0.4116, 0.3483],
        [0.4555, 0.3662, 0.3532],
        [0.4617, 0.3722, 0.3631],
        [0.4631, 0.3700, 0.3670],
        [0.4937, 0.3911, 0.2990],
        [0.4873, 0.3914, 0.2976],
        [0.4351, 0.3957, 0.3537],
        [0.4660, 0.3719, 0.3471],
        [0.5045, 0.4092, 0.2931],
        [0.4968, 0.3939, 0.3020],
        [0.4762, 0.3816, 0.3438],
        [0.4561, 0.3627, 0.3613],
        [0.4976, 0.4117, 0.2885],
        [0.4949, 0.3805, 0.3160],
        [0.4996, 0.4061, 0.2884],
        [0.4975, 0.3988, 0.2881],
        [0.4431, 0.3759, 0.3350],
        [0.5088, 0.4113, 0.2702],
        [0.4934, 0.3996, 0.3016],
        [0.4856, 0.3814, 0.3171],
        [0.5016, 0.3959, 0.2957]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 19: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16400.0, Mean: 2271.10498046875, Std: 1771.949951171875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 163860.84375, Mean: 22690.93359375, Std: 17704.5703125
[DEBUG] Top-3 class probabilities:
tensor([[0.4730, 0.3678, 0.3379],
        [0.4388, 0.4260, 0.3192],
        [0.4709, 0.3763, 0.3516],
        [0.4473, 0.3909, 0.3627],
        [0.4378, 0.4129, 0.3455],
        [0.4554, 0.3904, 0.3705],
        [0.4532, 0.3839, 0.3804],
        [0.5122, 0.3919, 0.2642],
        [0.4617, 0.3518, 0.3410],
        [0.4733, 0.3978, 0.3328],
        [0.4959, 0.4033, 0.3152],
        [0.4938, 0.4057, 0.2659],
        [0.5068, 0.4423, 0.2681],
        [0.4753, 0.3999, 0.3059],
        [0.4707, 0.3842, 0.3431],
        [0.4204, 0.4155, 0.3359],
        [0.4426, 0.3951, 0.3466],
        [0.4489, 0.3846, 0.3639],
        [0.4526, 0.4139, 0.3170],
        [0.5592, 0.3636, 0.2454],
        [0.4722, 0.3880, 0.3026],
        [0.4294, 0.4199, 0.3391],
        [0.4413, 0.4082, 0.3516],
        [0.4380, 0.4037, 0.3477],
        [0.4391, 0.4138, 0.3428],
        [0.4452, 0.3995, 0.3725],
        [0.4504, 0.3710, 0.3537],
        [0.4913, 0.3923, 0.3206],
        [0.4616, 0.3699, 0.3633],
        [0.4624, 0.3585, 0.3531],
        [0.5040, 0.3962, 0.2826],
        [0.5131, 0.4168, 0.2650],
        [0.4813, 0.3784, 0.3324],
        [0.4967, 0.4083, 0.2899],
        [0.5170, 0.3986, 0.2744],
        [0.5081, 0.4012, 0.2814],
        [0.4844, 0.3737, 0.3133],
        [0.4953, 0.3908, 0.2972],
        [0.5070, 0.3888, 0.2521],
        [0.4982, 0.3881, 0.2970],
        [0.4284, 0.4129, 0.3275],
        [0.4490, 0.3789, 0.3602],
        [0.4542, 0.3714, 0.3525],
        [0.5142, 0.4053, 0.2731],
        [0.5026, 0.4102, 0.2888],
        [0.4889, 0.3826, 0.3133],
        [0.4700, 0.3891, 0.3372],
        [0.4955, 0.3990, 0.2891],
        [0.4871, 0.3803, 0.2990],
        [0.4696, 0.3695, 0.3386],
        [0.4536, 0.3726, 0.3589],
        [0.4492, 0.3806, 0.3563],
        [0.4769, 0.4041, 0.3423],
        [0.4464, 0.3836, 0.3499],
        [0.4744, 0.3640, 0.3308],
        [0.4551, 0.3836, 0.3704],
        [0.4297, 0.4183, 0.3415],
        [0.4583, 0.3766, 0.3584],
        [0.4953, 0.3951, 0.2782],
        [0.4710, 0.3932, 0.3424],
        [0.4595, 0.3660, 0.3642],
        [0.4533, 0.3754, 0.3488],
        [0.4311, 0.4184, 0.3357],
        [0.4453, 0.3977, 0.3380],
        [0.5225, 0.3739, 0.2830],
        [0.4648, 0.4191, 0.3179],
        [0.4235, 0.4228, 0.3390],
        [0.4731, 0.3824, 0.3201],
        [0.4724, 0.3862, 0.3414],
        [0.4559, 0.3604, 0.3537],
        [0.4449, 0.4207, 0.3290],
        [0.4702, 0.3818, 0.3321],
        [0.4484, 0.3840, 0.3471],
        [0.4401, 0.3819, 0.3420],
        [0.4917, 0.4102, 0.3208],
        [0.4507, 0.3621, 0.3576],
        [0.5035, 0.4016, 0.2865],
        [0.4682, 0.3690, 0.3474],
        [0.5158, 0.3955, 0.2613],
        [0.4818, 0.3803, 0.3192],
        [0.4784, 0.3789, 0.3414],
        [0.4743, 0.3674, 0.3408],
        [0.5178, 0.4160, 0.2701],
        [0.4801, 0.3821, 0.3408],
        [0.5039, 0.3982, 0.2939],
        [0.4644, 0.3628, 0.3413],
        [0.4953, 0.4033, 0.2933],
        [0.4919, 0.3966, 0.3120],
        [0.4967, 0.3824, 0.3035],
        [0.5008, 0.3925, 0.2999],
        [0.4753, 0.3783, 0.3327],
        [0.5060, 0.3954, 0.2765],
        [0.4753, 0.3813, 0.3227],
        [0.4756, 0.3795, 0.3484],
        [0.4675, 0.3828, 0.3414],
        [0.4873, 0.3842, 0.2841],
        [0.5049, 0.3899, 0.2680],
        [0.4750, 0.3867, 0.3462],
        [0.5001, 0.3912, 0.2936],
        [0.4949, 0.4199, 0.3012],
        [0.4637, 0.3732, 0.3588],
        [0.4606, 0.3751, 0.3701],
        [0.4692, 0.3895, 0.3391],
        [0.4848, 0.4021, 0.3193],
        [0.4733, 0.3819, 0.3354],
        [0.4586, 0.3627, 0.3627],
        [0.4348, 0.3957, 0.3425],
        [0.4618, 0.4075, 0.3259],
        [0.4301, 0.3907, 0.3418],
        [0.4454, 0.3741, 0.3596],
        [0.4756, 0.3809, 0.3256],
        [0.4524, 0.3875, 0.3550],
        [0.4834, 0.3798, 0.3330],
        [0.4525, 0.3591, 0.3528],
        [0.4447, 0.3975, 0.3635],
        [0.4696, 0.3756, 0.3680],
        [0.5066, 0.3999, 0.2878],
        [0.5047, 0.3988, 0.3098],
        [0.4800, 0.3856, 0.3427],
        [0.4832, 0.3812, 0.3308],
        [0.4907, 0.3864, 0.3226],
        [0.4864, 0.3789, 0.3221],
        [0.4811, 0.3936, 0.3173],
        [0.4899, 0.4015, 0.3075],
        [0.4975, 0.4097, 0.2946],
        [0.4714, 0.3702, 0.3414],
        [0.4862, 0.3839, 0.3149],
        [0.4693, 0.3661, 0.3230]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 20: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 66.0, Max: 10384.0, Mean: 2312.83154296875, Std: 1820.0330810546875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 658.4636840820312, Max: 103751.53125, Mean: 23107.84765625, Std: 18184.99609375
[DEBUG] Top-3 class probabilities:
tensor([[0.5003, 0.3940, 0.2871],
        [0.5050, 0.4103, 0.2676],
        [0.5067, 0.4139, 0.2758],
        [0.4899, 0.3932, 0.2979],
        [0.5099, 0.4067, 0.2705],
        [0.4582, 0.3730, 0.3509],
        [0.4935, 0.4039, 0.3065],
        [0.5027, 0.3933, 0.2915],
        [0.5148, 0.4175, 0.2587],
        [0.4770, 0.3720, 0.3293],
        [0.4731, 0.3913, 0.3130],
        [0.5258, 0.4311, 0.2397],
        [0.4774, 0.3666, 0.3368],
        [0.4841, 0.3957, 0.3120],
        [0.4769, 0.3757, 0.3369],
        [0.4665, 0.3681, 0.3618],
        [0.4471, 0.3877, 0.3617],
        [0.4884, 0.4054, 0.2997],
        [0.4636, 0.3837, 0.3695],
        [0.4911, 0.3981, 0.3034],
        [0.5108, 0.3924, 0.2941],
        [0.4424, 0.3995, 0.3482],
        [0.4824, 0.3827, 0.3234],
        [0.4372, 0.3838, 0.3636],
        [0.4844, 0.3746, 0.3365],
        [0.4444, 0.4022, 0.3424],
        [0.4494, 0.3694, 0.3501],
        [0.4636, 0.3731, 0.3465],
        [0.4792, 0.3978, 0.3132],
        [0.4324, 0.4113, 0.3441],
        [0.4584, 0.3728, 0.3521],
        [0.4911, 0.3796, 0.3147],
        [0.4716, 0.3713, 0.3478],
        [0.5134, 0.3986, 0.2870],
        [0.5168, 0.4214, 0.2457],
        [0.5148, 0.3974, 0.2601],
        [0.4625, 0.3695, 0.3651],
        [0.4892, 0.3977, 0.3073],
        [0.4721, 0.3744, 0.3341],
        [0.4889, 0.3823, 0.3108],
        [0.4800, 0.3736, 0.3259],
        [0.4661, 0.3741, 0.3645],
        [0.4897, 0.3902, 0.3223],
        [0.4926, 0.4043, 0.3189],
        [0.4838, 0.3904, 0.3206],
        [0.4370, 0.4078, 0.3549],
        [0.4813, 0.4020, 0.3022],
        [0.5157, 0.4209, 0.2742],
        [0.4929, 0.3789, 0.2916],
        [0.5138, 0.3974, 0.2732],
        [0.5114, 0.4207, 0.2804],
        [0.5088, 0.3999, 0.2834],
        [0.4864, 0.3862, 0.3109],
        [0.4914, 0.3941, 0.2998],
        [0.4607, 0.3686, 0.3454],
        [0.5012, 0.4058, 0.2724],
        [0.4954, 0.3954, 0.2908],
        [0.4890, 0.3929, 0.2977],
        [0.4775, 0.3654, 0.3265],
        [0.4435, 0.3808, 0.3701],
        [0.5164, 0.4043, 0.2748],
        [0.4885, 0.3887, 0.3198],
        [0.4677, 0.3560, 0.3458],
        [0.4601, 0.3701, 0.3444],
        [0.4883, 0.4053, 0.3071],
        [0.4967, 0.3916, 0.3160],
        [0.4370, 0.4287, 0.3368],
        [0.4488, 0.3791, 0.3660],
        [0.4319, 0.4193, 0.3287],
        [0.4864, 0.3948, 0.3176],
        [0.4694, 0.3642, 0.3540],
        [0.4735, 0.3696, 0.3368],
        [0.5017, 0.3829, 0.2561],
        [0.4690, 0.3770, 0.3362],
        [0.4315, 0.4291, 0.3351],
        [0.4399, 0.4325, 0.3376],
        [0.4641, 0.4072, 0.3274],
        [0.4805, 0.3885, 0.3397],
        [0.4778, 0.3695, 0.3351],
        [0.4696, 0.3636, 0.3351],
        [0.4709, 0.3722, 0.3548],
        [0.4908, 0.3976, 0.3248],
        [0.5059, 0.4129, 0.2873],
        [0.4517, 0.3581, 0.3578],
        [0.4955, 0.3965, 0.2975],
        [0.4697, 0.3805, 0.3539],
        [0.4488, 0.3799, 0.3553],
        [0.4595, 0.3609, 0.3571],
        [0.4964, 0.3911, 0.2821],
        [0.4720, 0.3828, 0.3373],
        [0.4935, 0.3884, 0.2996],
        [0.4458, 0.3783, 0.3643],
        [0.5020, 0.3928, 0.2961],
        [0.4745, 0.3631, 0.3593],
        [0.4375, 0.3834, 0.3447],
        [0.4639, 0.3803, 0.3503],
        [0.5055, 0.3927, 0.2838],
        [0.5013, 0.4062, 0.2793],
        [0.5050, 0.4071, 0.2863],
        [0.4906, 0.3859, 0.2995],
        [0.4670, 0.3723, 0.3542],
        [0.5060, 0.3973, 0.2708],
        [0.4261, 0.4142, 0.3218],
        [0.4423, 0.3891, 0.3153],
        [0.4197, 0.3952, 0.3860],
        [0.4597, 0.3686, 0.3658],
        [0.4983, 0.4094, 0.2746],
        [0.4620, 0.3637, 0.3614],
        [0.4936, 0.3937, 0.3097],
        [0.4845, 0.3908, 0.3282],
        [0.4587, 0.3889, 0.3482],
        [0.5124, 0.3920, 0.2713],
        [0.4672, 0.3813, 0.3491],
        [0.4843, 0.3905, 0.3249],
        [0.4794, 0.3730, 0.3255],
        [0.4951, 0.4134, 0.2880],
        [0.4562, 0.3805, 0.3571],
        [0.4700, 0.3836, 0.3354],
        [0.4665, 0.3638, 0.3584],
        [0.4722, 0.3765, 0.3459],
        [0.4746, 0.3765, 0.3312],
        [0.4579, 0.3645, 0.3596],
        [0.4860, 0.3977, 0.3137],
        [0.4858, 0.3831, 0.3226],
        [0.4858, 0.3864, 0.3141],
        [0.4939, 0.3960, 0.2896],
        [0.4818, 0.3936, 0.3304],
        [0.5270, 0.4247, 0.2594]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 21: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15560.0, Mean: 2253.34912109375, Std: 1771.7581787109375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 155467.921875, Mean: 22513.52734375, Std: 17702.654296875
[DEBUG] Top-3 class probabilities:
tensor([[0.4764, 0.3806, 0.3268],
        [0.5201, 0.3977, 0.2641],
        [0.5236, 0.4185, 0.2473],
        [0.5077, 0.4057, 0.2736],
        [0.4715, 0.3674, 0.3448],
        [0.4770, 0.3834, 0.3310],
        [0.4910, 0.3864, 0.3219],
        [0.5110, 0.4038, 0.2676],
        [0.5118, 0.3981, 0.2802],
        [0.4764, 0.3924, 0.3217],
        [0.5168, 0.4009, 0.2571],
        [0.4840, 0.3947, 0.3147],
        [0.5077, 0.4157, 0.2696],
        [0.4866, 0.4015, 0.3121],
        [0.4929, 0.3987, 0.3106],
        [0.4682, 0.3514, 0.3282],
        [0.4759, 0.3805, 0.3280],
        [0.4697, 0.3811, 0.3158],
        [0.4465, 0.4036, 0.3290],
        [0.4433, 0.4002, 0.3309],
        [0.4562, 0.3858, 0.3211],
        [0.4080, 0.4008, 0.3960],
        [0.4527, 0.3884, 0.3717],
        [0.4789, 0.3762, 0.3308],
        [0.5185, 0.4148, 0.2496],
        [0.4991, 0.3901, 0.2894],
        [0.5391, 0.4230, 0.2351],
        [0.4667, 0.3902, 0.3077],
        [0.4800, 0.3829, 0.3389],
        [0.4757, 0.3864, 0.3305],
        [0.4475, 0.3753, 0.3429],
        [0.4852, 0.3999, 0.3072],
        [0.4756, 0.3953, 0.3301],
        [0.4433, 0.3809, 0.3641],
        [0.4854, 0.3674, 0.3138],
        [0.4862, 0.3815, 0.3105],
        [0.4482, 0.3764, 0.3532],
        [0.4478, 0.3889, 0.3517],
        [0.4869, 0.3852, 0.3195],
        [0.4703, 0.3780, 0.3328],
        [0.4390, 0.4224, 0.3409],
        [0.4616, 0.3651, 0.3604],
        [0.4911, 0.3844, 0.2896],
        [0.4540, 0.3590, 0.3534],
        [0.4438, 0.3801, 0.3531],
        [0.4764, 0.3702, 0.3047],
        [0.4974, 0.3962, 0.3095],
        [0.5001, 0.4087, 0.2917],
        [0.4833, 0.3871, 0.3005],
        [0.5038, 0.4055, 0.2601],
        [0.4776, 0.3832, 0.3336],
        [0.5176, 0.3973, 0.2570],
        [0.4828, 0.3707, 0.3213],
        [0.4773, 0.3886, 0.3320],
        [0.4560, 0.3670, 0.3597],
        [0.4360, 0.3835, 0.3723],
        [0.5107, 0.4061, 0.2618],
        [0.4692, 0.3774, 0.3281],
        [0.4784, 0.3722, 0.3259],
        [0.5073, 0.4233, 0.2708],
        [0.4757, 0.4081, 0.3229],
        [0.4870, 0.3743, 0.3257],
        [0.4348, 0.4059, 0.3505],
        [0.4285, 0.4079, 0.3424],
        [0.4220, 0.3954, 0.3894],
        [0.4365, 0.4004, 0.3817],
        [0.4205, 0.3929, 0.3745],
        [0.4096, 0.4086, 0.3869],
        [0.4370, 0.4031, 0.3655],
        [0.4893, 0.3892, 0.3131],
        [0.4828, 0.3885, 0.3087],
        [0.4696, 0.3805, 0.3454],
        [0.4759, 0.3991, 0.3085],
        [0.4534, 0.3741, 0.3733],
        [0.4896, 0.4056, 0.3118],
        [0.5041, 0.4120, 0.2759],
        [0.4515, 0.3648, 0.3585],
        [0.5131, 0.4175, 0.2708],
        [0.4843, 0.3907, 0.3282],
        [0.5252, 0.4112, 0.2585],
        [0.4823, 0.3765, 0.3212],
        [0.4761, 0.3887, 0.3344],
        [0.4737, 0.3911, 0.3326],
        [0.4821, 0.3669, 0.3227],
        [0.4553, 0.3821, 0.3691],
        [0.4692, 0.3685, 0.3565],
        [0.4284, 0.4216, 0.3347],
        [0.4985, 0.3906, 0.2939],
        [0.4747, 0.3837, 0.3187],
        [0.4329, 0.4083, 0.3400],
        [0.4359, 0.4046, 0.3510],
        [0.4843, 0.3902, 0.3148],
        [0.5110, 0.3968, 0.2636],
        [0.4768, 0.3758, 0.3202],
        [0.4644, 0.3635, 0.3590],
        [0.5005, 0.4007, 0.2853],
        [0.4934, 0.3808, 0.3025],
        [0.4743, 0.3891, 0.3365],
        [0.4879, 0.3847, 0.3191],
        [0.4350, 0.3786, 0.3655],
        [0.4635, 0.3891, 0.3463],
        [0.4375, 0.3725, 0.3715],
        [0.4202, 0.4163, 0.3425],
        [0.4181, 0.4004, 0.3435],
        [0.5047, 0.4209, 0.2816],
        [0.4755, 0.3716, 0.3263],
        [0.4420, 0.4000, 0.3498],
        [0.4133, 0.3959, 0.3844],
        [0.4099, 0.4023, 0.3824],
        [0.4260, 0.3914, 0.3721],
        [0.4574, 0.3894, 0.3583],
        [0.4801, 0.4001, 0.3094],
        [0.4375, 0.3881, 0.3504],
        [0.4881, 0.3834, 0.3205],
        [0.5204, 0.4239, 0.2427],
        [0.5019, 0.3998, 0.2740],
        [0.5039, 0.4031, 0.2781],
        [0.4977, 0.3936, 0.2882],
        [0.4491, 0.3762, 0.3665],
        [0.4962, 0.4028, 0.3035],
        [0.4880, 0.3840, 0.3148],
        [0.4707, 0.3641, 0.3609],
        [0.4615, 0.3665, 0.3518],
        [0.4857, 0.3929, 0.3163],
        [0.5088, 0.4100, 0.2735],
        [0.4616, 0.3681, 0.3587],
        [0.4300, 0.4037, 0.3376],
        [0.4601, 0.3723, 0.3458]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 22: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17296.0, Mean: 2245.933349609375, Std: 1766.4859619140625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 172813.296875, Mean: 22439.4296875, Std: 17649.9765625
[DEBUG] Top-3 class probabilities:
tensor([[0.4621, 0.3667, 0.3522],
        [0.4533, 0.3706, 0.3670],
        [0.4311, 0.4161, 0.3372],
        [0.4558, 0.3723, 0.3549],
        [0.4803, 0.3790, 0.3240],
        [0.4916, 0.3911, 0.2955],
        [0.4810, 0.3845, 0.3315],
        [0.4670, 0.3743, 0.3582],
        [0.4840, 0.3993, 0.3178],
        [0.4790, 0.3738, 0.3313],
        [0.4968, 0.3925, 0.2740],
        [0.5120, 0.4062, 0.2703],
        [0.5087, 0.4033, 0.2905],
        [0.4865, 0.3786, 0.3043],
        [0.4917, 0.3919, 0.3084],
        [0.4946, 0.3967, 0.3211],
        [0.4608, 0.3770, 0.3433],
        [0.4293, 0.3795, 0.3664],
        [0.4255, 0.3882, 0.3857],
        [0.4230, 0.3956, 0.3785],
        [0.4469, 0.3984, 0.3346],
        [0.4357, 0.4117, 0.3614],
        [0.4282, 0.3858, 0.3708],
        [0.4570, 0.3593, 0.3538],
        [0.4267, 0.3817, 0.3798],
        [0.4691, 0.4010, 0.3383],
        [0.4867, 0.3880, 0.3157],
        [0.5070, 0.4115, 0.2683],
        [0.4893, 0.3983, 0.2951],
        [0.4285, 0.4271, 0.3330],
        [0.4875, 0.3781, 0.3082],
        [0.5160, 0.4268, 0.2603],
        [0.5183, 0.4151, 0.2442],
        [0.5062, 0.4159, 0.2744],
        [0.4976, 0.3944, 0.2990],
        [0.4985, 0.3998, 0.2879],
        [0.4651, 0.3646, 0.3625],
        [0.4609, 0.3718, 0.3546],
        [0.4931, 0.3874, 0.3211],
        [0.4933, 0.3949, 0.2979],
        [0.4979, 0.3927, 0.2754],
        [0.4958, 0.3912, 0.2910],
        [0.4691, 0.3701, 0.3351],
        [0.4650, 0.3686, 0.3581],
        [0.4428, 0.4044, 0.3553],
        [0.4706, 0.3727, 0.3375],
        [0.4516, 0.3702, 0.3571],
        [0.4646, 0.3730, 0.3646],
        [0.4482, 0.3682, 0.3599],
        [0.4818, 0.4019, 0.3060],
        [0.4676, 0.3729, 0.3552],
        [0.4641, 0.3658, 0.3359],
        [0.4585, 0.3589, 0.3522],
        [0.4870, 0.3926, 0.3157],
        [0.4981, 0.3889, 0.2793],
        [0.4953, 0.3924, 0.2937],
        [0.4969, 0.3994, 0.2995],
        [0.4856, 0.3879, 0.3102],
        [0.4418, 0.3878, 0.3687],
        [0.4169, 0.4089, 0.3796],
        [0.4255, 0.3857, 0.3775],
        [0.4391, 0.3886, 0.3693],
        [0.4485, 0.3788, 0.3657],
        [0.4018, 0.4013, 0.3818],
        [0.4693, 0.3943, 0.3393],
        [0.4258, 0.3803, 0.3794],
        [0.4386, 0.3969, 0.3878],
        [0.4280, 0.4156, 0.3395],
        [0.4490, 0.3823, 0.3771],
        [0.4910, 0.4120, 0.3064],
        [0.5297, 0.4144, 0.2518],
        [0.4785, 0.3645, 0.3349],
        [0.4774, 0.3720, 0.3249],
        [0.5129, 0.4019, 0.2723],
        [0.5004, 0.4060, 0.3157],
        [0.4849, 0.3781, 0.3028],
        [0.5129, 0.4049, 0.2838],
        [0.4795, 0.3957, 0.3337],
        [0.5015, 0.4187, 0.2796],
        [0.5146, 0.4009, 0.2619],
        [0.4717, 0.3788, 0.3222],
        [0.4553, 0.3696, 0.3592],
        [0.4780, 0.3738, 0.3415],
        [0.4794, 0.3857, 0.3102],
        [0.4398, 0.3989, 0.3405],
        [0.4902, 0.3880, 0.2936],
        [0.4989, 0.4005, 0.3035],
        [0.4747, 0.3860, 0.3304],
        [0.4823, 0.3806, 0.3100],
        [0.4988, 0.3886, 0.2831],
        [0.4823, 0.3735, 0.3198],
        [0.4809, 0.3775, 0.3257],
        [0.4402, 0.4274, 0.3052],
        [0.4991, 0.3879, 0.3042],
        [0.4883, 0.3880, 0.3100],
        [0.4750, 0.3817, 0.3273],
        [0.4884, 0.3847, 0.3188],
        [0.4205, 0.4202, 0.3266],
        [0.4764, 0.3668, 0.3274],
        [0.5064, 0.4114, 0.2784],
        [0.4984, 0.4074, 0.2863],
        [0.4959, 0.4047, 0.3019],
        [0.5112, 0.4188, 0.2598],
        [0.4948, 0.3928, 0.3093],
        [0.4725, 0.3963, 0.3240],
        [0.4114, 0.3960, 0.3889],
        [0.4762, 0.3895, 0.3215],
        [0.4316, 0.4058, 0.3386],
        [0.4826, 0.3754, 0.3165],
        [0.4418, 0.4201, 0.3374],
        [0.4115, 0.3947, 0.3684],
        [0.4043, 0.4023, 0.3743],
        [0.4109, 0.4031, 0.3709],
        [0.4899, 0.3964, 0.3100],
        [0.5048, 0.3950, 0.2917],
        [0.5275, 0.4214, 0.2524],
        [0.5140, 0.4104, 0.2674],
        [0.5147, 0.3958, 0.2644],
        [0.4907, 0.4052, 0.2965],
        [0.4950, 0.3954, 0.2864],
        [0.4978, 0.3935, 0.2965],
        [0.4944, 0.3962, 0.2870],
        [0.4312, 0.4107, 0.3498],
        [0.5043, 0.4085, 0.2858],
        [0.4948, 0.3898, 0.3022],
        [0.4974, 0.4072, 0.2998],
        [0.5242, 0.4265, 0.2427],
        [0.4642, 0.3776, 0.3278]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 23: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16624.0, Mean: 2321.744873046875, Std: 1837.9111328125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 166098.953125, Mean: 23196.90625, Std: 18363.626953125
[DEBUG] Top-3 class probabilities:
tensor([[0.4829, 0.3683, 0.3275],
        [0.4639, 0.3721, 0.3628],
        [0.4962, 0.3972, 0.3098],
        [0.5043, 0.3968, 0.2943],
        [0.4891, 0.3802, 0.2898],
        [0.5138, 0.4074, 0.2613],
        [0.4913, 0.4205, 0.3073],
        [0.4665, 0.3793, 0.3420],
        [0.4504, 0.4213, 0.3208],
        [0.4813, 0.3886, 0.3139],
        [0.4736, 0.3805, 0.3202],
        [0.4947, 0.3867, 0.3069],
        [0.4631, 0.3692, 0.3330],
        [0.4525, 0.3681, 0.3543],
        [0.4737, 0.3675, 0.3363],
        [0.5106, 0.4086, 0.2779],
        [0.4796, 0.3907, 0.3345],
        [0.5029, 0.4003, 0.2742],
        [0.5008, 0.3938, 0.2711],
        [0.5024, 0.3978, 0.2748],
        [0.4492, 0.4065, 0.3570],
        [0.4910, 0.3935, 0.2880],
        [0.4531, 0.3581, 0.3450],
        [0.4255, 0.4143, 0.3377],
        [0.4252, 0.3866, 0.3817],
        [0.4369, 0.4022, 0.3740],
        [0.4663, 0.3830, 0.3646],
        [0.4991, 0.4071, 0.2860],
        [0.5177, 0.4218, 0.2503],
        [0.4772, 0.3756, 0.3239],
        [0.4957, 0.3728, 0.2900],
        [0.4855, 0.3889, 0.3046],
        [0.4973, 0.3848, 0.3003],
        [0.4894, 0.3868, 0.3144],
        [0.4949, 0.4047, 0.2995],
        [0.4940, 0.3886, 0.2734],
        [0.4607, 0.3719, 0.3538],
        [0.4765, 0.3833, 0.3162],
        [0.4700, 0.3886, 0.3110],
        [0.4538, 0.3748, 0.3436],
        [0.4697, 0.3680, 0.3492],
        [0.4530, 0.3709, 0.3510],
        [0.4872, 0.3928, 0.3109],
        [0.5255, 0.4088, 0.2501],
        [0.5123, 0.4099, 0.2906],
        [0.5040, 0.3958, 0.2959],
        [0.4542, 0.3627, 0.3620],
        [0.4683, 0.3659, 0.3601],
        [0.4413, 0.3995, 0.3434],
        [0.4902, 0.3899, 0.2965],
        [0.5034, 0.3899, 0.2964],
        [0.4715, 0.3804, 0.3459],
        [0.4827, 0.3991, 0.3131],
        [0.4239, 0.4203, 0.3480],
        [0.4445, 0.3695, 0.3420],
        [0.4847, 0.3711, 0.3371],
        [0.4680, 0.3746, 0.3373],
        [0.4677, 0.3807, 0.3344],
        [0.4751, 0.3801, 0.3508],
        [0.4659, 0.3642, 0.3424],
        [0.4640, 0.3738, 0.3427],
        [0.4692, 0.3726, 0.3254],
        [0.4930, 0.4020, 0.2904],
        [0.5203, 0.4012, 0.2544],
        [0.5025, 0.4067, 0.2834],
        [0.4746, 0.3771, 0.3112],
        [0.4079, 0.4057, 0.3830],
        [0.4149, 0.4115, 0.3632],
        [0.4907, 0.3837, 0.3109],
        [0.4698, 0.3808, 0.3467],
        [0.4027, 0.4027, 0.3842],
        [0.4117, 0.4076, 0.3803],
        [0.5026, 0.4018, 0.2885],
        [0.4927, 0.4010, 0.2923],
        [0.4800, 0.3969, 0.3096],
        [0.4935, 0.3875, 0.3014],
        [0.5159, 0.4086, 0.2693],
        [0.4865, 0.3737, 0.3112],
        [0.4600, 0.3830, 0.3555],
        [0.4857, 0.3938, 0.3158],
        [0.4684, 0.3649, 0.3479],
        [0.4881, 0.3876, 0.3230],
        [0.4650, 0.3712, 0.3644],
        [0.4753, 0.3896, 0.3305],
        [0.4947, 0.4054, 0.3039],
        [0.4746, 0.3793, 0.3289],
        [0.5075, 0.4085, 0.2593],
        [0.4847, 0.3761, 0.3172],
        [0.4728, 0.3709, 0.3331],
        [0.5230, 0.4158, 0.2521],
        [0.5102, 0.3991, 0.2777],
        [0.5104, 0.4074, 0.2692],
        [0.5010, 0.3942, 0.2881],
        [0.4682, 0.3832, 0.3302],
        [0.4528, 0.3868, 0.3595],
        [0.4624, 0.3758, 0.3609],
        [0.4731, 0.3823, 0.3318],
        [0.5065, 0.4040, 0.2689],
        [0.4854, 0.3949, 0.3402],
        [0.5011, 0.3992, 0.2981],
        [0.4742, 0.3795, 0.3364],
        [0.4878, 0.3728, 0.3039],
        [0.4791, 0.3843, 0.3163],
        [0.4584, 0.3665, 0.3662],
        [0.4604, 0.3728, 0.3694],
        [0.4549, 0.3923, 0.3290],
        [0.4474, 0.3819, 0.3480],
        [0.5113, 0.4025, 0.2630],
        [0.4980, 0.3840, 0.2972],
        [0.4862, 0.3899, 0.3044],
        [0.4854, 0.3920, 0.3172],
        [0.4990, 0.3966, 0.2969],
        [0.4969, 0.4001, 0.3137],
        [0.5292, 0.4286, 0.2398],
        [0.5003, 0.3880, 0.2755],
        [0.5289, 0.4109, 0.2494],
        [0.5212, 0.4097, 0.2407],
        [0.5364, 0.4135, 0.2462],
        [0.5239, 0.4123, 0.2484],
        [0.5293, 0.4229, 0.2507],
        [0.4853, 0.3729, 0.3011],
        [0.4807, 0.3816, 0.3196],
        [0.4897, 0.3921, 0.3189],
        [0.4602, 0.3701, 0.3573],
        [0.4823, 0.3906, 0.3420],
        [0.5037, 0.3988, 0.2799],
        [0.4638, 0.3698, 0.3542],
        [0.5002, 0.4180, 0.2865]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 24: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18480.0, Mean: 2355.58935546875, Std: 1857.9300537109375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 184643.3125, Mean: 23535.064453125, Std: 18563.646484375
[DEBUG] Top-3 class probabilities:
tensor([[0.4949, 0.4099, 0.2972],
        [0.4724, 0.3719, 0.3149],
        [0.5008, 0.3971, 0.2771],
        [0.4610, 0.3758, 0.3488],
        [0.5056, 0.3981, 0.2810],
        [0.4837, 0.3705, 0.3190],
        [0.5003, 0.4051, 0.2841],
        [0.4746, 0.3710, 0.3535],
        [0.4843, 0.3790, 0.3115],
        [0.4926, 0.4041, 0.3092],
        [0.5094, 0.4154, 0.2715],
        [0.5170, 0.4135, 0.2592],
        [0.4631, 0.3600, 0.3459],
        [0.5071, 0.4017, 0.2875],
        [0.5020, 0.4223, 0.2844],
        [0.5106, 0.3949, 0.2760],
        [0.5159, 0.4264, 0.2594],
        [0.4453, 0.3574, 0.3520],
        [0.4335, 0.4031, 0.3515],
        [0.5117, 0.4267, 0.2678],
        [0.4573, 0.3702, 0.3358],
        [0.5073, 0.4110, 0.2784],
        [0.4849, 0.3789, 0.3141],
        [0.4839, 0.3743, 0.3139],
        [0.4671, 0.3564, 0.3481],
        [0.5036, 0.4032, 0.2872],
        [0.4778, 0.3802, 0.3440],
        [0.5233, 0.4335, 0.2594],
        [0.4497, 0.3631, 0.3576],
        [0.4807, 0.4075, 0.3036],
        [0.4767, 0.3875, 0.3121],
        [0.4915, 0.3963, 0.3005],
        [0.5081, 0.3962, 0.2853],
        [0.5228, 0.4031, 0.2639],
        [0.5169, 0.4107, 0.2702],
        [0.5025, 0.3936, 0.2966],
        [0.5177, 0.4088, 0.2634],
        [0.4351, 0.4159, 0.3373],
        [0.4682, 0.3708, 0.3495],
        [0.4370, 0.4027, 0.3474],
        [0.4732, 0.3923, 0.3272],
        [0.4804, 0.3846, 0.3107],
        [0.4937, 0.4015, 0.3184],
        [0.4946, 0.4133, 0.2886],
        [0.4883, 0.3955, 0.3092],
        [0.4869, 0.4070, 0.3052],
        [0.5219, 0.4146, 0.2571],
        [0.4665, 0.3770, 0.3597],
        [0.4823, 0.3830, 0.3372],
        [0.4810, 0.3742, 0.3286],
        [0.5144, 0.3957, 0.2826],
        [0.4691, 0.3784, 0.3495],
        [0.4893, 0.3842, 0.3065],
        [0.4979, 0.4093, 0.3027],
        [0.4795, 0.3822, 0.3346],
        [0.4899, 0.3927, 0.2965],
        [0.4909, 0.3840, 0.3066],
        [0.4841, 0.3820, 0.3134],
        [0.4332, 0.3793, 0.3599],
        [0.4992, 0.4040, 0.2910],
        [0.4842, 0.3873, 0.3182],
        [0.4760, 0.3832, 0.3283],
        [0.5026, 0.4141, 0.2669],
        [0.5101, 0.4062, 0.2726],
        [0.4925, 0.3952, 0.3089],
        [0.4770, 0.3851, 0.3432],
        [0.4573, 0.3605, 0.3519],
        [0.4758, 0.3939, 0.3204],
        [0.4949, 0.3909, 0.2968],
        [0.5157, 0.4261, 0.2482],
        [0.5246, 0.4066, 0.2490],
        [0.4923, 0.3914, 0.3002],
        [0.4301, 0.4220, 0.3426],
        [0.4785, 0.3768, 0.3375],
        [0.4605, 0.3683, 0.3645],
        [0.4436, 0.4062, 0.3543],
        [0.4935, 0.3969, 0.3020],
        [0.4691, 0.3607, 0.3502],
        [0.4968, 0.4048, 0.3021],
        [0.5025, 0.3913, 0.2851],
        [0.4986, 0.4084, 0.2875],
        [0.5141, 0.4010, 0.2579],
        [0.5060, 0.3985, 0.2930],
        [0.5372, 0.4244, 0.2429],
        [0.5011, 0.3923, 0.2674],
        [0.5152, 0.3941, 0.2820],
        [0.4872, 0.3803, 0.3131],
        [0.4600, 0.3571, 0.3562],
        [0.4463, 0.3610, 0.3515],
        [0.5129, 0.4007, 0.2707],
        [0.4956, 0.3933, 0.3035],
        [0.4296, 0.4009, 0.3586],
        [0.4474, 0.3996, 0.3558],
        [0.4393, 0.4096, 0.3491],
        [0.4446, 0.3735, 0.3666],
        [0.5225, 0.4093, 0.2659],
        [0.4658, 0.3682, 0.3529],
        [0.4894, 0.3831, 0.3192],
        [0.4903, 0.3817, 0.3103],
        [0.4700, 0.3926, 0.3284],
        [0.4332, 0.4033, 0.3360],
        [0.4989, 0.3964, 0.3063],
        [0.4687, 0.3728, 0.3470],
        [0.4798, 0.3826, 0.3206],
        [0.5062, 0.3967, 0.2794],
        [0.4800, 0.4056, 0.3079],
        [0.4991, 0.3897, 0.2940],
        [0.4726, 0.3630, 0.3540],
        [0.4984, 0.4005, 0.2923],
        [0.4982, 0.4165, 0.2789],
        [0.4429, 0.3808, 0.3672],
        [0.4945, 0.4066, 0.3044],
        [0.4804, 0.3759, 0.3495],
        [0.4945, 0.3924, 0.2934],
        [0.5000, 0.3961, 0.2932],
        [0.4571, 0.3816, 0.3501],
        [0.5257, 0.4071, 0.2493],
        [0.4800, 0.3697, 0.3501],
        [0.4889, 0.3886, 0.3269],
        [0.4543, 0.3810, 0.3593],
        [0.4675, 0.3601, 0.3537],
        [0.4816, 0.3543, 0.3387],
        [0.4687, 0.3835, 0.3342],
        [0.4771, 0.3795, 0.3322],
        [0.5169, 0.4102, 0.2663],
        [0.4614, 0.3771, 0.3455],
        [0.4736, 0.3973, 0.3279],
        [0.5370, 0.4254, 0.2386]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 25: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 32.0, Max: 7707.0, Mean: 2306.053955078125, Std: 1847.066650390625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 318.7501220703125, Max: 77004.0859375, Mean: 23040.130859375, Std: 18455.103515625
[DEBUG] Top-3 class probabilities:
tensor([[0.4644, 0.3617, 0.3564],
        [0.4916, 0.3790, 0.2964],
        [0.4794, 0.3795, 0.3158],
        [0.5050, 0.4220, 0.2670],
        [0.5179, 0.4027, 0.2650],
        [0.5324, 0.4236, 0.2581],
        [0.4531, 0.3844, 0.3554],
        [0.4777, 0.4007, 0.3041],
        [0.4771, 0.3830, 0.3370],
        [0.4822, 0.3739, 0.2993],
        [0.4529, 0.3793, 0.3719],
        [0.4558, 0.4088, 0.3222],
        [0.4445, 0.3874, 0.3692],
        [0.4832, 0.3896, 0.3087],
        [0.4429, 0.3979, 0.3457],
        [0.4550, 0.3640, 0.3640],
        [0.4912, 0.3979, 0.2868],
        [0.4838, 0.3764, 0.3254],
        [0.4717, 0.3739, 0.3460],
        [0.5042, 0.4017, 0.2890],
        [0.5028, 0.4044, 0.2854],
        [0.5008, 0.4017, 0.2893],
        [0.4975, 0.4044, 0.3043],
        [0.4893, 0.4137, 0.2825],
        [0.5070, 0.3964, 0.2830],
        [0.4998, 0.3975, 0.2869],
        [0.4905, 0.4133, 0.2864],
        [0.4920, 0.3896, 0.3147],
        [0.4813, 0.3839, 0.3096],
        [0.4901, 0.4000, 0.3100],
        [0.4304, 0.4087, 0.3715],
        [0.4235, 0.3932, 0.3613],
        [0.4609, 0.3682, 0.3451],
        [0.4704, 0.3792, 0.3017],
        [0.4960, 0.3852, 0.2911],
        [0.4630, 0.3739, 0.3477],
        [0.5073, 0.4046, 0.2878],
        [0.4953, 0.3933, 0.3044],
        [0.5183, 0.4166, 0.2640],
        [0.5072, 0.3939, 0.2793],
        [0.4590, 0.3778, 0.3479],
        [0.4974, 0.4150, 0.3092],
        [0.4828, 0.3808, 0.3016],
        [0.4759, 0.3870, 0.3411],
        [0.5105, 0.3986, 0.2841],
        [0.4983, 0.3960, 0.2893],
        [0.4662, 0.4023, 0.3074],
        [0.4769, 0.3887, 0.3158],
        [0.5029, 0.4234, 0.2930],
        [0.4787, 0.3676, 0.3140],
        [0.4849, 0.3859, 0.3087],
        [0.4843, 0.3904, 0.3103],
        [0.4982, 0.3896, 0.2974],
        [0.4831, 0.3951, 0.3118],
        [0.5020, 0.4123, 0.2872],
        [0.4783, 0.3893, 0.3018],
        [0.4220, 0.3874, 0.3595],
        [0.4219, 0.4047, 0.3427],
        [0.4314, 0.3939, 0.3711],
        [0.5054, 0.3887, 0.2927],
        [0.4888, 0.3986, 0.3278],
        [0.4873, 0.3899, 0.3091],
        [0.4911, 0.3741, 0.2773],
        [0.4888, 0.4002, 0.2873],
        [0.4834, 0.3846, 0.3114],
        [0.5071, 0.3971, 0.2953],
        [0.5144, 0.4168, 0.2563],
        [0.4681, 0.3559, 0.3186],
        [0.5112, 0.4057, 0.2717],
        [0.4214, 0.3997, 0.3298],
        [0.4760, 0.3792, 0.3405],
        [0.4596, 0.3733, 0.3602],
        [0.4744, 0.3790, 0.3327],
        [0.4984, 0.4001, 0.2930],
        [0.4583, 0.3764, 0.3470],
        [0.4257, 0.3872, 0.3848],
        [0.4915, 0.3869, 0.2895],
        [0.5046, 0.4104, 0.2872],
        [0.4981, 0.3831, 0.3043],
        [0.5111, 0.4093, 0.2616],
        [0.5155, 0.4300, 0.2740],
        [0.4855, 0.3904, 0.3013],
        [0.4225, 0.4026, 0.3226],
        [0.5036, 0.4032, 0.2795],
        [0.5378, 0.4139, 0.2520],
        [0.5083, 0.4010, 0.2735],
        [0.4467, 0.3606, 0.3480],
        [0.4622, 0.3765, 0.3481],
        [0.4844, 0.3777, 0.3327],
        [0.4518, 0.3662, 0.3648],
        [0.4982, 0.3779, 0.3043],
        [0.4810, 0.3809, 0.3424],
        [0.4906, 0.3778, 0.3143],
        [0.4693, 0.3596, 0.3437],
        [0.4883, 0.3910, 0.3105],
        [0.4735, 0.3840, 0.3255],
        [0.4753, 0.3801, 0.3183],
        [0.5033, 0.3938, 0.2696],
        [0.4709, 0.3575, 0.3556],
        [0.4575, 0.3725, 0.3390],
        [0.4813, 0.3951, 0.3228],
        [0.4530, 0.3751, 0.3659],
        [0.4198, 0.3932, 0.3743],
        [0.4809, 0.4315, 0.2932],
        [0.4282, 0.3971, 0.3683],
        [0.4677, 0.3722, 0.3389],
        [0.4781, 0.3947, 0.3287],
        [0.4557, 0.3658, 0.3256],
        [0.4617, 0.3868, 0.3469],
        [0.4960, 0.4037, 0.3109],
        [0.4894, 0.3945, 0.2842],
        [0.4925, 0.4106, 0.3067],
        [0.5174, 0.4293, 0.2680],
        [0.4856, 0.3863, 0.3049],
        [0.5036, 0.4092, 0.3037],
        [0.4780, 0.3631, 0.3404],
        [0.4334, 0.3790, 0.3345],
        [0.4305, 0.4153, 0.3717],
        [0.5058, 0.4104, 0.2687],
        [0.4832, 0.3909, 0.3142],
        [0.4720, 0.3713, 0.3483],
        [0.4197, 0.4147, 0.3485],
        [0.4419, 0.3814, 0.3734],
        [0.5109, 0.4183, 0.2662],
        [0.4887, 0.4024, 0.3073],
        [0.4944, 0.3939, 0.2873],
        [0.4521, 0.3746, 0.3742],
        [0.4685, 0.3718, 0.3381]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 26: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7838.0, Mean: 2269.39990234375, Std: 1800.4302978515625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 78312.9765625, Mean: 22673.8984375, Std: 17989.1328125
[DEBUG] Top-3 class probabilities:
tensor([[0.4553, 0.3788, 0.3777],
        [0.4928, 0.3655, 0.3094],
        [0.4885, 0.4056, 0.3032],
        [0.5206, 0.4142, 0.2582],
        [0.4756, 0.3864, 0.3280],
        [0.4763, 0.4077, 0.3176],
        [0.4453, 0.3860, 0.3568],
        [0.5011, 0.4052, 0.2946],
        [0.4860, 0.3955, 0.3213],
        [0.4725, 0.3669, 0.3432],
        [0.4939, 0.3845, 0.3195],
        [0.5179, 0.4090, 0.2649],
        [0.4728, 0.3792, 0.3440],
        [0.4751, 0.3739, 0.3485],
        [0.5055, 0.4105, 0.2762],
        [0.4731, 0.3765, 0.3463],
        [0.4286, 0.4255, 0.3303],
        [0.4429, 0.3879, 0.3535],
        [0.4968, 0.4099, 0.2842],
        [0.4198, 0.4123, 0.3568],
        [0.4185, 0.4178, 0.3708],
        [0.4196, 0.4022, 0.3667],
        [0.4827, 0.4065, 0.3103],
        [0.4697, 0.3828, 0.3424],
        [0.4719, 0.3681, 0.3449],
        [0.4681, 0.3785, 0.3503],
        [0.4529, 0.3763, 0.3609],
        [0.4786, 0.3926, 0.3132],
        [0.5182, 0.4119, 0.2475],
        [0.4725, 0.3597, 0.3588],
        [0.5001, 0.4060, 0.3045],
        [0.4889, 0.3756, 0.3110],
        [0.4903, 0.3928, 0.3189],
        [0.4734, 0.3687, 0.3361],
        [0.4772, 0.3738, 0.3377],
        [0.4804, 0.3937, 0.3228],
        [0.4268, 0.4083, 0.3468],
        [0.5073, 0.4023, 0.2927],
        [0.4998, 0.4039, 0.2952],
        [0.4434, 0.3837, 0.3570],
        [0.4987, 0.4075, 0.2945],
        [0.4925, 0.3881, 0.2916],
        [0.4471, 0.3707, 0.3521],
        [0.4856, 0.3962, 0.3109],
        [0.4905, 0.3978, 0.3100],
        [0.5136, 0.4075, 0.2734],
        [0.4259, 0.3986, 0.3755],
        [0.4982, 0.4081, 0.2897],
        [0.4755, 0.3873, 0.3493],
        [0.4460, 0.3840, 0.3557],
        [0.5122, 0.4115, 0.2754],
        [0.4916, 0.4007, 0.3123],
        [0.4628, 0.3702, 0.3414],
        [0.4165, 0.4160, 0.3279],
        [0.4898, 0.3822, 0.3032],
        [0.4324, 0.4034, 0.3482],
        [0.4580, 0.3655, 0.3539],
        [0.4727, 0.3850, 0.3355],
        [0.4590, 0.3834, 0.3563],
        [0.4479, 0.3842, 0.3648],
        [0.4906, 0.3757, 0.3041],
        [0.4620, 0.3570, 0.3558],
        [0.5174, 0.3848, 0.2787],
        [0.4292, 0.4215, 0.3374],
        [0.4758, 0.3726, 0.3499],
        [0.4407, 0.4084, 0.3518],
        [0.4316, 0.4097, 0.3336],
        [0.4128, 0.4026, 0.3858],
        [0.4576, 0.3787, 0.3777],
        [0.5026, 0.4092, 0.2925],
        [0.5106, 0.3960, 0.2978],
        [0.4624, 0.3604, 0.3563],
        [0.4942, 0.3989, 0.2979],
        [0.4857, 0.3980, 0.3185],
        [0.4910, 0.3973, 0.2995],
        [0.4813, 0.3835, 0.3229],
        [0.4921, 0.3933, 0.2932],
        [0.4649, 0.3888, 0.3341],
        [0.4848, 0.4095, 0.3031],
        [0.4434, 0.3640, 0.3595],
        [0.4882, 0.3944, 0.3171],
        [0.5095, 0.4107, 0.2641],
        [0.4423, 0.3924, 0.3537],
        [0.4776, 0.3915, 0.3064],
        [0.4767, 0.3772, 0.3246],
        [0.4699, 0.3967, 0.3293],
        [0.4877, 0.4046, 0.3052],
        [0.4608, 0.3685, 0.3541],
        [0.4971, 0.4061, 0.2990],
        [0.4867, 0.3956, 0.3079],
        [0.4803, 0.3837, 0.3181],
        [0.4848, 0.3979, 0.3200],
        [0.5029, 0.3894, 0.2852],
        [0.5128, 0.4173, 0.2789],
        [0.5020, 0.4118, 0.2940],
        [0.5126, 0.4150, 0.2646],
        [0.5015, 0.3988, 0.2833],
        [0.5295, 0.4204, 0.2288],
        [0.4771, 0.3649, 0.3245],
        [0.5001, 0.3997, 0.3052],
        [0.4933, 0.4004, 0.2891],
        [0.5012, 0.3909, 0.3047],
        [0.5016, 0.3953, 0.3087],
        [0.4715, 0.3689, 0.3639],
        [0.4581, 0.3587, 0.3514],
        [0.4531, 0.3563, 0.3484],
        [0.4453, 0.3784, 0.3497],
        [0.4483, 0.4070, 0.3045],
        [0.4544, 0.4130, 0.3086],
        [0.4923, 0.3983, 0.2916],
        [0.4773, 0.4013, 0.3062],
        [0.4307, 0.4046, 0.3574],
        [0.4489, 0.3880, 0.3631],
        [0.4401, 0.4057, 0.3376],
        [0.4776, 0.3845, 0.3414],
        [0.5128, 0.4048, 0.2590],
        [0.4814, 0.3723, 0.3072],
        [0.4699, 0.3821, 0.3415],
        [0.4863, 0.3891, 0.3241],
        [0.4439, 0.3876, 0.3632],
        [0.4955, 0.4073, 0.2757],
        [0.4955, 0.4104, 0.2859],
        [0.4723, 0.3699, 0.3232],
        [0.4529, 0.3888, 0.3503],
        [0.4562, 0.3576, 0.3549],
        [0.4225, 0.4224, 0.3453],
        [0.4596, 0.3695, 0.3569],
        [0.4885, 0.3838, 0.3107]])
[DEBUG] Top-3 class indices:
tensor([[17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 27: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7896.0, Mean: 2238.0478515625, Std: 1739.6441650390625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 78892.4921875, Mean: 22360.63671875, Std: 17381.783203125
[DEBUG] Top-3 class probabilities:
tensor([[0.5092, 0.3944, 0.2689],
        [0.4574, 0.3780, 0.3263],
        [0.4802, 0.3776, 0.3218],
        [0.4590, 0.3666, 0.3438],
        [0.4973, 0.4024, 0.2872],
        [0.4559, 0.3652, 0.3615],
        [0.5087, 0.4038, 0.2736],
        [0.4969, 0.3868, 0.3053],
        [0.5194, 0.3989, 0.2543],
        [0.4944, 0.3926, 0.3096],
        [0.4830, 0.3875, 0.3045],
        [0.4841, 0.3937, 0.3132],
        [0.4796, 0.3750, 0.3378],
        [0.5252, 0.4203, 0.2470],
        [0.4744, 0.3822, 0.3499],
        [0.4703, 0.3810, 0.3497],
        [0.4939, 0.3735, 0.2996],
        [0.4978, 0.3726, 0.3097],
        [0.4661, 0.3707, 0.3350],
        [0.4852, 0.3905, 0.3273],
        [0.4677, 0.3981, 0.3274],
        [0.5003, 0.4115, 0.2963],
        [0.4895, 0.3742, 0.3162],
        [0.4763, 0.3739, 0.3309],
        [0.4505, 0.3945, 0.3530],
        [0.4569, 0.3773, 0.3524],
        [0.4488, 0.3739, 0.3593],
        [0.4628, 0.3555, 0.3459],
        [0.4364, 0.4156, 0.3231],
        [0.4582, 0.4259, 0.3294],
        [0.4978, 0.4634, 0.2683],
        [0.4698, 0.4023, 0.3285],
        [0.4382, 0.3955, 0.3391],
        [0.4520, 0.3846, 0.3775],
        [0.4193, 0.4162, 0.3553],
        [0.4357, 0.3984, 0.3695],
        [0.4557, 0.3628, 0.3545],
        [0.4630, 0.3929, 0.3443],
        [0.4836, 0.3833, 0.3081],
        [0.4682, 0.3792, 0.3416],
        [0.5004, 0.3931, 0.3055],
        [0.4589, 0.3727, 0.3618],
        [0.4383, 0.3785, 0.3414],
        [0.4822, 0.3988, 0.3327],
        [0.4640, 0.3822, 0.3750],
        [0.4417, 0.3885, 0.3721],
        [0.5144, 0.3988, 0.2568],
        [0.4746, 0.3865, 0.3364],
        [0.4293, 0.3961, 0.3400],
        [0.4425, 0.4069, 0.3457],
        [0.4709, 0.3871, 0.3609],
        [0.4947, 0.3768, 0.3048],
        [0.5075, 0.4155, 0.2724],
        [0.4780, 0.3823, 0.3166],
        [0.4825, 0.3861, 0.3072],
        [0.5009, 0.4042, 0.2784],
        [0.4461, 0.3816, 0.3453],
        [0.4993, 0.4087, 0.2886],
        [0.4559, 0.3822, 0.3743],
        [0.4995, 0.4178, 0.2757],
        [0.5109, 0.3927, 0.2777],
        [0.4382, 0.4189, 0.3178],
        [0.4976, 0.4005, 0.3025],
        [0.4974, 0.3834, 0.3020],
        [0.4671, 0.3691, 0.3409],
        [0.4618, 0.3643, 0.3633],
        [0.4536, 0.3641, 0.3452],
        [0.4511, 0.3763, 0.3561],
        [0.5071, 0.3995, 0.2781],
        [0.4718, 0.3627, 0.3234],
        [0.4620, 0.3688, 0.3553],
        [0.4642, 0.3667, 0.3627],
        [0.4630, 0.3693, 0.3484],
        [0.5103, 0.3965, 0.2748],
        [0.4602, 0.3601, 0.3422],
        [0.4244, 0.3981, 0.3840],
        [0.4872, 0.4433, 0.2851],
        [0.4351, 0.3954, 0.3739],
        [0.4219, 0.3868, 0.3766],
        [0.4481, 0.3812, 0.3590],
        [0.4352, 0.4180, 0.3511],
        [0.4498, 0.3733, 0.3642],
        [0.4320, 0.3783, 0.3681],
        [0.4445, 0.3739, 0.3734],
        [0.4731, 0.3848, 0.3449],
        [0.4815, 0.3880, 0.3180],
        [0.4806, 0.3821, 0.3015],
        [0.4808, 0.3936, 0.3212],
        [0.4906, 0.3923, 0.3011],
        [0.4909, 0.4008, 0.3052],
        [0.4598, 0.3889, 0.3734],
        [0.4295, 0.4102, 0.3465],
        [0.5166, 0.4085, 0.2656],
        [0.4662, 0.3973, 0.3048],
        [0.4950, 0.3760, 0.2970],
        [0.4416, 0.3866, 0.3610],
        [0.5054, 0.4079, 0.2819],
        [0.4950, 0.3919, 0.3122],
        [0.4304, 0.4216, 0.3230],
        [0.5121, 0.4027, 0.2769],
        [0.5094, 0.4057, 0.2912],
        [0.4587, 0.3680, 0.3444],
        [0.4472, 0.3718, 0.3717],
        [0.4749, 0.3911, 0.3420],
        [0.5224, 0.4186, 0.2526],
        [0.5024, 0.4040, 0.2755],
        [0.4596, 0.3798, 0.3523],
        [0.4819, 0.4006, 0.3134],
        [0.5213, 0.4143, 0.2618],
        [0.4779, 0.3683, 0.3378],
        [0.4637, 0.3666, 0.3489],
        [0.4592, 0.3705, 0.3672],
        [0.4701, 0.3818, 0.3432],
        [0.4623, 0.3571, 0.3527],
        [0.4615, 0.3783, 0.3622],
        [0.4696, 0.3754, 0.3544],
        [0.4735, 0.3695, 0.3255],
        [0.4702, 0.3659, 0.3496],
        [0.4617, 0.3742, 0.3408],
        [0.4276, 0.4041, 0.3506],
        [0.4469, 0.4015, 0.3571],
        [0.4655, 0.4005, 0.3214],
        [0.4063, 0.4054, 0.3764],
        [0.4307, 0.3820, 0.3710],
        [0.4603, 0.3709, 0.3655],
        [0.4270, 0.3984, 0.3715],
        [0.4258, 0.4137, 0.3520],
        [0.4508, 0.3730, 0.3641]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 28: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9032.0, Mean: 2190.937744140625, Std: 1727.4173583984375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 90242.921875, Mean: 21889.9375, Std: 17259.619140625
[DEBUG] Top-3 class probabilities:
tensor([[0.4714, 0.3877, 0.3394],
        [0.4882, 0.3902, 0.3091],
        [0.4780, 0.3951, 0.3149],
        [0.4467, 0.3777, 0.3630],
        [0.4195, 0.4152, 0.3673],
        [0.4323, 0.3964, 0.3767],
        [0.4195, 0.4038, 0.3475],
        [0.4781, 0.3931, 0.3332],
        [0.4868, 0.3875, 0.3000],
        [0.4906, 0.3926, 0.2987],
        [0.4987, 0.4051, 0.2859],
        [0.4769, 0.3975, 0.3273],
        [0.4413, 0.4221, 0.3458],
        [0.4340, 0.4170, 0.3555],
        [0.5172, 0.4229, 0.2616],
        [0.4862, 0.3771, 0.3140],
        [0.4853, 0.3906, 0.3238],
        [0.4870, 0.3942, 0.3346],
        [0.4909, 0.3988, 0.3079],
        [0.4690, 0.3857, 0.3358],
        [0.4150, 0.4049, 0.3637],
        [0.4397, 0.4056, 0.3762],
        [0.4309, 0.4150, 0.3389],
        [0.5149, 0.4122, 0.2697],
        [0.4904, 0.3827, 0.3070],
        [0.4769, 0.3721, 0.3268],
        [0.4391, 0.3783, 0.3468],
        [0.4873, 0.3704, 0.3179],
        [0.4705, 0.3640, 0.3410],
        [0.4726, 0.3756, 0.3375],
        [0.5120, 0.3943, 0.2542],
        [0.4957, 0.3973, 0.2865],
        [0.4709, 0.3838, 0.3188],
        [0.4650, 0.3673, 0.3438],
        [0.4821, 0.3916, 0.3249],
        [0.4743, 0.3638, 0.3548],
        [0.4649, 0.3734, 0.3626],
        [0.4718, 0.3763, 0.3428],
        [0.4639, 0.4377, 0.3097],
        [0.4151, 0.3920, 0.3846],
        [0.4441, 0.3870, 0.3709],
        [0.4250, 0.4019, 0.3580],
        [0.4218, 0.3973, 0.3933],
        [0.4473, 0.3838, 0.3718],
        [0.4377, 0.3897, 0.3759],
        [0.4442, 0.4004, 0.3585],
        [0.4477, 0.3771, 0.3758],
        [0.4575, 0.3649, 0.3601],
        [0.4813, 0.4095, 0.3191],
        [0.4375, 0.3798, 0.3666],
        [0.4807, 0.4095, 0.3354],
        [0.4175, 0.4154, 0.3692],
        [0.4777, 0.3960, 0.2918],
        [0.5000, 0.3962, 0.2881],
        [0.4572, 0.3639, 0.3556],
        [0.4938, 0.3912, 0.3048],
        [0.4523, 0.3580, 0.3531],
        [0.4604, 0.3599, 0.3582],
        [0.4427, 0.4018, 0.3609],
        [0.4385, 0.3830, 0.3795],
        [0.5042, 0.4177, 0.2998],
        [0.5094, 0.3867, 0.2760],
        [0.5104, 0.3985, 0.2761],
        [0.5055, 0.4009, 0.3092],
        [0.4869, 0.3865, 0.3113],
        [0.4878, 0.4004, 0.3190],
        [0.4756, 0.4183, 0.3174],
        [0.5247, 0.4155, 0.2488],
        [0.4395, 0.3855, 0.3462],
        [0.4704, 0.3730, 0.3307],
        [0.4954, 0.3850, 0.3029],
        [0.5179, 0.4321, 0.2552],
        [0.4798, 0.3634, 0.3121],
        [0.4938, 0.4186, 0.2681],
        [0.4573, 0.3744, 0.3654],
        [0.4997, 0.3979, 0.2949],
        [0.4928, 0.3817, 0.3168],
        [0.4529, 0.3645, 0.3573],
        [0.4541, 0.3881, 0.3483],
        [0.4450, 0.3939, 0.3423],
        [0.4504, 0.3917, 0.3481],
        [0.4747, 0.3845, 0.3275],
        [0.4908, 0.4005, 0.3173],
        [0.4323, 0.4144, 0.3381],
        [0.4397, 0.4151, 0.3350],
        [0.4186, 0.4011, 0.3799],
        [0.4163, 0.4075, 0.3792],
        [0.4439, 0.3893, 0.3763],
        [0.4446, 0.3900, 0.3561],
        [0.4345, 0.3874, 0.3712],
        [0.4727, 0.3717, 0.3665],
        [0.4102, 0.4092, 0.4048],
        [0.4416, 0.3912, 0.3769],
        [0.4376, 0.3818, 0.3714],
        [0.4919, 0.4157, 0.3085],
        [0.4803, 0.3870, 0.3307],
        [0.4514, 0.3740, 0.3517],
        [0.4997, 0.4045, 0.3027],
        [0.5222, 0.4121, 0.2488],
        [0.4384, 0.3974, 0.3417],
        [0.4508, 0.3746, 0.3597],
        [0.4379, 0.3951, 0.3605],
        [0.4582, 0.3694, 0.3527],
        [0.4287, 0.4031, 0.3420],
        [0.4575, 0.3907, 0.3790],
        [0.4441, 0.4005, 0.3679],
        [0.5167, 0.4061, 0.2447],
        [0.5256, 0.4000, 0.2503],
        [0.4640, 0.3630, 0.3594],
        [0.5226, 0.4106, 0.2594],
        [0.4519, 0.3698, 0.3669],
        [0.4395, 0.4071, 0.3328],
        [0.4451, 0.3838, 0.3548],
        [0.4700, 0.3779, 0.3587],
        [0.4323, 0.4013, 0.3366],
        [0.4467, 0.3860, 0.3578],
        [0.4723, 0.3739, 0.3324],
        [0.5080, 0.4076, 0.2774],
        [0.4455, 0.3720, 0.3684],
        [0.4664, 0.3752, 0.3305],
        [0.4710, 0.3803, 0.3375],
        [0.5041, 0.4128, 0.2783],
        [0.4726, 0.3612, 0.3320],
        [0.4424, 0.3806, 0.3470],
        [0.4423, 0.3817, 0.3515],
        [0.4921, 0.4062, 0.3060],
        [0.4843, 0.3912, 0.3176],
        [0.4916, 0.4039, 0.2969]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [ 6, 17, 11],
        [11,  6, 17],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 29: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18480.0, Mean: 2144.7802734375, Std: 1712.2445068359375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 184643.3125, Mean: 21428.75, Std: 17108.01953125
[DEBUG] Top-3 class probabilities:
tensor([[0.4925, 0.3924, 0.2851],
        [0.5005, 0.3954, 0.3014],
        [0.4442, 0.4125, 0.3501],
        [0.4173, 0.4092, 0.3664],
        [0.4297, 0.3908, 0.3817],
        [0.4917, 0.3700, 0.3447],
        [0.4331, 0.4066, 0.3457],
        [0.4197, 0.3922, 0.3787],
        [0.4718, 0.3703, 0.3624],
        [0.4263, 0.3946, 0.3923],
        [0.4257, 0.4005, 0.3750],
        [0.5098, 0.4042, 0.2584],
        [0.4495, 0.3791, 0.3698],
        [0.4779, 0.3948, 0.3447],
        [0.4173, 0.4122, 0.3773],
        [0.4811, 0.3897, 0.3234],
        [0.4468, 0.3985, 0.3461],
        [0.4473, 0.4100, 0.3291],
        [0.4600, 0.3889, 0.3586],
        [0.4903, 0.3902, 0.3288],
        [0.4825, 0.3790, 0.3301],
        [0.4874, 0.3874, 0.3146],
        [0.4618, 0.3946, 0.3493],
        [0.4692, 0.3766, 0.3479],
        [0.5141, 0.3907, 0.2718],
        [0.4991, 0.3924, 0.2878],
        [0.5028, 0.4048, 0.2739],
        [0.4959, 0.3937, 0.3029],
        [0.5004, 0.3919, 0.2936],
        [0.4713, 0.3699, 0.3232],
        [0.4721, 0.3727, 0.3373],
        [0.4783, 0.3869, 0.3315],
        [0.4326, 0.3976, 0.3480],
        [0.4704, 0.3771, 0.3588],
        [0.4660, 0.3587, 0.3442],
        [0.4653, 0.3710, 0.3516],
        [0.4612, 0.3667, 0.3589],
        [0.4943, 0.4130, 0.3075],
        [0.4597, 0.3592, 0.3584],
        [0.4802, 0.3799, 0.3281],
        [0.4376, 0.3910, 0.3614],
        [0.4805, 0.3990, 0.3198],
        [0.4889, 0.3910, 0.3003],
        [0.4886, 0.3925, 0.3168],
        [0.4742, 0.3829, 0.3452],
        [0.4925, 0.3984, 0.3006],
        [0.5082, 0.4148, 0.2947],
        [0.4432, 0.3734, 0.3619],
        [0.4332, 0.3883, 0.3768],
        [0.4192, 0.3987, 0.3881],
        [0.4585, 0.3679, 0.3577],
        [0.4502, 0.3897, 0.3869],
        [0.4123, 0.4092, 0.3961],
        [0.4306, 0.4010, 0.3915],
        [0.4892, 0.3843, 0.3221],
        [0.4626, 0.3866, 0.3461],
        [0.4398, 0.4113, 0.3401],
        [0.4819, 0.3884, 0.3188],
        [0.4462, 0.4121, 0.3402],
        [0.4959, 0.4087, 0.3024],
        [0.4971, 0.4030, 0.3083],
        [0.4713, 0.3787, 0.3422],
        [0.4489, 0.3713, 0.3610],
        [0.4733, 0.3853, 0.3428],
        [0.4433, 0.3790, 0.3602],
        [0.4760, 0.3815, 0.3321],
        [0.4542, 0.4164, 0.3347],
        [0.4605, 0.4066, 0.3206],
        [0.4350, 0.3905, 0.3583],
        [0.4728, 0.3900, 0.3354],
        [0.5096, 0.4038, 0.2583],
        [0.4808, 0.3619, 0.3367],
        [0.4820, 0.3794, 0.3177],
        [0.4890, 0.3874, 0.3045],
        [0.4853, 0.3940, 0.3274],
        [0.5027, 0.3922, 0.3093],
        [0.4749, 0.3724, 0.3440],
        [0.4568, 0.3801, 0.3598],
        [0.4736, 0.3837, 0.3400],
        [0.4670, 0.3802, 0.3496],
        [0.4620, 0.3644, 0.3481],
        [0.4563, 0.3754, 0.3512],
        [0.4753, 0.3772, 0.3206],
        [0.4832, 0.3945, 0.3233],
        [0.4564, 0.3601, 0.3590],
        [0.4995, 0.3927, 0.3035],
        [0.4603, 0.3761, 0.3645],
        [0.4809, 0.3918, 0.3074],
        [0.4593, 0.3594, 0.3589],
        [0.4562, 0.3814, 0.3505],
        [0.4893, 0.3906, 0.3025],
        [0.5072, 0.4165, 0.2715],
        [0.4964, 0.3876, 0.2976],
        [0.4672, 0.3869, 0.3415],
        [0.4579, 0.3862, 0.3401],
        [0.4358, 0.3877, 0.3847],
        [0.4619, 0.3723, 0.3689],
        [0.4894, 0.3577, 0.3324],
        [0.4482, 0.3824, 0.3568],
        [0.4554, 0.3723, 0.3672],
        [0.4379, 0.3841, 0.3823],
        [0.4722, 0.4140, 0.3381],
        [0.4415, 0.3934, 0.3766],
        [0.4639, 0.3858, 0.3502],
        [0.4401, 0.4053, 0.3634],
        [0.4229, 0.4128, 0.3567],
        [0.5265, 0.4066, 0.2478],
        [0.5014, 0.4052, 0.2756],
        [0.5016, 0.4025, 0.2888],
        [0.4690, 0.3655, 0.3421],
        [0.5154, 0.3934, 0.2661],
        [0.4329, 0.4222, 0.3438],
        [0.4434, 0.4028, 0.3500],
        [0.4226, 0.4147, 0.3430],
        [0.4314, 0.4154, 0.3554],
        [0.4570, 0.3770, 0.3613],
        [0.5085, 0.4127, 0.2714],
        [0.4797, 0.3792, 0.3309],
        [0.4577, 0.3682, 0.3588],
        [0.4924, 0.3723, 0.3173],
        [0.4547, 0.3711, 0.3533],
        [0.4679, 0.3734, 0.3608],
        [0.4683, 0.3578, 0.3509],
        [0.4478, 0.3668, 0.3591],
        [0.4827, 0.3899, 0.3177],
        [0.4568, 0.3596, 0.3500],
        [0.4775, 0.3862, 0.3261],
        [0.5116, 0.4028, 0.2760]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [ 6, 17, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [17, 11,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 30: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7752.0, Mean: 2194.435302734375, Std: 1727.8160400390625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 77453.703125, Mean: 21924.880859375, Std: 17263.6015625
[DEBUG] Top-3 class probabilities:
tensor([[0.4980, 0.4057, 0.2905],
        [0.4384, 0.3930, 0.3458],
        [0.5075, 0.4058, 0.2838],
        [0.4968, 0.3864, 0.2919],
        [0.4907, 0.3980, 0.3208],
        [0.4905, 0.3980, 0.2892],
        [0.4887, 0.3910, 0.3058],
        [0.4766, 0.3766, 0.3365],
        [0.4973, 0.4037, 0.2784],
        [0.5010, 0.3920, 0.3094],
        [0.4954, 0.4133, 0.2858],
        [0.5119, 0.4011, 0.2785],
        [0.4752, 0.3708, 0.3692],
        [0.4278, 0.4127, 0.3780],
        [0.4527, 0.3797, 0.3796],
        [0.4216, 0.4015, 0.3569],
        [0.4149, 0.4047, 0.3621],
        [0.4180, 0.4153, 0.3763],
        [0.4186, 0.4128, 0.3589],
        [0.4584, 0.3954, 0.3300],
        [0.5213, 0.4157, 0.2519],
        [0.4781, 0.3880, 0.3146],
        [0.5059, 0.4257, 0.2658],
        [0.4403, 0.3700, 0.3658],
        [0.5113, 0.3969, 0.2754],
        [0.4685, 0.3772, 0.3355],
        [0.4986, 0.3915, 0.2906],
        [0.5039, 0.4167, 0.2790],
        [0.4507, 0.3642, 0.3564],
        [0.4501, 0.3744, 0.3737],
        [0.4251, 0.4187, 0.3347],
        [0.4449, 0.3797, 0.3693],
        [0.4381, 0.3857, 0.3827],
        [0.5072, 0.3957, 0.2986],
        [0.5008, 0.4110, 0.2796],
        [0.4832, 0.4031, 0.3121],
        [0.4381, 0.3796, 0.3623],
        [0.4924, 0.3878, 0.2989],
        [0.5065, 0.4079, 0.2757],
        [0.4959, 0.3760, 0.3189],
        [0.4913, 0.3952, 0.3178],
        [0.4863, 0.3867, 0.3164],
        [0.4949, 0.3996, 0.2976],
        [0.4667, 0.3656, 0.3539],
        [0.4535, 0.3711, 0.3633],
        [0.4690, 0.3687, 0.3395],
        [0.4601, 0.3753, 0.3550],
        [0.4781, 0.3782, 0.3491],
        [0.4987, 0.4004, 0.3067],
        [0.4905, 0.3972, 0.2888],
        [0.4798, 0.3948, 0.3129],
        [0.4674, 0.3768, 0.3486],
        [0.4708, 0.3790, 0.3498],
        [0.4747, 0.3693, 0.3371],
        [0.4302, 0.4026, 0.3527],
        [0.4797, 0.3786, 0.3470],
        [0.4785, 0.3866, 0.3185],
        [0.4990, 0.3955, 0.3086],
        [0.4797, 0.3776, 0.3366],
        [0.4423, 0.3849, 0.3689],
        [0.4633, 0.3991, 0.3117],
        [0.4478, 0.3766, 0.3518],
        [0.3992, 0.3981, 0.3877],
        [0.4321, 0.3890, 0.3798],
        [0.4984, 0.3995, 0.2845],
        [0.4701, 0.3740, 0.3371],
        [0.4430, 0.3996, 0.3716],
        [0.4462, 0.3748, 0.3576],
        [0.4359, 0.3797, 0.3576],
        [0.4682, 0.3943, 0.3261],
        [0.4573, 0.3693, 0.3613],
        [0.4622, 0.3662, 0.3285],
        [0.4991, 0.4216, 0.2988],
        [0.4603, 0.4046, 0.3429],
        [0.4156, 0.4097, 0.4051],
        [0.4595, 0.3728, 0.3550],
        [0.4156, 0.4008, 0.3763],
        [0.4450, 0.3709, 0.3687],
        [0.4907, 0.4033, 0.3142],
        [0.4990, 0.4047, 0.3086],
        [0.5012, 0.3968, 0.2996],
        [0.4460, 0.3837, 0.3524],
        [0.4584, 0.3862, 0.3546],
        [0.4859, 0.3910, 0.3159],
        [0.4871, 0.3836, 0.3120],
        [0.4825, 0.3742, 0.3075],
        [0.4963, 0.3921, 0.2972],
        [0.4644, 0.3661, 0.3588],
        [0.4866, 0.3828, 0.3311],
        [0.4790, 0.3701, 0.3185],
        [0.4764, 0.3733, 0.3188],
        [0.4812, 0.3801, 0.3441],
        [0.4788, 0.4009, 0.3333],
        [0.5227, 0.4097, 0.2647],
        [0.4979, 0.4022, 0.3169],
        [0.4969, 0.3997, 0.3037],
        [0.4793, 0.3807, 0.3256],
        [0.4788, 0.3886, 0.3295],
        [0.4761, 0.3828, 0.3362],
        [0.4628, 0.3710, 0.3576],
        [0.5045, 0.4073, 0.2879],
        [0.4877, 0.4038, 0.2830],
        [0.5065, 0.4018, 0.2744],
        [0.5265, 0.4043, 0.2586],
        [0.4679, 0.3703, 0.3486],
        [0.4678, 0.4125, 0.3317],
        [0.4533, 0.3704, 0.3614],
        [0.5080, 0.3728, 0.2756],
        [0.4246, 0.3784, 0.3764],
        [0.4642, 0.4012, 0.3309],
        [0.4294, 0.3848, 0.3783],
        [0.5211, 0.4236, 0.2405],
        [0.4895, 0.4069, 0.3229],
        [0.4759, 0.3543, 0.3383],
        [0.4224, 0.4213, 0.3329],
        [0.4683, 0.3792, 0.3413],
        [0.4714, 0.3690, 0.3554],
        [0.4954, 0.3716, 0.3122],
        [0.4482, 0.4031, 0.3276],
        [0.4417, 0.3939, 0.3595],
        [0.4467, 0.4133, 0.3624],
        [0.4557, 0.4149, 0.3474],
        [0.4502, 0.3565, 0.3537],
        [0.4704, 0.3797, 0.3253],
        [0.4624, 0.3708, 0.3479],
        [0.4627, 0.3613, 0.3539],
        [0.4516, 0.3662, 0.3419],
        [0.4572, 0.3593, 0.3471]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [17, 11,  6],
        [11, 17,  6],
        [ 6, 17, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [ 6, 17, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 31: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13983.0, Mean: 2069.16455078125, Std: 1715.79248046875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 139711.203125, Mean: 20673.232421875, Std: 17143.466796875
[DEBUG] Top-3 class probabilities:
tensor([[0.4850, 0.3945, 0.3024],
        [0.4603, 0.3838, 0.3518],
        [0.4165, 0.4151, 0.3325],
        [0.4711, 0.3797, 0.3036],
        [0.5011, 0.4015, 0.2740],
        [0.4740, 0.3696, 0.3201],
        [0.4748, 0.3785, 0.3518],
        [0.4735, 0.3902, 0.3396],
        [0.5081, 0.4023, 0.2824],
        [0.4643, 0.3733, 0.3515],
        [0.5026, 0.4168, 0.2706],
        [0.4765, 0.3955, 0.3253],
        [0.4741, 0.3830, 0.3323],
        [0.4812, 0.3745, 0.3446],
        [0.4705, 0.3659, 0.3394],
        [0.4263, 0.4186, 0.3301],
        [0.4726, 0.3944, 0.3303],
        [0.5071, 0.4078, 0.2557],
        [0.4793, 0.3913, 0.3279],
        [0.4890, 0.3809, 0.3135],
        [0.4756, 0.3770, 0.3305],
        [0.4522, 0.4048, 0.3475],
        [0.4302, 0.3813, 0.3813],
        [0.4720, 0.3734, 0.3177],
        [0.4228, 0.4165, 0.3128],
        [0.4654, 0.3982, 0.3240],
        [0.4884, 0.3975, 0.3158],
        [0.4290, 0.3967, 0.3754],
        [0.4292, 0.3902, 0.3847],
        [0.4397, 0.3892, 0.3747],
        [0.4528, 0.3658, 0.3585],
        [0.4317, 0.4214, 0.3411],
        [0.4681, 0.3900, 0.3497],
        [0.4342, 0.4054, 0.3402],
        [0.4848, 0.3898, 0.3081],
        [0.4445, 0.3735, 0.3696],
        [0.4342, 0.4116, 0.3180],
        [0.4112, 0.4067, 0.3482],
        [0.4893, 0.4053, 0.3029],
        [0.4544, 0.3678, 0.3622],
        [0.4658, 0.4050, 0.3205],
        [0.4790, 0.3867, 0.3314],
        [0.5011, 0.4053, 0.2834],
        [0.4608, 0.3733, 0.3610],
        [0.4648, 0.3749, 0.3360],
        [0.4783, 0.3834, 0.3349],
        [0.5010, 0.4017, 0.2924],
        [0.4816, 0.3720, 0.3424],
        [0.4687, 0.3892, 0.3440],
        [0.4750, 0.3780, 0.3372],
        [0.4734, 0.3790, 0.3426],
        [0.4610, 0.3587, 0.3427],
        [0.4769, 0.4092, 0.3102],
        [0.4906, 0.3951, 0.3121],
        [0.4773, 0.3923, 0.3355],
        [0.4818, 0.4035, 0.3019],
        [0.4537, 0.3618, 0.3607],
        [0.5188, 0.4216, 0.2800],
        [0.4721, 0.3738, 0.3451],
        [0.4498, 0.3804, 0.3688],
        [0.4947, 0.4192, 0.2935],
        [0.4522, 0.3575, 0.3559],
        [0.4683, 0.3819, 0.3556],
        [0.4774, 0.3921, 0.3210],
        [0.4705, 0.3766, 0.3392],
        [0.4567, 0.3719, 0.3644],
        [0.4760, 0.4020, 0.3380],
        [0.4271, 0.4003, 0.3697],
        [0.4920, 0.3606, 0.3304],
        [0.4697, 0.3938, 0.3262],
        [0.4364, 0.4066, 0.3511],
        [0.4470, 0.3663, 0.3662],
        [0.4769, 0.3847, 0.3219],
        [0.4172, 0.4132, 0.4007],
        [0.4765, 0.3860, 0.3283],
        [0.4724, 0.3948, 0.3359],
        [0.4952, 0.4057, 0.2798],
        [0.4521, 0.3712, 0.3523],
        [0.4339, 0.3891, 0.3495],
        [0.4594, 0.3901, 0.3513],
        [0.5077, 0.4003, 0.2837],
        [0.4530, 0.3997, 0.3187],
        [0.4831, 0.3678, 0.3465],
        [0.4438, 0.4115, 0.3567],
        [0.4840, 0.4128, 0.3117],
        [0.4806, 0.3851, 0.3216],
        [0.4987, 0.4107, 0.2891],
        [0.4556, 0.3714, 0.3662],
        [0.4689, 0.3777, 0.3462],
        [0.4488, 0.3653, 0.3600],
        [0.4770, 0.3805, 0.3216],
        [0.4576, 0.3731, 0.3451],
        [0.4540, 0.3581, 0.3401],
        [0.4746, 0.3986, 0.2974],
        [0.4579, 0.3735, 0.3479],
        [0.4856, 0.3912, 0.3178],
        [0.4248, 0.4029, 0.3417],
        [0.4630, 0.3575, 0.3561],
        [0.4904, 0.3971, 0.3034],
        [0.4610, 0.3743, 0.3669],
        [0.4943, 0.3894, 0.3103],
        [0.4780, 0.3812, 0.3268],
        [0.4831, 0.3901, 0.3076],
        [0.4784, 0.3892, 0.3284],
        [0.4859, 0.4054, 0.3251],
        [0.4493, 0.3699, 0.3665],
        [0.4942, 0.4020, 0.2880],
        [0.5070, 0.4065, 0.2895],
        [0.4696, 0.3572, 0.3457],
        [0.4727, 0.3823, 0.3433],
        [0.5074, 0.4180, 0.2706],
        [0.5069, 0.4162, 0.2860],
        [0.4425, 0.4070, 0.3450],
        [0.4922, 0.3727, 0.3389],
        [0.5634, 0.3303, 0.2687],
        [0.6761, 0.3054, 0.2420],
        [0.6826, 0.3118, 0.2418],
        [0.6565, 0.2891, 0.2662],
        [0.5873, 0.3098, 0.2622],
        [0.5313, 0.3422, 0.3204],
        [0.4904, 0.3742, 0.3367],
        [0.5106, 0.3594, 0.3338],
        [0.4207, 0.4016, 0.3873],
        [0.4933, 0.3689, 0.3414],
        [0.4887, 0.3686, 0.3477],
        [0.6432, 0.2885, 0.2682],
        [0.6982, 0.2953, 0.2432],
        [0.5464, 0.3552, 0.2771]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 32: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6948.0, Mean: 1152.9332275390625, Std: 1218.6868896484375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 69420.4765625, Mean: 11518.638671875, Std: 12176.6005859375
[DEBUG] Top-3 class probabilities:
tensor([[0.5619, 0.3386, 0.2618],
        [0.5345, 0.3425, 0.3004],
        [0.4858, 0.3692, 0.3341],
        [0.5559, 0.3455, 0.2707],
        [0.5746, 0.3480, 0.2760],
        [0.6192, 0.3007, 0.2719],
        [0.5873, 0.3193, 0.2558],
        [0.5191, 0.3565, 0.3207],
        [0.4769, 0.3902, 0.3149],
        [0.5250, 0.3627, 0.3066],
        [0.5173, 0.3643, 0.3156],
        [0.4706, 0.3976, 0.3350],
        [0.4260, 0.4131, 0.3584],
        [0.4695, 0.3826, 0.3563],
        [0.4249, 0.3920, 0.3856],
        [0.4783, 0.3645, 0.3631],
        [0.4751, 0.3870, 0.3425],
        [0.5033, 0.3656, 0.3244],
        [0.4724, 0.3769, 0.3625],
        [0.5905, 0.3163, 0.2573],
        [0.6971, 0.3061, 0.2309],
        [0.6929, 0.2966, 0.2369],
        [0.6138, 0.3071, 0.2617],
        [0.5947, 0.3080, 0.2632],
        [0.6529, 0.3129, 0.2599],
        [0.7025, 0.3024, 0.2323],
        [0.6972, 0.3021, 0.2283],
        [0.6801, 0.2984, 0.2473],
        [0.6481, 0.2801, 0.2725],
        [0.4875, 0.3908, 0.3173],
        [0.4768, 0.3898, 0.3377],
        [0.4944, 0.3830, 0.3031],
        [0.5800, 0.3115, 0.2668],
        [0.7011, 0.3177, 0.2332],
        [0.6809, 0.3045, 0.2516],
        [0.6654, 0.2917, 0.2589],
        [0.4593, 0.3727, 0.3612],
        [0.4184, 0.4004, 0.3882],
        [0.4767, 0.3612, 0.3469],
        [0.5091, 0.3595, 0.3251],
        [0.5339, 0.3383, 0.3062],
        [0.5315, 0.3548, 0.3068],
        [0.5076, 0.3687, 0.3085],
        [0.5836, 0.3150, 0.2632],
        [0.5856, 0.3130, 0.2858],
        [0.6454, 0.2909, 0.2717],
        [0.5014, 0.3573, 0.3367],
        [0.4624, 0.3891, 0.3636],
        [0.4442, 0.3984, 0.3549],
        [0.5506, 0.3430, 0.2820],
        [0.5792, 0.3294, 0.2655],
        [0.5678, 0.3406, 0.2650],
        [0.4674, 0.3812, 0.3474],
        [0.4764, 0.3872, 0.3213],
        [0.5017, 0.3671, 0.3099],
        [0.4873, 0.3708, 0.3371],
        [0.4985, 0.3655, 0.3007],
        [0.5550, 0.3485, 0.2775],
        [0.5046, 0.3684, 0.3219],
        [0.4958, 0.3728, 0.3144],
        [0.5096, 0.3634, 0.3100],
        [0.4709, 0.3808, 0.3385],
        [0.4747, 0.3959, 0.3242],
        [0.5239, 0.3538, 0.2977],
        [0.5112, 0.3771, 0.2994],
        [0.5355, 0.3534, 0.2869],
        [0.6711, 0.3086, 0.2427],
        [0.6937, 0.2930, 0.2390],
        [0.6897, 0.3049, 0.2354],
        [0.6197, 0.2902, 0.2681],
        [0.5589, 0.3492, 0.2964],
        [0.6243, 0.2842, 0.2689],
        [0.6812, 0.2953, 0.2457],
        [0.5260, 0.3570, 0.3173],
        [0.5147, 0.3692, 0.3395],
        [0.4506, 0.3948, 0.3407],
        [0.5285, 0.3439, 0.3023],
        [0.6597, 0.3033, 0.2534],
        [0.6490, 0.2932, 0.2838],
        [0.6434, 0.2957, 0.2719],
        [0.6946, 0.3116, 0.2402],
        [0.6983, 0.3094, 0.2278],
        [0.6595, 0.2903, 0.2695],
        [0.5708, 0.3251, 0.2731],
        [0.4829, 0.3594, 0.3484],
        [0.5407, 0.3409, 0.3012],
        [0.5349, 0.3449, 0.3101],
        [0.5058, 0.3451, 0.3137],
        [0.5687, 0.3248, 0.2824],
        [0.5294, 0.3402, 0.3040],
        [0.6174, 0.2973, 0.2527],
        [0.5434, 0.3361, 0.2830],
        [0.5785, 0.3198, 0.2760],
        [0.4512, 0.3801, 0.3722],
        [0.5366, 0.3478, 0.2905],
        [0.5904, 0.3193, 0.2571],
        [0.5663, 0.3493, 0.2779],
        [0.5735, 0.3464, 0.2683],
        [0.5091, 0.3720, 0.3176],
        [0.4959, 0.3746, 0.3423],
        [0.4651, 0.3926, 0.3291],
        [0.4847, 0.3757, 0.3170],
        [0.5006, 0.3806, 0.3130],
        [0.5120, 0.3698, 0.3076],
        [0.5115, 0.3671, 0.3125],
        [0.5071, 0.3745, 0.3137],
        [0.5087, 0.3785, 0.3140],
        [0.5024, 0.3804, 0.3324],
        [0.5112, 0.3678, 0.3084],
        [0.4931, 0.3863, 0.3192],
        [0.4864, 0.3871, 0.3153],
        [0.5084, 0.3740, 0.3174],
        [0.5372, 0.3499, 0.2879],
        [0.6895, 0.2979, 0.2378],
        [0.6798, 0.2870, 0.2442],
        [0.5226, 0.3684, 0.3147],
        [0.5010, 0.3612, 0.3232],
        [0.4551, 0.3942, 0.3501],
        [0.5976, 0.3146, 0.2559],
        [0.6651, 0.2891, 0.2609],
        [0.6972, 0.3065, 0.2325],
        [0.7053, 0.2994, 0.2301],
        [0.7003, 0.3028, 0.2297],
        [0.6131, 0.3037, 0.2460],
        [0.4667, 0.3741, 0.3489],
        [0.4799, 0.3656, 0.3294],
        [0.5605, 0.3234, 0.2850],
        [0.5249, 0.3461, 0.3110]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 33: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8048.0, Mean: 872.076416015625, Std: 1153.1051025390625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 80411.2109375, Mean: 8712.435546875, Std: 11521.3349609375
[DEBUG] Top-3 class probabilities:
tensor([[0.4891, 0.3710, 0.3515],
        [0.4634, 0.3807, 0.3679],
        [0.4667, 0.3561, 0.3550],
        [0.4506, 0.3918, 0.3668],
        [0.5687, 0.3345, 0.2664],
        [0.5133, 0.3627, 0.3172],
        [0.5290, 0.3531, 0.2870],
        [0.5431, 0.3477, 0.2819],
        [0.5456, 0.3567, 0.2933],
        [0.5129, 0.3687, 0.2934],
        [0.4689, 0.3940, 0.3299],
        [0.4730, 0.3871, 0.3210],
        [0.4344, 0.4085, 0.3588],
        [0.4207, 0.4132, 0.3744],
        [0.4710, 0.3950, 0.3189],
        [0.5812, 0.3346, 0.2576],
        [0.6124, 0.3087, 0.2474],
        [0.6545, 0.2921, 0.2793],
        [0.6799, 0.2651, 0.2546],
        [0.5515, 0.3468, 0.2705],
        [0.4390, 0.4078, 0.3736],
        [0.5654, 0.3343, 0.2591],
        [0.6724, 0.2851, 0.2638],
        [0.4955, 0.3846, 0.3092],
        [0.5150, 0.3632, 0.3244],
        [0.5940, 0.3011, 0.2726],
        [0.6568, 0.3241, 0.2687],
        [0.6802, 0.3000, 0.2442],
        [0.6992, 0.3088, 0.2306],
        [0.7013, 0.3100, 0.2316],
        [0.6873, 0.3136, 0.2432],
        [0.5394, 0.3651, 0.3062],
        [0.5495, 0.3398, 0.2955],
        [0.4354, 0.3997, 0.3653],
        [0.4757, 0.3668, 0.3421],
        [0.6456, 0.2868, 0.2702],
        [0.7030, 0.2999, 0.2251],
        [0.7012, 0.3019, 0.2262],
        [0.6999, 0.3038, 0.2280],
        [0.7017, 0.3093, 0.2296],
        [0.6924, 0.3040, 0.2347],
        [0.5705, 0.3321, 0.2818],
        [0.4809, 0.3724, 0.3344],
        [0.4795, 0.3717, 0.3411],
        [0.5043, 0.3600, 0.3265],
        [0.4876, 0.3721, 0.3320],
        [0.5074, 0.3634, 0.3351],
        [0.4886, 0.3601, 0.3450],
        [0.5315, 0.3446, 0.3162],
        [0.6451, 0.2881, 0.2871],
        [0.5044, 0.3685, 0.3227],
        [0.4673, 0.3815, 0.3302],
        [0.4764, 0.3729, 0.3119],
        [0.5361, 0.3613, 0.2856],
        [0.5621, 0.3437, 0.2802],
        [0.6241, 0.2825, 0.2811],
        [0.6217, 0.2949, 0.2586],
        [0.5660, 0.3476, 0.2511],
        [0.4788, 0.3869, 0.3084],
        [0.4994, 0.3746, 0.3234],
        [0.6323, 0.2981, 0.2563],
        [0.6993, 0.3074, 0.2285],
        [0.6986, 0.3052, 0.2274],
        [0.7079, 0.3146, 0.2293],
        [0.7013, 0.3081, 0.2265],
        [0.6933, 0.2892, 0.2396],
        [0.5743, 0.3162, 0.2591],
        [0.5090, 0.3636, 0.3146],
        [0.6671, 0.3014, 0.2527],
        [0.7022, 0.3092, 0.2300],
        [0.6798, 0.2942, 0.2502],
        [0.5540, 0.3404, 0.3039],
        [0.4981, 0.3695, 0.3306],
        [0.5156, 0.3509, 0.2932],
        [0.5906, 0.3072, 0.2433],
        [0.5725, 0.3161, 0.2611],
        [0.6937, 0.2966, 0.2488],
        [0.6768, 0.2798, 0.2668],
        [0.6346, 0.2947, 0.2651],
        [0.6444, 0.2966, 0.2688],
        [0.5432, 0.3479, 0.2887],
        [0.5502, 0.3375, 0.2781],
        [0.7091, 0.2938, 0.2206],
        [0.7017, 0.3016, 0.2263],
        [0.6994, 0.3034, 0.2279],
        [0.6993, 0.3032, 0.2279],
        [0.6982, 0.3040, 0.2286],
        [0.6964, 0.2985, 0.2337],
        [0.5923, 0.3140, 0.2540],
        [0.5469, 0.3464, 0.2853],
        [0.6024, 0.3195, 0.2634],
        [0.5432, 0.3494, 0.2904],
        [0.4798, 0.3857, 0.3517],
        [0.4816, 0.3664, 0.3528],
        [0.4979, 0.3661, 0.3310],
        [0.5857, 0.3068, 0.2519],
        [0.5536, 0.3517, 0.2981],
        [0.5043, 0.3697, 0.3215],
        [0.5330, 0.3550, 0.2920],
        [0.6103, 0.3057, 0.2582],
        [0.6993, 0.3002, 0.2277],
        [0.6991, 0.2987, 0.2275],
        [0.6984, 0.3020, 0.2293],
        [0.6415, 0.2964, 0.2850],
        [0.4923, 0.3893, 0.3378],
        [0.5938, 0.3152, 0.2674],
        [0.6979, 0.3020, 0.2288],
        [0.6975, 0.3023, 0.2285],
        [0.6967, 0.3053, 0.2295],
        [0.6974, 0.3047, 0.2298],
        [0.6979, 0.3027, 0.2292],
        [0.6973, 0.2969, 0.2337],
        [0.6884, 0.2776, 0.2532],
        [0.6786, 0.2905, 0.2728],
        [0.6265, 0.2840, 0.2763],
        [0.6998, 0.3082, 0.2276],
        [0.6798, 0.2665, 0.2608],
        [0.5061, 0.3661, 0.3479],
        [0.4954, 0.3722, 0.3294],
        [0.4700, 0.3877, 0.3367],
        [0.5125, 0.3689, 0.3177],
        [0.6492, 0.2860, 0.2734],
        [0.6968, 0.3024, 0.2297],
        [0.6965, 0.3021, 0.2295],
        [0.7051, 0.2959, 0.2230],
        [0.7069, 0.2933, 0.2222],
        [0.6990, 0.3045, 0.2316],
        [0.7049, 0.2980, 0.2239]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 34: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7016.0, Mean: 429.42694091796875, Std: 862.8447875976562
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 70099.90625, Mean: 4289.6708984375, Std: 8621.177734375
[DEBUG] Top-3 class probabilities:
tensor([[0.7002, 0.3024, 0.2273],
        [0.6996, 0.3072, 0.2294],
        [0.6994, 0.3043, 0.2282],
        [0.6990, 0.3028, 0.2279],
        [0.6988, 0.3034, 0.2285],
        [0.6986, 0.3029, 0.2282],
        [0.6989, 0.3030, 0.2284],
        [0.6984, 0.3040, 0.2286],
        [0.6976, 0.3039, 0.2348],
        [0.6782, 0.2959, 0.2468],
        [0.6178, 0.2922, 0.2476],
        [0.5449, 0.3468, 0.3037],
        [0.5250, 0.3697, 0.2975],
        [0.5451, 0.3575, 0.3144],
        [0.5648, 0.3449, 0.2754],
        [0.5692, 0.3529, 0.2737],
        [0.6532, 0.2907, 0.2640],
        [0.6994, 0.2999, 0.2275],
        [0.6990, 0.2995, 0.2275],
        [0.6976, 0.3012, 0.2289],
        [0.7009, 0.2914, 0.2270],
        [0.6886, 0.2946, 0.2390],
        [0.6380, 0.2958, 0.2707],
        [0.6799, 0.3068, 0.2414],
        [0.6972, 0.3034, 0.2288],
        [0.6975, 0.3036, 0.2291],
        [0.6971, 0.3031, 0.2292],
        [0.6969, 0.3028, 0.2294],
        [0.6994, 0.3080, 0.2303],
        [0.6998, 0.3031, 0.2300],
        [0.6891, 0.3054, 0.2456],
        [0.6325, 0.2894, 0.2771],
        [0.6807, 0.2991, 0.2544],
        [0.6702, 0.2845, 0.2548],
        [0.5335, 0.3530, 0.3042],
        [0.4858, 0.3971, 0.3496],
        [0.4967, 0.3802, 0.3270],
        [0.5258, 0.3700, 0.3127],
        [0.5887, 0.3228, 0.2504],
        [0.6914, 0.3037, 0.2308],
        [0.6967, 0.3033, 0.2296],
        [0.7061, 0.2936, 0.2215],
        [0.7027, 0.2987, 0.2252],
        [0.7037, 0.2993, 0.2248],
        [0.7010, 0.3019, 0.2265],
        [0.7007, 0.3031, 0.2272],
        [0.6878, 0.2924, 0.2385],
        [0.7006, 0.2983, 0.2303],
        [0.6926, 0.2986, 0.2335],
        [0.7005, 0.2998, 0.2341],
        [0.6991, 0.3045, 0.2282],
        [0.6983, 0.3032, 0.2288],
        [0.6989, 0.3026, 0.2283],
        [0.6989, 0.3023, 0.2283],
        [0.6981, 0.3017, 0.2286],
        [0.6977, 0.3036, 0.2292],
        [0.6994, 0.2986, 0.2339],
        [0.5479, 0.3609, 0.2910],
        [0.5188, 0.3729, 0.3268],
        [0.5476, 0.3519, 0.2935],
        [0.4873, 0.3794, 0.3427],
        [0.5019, 0.3888, 0.3095],
        [0.5944, 0.3344, 0.2593],
        [0.6548, 0.2847, 0.2554],
        [0.6985, 0.3001, 0.2280],
        [0.6990, 0.2975, 0.2275],
        [0.6979, 0.2997, 0.2280],
        [0.6972, 0.3007, 0.2284],
        [0.6975, 0.3027, 0.2286],
        [0.6972, 0.3032, 0.2291],
        [0.6969, 0.3027, 0.2290],
        [0.6970, 0.3018, 0.2287],
        [0.6972, 0.3025, 0.2290],
        [0.6970, 0.3022, 0.2290],
        [0.6992, 0.3138, 0.2299],
        [0.6521, 0.2897, 0.2630],
        [0.6290, 0.2971, 0.2729],
        [0.6961, 0.3070, 0.2383],
        [0.6274, 0.2851, 0.2681],
        [0.4752, 0.3912, 0.3242],
        [0.6617, 0.2855, 0.2690],
        [0.5618, 0.3478, 0.2941],
        [0.5348, 0.3509, 0.2898],
        [0.6933, 0.3039, 0.2430],
        [0.6967, 0.3032, 0.2295],
        [0.7099, 0.2898, 0.2191],
        [0.7024, 0.2993, 0.2253],
        [0.7039, 0.3027, 0.2302],
        [0.7002, 0.3019, 0.2270],
        [0.6914, 0.2929, 0.2450],
        [0.6824, 0.2973, 0.2405],
        [0.6999, 0.3029, 0.2277],
        [0.6996, 0.3025, 0.2278],
        [0.6910, 0.2910, 0.2360],
        [0.6933, 0.3092, 0.2322],
        [0.6988, 0.3026, 0.2282],
        [0.6979, 0.3025, 0.2287],
        [0.6974, 0.3026, 0.2291],
        [0.6977, 0.3025, 0.2292],
        [0.6972, 0.3048, 0.2295],
        [0.6584, 0.2708, 0.2679],
        [0.4431, 0.3873, 0.3550],
        [0.4466, 0.3916, 0.3510],
        [0.5394, 0.3645, 0.2980],
        [0.5445, 0.3704, 0.2943],
        [0.5371, 0.3629, 0.2909],
        [0.5721, 0.3583, 0.2665],
        [0.5791, 0.3247, 0.2544],
        [0.6982, 0.3017, 0.2280],
        [0.6989, 0.2971, 0.2271],
        [0.6997, 0.2987, 0.2268],
        [0.7018, 0.2933, 0.2246],
        [0.6954, 0.2970, 0.2277],
        [0.6982, 0.2997, 0.2281],
        [0.6975, 0.3020, 0.2285],
        [0.6974, 0.3024, 0.2288],
        [0.6980, 0.3021, 0.2287],
        [0.6974, 0.3025, 0.2289],
        [0.6970, 0.3029, 0.2292],
        [0.6963, 0.3028, 0.2295],
        [0.6984, 0.3076, 0.2272],
        [0.7015, 0.3052, 0.2324],
        [0.6141, 0.2989, 0.2853],
        [0.6102, 0.2996, 0.2684],
        [0.6395, 0.2916, 0.2628],
        [0.6798, 0.2877, 0.2555],
        [0.6965, 0.3032, 0.2286],
        [0.6940, 0.2998, 0.2322]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 35: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7572.0, Mean: 448.0543212890625, Std: 831.3074340820312
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 75655.21875, Mean: 4475.78759765625, Std: 8306.0703125
[DEBUG] Top-3 class probabilities:
tensor([[0.6753, 0.2960, 0.2529],
        [0.6963, 0.3012, 0.2323],
        [0.7121, 0.2882, 0.2180],
        [0.7027, 0.3071, 0.2279],
        [0.7022, 0.2996, 0.2257],
        [0.7015, 0.2991, 0.2257],
        [0.7025, 0.3002, 0.2260],
        [0.6975, 0.3054, 0.2310],
        [0.6952, 0.3046, 0.2312],
        [0.6997, 0.3006, 0.2271],
        [0.6991, 0.3047, 0.2282],
        [0.7010, 0.3028, 0.2284],
        [0.6983, 0.3032, 0.2289],
        [0.6980, 0.3041, 0.2296],
        [0.6964, 0.3038, 0.2297],
        [0.6964, 0.3040, 0.2300],
        [0.6436, 0.2685, 0.2675],
        [0.4434, 0.3911, 0.3791],
        [0.4326, 0.3794, 0.3561],
        [0.4543, 0.3871, 0.3487],
        [0.5132, 0.3685, 0.3068],
        [0.5163, 0.3718, 0.3028],
        [0.4634, 0.4017, 0.3417],
        [0.5458, 0.3622, 0.2757],
        [0.6263, 0.3166, 0.2422],
        [0.6819, 0.3079, 0.2389],
        [0.7003, 0.2973, 0.2261],
        [0.6999, 0.2985, 0.2269],
        [0.6981, 0.2993, 0.2284],
        [0.6974, 0.2993, 0.2282],
        [0.6976, 0.3018, 0.2286],
        [0.6977, 0.3022, 0.2286],
        [0.6973, 0.3027, 0.2290],
        [0.6773, 0.2914, 0.2498],
        [0.6818, 0.3005, 0.2373],
        [0.6970, 0.3035, 0.2295],
        [0.6987, 0.2994, 0.2274],
        [0.6941, 0.3015, 0.2352],
        [0.6963, 0.3094, 0.2299],
        [0.6842, 0.2870, 0.2452],
        [0.6679, 0.2869, 0.2634],
        [0.6912, 0.3047, 0.2330],
        [0.7026, 0.2940, 0.2257],
        [0.6966, 0.3016, 0.2296],
        [0.6969, 0.3007, 0.2289],
        [0.6974, 0.3078, 0.2308],
        [0.7015, 0.3054, 0.2280],
        [0.7128, 0.2909, 0.2191],
        [0.7103, 0.2989, 0.2213],
        [0.7063, 0.3032, 0.2353],
        [0.7017, 0.3005, 0.2261],
        [0.7018, 0.3012, 0.2268],
        [0.6513, 0.2700, 0.2684],
        [0.6279, 0.2783, 0.2756],
        [0.6981, 0.2994, 0.2301],
        [0.6869, 0.2869, 0.2397],
        [0.6981, 0.3031, 0.2290],
        [0.7006, 0.3051, 0.2341],
        [0.6981, 0.3044, 0.2289],
        [0.6960, 0.3031, 0.2299],
        [0.6695, 0.2803, 0.2571],
        [0.5251, 0.3498, 0.3143],
        [0.4314, 0.3775, 0.3590],
        [0.4551, 0.3771, 0.3606],
        [0.4878, 0.3627, 0.3278],
        [0.5235, 0.3552, 0.3056],
        [0.4810, 0.3831, 0.3249],
        [0.4906, 0.3853, 0.3012],
        [0.5466, 0.3546, 0.2792],
        [0.5505, 0.3512, 0.2843],
        [0.6628, 0.2870, 0.2600],
        [0.6515, 0.2750, 0.2672],
        [0.6982, 0.2994, 0.2284],
        [0.6976, 0.3013, 0.2284],
        [0.6969, 0.3028, 0.2293],
        [0.6946, 0.2935, 0.2362],
        [0.6938, 0.3024, 0.2362],
        [0.7034, 0.3009, 0.2327],
        [0.6968, 0.3012, 0.2288],
        [0.6963, 0.3024, 0.2295],
        [0.6893, 0.3031, 0.2415],
        [0.6312, 0.2870, 0.2427],
        [0.6021, 0.3127, 0.2625],
        [0.6622, 0.3068, 0.2546],
        [0.7042, 0.3054, 0.2321],
        [0.6940, 0.3039, 0.2304],
        [0.6967, 0.3039, 0.2291],
        [0.6903, 0.3060, 0.2391],
        [0.6803, 0.2916, 0.2390],
        [0.7070, 0.2962, 0.2212],
        [0.6654, 0.2741, 0.2736],
        [0.6602, 0.3197, 0.2608],
        [0.6986, 0.3002, 0.2293],
        [0.6974, 0.2852, 0.2495],
        [0.6522, 0.2912, 0.2667],
        [0.6988, 0.3014, 0.2278],
        [0.6971, 0.3019, 0.2290],
        [0.6969, 0.3031, 0.2296],
        [0.6963, 0.3004, 0.2295],
        [0.6958, 0.3028, 0.2302],
        [0.6940, 0.3054, 0.2318],
        [0.6054, 0.3148, 0.2507],
        [0.4495, 0.3939, 0.3501],
        [0.4257, 0.3817, 0.3569],
        [0.4566, 0.3798, 0.3358],
        [0.4478, 0.3876, 0.3605],
        [0.4488, 0.3873, 0.3465],
        [0.5195, 0.3718, 0.3072],
        [0.4627, 0.3854, 0.3322],
        [0.6386, 0.3175, 0.2234],
        [0.5624, 0.3487, 0.2891],
        [0.6353, 0.3211, 0.2243],
        [0.6479, 0.3200, 0.2474],
        [0.6577, 0.2855, 0.2810],
        [0.7023, 0.3094, 0.2336],
        [0.6983, 0.3014, 0.2285],
        [0.6977, 0.3013, 0.2288],
        [0.6976, 0.3012, 0.2288],
        [0.6972, 0.3027, 0.2291],
        [0.6967, 0.3008, 0.2290],
        [0.6963, 0.3013, 0.2288],
        [0.6966, 0.3009, 0.2290],
        [0.6965, 0.2997, 0.2291],
        [0.6967, 0.3009, 0.2289],
        [0.6961, 0.3027, 0.2295],
        [0.6969, 0.3009, 0.2287],
        [0.6973, 0.3003, 0.2286],
        [0.6970, 0.3031, 0.2310]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 36: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15456.0, Mean: 944.8262329101562, Std: 1113.7789306640625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 154428.796875, Mean: 9439.3203125, Std: 11128.404296875
[DEBUG] Top-3 class probabilities:
tensor([[0.6688, 0.2750, 0.2550],
        [0.6958, 0.3041, 0.2302],
        [0.5128, 0.3692, 0.3217],
        [0.5731, 0.3236, 0.2653],
        [0.6599, 0.3036, 0.2584],
        [0.6690, 0.2969, 0.2588],
        [0.6737, 0.3050, 0.2452],
        [0.7071, 0.3048, 0.2311],
        [0.6978, 0.3020, 0.2294],
        [0.6974, 0.3026, 0.2295],
        [0.6970, 0.3022, 0.2292],
        [0.6543, 0.2876, 0.2576],
        [0.6722, 0.3109, 0.2456],
        [0.6965, 0.3047, 0.2305],
        [0.6950, 0.3049, 0.2308],
        [0.5256, 0.3457, 0.3004],
        [0.4498, 0.3862, 0.3525],
        [0.4608, 0.3864, 0.3408],
        [0.4863, 0.3872, 0.3217],
        [0.5392, 0.3506, 0.2876],
        [0.4850, 0.3707, 0.3160],
        [0.4848, 0.3753, 0.3349],
        [0.5081, 0.3787, 0.3138],
        [0.5994, 0.3390, 0.2420],
        [0.5904, 0.3410, 0.2466],
        [0.6337, 0.3199, 0.2423],
        [0.5778, 0.3480, 0.2469],
        [0.5730, 0.3486, 0.2727],
        [0.6985, 0.3014, 0.2281],
        [0.6977, 0.3027, 0.2289],
        [0.6970, 0.3018, 0.2287],
        [0.6981, 0.3015, 0.2284],
        [0.6971, 0.3011, 0.2291],
        [0.6963, 0.3011, 0.2295],
        [0.6965, 0.2998, 0.2291],
        [0.6968, 0.2991, 0.2288],
        [0.6965, 0.3020, 0.2295],
        [0.6955, 0.3021, 0.2297],
        [0.6950, 0.3014, 0.2295],
        [0.6990, 0.2982, 0.2309],
        [0.7009, 0.2997, 0.2297],
        [0.5897, 0.3215, 0.2594],
        [0.4740, 0.3849, 0.3551],
        [0.5137, 0.3703, 0.3193],
        [0.4911, 0.3753, 0.3319],
        [0.5113, 0.3593, 0.3133],
        [0.6643, 0.3069, 0.2505],
        [0.6752, 0.3019, 0.2437],
        [0.4708, 0.3789, 0.3374],
        [0.5845, 0.3122, 0.2658],
        [0.5297, 0.3511, 0.2936],
        [0.4580, 0.3884, 0.3523],
        [0.4579, 0.3756, 0.3417],
        [0.5026, 0.3743, 0.3107],
        [0.5627, 0.3459, 0.2833],
        [0.5117, 0.3681, 0.3060],
        [0.4729, 0.4034, 0.3087],
        [0.5265, 0.3811, 0.3029],
        [0.5078, 0.3716, 0.3104],
        [0.5127, 0.3712, 0.3258],
        [0.5965, 0.3320, 0.2589],
        [0.5508, 0.3632, 0.2921],
        [0.5543, 0.3528, 0.2857],
        [0.5619, 0.3668, 0.2838],
        [0.5798, 0.3187, 0.2488],
        [0.6945, 0.2988, 0.2395],
        [0.5952, 0.3057, 0.2624],
        [0.6340, 0.2852, 0.2752],
        [0.6965, 0.3007, 0.2296],
        [0.6962, 0.3020, 0.2303],
        [0.6954, 0.3037, 0.2299],
        [0.6956, 0.3005, 0.2296],
        [0.6962, 0.3010, 0.2300],
        [0.6969, 0.3053, 0.2294],
        [0.6728, 0.2816, 0.2606],
        [0.5010, 0.3716, 0.3487],
        [0.5025, 0.3708, 0.3391],
        [0.4466, 0.3863, 0.3854],
        [0.4659, 0.3833, 0.3671],
        [0.4624, 0.3776, 0.3688],
        [0.4802, 0.3846, 0.3304],
        [0.5282, 0.3571, 0.3086],
        [0.5084, 0.3768, 0.2944],
        [0.5795, 0.3352, 0.2554],
        [0.5937, 0.3189, 0.2632],
        [0.5234, 0.3605, 0.3043],
        [0.4569, 0.3922, 0.3545],
        [0.4751, 0.3693, 0.3352],
        [0.5169, 0.3556, 0.2952],
        [0.5504, 0.3438, 0.2984],
        [0.5499, 0.3371, 0.2990],
        [0.5048, 0.3642, 0.3335],
        [0.4914, 0.3727, 0.3293],
        [0.5759, 0.3371, 0.2786],
        [0.5419, 0.3403, 0.3009],
        [0.5249, 0.3764, 0.2977],
        [0.6146, 0.3184, 0.2422],
        [0.5580, 0.3551, 0.2814],
        [0.5241, 0.3749, 0.2949],
        [0.5038, 0.3826, 0.3298],
        [0.5855, 0.3472, 0.2495],
        [0.5458, 0.3509, 0.2851],
        [0.5587, 0.3418, 0.2885],
        [0.4653, 0.3913, 0.3102],
        [0.4981, 0.3845, 0.3198],
        [0.5083, 0.3646, 0.2886],
        [0.6359, 0.2996, 0.2788],
        [0.6719, 0.3040, 0.2558],
        [0.6953, 0.3056, 0.2307],
        [0.6967, 0.3048, 0.2293],
        [0.6963, 0.3042, 0.2297],
        [0.6980, 0.2988, 0.2303],
        [0.6985, 0.3030, 0.2285],
        [0.6762, 0.3132, 0.2439],
        [0.6957, 0.3039, 0.2307],
        [0.4767, 0.3663, 0.3581],
        [0.4727, 0.3721, 0.3622],
        [0.4520, 0.3908, 0.3844],
        [0.5456, 0.3372, 0.3010],
        [0.4858, 0.3673, 0.3281],
        [0.5198, 0.3593, 0.3245],
        [0.4648, 0.3873, 0.3648],
        [0.6171, 0.3030, 0.2817],
        [0.6198, 0.3053, 0.2452],
        [0.5333, 0.3704, 0.2919],
        [0.5252, 0.3677, 0.3063],
        [0.5151, 0.3797, 0.3108],
        [0.4844, 0.3858, 0.3298]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 37: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8904.0, Mean: 1338.2513427734375, Std: 1232.4862060546875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 88964.0, Mean: 13370.2578125, Std: 12314.4775390625
[DEBUG] Top-3 class probabilities:
tensor([[0.6003, 0.3227, 0.2509],
        [0.5218, 0.3357, 0.2831],
        [0.4473, 0.3773, 0.3323],
        [0.4502, 0.3748, 0.3562],
        [0.5062, 0.3601, 0.3399],
        [0.5304, 0.3468, 0.3123],
        [0.4343, 0.3963, 0.3768],
        [0.5339, 0.3665, 0.2995],
        [0.6270, 0.3031, 0.2657],
        [0.5213, 0.3672, 0.3035],
        [0.4732, 0.3732, 0.3370],
        [0.4658, 0.4015, 0.3549],
        [0.5296, 0.3596, 0.3020],
        [0.4888, 0.3847, 0.3230],
        [0.4670, 0.4037, 0.3193],
        [0.4837, 0.3896, 0.3257],
        [0.4431, 0.4201, 0.3306],
        [0.4528, 0.4053, 0.3244],
        [0.4672, 0.3952, 0.3320],
        [0.4514, 0.3975, 0.3480],
        [0.5512, 0.3376, 0.2683],
        [0.6864, 0.3048, 0.2376],
        [0.6973, 0.3054, 0.2295],
        [0.6934, 0.2960, 0.2397],
        [0.6779, 0.2988, 0.2516],
        [0.6265, 0.3091, 0.2790],
        [0.6964, 0.3036, 0.2301],
        [0.6645, 0.2727, 0.2699],
        [0.5293, 0.3559, 0.2994],
        [0.4783, 0.3836, 0.3395],
        [0.5992, 0.3015, 0.2712],
        [0.4871, 0.3609, 0.3545],
        [0.4807, 0.3780, 0.3520],
        [0.4996, 0.3612, 0.3441],
        [0.5834, 0.3207, 0.2717],
        [0.5426, 0.3476, 0.2991],
        [0.4932, 0.3786, 0.3288],
        [0.4971, 0.3603, 0.3385],
        [0.5386, 0.3528, 0.3023],
        [0.5370, 0.3499, 0.2851],
        [0.6030, 0.3054, 0.2595],
        [0.5099, 0.3705, 0.3328],
        [0.5003, 0.3674, 0.3398],
        [0.4750, 0.3753, 0.3570],
        [0.5751, 0.3216, 0.2873],
        [0.5686, 0.3301, 0.2839],
        [0.5492, 0.3251, 0.2753],
        [0.4575, 0.3792, 0.3448],
        [0.4440, 0.3805, 0.3515],
        [0.4457, 0.3805, 0.3609],
        [0.5591, 0.3470, 0.2975],
        [0.4351, 0.3947, 0.3722],
        [0.6085, 0.3276, 0.2407],
        [0.5639, 0.3259, 0.2608],
        [0.4949, 0.3737, 0.3313],
        [0.4891, 0.3844, 0.3358],
        [0.4996, 0.3798, 0.3310],
        [0.4705, 0.3998, 0.3266],
        [0.4699, 0.3903, 0.3437],
        [0.4448, 0.4038, 0.3546],
        [0.4486, 0.4078, 0.3385],
        [0.5414, 0.3589, 0.3034],
        [0.4662, 0.3941, 0.3216],
        [0.5047, 0.3794, 0.3215],
        [0.4456, 0.4097, 0.3544],
        [0.4983, 0.3849, 0.3227],
        [0.5314, 0.3425, 0.2844],
        [0.5810, 0.3047, 0.2734],
        [0.6963, 0.3067, 0.2391],
        [0.4358, 0.4071, 0.3296],
        [0.5893, 0.3027, 0.2777],
        [0.4941, 0.3732, 0.2904],
        [0.4687, 0.3907, 0.3380],
        [0.4958, 0.3736, 0.3363],
        [0.4929, 0.3746, 0.3410],
        [0.5154, 0.3621, 0.3142],
        [0.5073, 0.3657, 0.3366],
        [0.5466, 0.3436, 0.3040],
        [0.5927, 0.3178, 0.2537],
        [0.5295, 0.3644, 0.3148],
        [0.5189, 0.3648, 0.3160],
        [0.5158, 0.3595, 0.3301],
        [0.5000, 0.3812, 0.3237],
        [0.4768, 0.3959, 0.3625],
        [0.5340, 0.3501, 0.3200],
        [0.4783, 0.3683, 0.3477],
        [0.4819, 0.3736, 0.3537],
        [0.4543, 0.3814, 0.3661],
        [0.5091, 0.3598, 0.3317],
        [0.6044, 0.3242, 0.2636],
        [0.4372, 0.3877, 0.3724],
        [0.4593, 0.4035, 0.3324],
        [0.4664, 0.3859, 0.3350],
        [0.4886, 0.3704, 0.3144],
        [0.5578, 0.3341, 0.2738],
        [0.5819, 0.3174, 0.2628],
        [0.4814, 0.3712, 0.3483],
        [0.4616, 0.3852, 0.3746],
        [0.5119, 0.3703, 0.3001],
        [0.4511, 0.3834, 0.3804],
        [0.5048, 0.3631, 0.3253],
        [0.5200, 0.3651, 0.2961],
        [0.4998, 0.3793, 0.3246],
        [0.4907, 0.3790, 0.3233],
        [0.4770, 0.3916, 0.3506],
        [0.5183, 0.3706, 0.3172],
        [0.5133, 0.3780, 0.3202],
        [0.4924, 0.3813, 0.3415],
        [0.4463, 0.3929, 0.3791],
        [0.5566, 0.3332, 0.2846],
        [0.4690, 0.3785, 0.2996],
        [0.5040, 0.3746, 0.3249],
        [0.5478, 0.3621, 0.2776],
        [0.4732, 0.4039, 0.3337],
        [0.5112, 0.3766, 0.3111],
        [0.4755, 0.3656, 0.3539],
        [0.5262, 0.3588, 0.3190],
        [0.4936, 0.3606, 0.3383],
        [0.5970, 0.3200, 0.2582],
        [0.5187, 0.3524, 0.3164],
        [0.5176, 0.3558, 0.3138],
        [0.4933, 0.3670, 0.3344],
        [0.4909, 0.3918, 0.3303],
        [0.5161, 0.3568, 0.3181],
        [0.5951, 0.3137, 0.2585],
        [0.5041, 0.3745, 0.3361],
        [0.5005, 0.3751, 0.3453],
        [0.5264, 0.3572, 0.3090]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 38: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17664.0, Mean: 1360.7972412109375, Std: 1240.539306640625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 176490.1875, Mean: 13595.525390625, Std: 12394.94140625
[DEBUG] Top-3 class probabilities:
tensor([[0.5340, 0.3568, 0.2890],
        [0.4682, 0.3778, 0.3457],
        [0.5966, 0.3182, 0.2565],
        [0.5562, 0.3520, 0.2996],
        [0.4460, 0.3999, 0.3571],
        [0.4870, 0.3795, 0.3297],
        [0.5149, 0.3779, 0.3024],
        [0.4901, 0.3815, 0.3348],
        [0.5402, 0.3646, 0.2952],
        [0.4532, 0.3784, 0.3587],
        [0.4511, 0.3768, 0.3451],
        [0.5023, 0.3638, 0.3021],
        [0.4897, 0.3920, 0.3138],
        [0.4563, 0.3886, 0.3588],
        [0.4698, 0.4043, 0.3538],
        [0.5328, 0.3597, 0.3087],
        [0.5275, 0.3707, 0.3127],
        [0.4972, 0.3773, 0.3182],
        [0.4798, 0.4016, 0.3242],
        [0.5603, 0.3456, 0.2727],
        [0.6014, 0.3122, 0.2813],
        [0.5627, 0.3426, 0.2796],
        [0.5421, 0.3486, 0.2850],
        [0.4580, 0.3915, 0.3527],
        [0.5156, 0.3659, 0.3259],
        [0.5336, 0.3515, 0.2733],
        [0.6374, 0.2797, 0.2719],
        [0.4576, 0.4009, 0.3255],
        [0.4707, 0.3827, 0.3290],
        [0.5296, 0.3628, 0.3128],
        [0.5413, 0.3563, 0.2852],
        [0.6568, 0.2756, 0.2693],
        [0.5143, 0.3549, 0.3300],
        [0.4957, 0.3648, 0.3365],
        [0.5019, 0.3697, 0.3345],
        [0.5219, 0.3440, 0.3071],
        [0.5498, 0.3417, 0.3010],
        [0.4719, 0.3774, 0.3717],
        [0.5098, 0.3603, 0.3350],
        [0.4714, 0.3862, 0.3481],
        [0.5254, 0.3638, 0.3154],
        [0.4898, 0.3862, 0.3447],
        [0.4744, 0.3721, 0.3573],
        [0.5004, 0.3635, 0.3337],
        [0.4618, 0.3905, 0.3627],
        [0.4863, 0.3841, 0.3079],
        [0.5408, 0.3577, 0.2987],
        [0.6280, 0.2828, 0.2821],
        [0.6204, 0.3136, 0.2657],
        [0.5189, 0.3564, 0.3479],
        [0.4982, 0.3663, 0.3428],
        [0.4810, 0.3705, 0.3439],
        [0.5148, 0.3730, 0.3232],
        [0.5557, 0.3423, 0.2818],
        [0.5157, 0.3533, 0.3182],
        [0.4677, 0.3788, 0.3259],
        [0.4480, 0.4013, 0.3415],
        [0.4262, 0.4179, 0.3724],
        [0.4282, 0.4103, 0.3529],
        [0.4446, 0.3925, 0.3621],
        [0.4606, 0.3735, 0.3641],
        [0.4796, 0.3802, 0.3440],
        [0.4835, 0.3854, 0.3235],
        [0.4997, 0.3835, 0.3390],
        [0.4869, 0.3803, 0.3467],
        [0.4658, 0.3926, 0.3486],
        [0.4982, 0.3761, 0.3373],
        [0.5674, 0.3515, 0.2869],
        [0.4427, 0.4167, 0.3372],
        [0.4983, 0.3817, 0.3030],
        [0.5620, 0.3526, 0.3070],
        [0.4989, 0.3775, 0.3099],
        [0.5320, 0.3614, 0.2953],
        [0.7100, 0.2865, 0.2299],
        [0.6790, 0.2704, 0.2638],
        [0.7157, 0.2839, 0.2159],
        [0.6879, 0.2617, 0.2550],
        [0.4871, 0.3687, 0.3559],
        [0.5055, 0.3630, 0.3318],
        [0.4476, 0.3865, 0.3781],
        [0.4780, 0.3763, 0.3566],
        [0.4835, 0.3692, 0.3450],
        [0.5011, 0.3585, 0.3523],
        [0.5587, 0.3489, 0.2971],
        [0.4646, 0.3750, 0.3627],
        [0.4681, 0.3776, 0.3640],
        [0.5130, 0.3622, 0.3130],
        [0.4856, 0.3808, 0.3234],
        [0.4783, 0.3744, 0.3420],
        [0.4715, 0.3778, 0.3586],
        [0.4858, 0.3803, 0.3435],
        [0.4189, 0.3959, 0.3903],
        [0.6725, 0.3024, 0.2491],
        [0.6518, 0.2765, 0.2652],
        [0.4844, 0.3655, 0.3624],
        [0.5025, 0.3673, 0.3396],
        [0.4755, 0.3671, 0.3278],
        [0.4804, 0.3657, 0.3277],
        [0.4738, 0.3715, 0.3351],
        [0.5069, 0.3679, 0.3174],
        [0.4930, 0.3787, 0.3410],
        [0.4413, 0.3823, 0.3729],
        [0.4178, 0.4011, 0.3818],
        [0.4539, 0.3779, 0.3598],
        [0.4521, 0.3921, 0.3721],
        [0.4605, 0.3766, 0.3617],
        [0.4556, 0.4020, 0.3572],
        [0.5219, 0.3719, 0.3211],
        [0.4789, 0.3846, 0.3252],
        [0.4651, 0.4068, 0.3335],
        [0.5083, 0.3692, 0.3096],
        [0.5072, 0.3757, 0.3203],
        [0.5291, 0.3725, 0.3042],
        [0.4806, 0.3906, 0.3313],
        [0.5372, 0.3706, 0.2980],
        [0.4471, 0.4131, 0.3323],
        [0.4872, 0.3884, 0.3160],
        [0.6075, 0.3054, 0.2607],
        [0.6252, 0.2981, 0.2693],
        [0.6509, 0.2658, 0.2626],
        [0.6591, 0.2967, 0.2690],
        [0.4502, 0.3880, 0.3861],
        [0.5341, 0.3438, 0.2969],
        [0.4548, 0.3926, 0.3806],
        [0.4548, 0.3864, 0.3664],
        [0.5113, 0.3623, 0.3172],
        [0.5214, 0.3621, 0.3180],
        [0.5546, 0.3630, 0.3058]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 39: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17632.0, Mean: 1340.88916015625, Std: 1188.4283447265625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 176170.46875, Mean: 13396.61328125, Std: 11874.26953125
[DEBUG] Top-3 class probabilities:
tensor([[0.5079, 0.3602, 0.3351],
        [0.4707, 0.3803, 0.3476],
        [0.4775, 0.3728, 0.3624],
        [0.4663, 0.3941, 0.3484],
        [0.4666, 0.3904, 0.3364],
        [0.4522, 0.3925, 0.3624],
        [0.4378, 0.3792, 0.3593],
        [0.5656, 0.3049, 0.2711],
        [0.6960, 0.3049, 0.2302],
        [0.5657, 0.3370, 0.2626],
        [0.4671, 0.3661, 0.3604],
        [0.5166, 0.3534, 0.2686],
        [0.4636, 0.3713, 0.3442],
        [0.4751, 0.3760, 0.3302],
        [0.4823, 0.3709, 0.3399],
        [0.4653, 0.3826, 0.3539],
        [0.4945, 0.3718, 0.3359],
        [0.4218, 0.3932, 0.3728],
        [0.4437, 0.3784, 0.3750],
        [0.4489, 0.3985, 0.3621],
        [0.4896, 0.3862, 0.3340],
        [0.4828, 0.3904, 0.3375],
        [0.5038, 0.3733, 0.3166],
        [0.4797, 0.3793, 0.3317],
        [0.4846, 0.3818, 0.3211],
        [0.4903, 0.3774, 0.3296],
        [0.4987, 0.3810, 0.3137],
        [0.4992, 0.3686, 0.3110],
        [0.5073, 0.3729, 0.3165],
        [0.5082, 0.3548, 0.3286],
        [0.7043, 0.2918, 0.2427],
        [0.5428, 0.3403, 0.3065],
        [0.5036, 0.3641, 0.3217],
        [0.4744, 0.3731, 0.3610],
        [0.4962, 0.3724, 0.3443],
        [0.5280, 0.3435, 0.3193],
        [0.4755, 0.3722, 0.3367],
        [0.4973, 0.3590, 0.3243],
        [0.4907, 0.3769, 0.3342],
        [0.4751, 0.3734, 0.3511],
        [0.4747, 0.3823, 0.3482],
        [0.4675, 0.3811, 0.3553],
        [0.5814, 0.3175, 0.2670],
        [0.4968, 0.3634, 0.3384],
        [0.4730, 0.3610, 0.3547],
        [0.4776, 0.3748, 0.3532],
        [0.6365, 0.2832, 0.2677],
        [0.6000, 0.3054, 0.2796],
        [0.4298, 0.3981, 0.3797],
        [0.4695, 0.3719, 0.3557],
        [0.4436, 0.3773, 0.3766],
        [0.4316, 0.3823, 0.3612],
        [0.4843, 0.3735, 0.3453],
        [0.4981, 0.3651, 0.3277],
        [0.5106, 0.3665, 0.3191],
        [0.4574, 0.3799, 0.3684],
        [0.4780, 0.3821, 0.3335],
        [0.4860, 0.3773, 0.3431],
        [0.4952, 0.3684, 0.3294],
        [0.4917, 0.3865, 0.3292],
        [0.4404, 0.4025, 0.3475],
        [0.4824, 0.3878, 0.3332],
        [0.5032, 0.3726, 0.3337],
        [0.4542, 0.4044, 0.3511],
        [0.4655, 0.4075, 0.3244],
        [0.4451, 0.4197, 0.3423],
        [0.5006, 0.3829, 0.3034],
        [0.5121, 0.3791, 0.3030],
        [0.4732, 0.4038, 0.3213],
        [0.4671, 0.3974, 0.3435],
        [0.4855, 0.3795, 0.3450],
        [0.4486, 0.4023, 0.3462],
        [0.5978, 0.3206, 0.2574],
        [0.4441, 0.4062, 0.3477],
        [0.5123, 0.3618, 0.3331],
        [0.4330, 0.4132, 0.3677],
        [0.5392, 0.3369, 0.2937],
        [0.5045, 0.3643, 0.3266],
        [0.4486, 0.3796, 0.3557],
        [0.4430, 0.3873, 0.3435],
        [0.4555, 0.3895, 0.3498],
        [0.4696, 0.3782, 0.3526],
        [0.5795, 0.3122, 0.2556],
        [0.5044, 0.3471, 0.3242],
        [0.5727, 0.3283, 0.2711],
        [0.5993, 0.3145, 0.2709],
        [0.5435, 0.3328, 0.2851],
        [0.6835, 0.3044, 0.2478],
        [0.6429, 0.2800, 0.2776],
        [0.4859, 0.3617, 0.3346],
        [0.4328, 0.3794, 0.3633],
        [0.4665, 0.3675, 0.3607],
        [0.4443, 0.3958, 0.3631],
        [0.4807, 0.3590, 0.3279],
        [0.5463, 0.3496, 0.2943],
        [0.4794, 0.3688, 0.3463],
        [0.5371, 0.3557, 0.3062],
        [0.5614, 0.3406, 0.2945],
        [0.4505, 0.3852, 0.3802],
        [0.4912, 0.3636, 0.3522],
        [0.5035, 0.3526, 0.3496],
        [0.5077, 0.3541, 0.3353],
        [0.4715, 0.3775, 0.3664],
        [0.4834, 0.3756, 0.3265],
        [0.5011, 0.3719, 0.3292],
        [0.4479, 0.4064, 0.3475],
        [0.5178, 0.3828, 0.3035],
        [0.4893, 0.3878, 0.3129],
        [0.4669, 0.4040, 0.3174],
        [0.5209, 0.3550, 0.2699],
        [0.4845, 0.3823, 0.3153],
        [0.5189, 0.3601, 0.3220],
        [0.5099, 0.3734, 0.3376],
        [0.5669, 0.3422, 0.2728],
        [0.4660, 0.3987, 0.3376],
        [0.5898, 0.3158, 0.2742],
        [0.4886, 0.3767, 0.3294],
        [0.4882, 0.3757, 0.3372],
        [0.4753, 0.3962, 0.3511],
        [0.4594, 0.4058, 0.3556],
        [0.4942, 0.3731, 0.3124],
        [0.5033, 0.3629, 0.3361],
        [0.4739, 0.3833, 0.3461],
        [0.4691, 0.3722, 0.3428],
        [0.6177, 0.2901, 0.2703],
        [0.4698, 0.3646, 0.3499],
        [0.4722, 0.3966, 0.3620],
        [0.5410, 0.3460, 0.2890]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 40: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9760.0, Mean: 1289.329833984375, Std: 1139.0535888671875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 97516.7890625, Mean: 12881.4541015625, Std: 11380.9384765625
[DEBUG] Top-3 class probabilities:
tensor([[0.4832, 0.3723, 0.3465],
        [0.6480, 0.2960, 0.2676],
        [0.6019, 0.3000, 0.2587],
        [0.4767, 0.3530, 0.3329],
        [0.5058, 0.3762, 0.3159],
        [0.4728, 0.3702, 0.3501],
        [0.4758, 0.3715, 0.3366],
        [0.4565, 0.3856, 0.3629],
        [0.4464, 0.3755, 0.3663],
        [0.4537, 0.3911, 0.3647],
        [0.5082, 0.3585, 0.3213],
        [0.5971, 0.2965, 0.2692],
        [0.5804, 0.3157, 0.2664],
        [0.4483, 0.3867, 0.3680],
        [0.4895, 0.3533, 0.3499],
        [0.5130, 0.3437, 0.3360],
        [0.4276, 0.4048, 0.3888],
        [0.4752, 0.3717, 0.3452],
        [0.4686, 0.3718, 0.3622],
        [0.5049, 0.3583, 0.3386],
        [0.5409, 0.3349, 0.3033],
        [0.5722, 0.3186, 0.2635],
        [0.4816, 0.3791, 0.3091],
        [0.5608, 0.3383, 0.2795],
        [0.5340, 0.3532, 0.3229],
        [0.5475, 0.3333, 0.2736],
        [0.6460, 0.2824, 0.2660],
        [0.4977, 0.3667, 0.3330],
        [0.5196, 0.3728, 0.3052],
        [0.4789, 0.3886, 0.3210],
        [0.4823, 0.3831, 0.3408],
        [0.5373, 0.3560, 0.2982],
        [0.4438, 0.3997, 0.3484],
        [0.5352, 0.3463, 0.3077],
        [0.6918, 0.2942, 0.2519],
        [0.4748, 0.3787, 0.3601],
        [0.4872, 0.3770, 0.3572],
        [0.4586, 0.3929, 0.3627],
        [0.4760, 0.3836, 0.3329],
        [0.5078, 0.3663, 0.3269],
        [0.4928, 0.3656, 0.3438],
        [0.4947, 0.3539, 0.3297],
        [0.4935, 0.3589, 0.3503],
        [0.4852, 0.3668, 0.3478],
        [0.4921, 0.3630, 0.3406],
        [0.4903, 0.3600, 0.3592],
        [0.4620, 0.3694, 0.3679],
        [0.4755, 0.3632, 0.3572],
        [0.4866, 0.3794, 0.3541],
        [0.5045, 0.3638, 0.3374],
        [0.5134, 0.3456, 0.2973],
        [0.4266, 0.4048, 0.3724],
        [0.4326, 0.3765, 0.3739],
        [0.4399, 0.3767, 0.3730],
        [0.4592, 0.3733, 0.3702],
        [0.4691, 0.3661, 0.3531],
        [0.4553, 0.3791, 0.3751],
        [0.4813, 0.3697, 0.3666],
        [0.4605, 0.3775, 0.3735],
        [0.5127, 0.3609, 0.3321],
        [0.4780, 0.3718, 0.3426],
        [0.4193, 0.3896, 0.3111],
        [0.4749, 0.3771, 0.3390],
        [0.5055, 0.3947, 0.3111],
        [0.5168, 0.3721, 0.2825],
        [0.5317, 0.3465, 0.2821],
        [0.4874, 0.3712, 0.3388],
        [0.4582, 0.3912, 0.3715],
        [0.5417, 0.3486, 0.3013],
        [0.5041, 0.3677, 0.3448],
        [0.5432, 0.3508, 0.2984],
        [0.6444, 0.2858, 0.2807],
        [0.6357, 0.2820, 0.2785],
        [0.6287, 0.2976, 0.2921],
        [0.5001, 0.3670, 0.3321],
        [0.5722, 0.3196, 0.2568],
        [0.6941, 0.2880, 0.2468],
        [0.4754, 0.3815, 0.3622],
        [0.4743, 0.3859, 0.3581],
        [0.4747, 0.3780, 0.3700],
        [0.5477, 0.3427, 0.2853],
        [0.4755, 0.3835, 0.3454],
        [0.4820, 0.3653, 0.3650],
        [0.4792, 0.3739, 0.3673],
        [0.4998, 0.3824, 0.3344],
        [0.4997, 0.3698, 0.3254],
        [0.4858, 0.3657, 0.3608],
        [0.4447, 0.3789, 0.3687],
        [0.4856, 0.3723, 0.3623],
        [0.5314, 0.3613, 0.3122],
        [0.4748, 0.3848, 0.3400],
        [0.4964, 0.3671, 0.3511],
        [0.4569, 0.3787, 0.3655],
        [0.5224, 0.3532, 0.3110],
        [0.4770, 0.3695, 0.3409],
        [0.4401, 0.3886, 0.3626],
        [0.4560, 0.3738, 0.3653],
        [0.4069, 0.4017, 0.3853],
        [0.4913, 0.3667, 0.3540],
        [0.4728, 0.3814, 0.3651],
        [0.4766, 0.3690, 0.3630],
        [0.5173, 0.3551, 0.3245],
        [0.5120, 0.3652, 0.3295],
        [0.4686, 0.3805, 0.3536],
        [0.4868, 0.3628, 0.3284],
        [0.5389, 0.3535, 0.3098],
        [0.4814, 0.3696, 0.3402],
        [0.5497, 0.3474, 0.2830],
        [0.4548, 0.3834, 0.3535],
        [0.4914, 0.3879, 0.3353],
        [0.4678, 0.3772, 0.3322],
        [0.5779, 0.3108, 0.2535],
        [0.4908, 0.3828, 0.3489],
        [0.4956, 0.3603, 0.3429],
        [0.4533, 0.3809, 0.3700],
        [0.5572, 0.3430, 0.2776],
        [0.6433, 0.3100, 0.2401],
        [0.5668, 0.3546, 0.2771],
        [0.5681, 0.3303, 0.2832],
        [0.5899, 0.3257, 0.2682],
        [0.6076, 0.2932, 0.2715],
        [0.6650, 0.2994, 0.2606],
        [0.4882, 0.3787, 0.3230],
        [0.5286, 0.3541, 0.3139],
        [0.4618, 0.3880, 0.3706],
        [0.5174, 0.3475, 0.3244],
        [0.4855, 0.3719, 0.3568],
        [0.4944, 0.3718, 0.3461]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 41: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7672.0, Mean: 1296.1478271484375, Std: 1158.036376953125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 76654.375, Mean: 12949.5751953125, Std: 11570.6064453125
[DEBUG] Top-3 class probabilities:
tensor([[0.5023, 0.3690, 0.3252],
        [0.4715, 0.3865, 0.3613],
        [0.5336, 0.3569, 0.3091],
        [0.4825, 0.3917, 0.3403],
        [0.4723, 0.3909, 0.3483],
        [0.4938, 0.3743, 0.3331],
        [0.4961, 0.3763, 0.3383],
        [0.5330, 0.3515, 0.3125],
        [0.5159, 0.3594, 0.3169],
        [0.5416, 0.3329, 0.2900],
        [0.5124, 0.3436, 0.3165],
        [0.4881, 0.3650, 0.3383],
        [0.4528, 0.3862, 0.3545],
        [0.4889, 0.3727, 0.3328],
        [0.5517, 0.3355, 0.2959],
        [0.5747, 0.3221, 0.2781],
        [0.5728, 0.3359, 0.2765],
        [0.4841, 0.3691, 0.3482],
        [0.5230, 0.3571, 0.3197],
        [0.4760, 0.3895, 0.3489],
        [0.5214, 0.3751, 0.3265],
        [0.4904, 0.3807, 0.3310],
        [0.5475, 0.3451, 0.2937],
        [0.4934, 0.3888, 0.3441],
        [0.4334, 0.4098, 0.3553],
        [0.4517, 0.3920, 0.3460],
        [0.4396, 0.3990, 0.3417],
        [0.5216, 0.3661, 0.3049],
        [0.4894, 0.3828, 0.3433],
        [0.4894, 0.3777, 0.3270],
        [0.4946, 0.3660, 0.3306],
        [0.6387, 0.2818, 0.2646],
        [0.5691, 0.3401, 0.2913],
        [0.5061, 0.3680, 0.3357],
        [0.5009, 0.3588, 0.3463],
        [0.6037, 0.3049, 0.2636],
        [0.5291, 0.3523, 0.3081],
        [0.4982, 0.3615, 0.3330],
        [0.4935, 0.3772, 0.3146],
        [0.4879, 0.3691, 0.3385],
        [0.4981, 0.3604, 0.3284],
        [0.5379, 0.3511, 0.3118],
        [0.5168, 0.3613, 0.3428],
        [0.4906, 0.3708, 0.3567],
        [0.4599, 0.3916, 0.3804],
        [0.4905, 0.3696, 0.3482],
        [0.5110, 0.3573, 0.3273],
        [0.5894, 0.3220, 0.2728],
        [0.5625, 0.3450, 0.2785],
        [0.4531, 0.3881, 0.3748],
        [0.4599, 0.3856, 0.3768],
        [0.4643, 0.3800, 0.3594],
        [0.4845, 0.3859, 0.3280],
        [0.4254, 0.4095, 0.3532],
        [0.5266, 0.3674, 0.3249],
        [0.5137, 0.3468, 0.3138],
        [0.4869, 0.3691, 0.3501],
        [0.4792, 0.3694, 0.3643],
        [0.5087, 0.3644, 0.3274],
        [0.5521, 0.3368, 0.2963],
        [0.5225, 0.3530, 0.3122],
        [0.5752, 0.3184, 0.2713],
        [0.5800, 0.3210, 0.2880],
        [0.4771, 0.3654, 0.3394],
        [0.4775, 0.3792, 0.3615],
        [0.4353, 0.3953, 0.3726],
        [0.4608, 0.3778, 0.3582],
        [0.5029, 0.3805, 0.3217],
        [0.4949, 0.3791, 0.3223],
        [0.5295, 0.3403, 0.3009],
        [0.5385, 0.3532, 0.3161],
        [0.5855, 0.3267, 0.2716],
        [0.6284, 0.2941, 0.2863],
        [0.5277, 0.3521, 0.3333],
        [0.4753, 0.3828, 0.3627],
        [0.5071, 0.3771, 0.3284],
        [0.5114, 0.3689, 0.3199],
        [0.4890, 0.3691, 0.3322],
        [0.4451, 0.4158, 0.3525],
        [0.4456, 0.3996, 0.3730],
        [0.4970, 0.3724, 0.3365],
        [0.5045, 0.3581, 0.3272],
        [0.5284, 0.3578, 0.3331],
        [0.5429, 0.3463, 0.3043],
        [0.4571, 0.3889, 0.3686],
        [0.4710, 0.3722, 0.3678],
        [0.4810, 0.3727, 0.3669],
        [0.5195, 0.3602, 0.3286],
        [0.4805, 0.3692, 0.3589],
        [0.5397, 0.3354, 0.3002],
        [0.4780, 0.3722, 0.3501],
        [0.4464, 0.3935, 0.3703],
        [0.4819, 0.3689, 0.3648],
        [0.4658, 0.3717, 0.3620],
        [0.4807, 0.3719, 0.3544],
        [0.4838, 0.3910, 0.3162],
        [0.4822, 0.3738, 0.3329],
        [0.5524, 0.3355, 0.3068],
        [0.5733, 0.3295, 0.2936],
        [0.5000, 0.3546, 0.3430],
        [0.4863, 0.3628, 0.3589],
        [0.4801, 0.3861, 0.3500],
        [0.5121, 0.3629, 0.3390],
        [0.5393, 0.3522, 0.3037],
        [0.5247, 0.3588, 0.3028],
        [0.5594, 0.3295, 0.2683],
        [0.4157, 0.3935, 0.3759],
        [0.4589, 0.3797, 0.3626],
        [0.5654, 0.3385, 0.2863],
        [0.4657, 0.3729, 0.3610],
        [0.5518, 0.3323, 0.2959],
        [0.6340, 0.2862, 0.2816],
        [0.6175, 0.2799, 0.2788],
        [0.6400, 0.2975, 0.2642],
        [0.6758, 0.3063, 0.2439],
        [0.6787, 0.2875, 0.2566],
        [0.6207, 0.3035, 0.2639],
        [0.4894, 0.3765, 0.3562],
        [0.5323, 0.3415, 0.3015],
        [0.4724, 0.3709, 0.3528],
        [0.4762, 0.3740, 0.3509],
        [0.4558, 0.3953, 0.3512],
        [0.4774, 0.3885, 0.3461],
        [0.4701, 0.3842, 0.3668],
        [0.4714, 0.3752, 0.3725],
        [0.5393, 0.3388, 0.2998],
        [0.4824, 0.3611, 0.3555],
        [0.4968, 0.3613, 0.3366]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 42: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7540.0, Mean: 1219.0389404296875, Std: 1103.457763671875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 75335.4921875, Mean: 12179.138671875, Std: 11025.2802734375
[DEBUG] Top-3 class probabilities:
tensor([[0.5117, 0.3623, 0.3401],
        [0.5832, 0.3202, 0.2662],
        [0.4625, 0.3789, 0.3737],
        [0.4608, 0.3709, 0.3691],
        [0.5777, 0.3042, 0.2643],
        [0.5306, 0.3406, 0.3204],
        [0.5168, 0.3574, 0.3291],
        [0.5938, 0.3153, 0.2566],
        [0.5225, 0.3413, 0.3186],
        [0.5126, 0.3590, 0.3224],
        [0.4903, 0.3683, 0.3093],
        [0.5301, 0.3422, 0.2936],
        [0.5015, 0.3576, 0.3135],
        [0.4805, 0.3649, 0.3216],
        [0.4643, 0.3806, 0.3435],
        [0.4704, 0.3711, 0.3574],
        [0.5262, 0.3542, 0.3055],
        [0.4139, 0.4138, 0.3709],
        [0.5236, 0.3496, 0.2781],
        [0.5332, 0.3395, 0.3045],
        [0.6202, 0.2897, 0.2623],
        [0.6290, 0.2958, 0.2550],
        [0.5244, 0.3532, 0.3164],
        [0.5088, 0.3650, 0.3217],
        [0.4775, 0.3736, 0.3430],
        [0.5011, 0.3666, 0.3390],
        [0.4583, 0.3854, 0.3740],
        [0.4083, 0.4055, 0.3699],
        [0.5915, 0.3147, 0.2539],
        [0.6806, 0.2839, 0.2554],
        [0.5795, 0.3170, 0.2674],
        [0.5499, 0.3459, 0.3139],
        [0.5134, 0.3576, 0.3252],
        [0.4737, 0.3688, 0.3416],
        [0.5140, 0.3517, 0.3236],
        [0.4989, 0.3653, 0.3405],
        [0.4477, 0.3872, 0.3749],
        [0.4868, 0.3603, 0.3460],
        [0.4463, 0.3783, 0.3764],
        [0.4874, 0.3582, 0.3392],
        [0.5882, 0.3209, 0.2773],
        [0.5001, 0.3647, 0.3576],
        [0.4960, 0.3448, 0.3446],
        [0.4577, 0.3729, 0.3678],
        [0.5214, 0.3598, 0.3178],
        [0.4566, 0.3710, 0.3535],
        [0.5756, 0.3183, 0.2796],
        [0.6448, 0.2790, 0.2739],
        [0.6040, 0.3066, 0.2600],
        [0.5439, 0.3253, 0.2965],
        [0.6562, 0.2832, 0.2610],
        [0.5311, 0.3305, 0.2930],
        [0.4471, 0.3812, 0.3753],
        [0.5018, 0.3619, 0.3227],
        [0.4653, 0.3850, 0.3520],
        [0.5095, 0.3451, 0.3116],
        [0.4668, 0.3616, 0.3407],
        [0.4932, 0.3596, 0.3252],
        [0.6017, 0.3046, 0.2464],
        [0.5211, 0.3553, 0.3135],
        [0.5613, 0.3433, 0.2901],
        [0.5349, 0.3635, 0.3112],
        [0.6229, 0.3125, 0.2441],
        [0.5851, 0.2982, 0.2660],
        [0.5068, 0.3649, 0.3220],
        [0.5355, 0.3415, 0.3088],
        [0.4781, 0.3843, 0.3486],
        [0.4806, 0.3661, 0.3541],
        [0.6977, 0.3067, 0.2334],
        [0.5887, 0.3040, 0.2568],
        [0.4998, 0.3633, 0.3400],
        [0.5243, 0.3477, 0.3239],
        [0.4908, 0.3789, 0.3387],
        [0.4846, 0.3776, 0.3388],
        [0.4902, 0.3815, 0.3412],
        [0.5062, 0.3690, 0.3234],
        [0.4984, 0.3835, 0.3107],
        [0.4576, 0.3838, 0.3519],
        [0.4596, 0.3781, 0.3764],
        [0.4608, 0.3690, 0.3619],
        [0.4755, 0.3536, 0.3435],
        [0.5315, 0.3543, 0.3235],
        [0.5096, 0.3591, 0.3315],
        [0.4849, 0.3733, 0.3587],
        [0.5011, 0.3633, 0.3263],
        [0.5097, 0.3688, 0.3243],
        [0.5574, 0.3438, 0.2874],
        [0.6434, 0.2801, 0.2786],
        [0.5298, 0.3410, 0.3133],
        [0.4731, 0.3629, 0.3403],
        [0.4954, 0.3590, 0.3435],
        [0.4736, 0.3753, 0.3447],
        [0.5543, 0.3303, 0.2860],
        [0.4931, 0.3451, 0.3235],
        [0.4838, 0.3565, 0.3259],
        [0.4835, 0.3708, 0.3317],
        [0.5017, 0.3599, 0.3082],
        [0.5806, 0.3190, 0.2540],
        [0.5351, 0.3337, 0.2889],
        [0.4324, 0.3744, 0.3577],
        [0.5457, 0.3477, 0.3046],
        [0.4517, 0.3658, 0.3452],
        [0.5348, 0.3472, 0.3128],
        [0.6209, 0.2905, 0.2827],
        [0.5781, 0.3175, 0.2664],
        [0.5883, 0.3235, 0.2736],
        [0.4298, 0.3914, 0.3817],
        [0.5234, 0.3497, 0.3022],
        [0.5301, 0.3383, 0.2847],
        [0.4655, 0.3777, 0.3602],
        [0.4625, 0.3831, 0.3616],
        [0.5256, 0.3530, 0.3094],
        [0.4756, 0.3637, 0.3534],
        [0.5011, 0.3704, 0.3363],
        [0.4816, 0.3739, 0.3545],
        [0.4617, 0.3688, 0.3604],
        [0.5216, 0.3620, 0.3227],
        [0.4358, 0.4114, 0.3563],
        [0.4729, 0.3806, 0.3511],
        [0.4551, 0.3841, 0.3654],
        [0.4797, 0.3657, 0.3500],
        [0.4780, 0.3662, 0.3662],
        [0.6323, 0.2850, 0.2598],
        [0.5218, 0.3520, 0.3201],
        [0.5207, 0.3567, 0.3264],
        [0.5408, 0.3513, 0.3087],
        [0.5559, 0.3357, 0.2846],
        [0.5320, 0.3440, 0.2855]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 43: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7128.0, Mean: 1208.205078125, Std: 1101.69580078125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 71218.9609375, Mean: 12070.890625, Std: 11007.67578125
[DEBUG] Top-3 class probabilities:
tensor([[0.5019, 0.3697, 0.3342],
        [0.5095, 0.3475, 0.3183],
        [0.4708, 0.3781, 0.3530],
        [0.5365, 0.3572, 0.3012],
        [0.4857, 0.3816, 0.3448],
        [0.4521, 0.3724, 0.3682],
        [0.4951, 0.3539, 0.3438],
        [0.4902, 0.3826, 0.3147],
        [0.5584, 0.3550, 0.2875],
        [0.4979, 0.3591, 0.3385],
        [0.4461, 0.4265, 0.3375],
        [0.4567, 0.3795, 0.3408],
        [0.4255, 0.4040, 0.3548],
        [0.5378, 0.3376, 0.2381],
        [0.5856, 0.3342, 0.2358],
        [0.6720, 0.3013, 0.2451],
        [0.6575, 0.2875, 0.2721],
        [0.6829, 0.3095, 0.2379],
        [0.5843, 0.3097, 0.2686],
        [0.4578, 0.3664, 0.3601],
        [0.4227, 0.4025, 0.3753],
        [0.4567, 0.3751, 0.3733],
        [0.4429, 0.3885, 0.3614],
        [0.5442, 0.3252, 0.2913],
        [0.5120, 0.3471, 0.3413],
        [0.5047, 0.3545, 0.3228],
        [0.5603, 0.3269, 0.2767],
        [0.4487, 0.3781, 0.3753],
        [0.4618, 0.3824, 0.3679],
        [0.4844, 0.3638, 0.3328],
        [0.4702, 0.3900, 0.3462],
        [0.4437, 0.3851, 0.3727],
        [0.4553, 0.3867, 0.3697],
        [0.4639, 0.3669, 0.3631],
        [0.5964, 0.3037, 0.2660],
        [0.6707, 0.2774, 0.2687],
        [0.4445, 0.3778, 0.3744],
        [0.5843, 0.3044, 0.2611],
        [0.5366, 0.3555, 0.3107],
        [0.5407, 0.3389, 0.2988],
        [0.5240, 0.3561, 0.3098],
        [0.5558, 0.3290, 0.2845],
        [0.6529, 0.3044, 0.2690],
        [0.5207, 0.3594, 0.3172],
        [0.5111, 0.3495, 0.3262],
        [0.5040, 0.3605, 0.3067],
        [0.4723, 0.3757, 0.3648],
        [0.4411, 0.3846, 0.3742],
        [0.5553, 0.3401, 0.2909],
        [0.4715, 0.3827, 0.3246],
        [0.4963, 0.3552, 0.3279],
        [0.4864, 0.3829, 0.3327],
        [0.4460, 0.3900, 0.3520],
        [0.4369, 0.4021, 0.3333],
        [0.5238, 0.3543, 0.2696],
        [0.5281, 0.3295, 0.2938],
        [0.6473, 0.2937, 0.2687],
        [0.6889, 0.2853, 0.2412],
        [0.6902, 0.3017, 0.2350],
        [0.4578, 0.3859, 0.3485],
        [0.4458, 0.3815, 0.3523],
        [0.4424, 0.3807, 0.3658],
        [0.4704, 0.3668, 0.3529],
        [0.4432, 0.3848, 0.3711],
        [0.4649, 0.3767, 0.3765],
        [0.4545, 0.3825, 0.3738],
        [0.4504, 0.3763, 0.3725],
        [0.5095, 0.3607, 0.3227],
        [0.5143, 0.3599, 0.3191],
        [0.4448, 0.3826, 0.3674],
        [0.5059, 0.3665, 0.3284],
        [0.5539, 0.3460, 0.2815],
        [0.5123, 0.3520, 0.3293],
        [0.4684, 0.3709, 0.3412],
        [0.5162, 0.3464, 0.3192],
        [0.6458, 0.2827, 0.2826],
        [0.4054, 0.4028, 0.3718],
        [0.4984, 0.3531, 0.3253],
        [0.4724, 0.3740, 0.3570],
        [0.4578, 0.3822, 0.3802],
        [0.6467, 0.2850, 0.2715],
        [0.4881, 0.3882, 0.3374],
        [0.5633, 0.3253, 0.2628],
        [0.5472, 0.3306, 0.3048],
        [0.4801, 0.3575, 0.3418],
        [0.4491, 0.3829, 0.3762],
        [0.4972, 0.3592, 0.3369],
        [0.5026, 0.3543, 0.3329],
        [0.5338, 0.3395, 0.3054],
        [0.5122, 0.3673, 0.3258],
        [0.5556, 0.3495, 0.2737],
        [0.5128, 0.3460, 0.3090],
        [0.4384, 0.3891, 0.3756],
        [0.4897, 0.3659, 0.3283],
        [0.5771, 0.3286, 0.2300],
        [0.4772, 0.3765, 0.3181],
        [0.4273, 0.3865, 0.3809],
        [0.5707, 0.3064, 0.2714],
        [0.6902, 0.3069, 0.2353],
        [0.6176, 0.2905, 0.2879],
        [0.4534, 0.4021, 0.3294],
        [0.4454, 0.3829, 0.3627],
        [0.4378, 0.3931, 0.3670],
        [0.4783, 0.3830, 0.3481],
        [0.4930, 0.3722, 0.3607],
        [0.4723, 0.3738, 0.3524],
        [0.4349, 0.3888, 0.3850],
        [0.5236, 0.3505, 0.3234],
        [0.6676, 0.3032, 0.2550],
        [0.6056, 0.3080, 0.2648],
        [0.4515, 0.3869, 0.3805],
        [0.5000, 0.3651, 0.3457],
        [0.5084, 0.3638, 0.3237],
        [0.5670, 0.3403, 0.2837],
        [0.4157, 0.3969, 0.3917],
        [0.4371, 0.3868, 0.3701],
        [0.5488, 0.3380, 0.2987],
        [0.4751, 0.3598, 0.3500],
        [0.5345, 0.3411, 0.3032],
        [0.4996, 0.3644, 0.3405],
        [0.4537, 0.3907, 0.3710],
        [0.5182, 0.3443, 0.3147],
        [0.4951, 0.3774, 0.3436],
        [0.5024, 0.3643, 0.3409],
        [0.4406, 0.3952, 0.3652],
        [0.6259, 0.2844, 0.2801],
        [0.6239, 0.3020, 0.2474],
        [0.5035, 0.3657, 0.3305]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 44: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7236.0, Mean: 1295.9044189453125, Std: 1102.4324951171875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 72298.0546875, Mean: 12947.1455078125, Std: 11015.0361328125
[DEBUG] Top-3 class probabilities:
tensor([[0.5211, 0.3544, 0.3067],
        [0.5429, 0.3389, 0.3111],
        [0.5163, 0.3426, 0.3239],
        [0.4708, 0.3715, 0.3596],
        [0.4522, 0.3740, 0.3674],
        [0.4692, 0.3881, 0.3488],
        [0.5208, 0.3624, 0.3317],
        [0.4653, 0.4141, 0.3262],
        [0.5394, 0.3568, 0.2845],
        [0.5015, 0.3746, 0.3222],
        [0.4393, 0.3842, 0.3776],
        [0.4466, 0.4010, 0.3446],
        [0.4733, 0.3718, 0.3434],
        [0.4752, 0.3638, 0.3410],
        [0.4703, 0.3669, 0.3565],
        [0.4785, 0.3544, 0.3387],
        [0.5164, 0.3351, 0.2944],
        [0.4117, 0.3940, 0.3670],
        [0.4347, 0.3824, 0.3783],
        [0.4918, 0.3646, 0.3305],
        [0.4938, 0.3662, 0.3266],
        [0.5163, 0.3602, 0.3172],
        [0.4941, 0.3586, 0.3345],
        [0.4733, 0.3634, 0.3590],
        [0.4846, 0.3745, 0.3488],
        [0.5831, 0.3148, 0.2797],
        [0.6511, 0.2820, 0.2786],
        [0.4692, 0.3705, 0.3581],
        [0.4900, 0.3602, 0.3475],
        [0.4752, 0.3589, 0.3586],
        [0.5229, 0.3694, 0.3195],
        [0.5065, 0.3654, 0.3310],
        [0.4437, 0.3812, 0.3638],
        [0.5322, 0.3338, 0.3024],
        [0.4771, 0.3695, 0.3433],
        [0.4582, 0.3809, 0.3523],
        [0.5112, 0.3604, 0.3370],
        [0.4948, 0.3591, 0.3398],
        [0.4788, 0.3621, 0.3580],
        [0.4691, 0.3704, 0.3557],
        [0.4614, 0.3614, 0.3535],
        [0.4342, 0.3820, 0.3778],
        [0.5374, 0.3332, 0.2877],
        [0.6008, 0.3069, 0.2814],
        [0.5835, 0.3313, 0.2670],
        [0.4956, 0.3732, 0.3367],
        [0.5053, 0.3649, 0.3312],
        [0.4751, 0.3707, 0.3377],
        [0.5285, 0.3448, 0.3061],
        [0.4662, 0.3986, 0.3439],
        [0.5338, 0.3450, 0.2926],
        [0.4727, 0.3767, 0.3555],
        [0.5221, 0.3801, 0.3185],
        [0.4590, 0.3858, 0.3589],
        [0.5023, 0.3723, 0.3311],
        [0.4407, 0.4035, 0.3647],
        [0.4929, 0.3676, 0.3294],
        [0.4710, 0.3906, 0.3615],
        [0.4700, 0.3754, 0.3464],
        [0.4455, 0.3787, 0.3738],
        [0.4769, 0.3647, 0.3454],
        [0.5052, 0.3566, 0.3209],
        [0.6350, 0.2904, 0.2806],
        [0.5238, 0.3464, 0.2839],
        [0.4612, 0.3884, 0.3750],
        [0.4812, 0.3749, 0.3450],
        [0.4640, 0.3785, 0.3592],
        [0.4524, 0.3778, 0.3642],
        [0.4824, 0.3678, 0.3447],
        [0.5364, 0.3404, 0.3009],
        [0.4866, 0.3578, 0.3533],
        [0.5972, 0.3016, 0.2635],
        [0.5666, 0.3313, 0.2788],
        [0.4540, 0.3755, 0.3628],
        [0.4778, 0.3719, 0.3587],
        [0.4591, 0.3726, 0.3647],
        [0.4930, 0.3645, 0.3314],
        [0.4611, 0.3724, 0.3428],
        [0.5509, 0.3352, 0.2953],
        [0.4769, 0.3676, 0.3477],
        [0.5041, 0.3656, 0.3281],
        [0.5482, 0.3389, 0.3012],
        [0.5592, 0.3396, 0.2947],
        [0.4860, 0.3665, 0.3482],
        [0.5014, 0.3619, 0.3286],
        [0.4683, 0.3723, 0.3561],
        [0.4720, 0.3732, 0.3528],
        [0.4427, 0.3782, 0.3669],
        [0.4418, 0.3859, 0.3678],
        [0.5414, 0.3319, 0.2934],
        [0.4526, 0.3917, 0.3671],
        [0.4963, 0.3668, 0.3189],
        [0.4974, 0.3610, 0.3312],
        [0.4747, 0.3725, 0.3600],
        [0.4978, 0.3674, 0.3313],
        [0.5326, 0.3568, 0.3223],
        [0.5120, 0.3559, 0.3410],
        [0.5656, 0.3313, 0.2765],
        [0.4871, 0.3608, 0.3198],
        [0.4456, 0.3868, 0.3813],
        [0.4750, 0.3800, 0.3519],
        [0.4520, 0.3811, 0.3624],
        [0.4476, 0.3829, 0.3636],
        [0.5008, 0.3455, 0.3283],
        [0.5512, 0.3485, 0.2774],
        [0.4894, 0.3637, 0.3281],
        [0.4868, 0.3527, 0.2836],
        [0.4473, 0.3868, 0.3649],
        [0.4890, 0.3539, 0.3388],
        [0.4855, 0.3659, 0.3373],
        [0.4855, 0.3698, 0.3345],
        [0.4510, 0.3729, 0.3696],
        [0.4918, 0.3520, 0.3316],
        [0.5961, 0.3125, 0.2586],
        [0.4891, 0.3650, 0.3509],
        [0.5019, 0.3524, 0.3356],
        [0.4791, 0.3749, 0.3359],
        [0.4886, 0.3703, 0.3286],
        [0.4501, 0.3673, 0.3661],
        [0.4689, 0.3762, 0.3483],
        [0.4788, 0.3741, 0.3299],
        [0.5529, 0.3273, 0.2837],
        [0.5175, 0.3680, 0.3328],
        [0.4802, 0.3859, 0.3588],
        [0.6337, 0.2983, 0.2958],
        [0.5031, 0.3548, 0.3102],
        [0.5350, 0.3545, 0.3296],
        [0.4475, 0.3860, 0.3779]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 45: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7572.0, Mean: 1258.8018798828125, Std: 1064.4139404296875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 75655.21875, Mean: 12576.4306640625, Std: 10635.1708984375
[DEBUG] Top-3 class probabilities:
tensor([[0.5012, 0.3550, 0.3386],
        [0.4814, 0.3646, 0.3032],
        [0.4572, 0.3713, 0.3694],
        [0.5591, 0.3475, 0.2915],
        [0.4779, 0.3593, 0.3536],
        [0.5136, 0.3680, 0.3154],
        [0.5266, 0.3519, 0.3311],
        [0.5082, 0.3605, 0.3451],
        [0.5380, 0.3489, 0.3118],
        [0.4986, 0.3837, 0.3218],
        [0.6146, 0.3140, 0.2516],
        [0.4355, 0.3955, 0.3761],
        [0.4591, 0.3860, 0.3633],
        [0.4545, 0.3785, 0.3673],
        [0.5432, 0.3349, 0.2968],
        [0.5353, 0.3517, 0.3051],
        [0.4587, 0.3816, 0.3578],
        [0.5822, 0.3123, 0.2552],
        [0.5827, 0.3141, 0.2580],
        [0.4923, 0.3616, 0.3432],
        [0.5347, 0.3532, 0.2982],
        [0.5347, 0.3372, 0.2918],
        [0.5566, 0.3256, 0.2861],
        [0.4912, 0.3659, 0.3269],
        [0.5066, 0.3581, 0.3325],
        [0.5412, 0.3289, 0.2910],
        [0.4908, 0.3786, 0.3540],
        [0.4981, 0.3553, 0.3435],
        [0.5541, 0.3324, 0.2786],
        [0.4675, 0.3745, 0.3595],
        [0.5188, 0.3538, 0.3224],
        [0.5083, 0.3596, 0.3400],
        [0.5344, 0.3434, 0.3023],
        [0.5312, 0.3573, 0.3207],
        [0.5568, 0.3227, 0.2858],
        [0.6059, 0.2900, 0.2711],
        [0.5168, 0.3467, 0.3182],
        [0.5175, 0.3616, 0.3148],
        [0.4957, 0.3599, 0.3254],
        [0.4836, 0.3825, 0.3194],
        [0.4887, 0.3704, 0.2845],
        [0.4616, 0.3883, 0.3413],
        [0.4132, 0.4044, 0.3775],
        [0.4352, 0.3819, 0.3807],
        [0.4219, 0.3952, 0.3862],
        [0.4515, 0.3874, 0.3767],
        [0.5075, 0.3672, 0.3295],
        [0.4999, 0.3780, 0.3444],
        [0.5140, 0.3651, 0.3182],
        [0.4971, 0.3642, 0.3317],
        [0.5412, 0.3492, 0.2888],
        [0.5387, 0.3494, 0.3125],
        [0.5235, 0.3401, 0.3137],
        [0.4308, 0.4040, 0.3743],
        [0.4512, 0.3718, 0.3588],
        [0.5040, 0.3621, 0.3401],
        [0.6935, 0.2968, 0.2450],
        [0.5687, 0.3266, 0.2870],
        [0.4518, 0.3797, 0.3713],
        [0.4564, 0.3758, 0.3682],
        [0.5725, 0.3178, 0.2630],
        [0.5575, 0.3391, 0.2868],
        [0.4899, 0.3587, 0.3383],
        [0.4816, 0.3689, 0.3480],
        [0.4771, 0.3703, 0.3412],
        [0.4735, 0.3593, 0.3442],
        [0.5230, 0.3627, 0.3274],
        [0.5138, 0.3593, 0.3144],
        [0.4920, 0.3550, 0.3425],
        [0.5650, 0.3440, 0.3026],
        [0.5096, 0.3545, 0.3327],
        [0.5123, 0.3742, 0.3246],
        [0.5106, 0.3612, 0.3284],
        [0.4889, 0.3697, 0.3373],
        [0.5030, 0.3645, 0.3337],
        [0.5351, 0.3622, 0.3187],
        [0.4633, 0.3748, 0.3583],
        [0.4877, 0.3642, 0.3406],
        [0.4679, 0.3607, 0.3392],
        [0.4568, 0.3754, 0.3586],
        [0.4795, 0.3762, 0.3095],
        [0.4828, 0.3842, 0.3193],
        [0.4641, 0.3906, 0.3590],
        [0.4301, 0.3962, 0.3645],
        [0.4940, 0.3686, 0.3174],
        [0.5007, 0.3774, 0.3263],
        [0.5480, 0.3496, 0.2961],
        [0.4618, 0.3827, 0.3629],
        [0.5362, 0.3544, 0.3159],
        [0.5373, 0.3471, 0.2986],
        [0.5690, 0.3258, 0.2782],
        [0.5769, 0.3302, 0.2726],
        [0.5693, 0.3363, 0.2753],
        [0.5450, 0.3442, 0.2971],
        [0.5262, 0.3537, 0.3247],
        [0.6220, 0.2951, 0.2502],
        [0.4673, 0.3713, 0.3437],
        [0.4458, 0.3714, 0.3563],
        [0.5066, 0.3454, 0.3304],
        [0.5262, 0.3422, 0.3140],
        [0.6769, 0.2893, 0.2454],
        [0.4860, 0.3546, 0.3270],
        [0.5284, 0.3541, 0.3172],
        [0.4921, 0.3691, 0.3443],
        [0.4949, 0.3837, 0.3278],
        [0.4661, 0.3724, 0.3672],
        [0.4617, 0.3778, 0.3582],
        [0.4399, 0.3858, 0.3695],
        [0.4628, 0.3807, 0.3613],
        [0.4380, 0.3844, 0.3839],
        [0.5050, 0.3572, 0.3387],
        [0.5054, 0.3684, 0.3362],
        [0.5040, 0.3580, 0.3262],
        [0.5398, 0.3342, 0.2992],
        [0.5612, 0.3179, 0.2814],
        [0.4671, 0.3656, 0.3636],
        [0.4708, 0.3715, 0.3435],
        [0.4865, 0.3603, 0.3463],
        [0.5241, 0.3565, 0.3180],
        [0.4919, 0.3754, 0.3423],
        [0.5366, 0.3456, 0.2995],
        [0.4838, 0.3738, 0.3463],
        [0.4902, 0.3703, 0.3347],
        [0.4636, 0.3786, 0.3567],
        [0.4666, 0.3900, 0.3471],
        [0.4910, 0.3678, 0.3260],
        [0.5011, 0.3643, 0.3339],
        [0.5014, 0.3780, 0.3164]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 46: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7797.0, Mean: 1333.9246826171875, Std: 1206.2342529296875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 77903.328125, Mean: 13327.02734375, Std: 12052.1796875
[DEBUG] Top-3 class probabilities:
tensor([[0.5232, 0.3582, 0.3110],
        [0.5894, 0.3194, 0.2679],
        [0.5720, 0.3311, 0.2700],
        [0.5281, 0.3528, 0.3102],
        [0.4962, 0.3684, 0.3357],
        [0.6352, 0.2823, 0.2807],
        [0.6683, 0.2891, 0.2579],
        [0.4606, 0.3721, 0.3614],
        [0.4878, 0.3667, 0.3562],
        [0.4894, 0.3712, 0.3626],
        [0.4820, 0.3767, 0.3448],
        [0.4847, 0.3766, 0.3454],
        [0.4820, 0.3679, 0.3404],
        [0.4815, 0.3732, 0.3420],
        [0.5552, 0.3394, 0.2888],
        [0.5016, 0.3514, 0.3400],
        [0.5186, 0.3572, 0.3394],
        [0.4347, 0.3839, 0.3795],
        [0.4852, 0.3782, 0.3497],
        [0.4575, 0.3660, 0.3626],
        [0.5239, 0.3610, 0.3213],
        [0.4888, 0.3710, 0.3442],
        [0.5088, 0.3694, 0.3110],
        [0.4854, 0.3717, 0.3377],
        [0.4744, 0.3695, 0.3628],
        [0.5013, 0.3598, 0.3280],
        [0.4595, 0.3705, 0.3464],
        [0.5165, 0.3703, 0.3225],
        [0.5434, 0.3484, 0.2821],
        [0.4828, 0.3806, 0.3160],
        [0.4277, 0.3974, 0.3637],
        [0.4665, 0.3759, 0.3451],
        [0.4366, 0.3889, 0.3527],
        [0.4429, 0.4038, 0.3517],
        [0.4344, 0.4055, 0.3772],
        [0.4436, 0.3824, 0.3788],
        [0.5205, 0.3602, 0.3191],
        [0.5724, 0.3279, 0.2748],
        [0.4846, 0.3661, 0.3213],
        [0.5513, 0.3484, 0.2857],
        [0.4507, 0.3834, 0.3563],
        [0.5859, 0.3332, 0.2561],
        [0.4712, 0.3735, 0.3398],
        [0.5566, 0.3315, 0.2739],
        [0.5747, 0.3140, 0.2725],
        [0.6742, 0.3076, 0.2560],
        [0.5203, 0.3410, 0.3138],
        [0.4882, 0.3479, 0.3477],
        [0.4553, 0.3645, 0.3640],
        [0.4852, 0.3771, 0.3451],
        [0.4510, 0.3753, 0.3734],
        [0.4275, 0.3868, 0.3764],
        [0.4831, 0.3697, 0.3559],
        [0.4928, 0.3666, 0.3348],
        [0.4934, 0.3672, 0.3395],
        [0.4892, 0.3785, 0.3386],
        [0.5004, 0.3565, 0.3225],
        [0.4831, 0.3687, 0.3478],
        [0.4578, 0.3791, 0.3702],
        [0.5665, 0.3173, 0.2731],
        [0.5429, 0.3388, 0.3062],
        [0.4674, 0.3806, 0.3638],
        [0.4977, 0.3596, 0.3259],
        [0.5275, 0.3408, 0.3074],
        [0.5646, 0.3312, 0.2945],
        [0.4931, 0.3712, 0.3458],
        [0.5067, 0.3639, 0.3328],
        [0.5350, 0.3485, 0.3026],
        [0.4780, 0.3763, 0.3355],
        [0.5052, 0.3724, 0.3138],
        [0.5714, 0.3403, 0.2832],
        [0.4711, 0.3683, 0.3537],
        [0.4927, 0.3595, 0.3282],
        [0.5044, 0.3844, 0.3118],
        [0.4789, 0.3911, 0.3379],
        [0.5442, 0.3582, 0.2914],
        [0.4891, 0.3657, 0.3648],
        [0.5494, 0.3389, 0.2961],
        [0.5028, 0.3706, 0.3356],
        [0.5378, 0.3529, 0.3041],
        [0.6840, 0.3163, 0.2517],
        [0.4806, 0.3681, 0.3587],
        [0.5908, 0.2944, 0.2728],
        [0.5035, 0.3412, 0.3291],
        [0.5599, 0.3225, 0.2829],
        [0.4807, 0.3805, 0.3425],
        [0.5267, 0.3471, 0.3177],
        [0.5370, 0.3442, 0.3126],
        [0.4757, 0.3847, 0.3675],
        [0.4574, 0.3866, 0.3680],
        [0.5241, 0.3558, 0.3076],
        [0.5513, 0.3424, 0.2886],
        [0.5344, 0.3537, 0.3119],
        [0.5275, 0.3544, 0.3132],
        [0.4787, 0.3765, 0.3539],
        [0.4117, 0.3970, 0.3860],
        [0.4248, 0.3971, 0.3665],
        [0.5056, 0.3462, 0.3310],
        [0.4771, 0.3794, 0.3514],
        [0.4695, 0.3748, 0.3505],
        [0.5003, 0.3557, 0.3314],
        [0.6180, 0.3014, 0.2865],
        [0.5008, 0.3713, 0.3293],
        [0.4347, 0.3860, 0.3798],
        [0.4819, 0.3830, 0.3430],
        [0.5163, 0.3516, 0.3089],
        [0.4972, 0.3736, 0.3221],
        [0.5227, 0.3736, 0.2988],
        [0.5768, 0.3300, 0.2693],
        [0.5140, 0.3663, 0.3300],
        [0.5063, 0.3698, 0.3401],
        [0.4779, 0.3762, 0.3583],
        [0.4976, 0.3761, 0.3470],
        [0.6169, 0.2930, 0.2645],
        [0.6764, 0.2881, 0.2577],
        [0.5299, 0.3669, 0.3172],
        [0.5236, 0.3645, 0.3350],
        [0.5961, 0.3272, 0.2595],
        [0.5134, 0.3491, 0.3258],
        [0.5047, 0.3553, 0.3340],
        [0.5225, 0.3572, 0.3056],
        [0.5329, 0.4293, 0.2493],
        [0.5020, 0.3887, 0.2796],
        [0.4893, 0.3997, 0.3044],
        [0.4985, 0.4042, 0.2902],
        [0.4951, 0.3855, 0.2935],
        [0.4772, 0.3718, 0.3369],
        [0.4909, 0.3566, 0.2826]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  8]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 47: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8092.0, Mean: 2247.39892578125, Std: 1776.50537109375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 80850.8359375, Mean: 22454.076171875, Std: 17750.0859375
[DEBUG] Top-3 class probabilities:
tensor([[0.5538, 0.3269, 0.3217],
        [0.4582, 0.3594, 0.3490],
        [0.5087, 0.3846, 0.2691],
        [0.5142, 0.4053, 0.2587],
        [0.5111, 0.4039, 0.2726],
        [0.5082, 0.4151, 0.2835],
        [0.5229, 0.4185, 0.2531],
        [0.5154, 0.4181, 0.2624],
        [0.5177, 0.4065, 0.2548],
        [0.4918, 0.3774, 0.2911],
        [0.5091, 0.4063, 0.2617],
        [0.5177, 0.4225, 0.2583],
        [0.5200, 0.4155, 0.2591],
        [0.5109, 0.4094, 0.2835],
        [0.5106, 0.4040, 0.2750],
        [0.4349, 0.3791, 0.3416],
        [0.5081, 0.4044, 0.2712],
        [0.4840, 0.3914, 0.3092],
        [0.5087, 0.4120, 0.2753],
        [0.4770, 0.3793, 0.3346],
        [0.5139, 0.4061, 0.2747],
        [0.4265, 0.4134, 0.3578],
        [0.4486, 0.4144, 0.3553],
        [0.4902, 0.3987, 0.3172],
        [0.4923, 0.3835, 0.3006],
        [0.4870, 0.3763, 0.3341],
        [0.5094, 0.4057, 0.2756],
        [0.4259, 0.4014, 0.3280],
        [0.4591, 0.3564, 0.3497],
        [0.4413, 0.3692, 0.3622],
        [0.4819, 0.4002, 0.2938],
        [0.5013, 0.3983, 0.2676],
        [0.4928, 0.3933, 0.2959],
        [0.5282, 0.4326, 0.2369],
        [0.5042, 0.4021, 0.2793],
        [0.4989, 0.4016, 0.2835],
        [0.4920, 0.3906, 0.2974],
        [0.4701, 0.3683, 0.3402],
        [0.4913, 0.3892, 0.2976],
        [0.4876, 0.3934, 0.3020],
        [0.4762, 0.3685, 0.3321],
        [0.4891, 0.3784, 0.2893],
        [0.4655, 0.3734, 0.3582],
        [0.4800, 0.3912, 0.3098],
        [0.5157, 0.4189, 0.2512],
        [0.4892, 0.3772, 0.2907],
        [0.4870, 0.3780, 0.3129],
        [0.5111, 0.4179, 0.2663],
        [0.4923, 0.3905, 0.2885],
        [0.4995, 0.3942, 0.2837],
        [0.5197, 0.4160, 0.2446],
        [0.5140, 0.4084, 0.2632],
        [0.5324, 0.4196, 0.2367],
        [0.5039, 0.4047, 0.2718],
        [0.4913, 0.3781, 0.3250],
        [0.4889, 0.3882, 0.3258],
        [0.5218, 0.4256, 0.2643],
        [0.5222, 0.4134, 0.2565],
        [0.4998, 0.3921, 0.2928],
        [0.4547, 0.3607, 0.3590],
        [0.4302, 0.3993, 0.3420],
        [0.4972, 0.3846, 0.3065],
        [0.5190, 0.4061, 0.2663],
        [0.4732, 0.3925, 0.3316],
        [0.4990, 0.3977, 0.2774],
        [0.4499, 0.3769, 0.3558],
        [0.4240, 0.4188, 0.3637],
        [0.4941, 0.3905, 0.3180],
        [0.5049, 0.3771, 0.3075],
        [0.4967, 0.4010, 0.2857],
        [0.4554, 0.3713, 0.3687],
        [0.4339, 0.4130, 0.3296],
        [0.4759, 0.3890, 0.3241],
        [0.5038, 0.4029, 0.2832],
        [0.4778, 0.3948, 0.3170],
        [0.4278, 0.4165, 0.3469],
        [0.4294, 0.3879, 0.3576],
        [0.4953, 0.3928, 0.2914],
        [0.4840, 0.3686, 0.3379],
        [0.5236, 0.4053, 0.2638],
        [0.5109, 0.4015, 0.2710],
        [0.4934, 0.3844, 0.3129],
        [0.4520, 0.3704, 0.3455],
        [0.4932, 0.3847, 0.3152],
        [0.5292, 0.4212, 0.2483],
        [0.5019, 0.3950, 0.2814],
        [0.4872, 0.3896, 0.3112],
        [0.4925, 0.3966, 0.3042],
        [0.4744, 0.3864, 0.3125],
        [0.4868, 0.3963, 0.3044],
        [0.4996, 0.4030, 0.2839],
        [0.5063, 0.4017, 0.2822],
        [0.4920, 0.3788, 0.3139],
        [0.4863, 0.3934, 0.3050],
        [0.4786, 0.3711, 0.3475],
        [0.4859, 0.3911, 0.3183],
        [0.5111, 0.4258, 0.2498],
        [0.5299, 0.4255, 0.2399],
        [0.4958, 0.3851, 0.3120],
        [0.4737, 0.3672, 0.3348],
        [0.5213, 0.4078, 0.2537],
        [0.4618, 0.3715, 0.3517],
        [0.4783, 0.3829, 0.3109],
        [0.4967, 0.4073, 0.2916],
        [0.4723, 0.3827, 0.3210],
        [0.4977, 0.3957, 0.2854],
        [0.4781, 0.3825, 0.3345],
        [0.4725, 0.3781, 0.3343],
        [0.4742, 0.3851, 0.3294],
        [0.4905, 0.3932, 0.3079],
        [0.5134, 0.4037, 0.2659],
        [0.4262, 0.4085, 0.3579],
        [0.5424, 0.3710, 0.2567],
        [0.4261, 0.4030, 0.3399],
        [0.5008, 0.3946, 0.2915],
        [0.4962, 0.3612, 0.2463],
        [0.4750, 0.3875, 0.2942],
        [0.4983, 0.3882, 0.2941],
        [0.4484, 0.4044, 0.3402],
        [0.4990, 0.4081, 0.2891],
        [0.4653, 0.3698, 0.3483],
        [0.4543, 0.3691, 0.3581],
        [0.4513, 0.3710, 0.3603],
        [0.4756, 0.3757, 0.3248],
        [0.4553, 0.3934, 0.3603],
        [0.5059, 0.4005, 0.2910],
        [0.5197, 0.4031, 0.2563],
        [0.4907, 0.3821, 0.2966]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 48: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8200.0, Mean: 2291.458740234375, Std: 1784.131103515625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 81929.9375, Mean: 22894.30078125, Std: 17826.279296875
[DEBUG] Top-3 class probabilities:
tensor([[0.4771, 0.3809, 0.3295],
        [0.4351, 0.3828, 0.3455],
        [0.5004, 0.4130, 0.2737],
        [0.5116, 0.4097, 0.2714],
        [0.4987, 0.3874, 0.2825],
        [0.5114, 0.4036, 0.2641],
        [0.4850, 0.3915, 0.3366],
        [0.4471, 0.3774, 0.3481],
        [0.4960, 0.4064, 0.2925],
        [0.4861, 0.3907, 0.3088],
        [0.5007, 0.3868, 0.2913],
        [0.4528, 0.3658, 0.3658],
        [0.5156, 0.4094, 0.2725],
        [0.4955, 0.3824, 0.3019],
        [0.5175, 0.3978, 0.2723],
        [0.4973, 0.3828, 0.2854],
        [0.5146, 0.4183, 0.2605],
        [0.5106, 0.4014, 0.2665],
        [0.5030, 0.3920, 0.2821],
        [0.5016, 0.3924, 0.2605],
        [0.4526, 0.3681, 0.3584],
        [0.5263, 0.4280, 0.2565],
        [0.4929, 0.3954, 0.3131],
        [0.5002, 0.3894, 0.2952],
        [0.5238, 0.4261, 0.2435],
        [0.4958, 0.3904, 0.2797],
        [0.4985, 0.4022, 0.3113],
        [0.4916, 0.4019, 0.2822],
        [0.5242, 0.4120, 0.2481],
        [0.4502, 0.3752, 0.3596],
        [0.5036, 0.3872, 0.2898],
        [0.4262, 0.4007, 0.3579],
        [0.4280, 0.4037, 0.3450],
        [0.5194, 0.3616, 0.2537],
        [0.4355, 0.3948, 0.3391],
        [0.5170, 0.3488, 0.2322],
        [0.5619, 0.3416, 0.2311],
        [0.4992, 0.3987, 0.2880],
        [0.4973, 0.4018, 0.2722],
        [0.5107, 0.4175, 0.2515],
        [0.4770, 0.3695, 0.3394],
        [0.5112, 0.4166, 0.2677],
        [0.4783, 0.3687, 0.3438],
        [0.4843, 0.3959, 0.2932],
        [0.5191, 0.4030, 0.2643],
        [0.4875, 0.3783, 0.3066],
        [0.4407, 0.3975, 0.3394],
        [0.4941, 0.4034, 0.2859],
        [0.4907, 0.3973, 0.3024],
        [0.4808, 0.3751, 0.3301],
        [0.5068, 0.4015, 0.2582],
        [0.4622, 0.3629, 0.3491],
        [0.4829, 0.3872, 0.2900],
        [0.4356, 0.3822, 0.3583],
        [0.4997, 0.4053, 0.2702],
        [0.4725, 0.3959, 0.3342],
        [0.4736, 0.3766, 0.3395],
        [0.5297, 0.4215, 0.2463],
        [0.5205, 0.4076, 0.2567],
        [0.5065, 0.4068, 0.2628],
        [0.5220, 0.4183, 0.2440],
        [0.4987, 0.4016, 0.2882],
        [0.4802, 0.3774, 0.3090],
        [0.4788, 0.3785, 0.3196],
        [0.5075, 0.3892, 0.2566],
        [0.4899, 0.3947, 0.3003],
        [0.4856, 0.3780, 0.3169],
        [0.5034, 0.4076, 0.2877],
        [0.4821, 0.3802, 0.3265],
        [0.4966, 0.4093, 0.2888],
        [0.4407, 0.4027, 0.3348],
        [0.5019, 0.4038, 0.3006],
        [0.4970, 0.4039, 0.2810],
        [0.5125, 0.4181, 0.2647],
        [0.4647, 0.3549, 0.3423],
        [0.4560, 0.3710, 0.3524],
        [0.4639, 0.3755, 0.3522],
        [0.4856, 0.3826, 0.3174],
        [0.4820, 0.4053, 0.3189],
        [0.5028, 0.4031, 0.2696],
        [0.4999, 0.3772, 0.3032],
        [0.4874, 0.3781, 0.3114],
        [0.4965, 0.3991, 0.2954],
        [0.4782, 0.3787, 0.3103],
        [0.5037, 0.3916, 0.2907],
        [0.4194, 0.4122, 0.3401],
        [0.5079, 0.3922, 0.2865],
        [0.5093, 0.4113, 0.2615],
        [0.4944, 0.3867, 0.3136],
        [0.4936, 0.4057, 0.2958],
        [0.4542, 0.3677, 0.3676],
        [0.4935, 0.3914, 0.2954],
        [0.4790, 0.4019, 0.3072],
        [0.4717, 0.3767, 0.3092],
        [0.5126, 0.4203, 0.2784],
        [0.4953, 0.4130, 0.2936],
        [0.5110, 0.4106, 0.2615],
        [0.4979, 0.3797, 0.2842],
        [0.5127, 0.4123, 0.2815],
        [0.4620, 0.3583, 0.3571],
        [0.4921, 0.3961, 0.3055],
        [0.4543, 0.3901, 0.3199],
        [0.4610, 0.3730, 0.3466],
        [0.4695, 0.3962, 0.3257],
        [0.4944, 0.3814, 0.2999],
        [0.5168, 0.4295, 0.2398],
        [0.5143, 0.4116, 0.2616],
        [0.5075, 0.4071, 0.2870],
        [0.4594, 0.3636, 0.3604],
        [0.4519, 0.3701, 0.3569],
        [0.5096, 0.4095, 0.2656],
        [0.5076, 0.4000, 0.2855],
        [0.4983, 0.3991, 0.2908],
        [0.5036, 0.3970, 0.2853],
        [0.5026, 0.3817, 0.2849],
        [0.4972, 0.3905, 0.2871],
        [0.4963, 0.3906, 0.3138],
        [0.4918, 0.3935, 0.2943],
        [0.4948, 0.3880, 0.2880],
        [0.4154, 0.4127, 0.3493],
        [0.4898, 0.3701, 0.3110],
        [0.4851, 0.3967, 0.3152],
        [0.5236, 0.4236, 0.2596],
        [0.4589, 0.3804, 0.3302],
        [0.4544, 0.3758, 0.3678],
        [0.4758, 0.3827, 0.3065],
        [0.4955, 0.3978, 0.2698],
        [0.5218, 0.4255, 0.2471]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 49: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20192.0, Mean: 2334.367919921875, Std: 1794.796875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 201748.890625, Mean: 23323.03125, Std: 17932.845703125
[DEBUG] Top-3 class probabilities:
tensor([[0.5125, 0.4082, 0.2604],
        [0.4936, 0.3908, 0.2935],
        [0.4799, 0.3717, 0.3100],
        [0.4793, 0.3753, 0.3283],
        [0.5080, 0.3917, 0.2907],
        [0.4818, 0.3749, 0.3122],
        [0.4938, 0.3927, 0.2946],
        [0.4596, 0.3750, 0.3647],
        [0.4651, 0.3755, 0.3616],
        [0.4709, 0.3771, 0.3442],
        [0.4618, 0.3615, 0.3610],
        [0.4826, 0.3972, 0.2920],
        [0.4853, 0.3995, 0.3111],
        [0.4958, 0.4042, 0.3082],
        [0.4788, 0.3837, 0.3164],
        [0.5154, 0.4259, 0.2533],
        [0.5079, 0.4156, 0.2632],
        [0.4692, 0.3655, 0.3554],
        [0.5124, 0.4120, 0.2775],
        [0.5103, 0.4091, 0.2659],
        [0.4833, 0.3730, 0.3301],
        [0.5264, 0.4186, 0.2480],
        [0.4824, 0.3847, 0.3208],
        [0.5043, 0.3945, 0.2856],
        [0.5243, 0.4196, 0.2445],
        [0.5041, 0.4009, 0.2897],
        [0.5026, 0.4021, 0.2719],
        [0.4919, 0.3899, 0.2992],
        [0.4776, 0.3803, 0.3308],
        [0.4624, 0.3568, 0.3420],
        [0.5020, 0.4049, 0.2922],
        [0.4999, 0.3856, 0.2932],
        [0.4925, 0.4035, 0.3069],
        [0.4973, 0.3932, 0.3016],
        [0.5060, 0.4163, 0.2882],
        [0.4848, 0.3777, 0.3022],
        [0.4935, 0.3790, 0.3030],
        [0.5121, 0.4191, 0.2867],
        [0.5028, 0.3933, 0.2886],
        [0.4860, 0.3695, 0.3247],
        [0.4828, 0.3916, 0.3220],
        [0.5163, 0.4192, 0.2591],
        [0.4938, 0.4166, 0.2852],
        [0.4482, 0.3763, 0.3364],
        [0.4920, 0.3928, 0.3043],
        [0.4902, 0.3953, 0.3148],
        [0.4923, 0.3822, 0.3079],
        [0.5020, 0.4005, 0.2914],
        [0.4949, 0.4023, 0.3012],
        [0.5113, 0.4035, 0.2781],
        [0.4919, 0.3931, 0.2970],
        [0.5009, 0.3984, 0.2890],
        [0.4714, 0.3797, 0.3356],
        [0.4788, 0.3818, 0.3111],
        [0.4703, 0.3584, 0.3341],
        [0.4777, 0.3795, 0.3447],
        [0.5018, 0.3977, 0.2843],
        [0.5015, 0.4108, 0.2645],
        [0.5134, 0.4053, 0.2704],
        [0.4822, 0.3950, 0.2877],
        [0.5042, 0.4072, 0.2750],
        [0.5066, 0.4009, 0.2813],
        [0.5020, 0.3994, 0.2817],
        [0.4926, 0.4016, 0.3000],
        [0.5196, 0.4352, 0.2574],
        [0.5178, 0.4137, 0.2477],
        [0.5321, 0.4255, 0.2408],
        [0.4898, 0.3908, 0.3118],
        [0.4673, 0.3814, 0.3340],
        [0.5156, 0.4105, 0.2670],
        [0.5017, 0.3992, 0.3073],
        [0.5060, 0.4020, 0.2905],
        [0.4872, 0.3849, 0.3294],
        [0.4676, 0.3621, 0.3301],
        [0.4373, 0.3856, 0.3696],
        [0.5200, 0.4155, 0.2587],
        [0.4878, 0.3795, 0.3135],
        [0.5046, 0.4011, 0.2779],
        [0.5058, 0.4016, 0.2739],
        [0.4664, 0.3756, 0.3314],
        [0.4618, 0.3831, 0.3448],
        [0.5074, 0.4046, 0.2744],
        [0.4653, 0.3684, 0.3226],
        [0.4737, 0.3876, 0.3190],
        [0.4525, 0.3740, 0.3631],
        [0.5177, 0.4172, 0.2579],
        [0.5135, 0.4104, 0.2715],
        [0.5138, 0.4086, 0.2789],
        [0.4985, 0.3914, 0.2815],
        [0.4621, 0.3683, 0.3598],
        [0.5050, 0.4034, 0.2666],
        [0.4625, 0.3748, 0.3640],
        [0.5026, 0.4021, 0.3043],
        [0.5071, 0.4002, 0.2918],
        [0.5021, 0.4124, 0.2832],
        [0.4936, 0.3968, 0.2995],
        [0.5011, 0.4026, 0.2808],
        [0.4448, 0.3785, 0.3419],
        [0.4503, 0.3828, 0.2950],
        [0.5180, 0.4187, 0.2511],
        [0.5024, 0.4067, 0.2937],
        [0.5131, 0.4161, 0.2714],
        [0.4868, 0.4019, 0.3000],
        [0.5111, 0.4054, 0.2700],
        [0.4855, 0.3876, 0.3169],
        [0.4943, 0.4069, 0.2869],
        [0.5000, 0.4014, 0.2829],
        [0.4877, 0.3754, 0.2842],
        [0.4954, 0.4041, 0.3029],
        [0.5041, 0.4108, 0.2825],
        [0.5052, 0.3888, 0.2804],
        [0.4657, 0.3719, 0.3337],
        [0.5076, 0.4199, 0.2708],
        [0.5108, 0.3862, 0.2634],
        [0.4830, 0.3977, 0.3063],
        [0.4698, 0.3926, 0.3337],
        [0.5068, 0.4014, 0.2775],
        [0.4818, 0.3519, 0.3026],
        [0.5027, 0.4085, 0.2855],
        [0.5040, 0.4106, 0.2817],
        [0.4466, 0.3740, 0.3707],
        [0.4971, 0.3946, 0.2990],
        [0.5073, 0.4137, 0.2593],
        [0.4976, 0.4156, 0.2838],
        [0.4732, 0.3653, 0.3232],
        [0.4256, 0.3659, 0.3211],
        [0.5136, 0.4050, 0.2769],
        [0.4993, 0.3980, 0.2893]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 50: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9536.0, Mean: 2314.84765625, Std: 1766.55859375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 95278.6796875, Mean: 23127.9921875, Std: 17650.701171875
[DEBUG] Top-3 class probabilities:
tensor([[0.4786, 0.3891, 0.3445],
        [0.5104, 0.3864, 0.2904],
        [0.4785, 0.3990, 0.3096],
        [0.5112, 0.4219, 0.2539],
        [0.4923, 0.3888, 0.2989],
        [0.5107, 0.4104, 0.2600],
        [0.4713, 0.3845, 0.3154],
        [0.4550, 0.3781, 0.3531],
        [0.5160, 0.4006, 0.2730],
        [0.4678, 0.3777, 0.3216],
        [0.4903, 0.3877, 0.3193],
        [0.4974, 0.4018, 0.2960],
        [0.4418, 0.3907, 0.3412],
        [0.4866, 0.4037, 0.2924],
        [0.5052, 0.4068, 0.2764],
        [0.4215, 0.4015, 0.3369],
        [0.4800, 0.3727, 0.3305],
        [0.4826, 0.3845, 0.3038],
        [0.4825, 0.3733, 0.3225],
        [0.4557, 0.3563, 0.3556],
        [0.4588, 0.3725, 0.3513],
        [0.4799, 0.3850, 0.3201],
        [0.5078, 0.4126, 0.2790],
        [0.4944, 0.4015, 0.2823],
        [0.5178, 0.4113, 0.2632],
        [0.5014, 0.3981, 0.2699],
        [0.4815, 0.3749, 0.3074],
        [0.4895, 0.3944, 0.3074],
        [0.4833, 0.3988, 0.2976],
        [0.4765, 0.3723, 0.3241],
        [0.4774, 0.3664, 0.3274],
        [0.4536, 0.3533, 0.3423],
        [0.4877, 0.3979, 0.2963],
        [0.4789, 0.3736, 0.3222],
        [0.4593, 0.3724, 0.3695],
        [0.4754, 0.3672, 0.3281],
        [0.4640, 0.3820, 0.3447],
        [0.4696, 0.3751, 0.3358],
        [0.4799, 0.3922, 0.2808],
        [0.5304, 0.4083, 0.2550],
        [0.4976, 0.3957, 0.2867],
        [0.4881, 0.3970, 0.2900],
        [0.4849, 0.3816, 0.3065],
        [0.5213, 0.4027, 0.2523],
        [0.5247, 0.3984, 0.2443],
        [0.4845, 0.3860, 0.3181],
        [0.4956, 0.3808, 0.2905],
        [0.4827, 0.3917, 0.3226],
        [0.4660, 0.3795, 0.3417],
        [0.5266, 0.4111, 0.2512],
        [0.4669, 0.3648, 0.3507],
        [0.4656, 0.3674, 0.3641],
        [0.4564, 0.3649, 0.3570],
        [0.4591, 0.3928, 0.3481],
        [0.5073, 0.4097, 0.2762],
        [0.5003, 0.4054, 0.2826],
        [0.4918, 0.3997, 0.3034],
        [0.4892, 0.3890, 0.3246],
        [0.4952, 0.3799, 0.3143],
        [0.4942, 0.4001, 0.2931],
        [0.4457, 0.3875, 0.3549],
        [0.4701, 0.3738, 0.3540],
        [0.4824, 0.3889, 0.3111],
        [0.5132, 0.4028, 0.2827],
        [0.4963, 0.3945, 0.2897],
        [0.5090, 0.4045, 0.2831],
        [0.4989, 0.4116, 0.2777],
        [0.4979, 0.4046, 0.2886],
        [0.4877, 0.3803, 0.3088],
        [0.5173, 0.4137, 0.2547],
        [0.4984, 0.3985, 0.2847],
        [0.5235, 0.4259, 0.2547],
        [0.5121, 0.4142, 0.2613],
        [0.5120, 0.4041, 0.2779],
        [0.4996, 0.4145, 0.2946],
        [0.4978, 0.3940, 0.2909],
        [0.4848, 0.3887, 0.3075],
        [0.4568, 0.3772, 0.3576],
        [0.5064, 0.3831, 0.2821],
        [0.4647, 0.3771, 0.3630],
        [0.4991, 0.3929, 0.2853],
        [0.4795, 0.3642, 0.3354],
        [0.5127, 0.4170, 0.2665],
        [0.5024, 0.4091, 0.2866],
        [0.4697, 0.3959, 0.3365],
        [0.4953, 0.3895, 0.2734],
        [0.4931, 0.3862, 0.2937],
        [0.4738, 0.3745, 0.3325],
        [0.4521, 0.3713, 0.3624],
        [0.4765, 0.3598, 0.3405],
        [0.4705, 0.3862, 0.3180],
        [0.4902, 0.4245, 0.3193],
        [0.4597, 0.3733, 0.3594],
        [0.4640, 0.3564, 0.3441],
        [0.4634, 0.3648, 0.3269],
        [0.4960, 0.3921, 0.3001],
        [0.5060, 0.4064, 0.2702],
        [0.4491, 0.3880, 0.3512],
        [0.4870, 0.3835, 0.3046],
        [0.4790, 0.3952, 0.3283],
        [0.4722, 0.3790, 0.3434],
        [0.4910, 0.3969, 0.3065],
        [0.4723, 0.3639, 0.3410],
        [0.4895, 0.3781, 0.3105],
        [0.4600, 0.3763, 0.3700],
        [0.4811, 0.3731, 0.3349],
        [0.4562, 0.3659, 0.3634],
        [0.4599, 0.3605, 0.3565],
        [0.4788, 0.3723, 0.3329],
        [0.4869, 0.3859, 0.3207],
        [0.4893, 0.4023, 0.2889],
        [0.4898, 0.4077, 0.2829],
        [0.5169, 0.4207, 0.2672],
        [0.4935, 0.3943, 0.2834],
        [0.4936, 0.4119, 0.3004],
        [0.5168, 0.4150, 0.2571],
        [0.5144, 0.4108, 0.2586],
        [0.4934, 0.4012, 0.2774],
        [0.4968, 0.4058, 0.2761],
        [0.5116, 0.4137, 0.2648],
        [0.4878, 0.3865, 0.3086],
        [0.4428, 0.3899, 0.3562],
        [0.4342, 0.3945, 0.3535],
        [0.4473, 0.3647, 0.3516],
        [0.4776, 0.3948, 0.3016],
        [0.5129, 0.4242, 0.2578],
        [0.5059, 0.4059, 0.2715],
        [0.5221, 0.4028, 0.2494]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 51: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10214.0, Mean: 2308.322265625, Std: 1772.2044677734375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 102052.96875, Mean: 23062.791015625, Std: 17707.11328125
[DEBUG] Top-3 class probabilities:
tensor([[0.5169, 0.4019, 0.2566],
        [0.4984, 0.3903, 0.3040],
        [0.5244, 0.4177, 0.2567],
        [0.4802, 0.3937, 0.3148],
        [0.4952, 0.3898, 0.2934],
        [0.5005, 0.3931, 0.2899],
        [0.4932, 0.4021, 0.2977],
        [0.4721, 0.4217, 0.3133],
        [0.4765, 0.4142, 0.2964],
        [0.4609, 0.3656, 0.3551],
        [0.4469, 0.3737, 0.3534],
        [0.4736, 0.3683, 0.3224],
        [0.4702, 0.3669, 0.3363],
        [0.5129, 0.3996, 0.2620],
        [0.4642, 0.3819, 0.3550],
        [0.4440, 0.3745, 0.3624],
        [0.4293, 0.4256, 0.3341],
        [0.4960, 0.3966, 0.3065],
        [0.4954, 0.3835, 0.3045],
        [0.4713, 0.3856, 0.3080],
        [0.5058, 0.4063, 0.2873],
        [0.4964, 0.3807, 0.2914],
        [0.5025, 0.3766, 0.2842],
        [0.4562, 0.3737, 0.3513],
        [0.4873, 0.3813, 0.3145],
        [0.5006, 0.3983, 0.2924],
        [0.5104, 0.4157, 0.2772],
        [0.4625, 0.3739, 0.3527],
        [0.4899, 0.3990, 0.3019],
        [0.4592, 0.3611, 0.3559],
        [0.5054, 0.4172, 0.2889],
        [0.4976, 0.4042, 0.2978],
        [0.5084, 0.4180, 0.2562],
        [0.5140, 0.4182, 0.2627],
        [0.5262, 0.4100, 0.2558],
        [0.4868, 0.3836, 0.3215],
        [0.4584, 0.3619, 0.3611],
        [0.4683, 0.3433, 0.3409],
        [0.4614, 0.3661, 0.3637],
        [0.4973, 0.4193, 0.2953],
        [0.4441, 0.3902, 0.3477],
        [0.4730, 0.3643, 0.3203],
        [0.4838, 0.3767, 0.3304],
        [0.4964, 0.3951, 0.2846],
        [0.5054, 0.4094, 0.2694],
        [0.4902, 0.3967, 0.2848],
        [0.4902, 0.3817, 0.3069],
        [0.4528, 0.3739, 0.3591],
        [0.4767, 0.3694, 0.3270],
        [0.5157, 0.4070, 0.2601],
        [0.4874, 0.3916, 0.3059],
        [0.4943, 0.4033, 0.3000],
        [0.5288, 0.4099, 0.2627],
        [0.4768, 0.3845, 0.3264],
        [0.4576, 0.3702, 0.3625],
        [0.4881, 0.3974, 0.2972],
        [0.4544, 0.3778, 0.3735],
        [0.4859, 0.3770, 0.3062],
        [0.4695, 0.3812, 0.3228],
        [0.4900, 0.3844, 0.3087],
        [0.4246, 0.4171, 0.3346],
        [0.4917, 0.3899, 0.3101],
        [0.4809, 0.3722, 0.3243],
        [0.4848, 0.3839, 0.3033],
        [0.4781, 0.3612, 0.3267],
        [0.4923, 0.3882, 0.3162],
        [0.4603, 0.3575, 0.3455],
        [0.4593, 0.3804, 0.3379],
        [0.4913, 0.3886, 0.2942],
        [0.4645, 0.3588, 0.3571],
        [0.4754, 0.3720, 0.3412],
        [0.4634, 0.3709, 0.3447],
        [0.5085, 0.4108, 0.2640],
        [0.4888, 0.3959, 0.2952],
        [0.4825, 0.3743, 0.3240],
        [0.4911, 0.3927, 0.2983],
        [0.5059, 0.4062, 0.2717],
        [0.5088, 0.4134, 0.2497],
        [0.5110, 0.4040, 0.2550],
        [0.5103, 0.4033, 0.2552],
        [0.5242, 0.4214, 0.2512],
        [0.5283, 0.4277, 0.2436],
        [0.5107, 0.3965, 0.2744],
        [0.4860, 0.3913, 0.2833],
        [0.4528, 0.3785, 0.3466],
        [0.4689, 0.3689, 0.3357],
        [0.4859, 0.3810, 0.3074],
        [0.4862, 0.3869, 0.3117],
        [0.5033, 0.4156, 0.2478],
        [0.5000, 0.3915, 0.2851],
        [0.5040, 0.3995, 0.2698],
        [0.5163, 0.4084, 0.2518],
        [0.5044, 0.4008, 0.2738],
        [0.4416, 0.3973, 0.3260],
        [0.4328, 0.4178, 0.3354],
        [0.4730, 0.3935, 0.3127],
        [0.4759, 0.3689, 0.3326],
        [0.4728, 0.3833, 0.3302],
        [0.5031, 0.4011, 0.2781],
        [0.5024, 0.4043, 0.2895],
        [0.5034, 0.3991, 0.2854],
        [0.4970, 0.3886, 0.2795],
        [0.4615, 0.3659, 0.3475],
        [0.4736, 0.3905, 0.3466],
        [0.4893, 0.3967, 0.3083],
        [0.4909, 0.4012, 0.3160],
        [0.4429, 0.3736, 0.3522],
        [0.4785, 0.3657, 0.3259],
        [0.4692, 0.3891, 0.3407],
        [0.4771, 0.3943, 0.3273],
        [0.4666, 0.3854, 0.3527],
        [0.4626, 0.3651, 0.3555],
        [0.4435, 0.4280, 0.3062],
        [0.4372, 0.3910, 0.3484],
        [0.4855, 0.3915, 0.3127],
        [0.5123, 0.3892, 0.2787],
        [0.4796, 0.3722, 0.2998],
        [0.5216, 0.3965, 0.2556],
        [0.4711, 0.3713, 0.3113],
        [0.4457, 0.3742, 0.3508],
        [0.5060, 0.4141, 0.2747],
        [0.5120, 0.4083, 0.2564],
        [0.5270, 0.4317, 0.2571],
        [0.5080, 0.4100, 0.2818],
        [0.4742, 0.3710, 0.3398],
        [0.4729, 0.3753, 0.3400],
        [0.4746, 0.3664, 0.3172],
        [0.4881, 0.3962, 0.2967]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 52: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15264.0, Mean: 2280.377197265625, Std: 1761.0362548828125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 152510.421875, Mean: 22783.57421875, Std: 17595.525390625
[DEBUG] Top-3 class probabilities:
tensor([[0.5142, 0.4167, 0.2765],
        [0.5115, 0.3970, 0.2534],
        [0.4808, 0.3619, 0.3119],
        [0.4855, 0.3859, 0.3080],
        [0.4829, 0.3687, 0.3164],
        [0.4551, 0.3801, 0.3629],
        [0.5055, 0.4021, 0.2808],
        [0.5098, 0.4339, 0.2615],
        [0.4657, 0.3750, 0.3676],
        [0.4320, 0.4222, 0.3268],
        [0.5045, 0.3981, 0.2840],
        [0.4443, 0.3821, 0.3613],
        [0.5079, 0.3899, 0.2828],
        [0.4546, 0.3683, 0.3530],
        [0.4718, 0.3792, 0.3286],
        [0.4731, 0.4031, 0.3300],
        [0.4820, 0.4032, 0.3183],
        [0.5048, 0.4084, 0.2810],
        [0.4447, 0.4034, 0.3582],
        [0.4772, 0.3900, 0.3381],
        [0.4771, 0.3941, 0.3204],
        [0.4637, 0.3946, 0.3322],
        [0.4853, 0.3980, 0.3132],
        [0.4743, 0.3704, 0.3269],
        [0.4829, 0.3850, 0.3215],
        [0.4721, 0.3734, 0.3371],
        [0.4741, 0.3824, 0.3249],
        [0.4794, 0.3950, 0.3036],
        [0.4800, 0.3802, 0.3353],
        [0.4803, 0.3550, 0.3165],
        [0.4396, 0.4058, 0.3287],
        [0.4345, 0.3861, 0.3501],
        [0.4623, 0.3751, 0.3386],
        [0.4513, 0.3740, 0.3491],
        [0.5034, 0.4101, 0.2884],
        [0.4917, 0.4058, 0.2913],
        [0.4495, 0.3849, 0.3493],
        [0.4905, 0.3954, 0.2766],
        [0.4697, 0.3802, 0.3313],
        [0.5108, 0.4065, 0.2554],
        [0.5244, 0.4159, 0.2406],
        [0.4919, 0.4000, 0.2888],
        [0.5196, 0.4252, 0.2496],
        [0.4999, 0.3882, 0.2783],
        [0.4371, 0.3721, 0.3332],
        [0.4569, 0.3973, 0.3287],
        [0.4929, 0.4068, 0.3092],
        [0.4804, 0.3760, 0.3266],
        [0.5148, 0.4089, 0.2552],
        [0.4996, 0.3878, 0.3100],
        [0.4606, 0.3809, 0.3579],
        [0.4801, 0.4059, 0.3101],
        [0.4769, 0.3899, 0.3197],
        [0.4954, 0.4030, 0.2711],
        [0.4905, 0.4013, 0.2998],
        [0.5051, 0.3977, 0.2760],
        [0.4870, 0.3922, 0.3184],
        [0.4800, 0.4019, 0.3038],
        [0.5055, 0.4180, 0.2732],
        [0.5150, 0.4145, 0.2743],
        [0.4637, 0.3821, 0.3434],
        [0.4853, 0.3877, 0.3101],
        [0.4558, 0.4156, 0.3177],
        [0.4826, 0.4057, 0.2811],
        [0.4725, 0.3666, 0.3433],
        [0.4528, 0.3691, 0.3580],
        [0.4866, 0.4076, 0.2963],
        [0.4914, 0.3869, 0.2988],
        [0.4836, 0.3904, 0.3052],
        [0.4937, 0.3692, 0.3223],
        [0.4525, 0.3792, 0.3366],
        [0.4469, 0.3984, 0.3382],
        [0.4884, 0.3880, 0.3175],
        [0.4782, 0.3791, 0.3207],
        [0.4983, 0.4085, 0.3027],
        [0.4826, 0.3891, 0.3042],
        [0.4455, 0.3751, 0.3585],
        [0.4711, 0.3609, 0.3381],
        [0.4939, 0.3998, 0.2895],
        [0.4433, 0.3937, 0.3513],
        [0.4864, 0.3893, 0.3203],
        [0.4636, 0.3755, 0.3382],
        [0.4772, 0.3815, 0.2913],
        [0.5086, 0.4036, 0.2644],
        [0.4655, 0.3648, 0.3555],
        [0.5078, 0.3930, 0.2965],
        [0.5336, 0.4263, 0.2557],
        [0.5005, 0.3917, 0.2903],
        [0.4696, 0.3948, 0.3269],
        [0.4998, 0.3817, 0.3055],
        [0.4543, 0.3659, 0.3636],
        [0.4598, 0.3700, 0.3647],
        [0.4821, 0.3913, 0.3371],
        [0.4524, 0.3593, 0.3330],
        [0.4920, 0.3890, 0.2978],
        [0.4953, 0.3772, 0.3008],
        [0.4895, 0.4013, 0.3035],
        [0.4595, 0.3745, 0.3524],
        [0.4824, 0.3840, 0.3343],
        [0.4983, 0.4027, 0.2628],
        [0.4999, 0.4024, 0.2843],
        [0.5096, 0.4193, 0.2641],
        [0.5049, 0.4030, 0.2871],
        [0.4936, 0.4202, 0.2817],
        [0.4895, 0.3847, 0.2865],
        [0.4922, 0.3949, 0.3029],
        [0.4396, 0.3909, 0.3712],
        [0.4751, 0.3790, 0.3396],
        [0.4504, 0.3767, 0.3459],
        [0.4358, 0.4150, 0.3281],
        [0.4761, 0.3850, 0.3268],
        [0.4668, 0.3672, 0.3499],
        [0.4840, 0.3815, 0.3077],
        [0.4752, 0.3880, 0.2985],
        [0.4500, 0.3683, 0.3569],
        [0.4781, 0.3740, 0.3292],
        [0.4692, 0.3596, 0.3379],
        [0.4397, 0.3992, 0.3436],
        [0.5078, 0.4136, 0.2688],
        [0.4863, 0.3990, 0.3138],
        [0.4852, 0.3921, 0.3202],
        [0.4545, 0.3789, 0.3664],
        [0.4659, 0.3655, 0.3566],
        [0.4851, 0.3883, 0.3115],
        [0.4787, 0.3802, 0.3179],
        [0.4396, 0.3821, 0.3628],
        [0.5079, 0.4020, 0.2747],
        [0.5205, 0.4239, 0.2456]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 53: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8321.0, Mean: 2298.931396484375, Std: 1785.9764404296875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 83138.9140625, Mean: 22968.96484375, Std: 17844.71484375
[DEBUG] Top-3 class probabilities:
tensor([[0.5134, 0.4022, 0.2665],
        [0.5078, 0.3934, 0.2867],
        [0.5204, 0.4189, 0.2446],
        [0.4840, 0.3769, 0.3071],
        [0.4712, 0.3893, 0.3147],
        [0.4854, 0.3905, 0.2843],
        [0.5259, 0.4157, 0.2441],
        [0.4791, 0.3680, 0.3246],
        [0.4750, 0.3819, 0.3434],
        [0.4792, 0.3833, 0.3215],
        [0.4699, 0.3757, 0.3298],
        [0.4900, 0.3724, 0.3096],
        [0.5083, 0.4032, 0.2901],
        [0.4890, 0.3790, 0.3150],
        [0.4981, 0.4062, 0.2883],
        [0.4940, 0.3948, 0.2936],
        [0.4776, 0.3910, 0.3301],
        [0.4857, 0.3930, 0.3115],
        [0.4795, 0.3684, 0.3460],
        [0.5029, 0.3968, 0.2696],
        [0.4746, 0.3875, 0.3238],
        [0.4965, 0.3993, 0.2937],
        [0.4693, 0.3811, 0.3352],
        [0.4330, 0.3915, 0.3502],
        [0.4801, 0.4139, 0.3314],
        [0.4858, 0.3992, 0.3100],
        [0.4564, 0.3714, 0.3664],
        [0.5014, 0.3970, 0.2896],
        [0.4938, 0.4040, 0.2985],
        [0.4780, 0.3709, 0.3379],
        [0.4257, 0.3866, 0.3439],
        [0.4669, 0.3642, 0.3495],
        [0.4777, 0.3836, 0.3258],
        [0.4660, 0.3721, 0.3352],
        [0.4645, 0.3727, 0.3572],
        [0.4639, 0.3793, 0.3361],
        [0.4937, 0.3949, 0.2935],
        [0.4714, 0.3556, 0.3249],
        [0.4762, 0.3890, 0.3168],
        [0.5032, 0.3972, 0.2792],
        [0.5065, 0.3884, 0.3045],
        [0.4704, 0.3710, 0.3314],
        [0.4825, 0.3896, 0.3058],
        [0.4664, 0.3774, 0.3733],
        [0.5188, 0.3986, 0.2567],
        [0.5316, 0.4245, 0.2435],
        [0.5151, 0.4110, 0.2452],
        [0.4997, 0.3920, 0.3000],
        [0.5127, 0.4169, 0.2506],
        [0.4953, 0.3938, 0.2831],
        [0.4780, 0.3809, 0.3521],
        [0.4855, 0.4000, 0.3150],
        [0.4677, 0.3672, 0.3193],
        [0.4662, 0.3583, 0.3508],
        [0.4675, 0.3654, 0.3547],
        [0.4793, 0.3808, 0.3083],
        [0.4598, 0.3795, 0.3455],
        [0.4992, 0.3940, 0.3132],
        [0.4951, 0.3970, 0.2923],
        [0.5053, 0.4053, 0.2676],
        [0.4859, 0.3965, 0.2980],
        [0.4886, 0.3975, 0.3009],
        [0.4667, 0.3657, 0.3571],
        [0.5018, 0.3949, 0.2736],
        [0.4869, 0.3813, 0.3259],
        [0.4776, 0.3825, 0.3234],
        [0.4865, 0.3970, 0.3127],
        [0.4731, 0.3919, 0.3120],
        [0.4639, 0.3788, 0.3372],
        [0.4848, 0.3851, 0.3053],
        [0.4655, 0.3547, 0.3527],
        [0.4552, 0.3711, 0.3500],
        [0.4694, 0.3621, 0.3429],
        [0.4475, 0.3773, 0.3752],
        [0.4481, 0.3640, 0.3411],
        [0.5080, 0.4404, 0.2668],
        [0.4490, 0.3707, 0.3507],
        [0.5001, 0.3962, 0.2952],
        [0.4617, 0.3686, 0.3607],
        [0.5043, 0.3998, 0.2898],
        [0.4636, 0.3713, 0.3533],
        [0.4903, 0.3959, 0.3123],
        [0.5048, 0.3965, 0.2847],
        [0.4443, 0.3770, 0.3556],
        [0.4911, 0.3961, 0.2924],
        [0.4838, 0.3807, 0.3306],
        [0.5102, 0.4069, 0.2881],
        [0.4870, 0.3890, 0.3051],
        [0.4439, 0.3743, 0.3507],
        [0.4948, 0.3796, 0.2979],
        [0.4996, 0.4187, 0.2916],
        [0.5033, 0.4066, 0.2697],
        [0.5274, 0.4191, 0.2543],
        [0.5091, 0.4133, 0.2681],
        [0.4976, 0.3976, 0.3041],
        [0.4828, 0.3906, 0.3107],
        [0.5029, 0.4134, 0.3013],
        [0.5189, 0.4094, 0.2592],
        [0.4797, 0.3953, 0.3048],
        [0.4631, 0.3576, 0.3517],
        [0.4765, 0.3852, 0.3205],
        [0.4515, 0.3796, 0.3617],
        [0.4654, 0.3606, 0.3395],
        [0.4686, 0.3726, 0.3391],
        [0.4958, 0.4021, 0.3139],
        [0.4554, 0.4154, 0.2989],
        [0.4617, 0.3576, 0.3432],
        [0.4631, 0.3728, 0.3517],
        [0.4620, 0.3716, 0.3629],
        [0.4761, 0.3850, 0.3232],
        [0.4652, 0.3918, 0.3433],
        [0.4821, 0.3925, 0.3337],
        [0.4276, 0.4005, 0.3530],
        [0.4539, 0.3635, 0.3623],
        [0.4807, 0.3834, 0.3238],
        [0.4751, 0.3878, 0.3206],
        [0.4566, 0.3785, 0.3485],
        [0.4938, 0.3926, 0.3159],
        [0.4776, 0.3784, 0.3071],
        [0.4801, 0.3868, 0.3089],
        [0.5300, 0.4151, 0.2603],
        [0.4618, 0.3702, 0.3523],
        [0.4822, 0.3944, 0.3174],
        [0.4688, 0.3802, 0.3459],
        [0.4807, 0.3824, 0.3247],
        [0.4980, 0.3932, 0.2946],
        [0.4785, 0.3880, 0.3228],
        [0.4910, 0.3946, 0.3232]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 54: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 2257.285888671875, Std: 1763.3978271484375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 205745.53125, Mean: 22552.85546875, Std: 17619.119140625
[DEBUG] Top-3 class probabilities:
tensor([[0.4840, 0.3731, 0.3158],
        [0.4809, 0.3694, 0.3201],
        [0.4698, 0.3676, 0.3487],
        [0.5130, 0.4101, 0.2742],
        [0.4955, 0.3975, 0.2964],
        [0.5165, 0.4047, 0.2721],
        [0.5108, 0.3907, 0.2453],
        [0.5083, 0.4006, 0.2761],
        [0.5052, 0.4015, 0.2681],
        [0.4668, 0.3741, 0.3285],
        [0.4951, 0.4038, 0.2986],
        [0.4539, 0.3634, 0.3596],
        [0.4951, 0.4010, 0.2992],
        [0.4773, 0.3956, 0.3305],
        [0.4323, 0.3888, 0.3427],
        [0.4663, 0.3812, 0.3582],
        [0.4606, 0.3753, 0.3665],
        [0.4798, 0.3685, 0.3331],
        [0.4376, 0.3808, 0.3513],
        [0.4906, 0.3826, 0.3204],
        [0.4745, 0.3840, 0.3319],
        [0.4819, 0.3765, 0.3059],
        [0.4359, 0.4243, 0.3359],
        [0.4270, 0.3925, 0.3319],
        [0.5378, 0.3820, 0.2747],
        [0.4300, 0.4136, 0.3478],
        [0.4604, 0.3737, 0.3428],
        [0.4818, 0.3927, 0.3091],
        [0.4923, 0.4001, 0.3052],
        [0.4674, 0.3818, 0.3407],
        [0.4738, 0.3675, 0.3348],
        [0.4887, 0.4063, 0.2821],
        [0.4905, 0.3705, 0.2940],
        [0.4676, 0.3699, 0.3336],
        [0.4422, 0.3739, 0.3682],
        [0.4859, 0.3808, 0.3162],
        [0.4959, 0.3776, 0.3028],
        [0.4520, 0.3549, 0.3547],
        [0.4768, 0.3855, 0.3244],
        [0.4907, 0.4044, 0.3287],
        [0.5020, 0.3910, 0.2721],
        [0.5078, 0.4084, 0.2847],
        [0.4969, 0.4000, 0.3032],
        [0.4951, 0.4128, 0.2925],
        [0.4941, 0.3956, 0.2968],
        [0.5065, 0.4063, 0.2726],
        [0.5132, 0.3972, 0.2719],
        [0.4844, 0.3636, 0.3000],
        [0.5090, 0.4055, 0.2904],
        [0.5165, 0.4054, 0.2709],
        [0.5225, 0.4148, 0.2503],
        [0.5231, 0.4112, 0.2460],
        [0.4552, 0.3693, 0.3469],
        [0.5188, 0.4107, 0.2646],
        [0.5093, 0.3985, 0.2813],
        [0.5245, 0.4280, 0.2437],
        [0.4901, 0.3966, 0.3080],
        [0.4945, 0.3984, 0.3056],
        [0.4648, 0.3646, 0.3484],
        [0.4842, 0.3887, 0.3141],
        [0.5046, 0.3860, 0.2846],
        [0.4736, 0.3824, 0.3220],
        [0.5016, 0.3925, 0.2858],
        [0.5107, 0.4149, 0.2725],
        [0.4465, 0.3740, 0.3622],
        [0.4843, 0.3902, 0.3205],
        [0.4619, 0.3943, 0.3592],
        [0.4749, 0.3716, 0.3409],
        [0.4755, 0.4088, 0.3173],
        [0.5753, 0.3542, 0.2583],
        [0.5190, 0.3814, 0.2669],
        [0.4376, 0.4104, 0.3360],
        [0.4470, 0.3725, 0.3492],
        [0.4358, 0.3953, 0.3538],
        [0.4684, 0.3864, 0.3252],
        [0.4548, 0.3730, 0.3653],
        [0.4624, 0.3689, 0.3600],
        [0.4991, 0.3851, 0.3122],
        [0.4879, 0.3973, 0.2787],
        [0.4568, 0.3759, 0.3597],
        [0.4675, 0.3684, 0.3399],
        [0.4870, 0.3843, 0.3171],
        [0.5006, 0.3958, 0.2928],
        [0.4731, 0.3626, 0.3448],
        [0.4950, 0.3964, 0.2815],
        [0.4457, 0.3937, 0.3387],
        [0.4791, 0.3885, 0.3265],
        [0.4984, 0.3815, 0.3064],
        [0.4919, 0.3944, 0.3104],
        [0.5168, 0.4212, 0.2503],
        [0.4933, 0.3844, 0.2926],
        [0.5064, 0.4083, 0.2820],
        [0.4946, 0.3904, 0.2980],
        [0.4923, 0.3900, 0.3108],
        [0.4719, 0.3817, 0.3528],
        [0.4684, 0.3713, 0.3391],
        [0.4937, 0.3984, 0.2965],
        [0.5070, 0.3865, 0.2955],
        [0.4992, 0.3941, 0.2889],
        [0.4727, 0.3762, 0.3323],
        [0.4853, 0.3862, 0.3110],
        [0.5040, 0.4025, 0.2647],
        [0.4763, 0.3789, 0.3246],
        [0.5068, 0.4132, 0.2790],
        [0.4968, 0.3999, 0.2784],
        [0.5041, 0.4073, 0.2762],
        [0.4905, 0.3823, 0.3134],
        [0.4458, 0.3733, 0.3586],
        [0.4587, 0.3738, 0.3521],
        [0.4521, 0.3816, 0.3587],
        [0.4860, 0.3866, 0.3254],
        [0.4414, 0.3745, 0.3532],
        [0.5133, 0.4207, 0.2575],
        [0.5026, 0.3922, 0.2716],
        [0.4282, 0.4115, 0.3281],
        [0.4315, 0.4269, 0.3421],
        [0.5367, 0.3762, 0.2797],
        [0.4525, 0.4114, 0.3193],
        [0.4454, 0.3928, 0.3495],
        [0.4700, 0.3711, 0.3453],
        [0.4805, 0.3684, 0.3318],
        [0.4923, 0.3878, 0.2867],
        [0.4683, 0.3822, 0.3471],
        [0.4476, 0.3879, 0.3738],
        [0.4404, 0.3975, 0.3509],
        [0.4550, 0.3954, 0.3549],
        [0.4572, 0.3725, 0.3724],
        [0.4648, 0.3549, 0.3491]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 55: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19680.0, Mean: 2254.705322265625, Std: 1749.99462890625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 196633.203125, Mean: 22527.078125, Std: 17485.201171875
[DEBUG] Top-3 class probabilities:
tensor([[0.4582, 0.3617, 0.3595],
        [0.4761, 0.3943, 0.3293],
        [0.4801, 0.3777, 0.3267],
        [0.4764, 0.3921, 0.3236],
        [0.4961, 0.3950, 0.2940],
        [0.4631, 0.3698, 0.3476],
        [0.4934, 0.3901, 0.3126],
        [0.4577, 0.3707, 0.3338],
        [0.5014, 0.4021, 0.2798],
        [0.5070, 0.4117, 0.2616],
        [0.4778, 0.3720, 0.3110],
        [0.4753, 0.4033, 0.3041],
        [0.4756, 0.3764, 0.3240],
        [0.4500, 0.3612, 0.3600],
        [0.5008, 0.3930, 0.2752],
        [0.4892, 0.3906, 0.2971],
        [0.4796, 0.3940, 0.3051],
        [0.4786, 0.3852, 0.3130],
        [0.5141, 0.4039, 0.2561],
        [0.5161, 0.4165, 0.2619],
        [0.4955, 0.3985, 0.2800],
        [0.4614, 0.3867, 0.3336],
        [0.4963, 0.4133, 0.2971],
        [0.4980, 0.3825, 0.2966],
        [0.5026, 0.3992, 0.2956],
        [0.4981, 0.4166, 0.3095],
        [0.4320, 0.3980, 0.3368],
        [0.4584, 0.3829, 0.3512],
        [0.4679, 0.3805, 0.3314],
        [0.4853, 0.3841, 0.3029],
        [0.4903, 0.3878, 0.3272],
        [0.4756, 0.3938, 0.3185],
        [0.4569, 0.3796, 0.3421],
        [0.4746, 0.3933, 0.3393],
        [0.4554, 0.3889, 0.3546],
        [0.4524, 0.4129, 0.3254],
        [0.4959, 0.4046, 0.2978],
        [0.4503, 0.3723, 0.3642],
        [0.4516, 0.3792, 0.3712],
        [0.4829, 0.3948, 0.3101],
        [0.4511, 0.3704, 0.3677],
        [0.4612, 0.3621, 0.3599],
        [0.4487, 0.3721, 0.3615],
        [0.4825, 0.3889, 0.3155],
        [0.4577, 0.3743, 0.3582],
        [0.4370, 0.3939, 0.3422],
        [0.4625, 0.3801, 0.3518],
        [0.4550, 0.3824, 0.3633],
        [0.4955, 0.4017, 0.3075],
        [0.4712, 0.3863, 0.3416],
        [0.5072, 0.4150, 0.2874],
        [0.4976, 0.4164, 0.2870],
        [0.4944, 0.3847, 0.2870],
        [0.5024, 0.4087, 0.2918],
        [0.5104, 0.4056, 0.2873],
        [0.4768, 0.3877, 0.3122],
        [0.4658, 0.3631, 0.3555],
        [0.4760, 0.3785, 0.3279],
        [0.4402, 0.3873, 0.3553],
        [0.4801, 0.3817, 0.3099],
        [0.5112, 0.4108, 0.2899],
        [0.4552, 0.3779, 0.3494],
        [0.4757, 0.3673, 0.3166],
        [0.4842, 0.3966, 0.3149],
        [0.5019, 0.4017, 0.2676],
        [0.5223, 0.4138, 0.2764],
        [0.4795, 0.3793, 0.3206],
        [0.4716, 0.3780, 0.3331],
        [0.4878, 0.4068, 0.2978],
        [0.4646, 0.3728, 0.3347],
        [0.4935, 0.3992, 0.2714],
        [0.4630, 0.3727, 0.3411],
        [0.4550, 0.3736, 0.3676],
        [0.4826, 0.3795, 0.3269],
        [0.4598, 0.3766, 0.3659],
        [0.4549, 0.3803, 0.3667],
        [0.4515, 0.3704, 0.3649],
        [0.4493, 0.4044, 0.3585],
        [0.4619, 0.3831, 0.3456],
        [0.4541, 0.3620, 0.3580],
        [0.4675, 0.3924, 0.3474],
        [0.4764, 0.3955, 0.3401],
        [0.4604, 0.3960, 0.3446],
        [0.4657, 0.3671, 0.3484],
        [0.4648, 0.3642, 0.3514],
        [0.4684, 0.3825, 0.3438],
        [0.4681, 0.3621, 0.3358],
        [0.4453, 0.3706, 0.3429],
        [0.4439, 0.3801, 0.3756],
        [0.4398, 0.4178, 0.3425],
        [0.4277, 0.3835, 0.3697],
        [0.4537, 0.3631, 0.3577],
        [0.4710, 0.3678, 0.3419],
        [0.4975, 0.3954, 0.3025],
        [0.4183, 0.4035, 0.3312],
        [0.4939, 0.3907, 0.3137],
        [0.5133, 0.3975, 0.2854],
        [0.4730, 0.3681, 0.3282],
        [0.4429, 0.3954, 0.3272],
        [0.4724, 0.3775, 0.3245],
        [0.5150, 0.4071, 0.2734],
        [0.4908, 0.3900, 0.2936],
        [0.4589, 0.3698, 0.3528],
        [0.4830, 0.3765, 0.3059],
        [0.4602, 0.3617, 0.3559],
        [0.4870, 0.3824, 0.3076],
        [0.4978, 0.4045, 0.2726],
        [0.4637, 0.3650, 0.3438],
        [0.4918, 0.4057, 0.2659],
        [0.4535, 0.3663, 0.3576],
        [0.4842, 0.3893, 0.2888],
        [0.5113, 0.4099, 0.2540],
        [0.4929, 0.3873, 0.3020],
        [0.4980, 0.4106, 0.2830],
        [0.4952, 0.3898, 0.3073],
        [0.5098, 0.4055, 0.2606],
        [0.4575, 0.3394, 0.3343],
        [0.4670, 0.3870, 0.3375],
        [0.4774, 0.3794, 0.3194],
        [0.4699, 0.3758, 0.3431],
        [0.4766, 0.3803, 0.3301],
        [0.4396, 0.4010, 0.3457],
        [0.4756, 0.3702, 0.3549],
        [0.4506, 0.3824, 0.3728],
        [0.4443, 0.3684, 0.3643],
        [0.4936, 0.3990, 0.3103],
        [0.5049, 0.4097, 0.2695],
        [0.4831, 0.3813, 0.2988]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 56: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20624.0, Mean: 2250.105712890625, Std: 1744.6981201171875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 206065.25, Mean: 22481.115234375, Std: 17432.28125
[DEBUG] Top-3 class probabilities:
tensor([[0.4774, 0.3896, 0.3231],
        [0.4753, 0.3729, 0.3194],
        [0.4758, 0.3814, 0.3288],
        [0.4918, 0.3972, 0.2928],
        [0.4680, 0.3971, 0.3155],
        [0.4647, 0.4034, 0.3120],
        [0.4723, 0.3982, 0.3352],
        [0.4254, 0.4215, 0.3447],
        [0.4289, 0.4169, 0.3218],
        [0.4670, 0.3939, 0.3419],
        [0.4714, 0.3621, 0.3362],
        [0.4382, 0.3830, 0.3359],
        [0.4953, 0.4011, 0.3126],
        [0.4753, 0.3798, 0.3220],
        [0.4956, 0.3904, 0.2779],
        [0.4383, 0.3910, 0.3585],
        [0.4838, 0.3825, 0.3337],
        [0.5013, 0.4012, 0.2907],
        [0.4934, 0.3890, 0.2891],
        [0.4811, 0.4020, 0.2917],
        [0.4492, 0.3839, 0.3656],
        [0.4414, 0.3976, 0.3469],
        [0.4698, 0.3706, 0.3396],
        [0.4344, 0.3990, 0.3683],
        [0.4649, 0.3947, 0.3482],
        [0.4958, 0.3963, 0.3031],
        [0.4861, 0.4074, 0.3173],
        [0.4775, 0.3785, 0.3186],
        [0.4868, 0.3800, 0.2844],
        [0.4944, 0.4072, 0.2791],
        [0.4967, 0.4041, 0.2850],
        [0.4777, 0.3864, 0.3283],
        [0.4829, 0.3864, 0.3372],
        [0.4720, 0.3741, 0.3397],
        [0.4732, 0.3860, 0.3429],
        [0.4668, 0.3622, 0.3513],
        [0.5032, 0.4031, 0.2833],
        [0.4502, 0.3650, 0.3587],
        [0.4343, 0.4009, 0.3495],
        [0.4855, 0.3878, 0.2918],
        [0.4885, 0.3979, 0.2926],
        [0.4662, 0.3587, 0.3493],
        [0.4816, 0.3778, 0.3147],
        [0.4384, 0.3778, 0.3331],
        [0.4526, 0.3818, 0.3699],
        [0.5123, 0.4071, 0.2582],
        [0.4590, 0.3702, 0.3556],
        [0.4857, 0.3837, 0.3191],
        [0.4716, 0.3756, 0.3370],
        [0.5095, 0.4011, 0.2456],
        [0.4959, 0.4004, 0.2797],
        [0.5047, 0.3973, 0.2947],
        [0.4784, 0.3823, 0.3394],
        [0.4813, 0.3855, 0.3295],
        [0.4587, 0.3715, 0.3461],
        [0.4571, 0.3707, 0.3612],
        [0.5037, 0.4168, 0.2956],
        [0.4815, 0.3865, 0.3219],
        [0.5189, 0.3981, 0.2603],
        [0.5049, 0.4060, 0.2771],
        [0.4913, 0.3884, 0.2960],
        [0.5152, 0.4118, 0.2660],
        [0.5045, 0.4079, 0.2601],
        [0.4908, 0.3802, 0.3035],
        [0.4833, 0.3884, 0.3179],
        [0.4760, 0.3945, 0.3119],
        [0.4928, 0.3846, 0.3164],
        [0.4616, 0.3689, 0.3519],
        [0.4456, 0.3669, 0.3585],
        [0.4541, 0.3872, 0.3639],
        [0.4714, 0.3616, 0.3445],
        [0.4878, 0.4030, 0.3044],
        [0.5097, 0.4040, 0.2986],
        [0.4761, 0.3735, 0.3004],
        [0.5096, 0.4195, 0.2727],
        [0.5193, 0.4119, 0.2599],
        [0.4808, 0.3795, 0.3006],
        [0.4649, 0.3788, 0.3572],
        [0.4776, 0.3711, 0.3295],
        [0.4694, 0.3757, 0.3425],
        [0.4262, 0.4182, 0.3392],
        [0.4820, 0.3978, 0.3154],
        [0.4587, 0.3705, 0.3628],
        [0.4806, 0.3747, 0.3346],
        [0.4816, 0.4033, 0.3084],
        [0.4832, 0.3873, 0.3154],
        [0.5002, 0.3861, 0.3107],
        [0.4832, 0.3887, 0.3188],
        [0.4431, 0.3921, 0.3501],
        [0.4534, 0.3617, 0.3478],
        [0.4800, 0.4075, 0.3299],
        [0.4878, 0.3934, 0.3144],
        [0.4895, 0.3902, 0.3262],
        [0.4569, 0.3807, 0.3440],
        [0.4684, 0.3770, 0.3493],
        [0.4643, 0.3723, 0.3306],
        [0.4764, 0.3692, 0.3167],
        [0.4919, 0.4027, 0.2936],
        [0.5096, 0.4077, 0.2719],
        [0.4585, 0.3651, 0.3492],
        [0.4881, 0.3792, 0.3149],
        [0.4735, 0.3857, 0.3401],
        [0.5170, 0.4060, 0.2695],
        [0.5228, 0.4180, 0.2497],
        [0.4750, 0.3694, 0.3216],
        [0.4560, 0.3849, 0.3424],
        [0.4900, 0.3739, 0.3287],
        [0.4703, 0.3807, 0.3402],
        [0.4981, 0.4053, 0.2957],
        [0.4919, 0.3824, 0.3108],
        [0.4342, 0.4147, 0.3081],
        [0.4370, 0.4186, 0.3252],
        [0.4880, 0.4024, 0.2872],
        [0.4396, 0.4141, 0.3308],
        [0.4644, 0.3940, 0.3536],
        [0.4310, 0.3889, 0.3797],
        [0.4489, 0.3653, 0.3642],
        [0.4647, 0.4001, 0.3571],
        [0.4970, 0.4036, 0.2918],
        [0.4996, 0.4143, 0.2968],
        [0.5036, 0.4066, 0.2682],
        [0.4995, 0.4013, 0.2903],
        [0.5269, 0.4193, 0.2467],
        [0.4681, 0.3806, 0.3272],
        [0.4878, 0.3969, 0.3156],
        [0.4552, 0.3603, 0.3529],
        [0.4447, 0.4260, 0.3332],
        [0.4770, 0.3970, 0.3171]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 57: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12056.0, Mean: 2227.756103515625, Std: 1712.5125732421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 120457.4453125, Mean: 22257.8125, Std: 17110.697265625
[DEBUG] Top-3 class probabilities:
tensor([[0.4606, 0.3646, 0.3476],
        [0.5143, 0.4258, 0.2709],
        [0.4965, 0.3848, 0.2573],
        [0.4889, 0.3877, 0.3309],
        [0.4666, 0.3791, 0.3414],
        [0.4558, 0.3717, 0.3579],
        [0.4735, 0.4045, 0.3187],
        [0.4484, 0.3856, 0.3546],
        [0.4645, 0.3676, 0.3512],
        [0.4631, 0.3780, 0.3542],
        [0.4574, 0.3823, 0.3375],
        [0.4417, 0.3985, 0.3567],
        [0.4775, 0.3870, 0.3229],
        [0.4968, 0.4115, 0.2957],
        [0.4588, 0.3734, 0.3698],
        [0.4718, 0.3754, 0.3255],
        [0.4768, 0.3787, 0.3446],
        [0.4956, 0.3931, 0.2933],
        [0.5032, 0.3956, 0.2996],
        [0.4960, 0.4029, 0.3082],
        [0.4680, 0.3589, 0.3356],
        [0.4891, 0.4002, 0.3029],
        [0.4843, 0.3911, 0.3274],
        [0.4664, 0.3879, 0.3434],
        [0.4958, 0.3939, 0.3020],
        [0.4902, 0.3986, 0.2998],
        [0.5006, 0.4094, 0.2811],
        [0.4575, 0.3815, 0.3561],
        [0.4692, 0.4025, 0.3292],
        [0.4664, 0.4038, 0.3333],
        [0.4718, 0.3904, 0.3366],
        [0.4904, 0.3617, 0.3112],
        [0.4547, 0.3880, 0.3791],
        [0.4589, 0.3676, 0.3648],
        [0.4737, 0.3656, 0.3225],
        [0.5023, 0.4013, 0.2803],
        [0.4857, 0.3848, 0.3042],
        [0.4889, 0.4003, 0.3173],
        [0.5159, 0.4159, 0.2415],
        [0.4876, 0.4108, 0.2922],
        [0.4574, 0.3755, 0.3511],
        [0.4557, 0.3535, 0.3516],
        [0.4654, 0.3637, 0.3382],
        [0.4377, 0.3915, 0.3538],
        [0.4784, 0.3744, 0.3155],
        [0.5121, 0.4108, 0.2745],
        [0.4774, 0.3687, 0.3102],
        [0.4583, 0.3559, 0.3446],
        [0.4625, 0.3602, 0.3496],
        [0.4880, 0.4001, 0.2967],
        [0.4760, 0.4285, 0.3101],
        [0.4827, 0.4078, 0.3006],
        [0.4802, 0.4048, 0.3256],
        [0.4732, 0.3662, 0.3509],
        [0.4643, 0.3755, 0.3513],
        [0.4738, 0.3657, 0.3319],
        [0.4885, 0.3949, 0.2891],
        [0.4972, 0.4058, 0.2963],
        [0.4592, 0.3726, 0.3697],
        [0.4579, 0.4208, 0.3094],
        [0.4414, 0.3743, 0.3529],
        [0.4770, 0.3798, 0.3345],
        [0.4848, 0.3932, 0.3235],
        [0.4934, 0.4042, 0.2942],
        [0.4808, 0.4002, 0.3123],
        [0.5061, 0.4067, 0.2727],
        [0.4807, 0.3754, 0.3042],
        [0.5017, 0.4033, 0.2971],
        [0.4909, 0.3993, 0.3074],
        [0.4635, 0.3694, 0.3388],
        [0.4525, 0.3874, 0.3752],
        [0.4211, 0.4113, 0.3383],
        [0.4664, 0.3800, 0.3534],
        [0.4712, 0.3879, 0.3251],
        [0.4536, 0.3866, 0.3556],
        [0.4495, 0.4175, 0.3273],
        [0.4243, 0.4131, 0.3402],
        [0.4773, 0.3820, 0.3170],
        [0.4755, 0.3758, 0.3327],
        [0.4563, 0.3701, 0.3593],
        [0.4867, 0.3953, 0.3130],
        [0.4999, 0.4098, 0.2770],
        [0.4718, 0.3685, 0.3297],
        [0.4344, 0.3944, 0.3355],
        [0.4518, 0.3763, 0.3527],
        [0.4735, 0.3753, 0.3204],
        [0.4652, 0.3705, 0.3640],
        [0.4813, 0.3811, 0.3146],
        [0.4836, 0.4329, 0.2930],
        [0.4862, 0.3922, 0.3321],
        [0.4730, 0.3820, 0.3313],
        [0.4693, 0.3784, 0.3494],
        [0.4717, 0.3740, 0.3677],
        [0.4404, 0.3713, 0.3600],
        [0.4402, 0.3889, 0.3701],
        [0.4700, 0.3786, 0.3338],
        [0.4889, 0.3911, 0.3037],
        [0.4910, 0.3871, 0.3136],
        [0.4502, 0.3779, 0.3683],
        [0.4833, 0.4073, 0.3110],
        [0.4715, 0.4076, 0.3187],
        [0.4637, 0.3881, 0.3554],
        [0.5047, 0.3996, 0.2680],
        [0.4761, 0.3752, 0.3333],
        [0.4912, 0.3635, 0.2673],
        [0.4469, 0.3802, 0.3597],
        [0.4947, 0.3762, 0.3155],
        [0.5020, 0.4054, 0.2882],
        [0.4981, 0.4071, 0.2869],
        [0.4654, 0.3992, 0.3445],
        [0.4716, 0.3746, 0.3408],
        [0.4908, 0.3987, 0.3146],
        [0.4825, 0.3785, 0.3293],
        [0.4851, 0.4007, 0.3139],
        [0.4776, 0.3737, 0.3063],
        [0.4535, 0.4011, 0.3277],
        [0.4461, 0.3808, 0.3486],
        [0.4334, 0.4178, 0.3526],
        [0.4460, 0.3793, 0.3548],
        [0.4686, 0.3938, 0.3233],
        [0.4499, 0.3827, 0.3633],
        [0.4505, 0.3841, 0.3558],
        [0.4658, 0.3772, 0.3372],
        [0.4483, 0.3848, 0.3534],
        [0.4511, 0.3911, 0.3585],
        [0.4658, 0.4095, 0.3059],
        [0.4649, 0.3599, 0.3547],
        [0.5049, 0.3937, 0.2753]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6,  8],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 58: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15264.0, Mean: 2179.216064453125, Std: 1660.8382568359375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 152510.421875, Mean: 21772.82421875, Std: 16594.388671875
[DEBUG] Top-3 class probabilities:
tensor([[0.4884, 0.3933, 0.3165],
        [0.4884, 0.3669, 0.3125],
        [0.4568, 0.3707, 0.3461],
        [0.4439, 0.3734, 0.3620],
        [0.4865, 0.4030, 0.3120],
        [0.4591, 0.3634, 0.3541],
        [0.4633, 0.3691, 0.3587],
        [0.4528, 0.3732, 0.3539],
        [0.4353, 0.4040, 0.3250],
        [0.4450, 0.3840, 0.3667],
        [0.4801, 0.3879, 0.3092],
        [0.4587, 0.3654, 0.3571],
        [0.4546, 0.3661, 0.3608],
        [0.4711, 0.3941, 0.3365],
        [0.4771, 0.3875, 0.3304],
        [0.4960, 0.3931, 0.2953],
        [0.5002, 0.4161, 0.2667],
        [0.4439, 0.3750, 0.3684],
        [0.4352, 0.3862, 0.3618],
        [0.4832, 0.4105, 0.3063],
        [0.4602, 0.3642, 0.3519],
        [0.4643, 0.3736, 0.3506],
        [0.4828, 0.3880, 0.3230],
        [0.4819, 0.4004, 0.3131],
        [0.4863, 0.3642, 0.3228],
        [0.4710, 0.3706, 0.3449],
        [0.4768, 0.4052, 0.3159],
        [0.4700, 0.3859, 0.3572],
        [0.4757, 0.3830, 0.3254],
        [0.4792, 0.3912, 0.3316],
        [0.4697, 0.3843, 0.3177],
        [0.4700, 0.3717, 0.3319],
        [0.4272, 0.4010, 0.3539],
        [0.4355, 0.4179, 0.3271],
        [0.4596, 0.3584, 0.3466],
        [0.4797, 0.3514, 0.3428],
        [0.4408, 0.4105, 0.3547],
        [0.4912, 0.4180, 0.2985],
        [0.4511, 0.3803, 0.3486],
        [0.4922, 0.4004, 0.2982],
        [0.4341, 0.3965, 0.3423],
        [0.4376, 0.4139, 0.3330],
        [0.4498, 0.3732, 0.3583],
        [0.4791, 0.3941, 0.3386],
        [0.4559, 0.3724, 0.3382],
        [0.4685, 0.3816, 0.3500],
        [0.4558, 0.3579, 0.3557],
        [0.4348, 0.3686, 0.3585],
        [0.4361, 0.4161, 0.3489],
        [0.5213, 0.4230, 0.2532],
        [0.5038, 0.4033, 0.2869],
        [0.4477, 0.3770, 0.3762],
        [0.4702, 0.3783, 0.3408],
        [0.4805, 0.3837, 0.3359],
        [0.4381, 0.3979, 0.3463],
        [0.4630, 0.3741, 0.3551],
        [0.4622, 0.3846, 0.3400],
        [0.4727, 0.3758, 0.3259],
        [0.4965, 0.3911, 0.2828],
        [0.4922, 0.4090, 0.2817],
        [0.4676, 0.3740, 0.3570],
        [0.4510, 0.3805, 0.3568],
        [0.4454, 0.3772, 0.3640],
        [0.4185, 0.4082, 0.3303],
        [0.4576, 0.3664, 0.3547],
        [0.4817, 0.3780, 0.3144],
        [0.4648, 0.3849, 0.3369],
        [0.4517, 0.3657, 0.3590],
        [0.4868, 0.4018, 0.3052],
        [0.4176, 0.4096, 0.3407],
        [0.4422, 0.3718, 0.3565],
        [0.4643, 0.3762, 0.3570],
        [0.4473, 0.3904, 0.3597],
        [0.4891, 0.3869, 0.2997],
        [0.4734, 0.3827, 0.3314],
        [0.4700, 0.3782, 0.3326],
        [0.4501, 0.3532, 0.3432],
        [0.4475, 0.3829, 0.3614],
        [0.4407, 0.4073, 0.3603],
        [0.4363, 0.3985, 0.3353],
        [0.4568, 0.3717, 0.3671],
        [0.4511, 0.4101, 0.3362],
        [0.4567, 0.3744, 0.3717],
        [0.4712, 0.3869, 0.3320],
        [0.4477, 0.3565, 0.3448],
        [0.4323, 0.3940, 0.3481],
        [0.4432, 0.3769, 0.3617],
        [0.4379, 0.4005, 0.3528],
        [0.4708, 0.3658, 0.3575],
        [0.4472, 0.3793, 0.3650],
        [0.4753, 0.3910, 0.3390],
        [0.4307, 0.3981, 0.3480],
        [0.4903, 0.3966, 0.3107],
        [0.4571, 0.3727, 0.3351],
        [0.4577, 0.3692, 0.3442],
        [0.4501, 0.3766, 0.3619],
        [0.4553, 0.3674, 0.3563],
        [0.4473, 0.3884, 0.3549],
        [0.4530, 0.3741, 0.3741],
        [0.4283, 0.3980, 0.3338],
        [0.4701, 0.3874, 0.3433],
        [0.4668, 0.3739, 0.3482],
        [0.4942, 0.3911, 0.3100],
        [0.4881, 0.3952, 0.3059],
        [0.4391, 0.3824, 0.3472],
        [0.4191, 0.4048, 0.3695],
        [0.4844, 0.4149, 0.3034],
        [0.4228, 0.4114, 0.3417],
        [0.4530, 0.3761, 0.3702],
        [0.4989, 0.4073, 0.2903],
        [0.4301, 0.4280, 0.3300],
        [0.4809, 0.3741, 0.3284],
        [0.4958, 0.3957, 0.3021],
        [0.4750, 0.3539, 0.3327],
        [0.4727, 0.3953, 0.2997],
        [0.4508, 0.3763, 0.3721],
        [0.4589, 0.3783, 0.3722],
        [0.4221, 0.4198, 0.3352],
        [0.4339, 0.4264, 0.3321],
        [0.4406, 0.3953, 0.3510],
        [0.4364, 0.4223, 0.3197],
        [0.4925, 0.4003, 0.3044],
        [0.5013, 0.4099, 0.2774],
        [0.4307, 0.4003, 0.3576],
        [0.4790, 0.3802, 0.3209],
        [0.4332, 0.4025, 0.3586],
        [0.4343, 0.4141, 0.3231],
        [0.4464, 0.3655, 0.3644]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 59: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18976.0, Mean: 2164.977783203125, Std: 1646.948974609375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 189599.140625, Mean: 21630.556640625, Std: 16455.61328125
[DEBUG] Top-3 class probabilities:
tensor([[0.4493, 0.3687, 0.3684],
        [0.4596, 0.3645, 0.3546],
        [0.4655, 0.3820, 0.3699],
        [0.4658, 0.3892, 0.3477],
        [0.4676, 0.3896, 0.3457],
        [0.4759, 0.3796, 0.3364],
        [0.4531, 0.3826, 0.3571],
        [0.4885, 0.3930, 0.3090],
        [0.4714, 0.3789, 0.3483],
        [0.4637, 0.3622, 0.3535],
        [0.4544, 0.3566, 0.3539],
        [0.4313, 0.4049, 0.3429],
        [0.4571, 0.3697, 0.3650],
        [0.4450, 0.3802, 0.3509],
        [0.4696, 0.3905, 0.3205],
        [0.4588, 0.3749, 0.3512],
        [0.4159, 0.4138, 0.3169],
        [0.4708, 0.3737, 0.3339],
        [0.4613, 0.3859, 0.3330],
        [0.4937, 0.4027, 0.3002],
        [0.5069, 0.4085, 0.2638],
        [0.4724, 0.3821, 0.3288],
        [0.4533, 0.3808, 0.3539],
        [0.5084, 0.4078, 0.2491],
        [0.4471, 0.3751, 0.3490],
        [0.4663, 0.3710, 0.3622],
        [0.4820, 0.3761, 0.3385],
        [0.4300, 0.4156, 0.3301],
        [0.4604, 0.3655, 0.3451],
        [0.4771, 0.3713, 0.3173],
        [0.4689, 0.3714, 0.3484],
        [0.5255, 0.3736, 0.2808],
        [0.4336, 0.3721, 0.3416],
        [0.4586, 0.3463, 0.3417],
        [0.4968, 0.3981, 0.2954],
        [0.4923, 0.3860, 0.3096],
        [0.4544, 0.3644, 0.3423],
        [0.4354, 0.4296, 0.3344],
        [0.4536, 0.3646, 0.3586],
        [0.4556, 0.3545, 0.3492],
        [0.4718, 0.3958, 0.3213],
        [0.4913, 0.3873, 0.3294],
        [0.4515, 0.3763, 0.3744],
        [0.4934, 0.3988, 0.3075],
        [0.4770, 0.3772, 0.3288],
        [0.4473, 0.3775, 0.3555],
        [0.4572, 0.4010, 0.3547],
        [0.4522, 0.3793, 0.3660],
        [0.4357, 0.4125, 0.3534],
        [0.4648, 0.3743, 0.3499],
        [0.4426, 0.4050, 0.3566],
        [0.4650, 0.3701, 0.3468],
        [0.4721, 0.3706, 0.3409],
        [0.4633, 0.3760, 0.3532],
        [0.4572, 0.3592, 0.3566],
        [0.4614, 0.3880, 0.3633],
        [0.4460, 0.3865, 0.3685],
        [0.4441, 0.4147, 0.3285],
        [0.4367, 0.4058, 0.3549],
        [0.4316, 0.4195, 0.3317],
        [0.4376, 0.3967, 0.3322],
        [0.4601, 0.3564, 0.3534],
        [0.4823, 0.4100, 0.3047],
        [0.4897, 0.3926, 0.2863],
        [0.4668, 0.3586, 0.3365],
        [0.4775, 0.3987, 0.3158],
        [0.4703, 0.3739, 0.3229],
        [0.4786, 0.4025, 0.3153],
        [0.4744, 0.3750, 0.3312],
        [0.4524, 0.3574, 0.3550],
        [0.4562, 0.3778, 0.3395],
        [0.4881, 0.4026, 0.3003],
        [0.4786, 0.3717, 0.3189],
        [0.4786, 0.3855, 0.3223],
        [0.4635, 0.3703, 0.3560],
        [0.4518, 0.3894, 0.3732],
        [0.4527, 0.3888, 0.3635],
        [0.4385, 0.3962, 0.3647],
        [0.4908, 0.4246, 0.2890],
        [0.4592, 0.3716, 0.3641],
        [0.4661, 0.3781, 0.3363],
        [0.4656, 0.3817, 0.3529],
        [0.5011, 0.3817, 0.2861],
        [0.4339, 0.3925, 0.3383],
        [0.4219, 0.4183, 0.3313],
        [0.4633, 0.3736, 0.3569],
        [0.4689, 0.3749, 0.3558],
        [0.4451, 0.3968, 0.3394],
        [0.4329, 0.4292, 0.3292],
        [0.4371, 0.3916, 0.3636],
        [0.4675, 0.3687, 0.3548],
        [0.4595, 0.3774, 0.3412],
        [0.4595, 0.3645, 0.3607],
        [0.4635, 0.3700, 0.3442],
        [0.4666, 0.3997, 0.3166],
        [0.4670, 0.3793, 0.3367],
        [0.4570, 0.3797, 0.3792],
        [0.4699, 0.3649, 0.3253],
        [0.4277, 0.4120, 0.3584],
        [0.4755, 0.3964, 0.3205],
        [0.4691, 0.3750, 0.3252],
        [0.4267, 0.4253, 0.3200],
        [0.4487, 0.3607, 0.3510],
        [0.4521, 0.3635, 0.3608],
        [0.4881, 0.3794, 0.3137],
        [0.4910, 0.4094, 0.3060],
        [0.4630, 0.3580, 0.3317],
        [0.4890, 0.3862, 0.3128],
        [0.4968, 0.3966, 0.2738],
        [0.5168, 0.4159, 0.2726],
        [0.4619, 0.3742, 0.3678],
        [0.4578, 0.3866, 0.3520],
        [0.4658, 0.3795, 0.3329],
        [0.4426, 0.3794, 0.3628],
        [0.4285, 0.3680, 0.3228],
        [0.4326, 0.4158, 0.3126],
        [0.4263, 0.4178, 0.3516],
        [0.4317, 0.4006, 0.3695],
        [0.4698, 0.4332, 0.3297],
        [0.4238, 0.4111, 0.3686],
        [0.4595, 0.3730, 0.3576],
        [0.4394, 0.3719, 0.3698],
        [0.4705, 0.4053, 0.3238],
        [0.4390, 0.4043, 0.3437],
        [0.5042, 0.3760, 0.2687],
        [0.5119, 0.3863, 0.2884],
        [0.5411, 0.3841, 0.2813],
        [0.4708, 0.3992, 0.3052]])
[DEBUG] Top-3 class indices:
tensor([[17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 60: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17520.0, Mean: 2147.77880859375, Std: 1599.6788330078125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 175051.40625, Mean: 21458.7109375, Std: 15983.3095703125
[DEBUG] Top-3 class probabilities:
tensor([[0.4430, 0.3747, 0.3482],
        [0.4542, 0.3770, 0.3429],
        [0.4331, 0.3974, 0.3591],
        [0.4234, 0.4137, 0.3390],
        [0.4389, 0.4192, 0.3312],
        [0.4446, 0.4110, 0.3307],
        [0.4387, 0.3823, 0.3611],
        [0.4455, 0.3711, 0.3477],
        [0.4548, 0.3736, 0.3534],
        [0.4617, 0.3700, 0.3574],
        [0.4685, 0.3753, 0.3388],
        [0.4525, 0.3678, 0.3548],
        [0.4679, 0.3887, 0.3568],
        [0.4858, 0.3997, 0.3070],
        [0.4393, 0.4122, 0.3286],
        [0.4528, 0.3887, 0.3633],
        [0.4519, 0.3611, 0.3608],
        [0.4641, 0.3964, 0.3300],
        [0.4680, 0.3996, 0.3239],
        [0.4696, 0.3849, 0.3319],
        [0.4951, 0.3926, 0.2793],
        [0.4637, 0.3716, 0.3579],
        [0.4736, 0.3572, 0.3125],
        [0.4872, 0.3898, 0.3097],
        [0.4814, 0.3819, 0.3181],
        [0.5089, 0.4059, 0.2781],
        [0.4815, 0.3984, 0.3179],
        [0.5015, 0.4008, 0.3077],
        [0.5032, 0.4049, 0.2901],
        [0.4492, 0.3707, 0.3577],
        [0.4913, 0.3626, 0.2726],
        [0.5305, 0.3742, 0.2733],
        [0.4447, 0.4214, 0.3341],
        [0.4691, 0.3746, 0.3620],
        [0.4949, 0.3846, 0.3013],
        [0.5118, 0.4171, 0.2543],
        [0.4803, 0.4070, 0.3220],
        [0.4787, 0.3831, 0.3239],
        [0.4733, 0.4139, 0.3484],
        [0.4438, 0.3990, 0.3471],
        [0.4309, 0.3664, 0.3206],
        [0.5496, 0.3743, 0.2732],
        [0.5211, 0.3816, 0.2799],
        [0.4721, 0.3837, 0.3219],
        [0.4520, 0.3691, 0.3487],
        [0.4819, 0.3956, 0.3272],
        [0.4584, 0.3709, 0.3537],
        [0.4563, 0.3683, 0.3572],
        [0.4537, 0.3714, 0.3542],
        [0.4759, 0.4039, 0.3049],
        [0.4501, 0.4020, 0.3203],
        [0.4468, 0.3782, 0.3640],
        [0.4671, 0.3743, 0.3557],
        [0.4718, 0.3773, 0.3350],
        [0.4909, 0.3762, 0.2927],
        [0.4569, 0.3744, 0.3632],
        [0.4862, 0.4028, 0.2876],
        [0.5684, 0.3616, 0.2623],
        [0.5006, 0.3772, 0.2802],
        [0.4473, 0.3856, 0.3492],
        [0.4341, 0.3944, 0.3208],
        [0.4424, 0.3888, 0.3804],
        [0.4772, 0.3739, 0.3316],
        [0.4685, 0.3693, 0.3269],
        [0.4598, 0.3719, 0.3575],
        [0.4915, 0.3590, 0.2987],
        [0.4868, 0.3928, 0.2943],
        [0.4906, 0.4027, 0.2881],
        [0.4407, 0.3783, 0.3494],
        [0.4937, 0.3836, 0.3168],
        [0.4813, 0.3836, 0.3100],
        [0.4756, 0.3619, 0.3479],
        [0.4712, 0.3715, 0.3693],
        [0.4623, 0.3716, 0.3452],
        [0.4329, 0.3988, 0.3436],
        [0.4634, 0.3851, 0.3377],
        [0.4271, 0.4225, 0.3445],
        [0.4683, 0.3961, 0.3328],
        [0.4798, 0.3815, 0.3375],
        [0.4653, 0.3702, 0.3490],
        [0.4881, 0.3971, 0.3108],
        [0.4672, 0.3752, 0.3406],
        [0.4810, 0.4146, 0.3226],
        [0.4395, 0.4225, 0.3085],
        [0.5221, 0.3691, 0.2820],
        [0.5003, 0.4042, 0.3025],
        [0.5355, 0.3693, 0.2771],
        [0.4397, 0.4085, 0.3413],
        [0.4196, 0.4089, 0.3203],
        [0.4276, 0.4025, 0.3610],
        [0.4569, 0.3816, 0.3564],
        [0.4498, 0.3693, 0.3616],
        [0.4270, 0.4129, 0.3301],
        [0.4671, 0.3767, 0.3373],
        [0.4391, 0.3941, 0.3504],
        [0.4652, 0.3708, 0.3371],
        [0.4615, 0.3670, 0.3638],
        [0.4586, 0.3847, 0.3665],
        [0.4890, 0.4056, 0.3148],
        [0.4196, 0.3918, 0.3294],
        [0.5824, 0.3596, 0.2502],
        [0.6451, 0.3172, 0.2113],
        [0.5185, 0.3785, 0.2779],
        [0.4714, 0.3946, 0.3214],
        [0.4875, 0.3779, 0.3258],
        [0.4573, 0.3550, 0.3460],
        [0.4938, 0.4061, 0.2936],
        [0.4304, 0.4272, 0.3443],
        [0.4632, 0.3717, 0.3547],
        [0.4391, 0.3906, 0.3473],
        [0.4253, 0.4165, 0.3261],
        [0.4342, 0.3978, 0.3226],
        [0.5061, 0.4054, 0.2902],
        [0.4793, 0.3798, 0.3270],
        [0.5012, 0.3941, 0.2812],
        [0.4824, 0.3744, 0.3153],
        [0.4777, 0.3668, 0.3400],
        [0.4290, 0.4004, 0.3371],
        [0.4487, 0.3785, 0.3532],
        [0.4921, 0.4167, 0.3112],
        [0.4518, 0.3780, 0.3662],
        [0.4831, 0.3883, 0.3240],
        [0.4814, 0.3880, 0.3049],
        [0.4653, 0.3653, 0.3596],
        [0.4403, 0.4001, 0.3659],
        [0.4722, 0.4307, 0.3317],
        [0.4563, 0.4248, 0.3493],
        [0.4563, 0.3626, 0.3578]])
[DEBUG] Top-3 class indices:
tensor([[17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6,  8],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 61: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20608.0, Mean: 2136.595458984375, Std: 1600.892822265625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 205905.390625, Mean: 21346.96875, Std: 15995.4404296875
[DEBUG] Top-3 class probabilities:
tensor([[0.4773, 0.3954, 0.2893],
        [0.4646, 0.4017, 0.2987],
        [0.4383, 0.3995, 0.2947],
        [0.5198, 0.3572, 0.2544],
        [0.4631, 0.3695, 0.3648],
        [0.4682, 0.4091, 0.3433],
        [0.4693, 0.3844, 0.3198],
        [0.4558, 0.3705, 0.3636],
        [0.4286, 0.4008, 0.3469],
        [0.4244, 0.4040, 0.3488],
        [0.4491, 0.3853, 0.3655],
        [0.4297, 0.4123, 0.3214],
        [0.4663, 0.3711, 0.3321],
        [0.4893, 0.3813, 0.3115],
        [0.4569, 0.3529, 0.3504],
        [0.4842, 0.3991, 0.2981],
        [0.4593, 0.3751, 0.3653],
        [0.4671, 0.4002, 0.2826],
        [0.5922, 0.3473, 0.2348],
        [0.5018, 0.3859, 0.2956],
        [0.4692, 0.3565, 0.3306],
        [0.4774, 0.3912, 0.3163],
        [0.4751, 0.3669, 0.3243],
        [0.4327, 0.4049, 0.3645],
        [0.4808, 0.4010, 0.3138],
        [0.4567, 0.3700, 0.3641],
        [0.4660, 0.3659, 0.3350],
        [0.4763, 0.3789, 0.3355],
        [0.4445, 0.3804, 0.3517],
        [0.4475, 0.3824, 0.3513],
        [0.4781, 0.3804, 0.3303],
        [0.4478, 0.3702, 0.3662],
        [0.4935, 0.4053, 0.3021],
        [0.4479, 0.3888, 0.3284],
        [0.4606, 0.3613, 0.3568],
        [0.4412, 0.3764, 0.3652],
        [0.4854, 0.3800, 0.3068],
        [0.4636, 0.3692, 0.3394],
        [0.4605, 0.3863, 0.3642],
        [0.4676, 0.3636, 0.3321],
        [0.4504, 0.3838, 0.3569],
        [0.4546, 0.4016, 0.3296],
        [0.4274, 0.4208, 0.3691],
        [0.4746, 0.3626, 0.2913],
        [0.4393, 0.3642, 0.3416],
        [0.4550, 0.3890, 0.2942],
        [0.4163, 0.3511, 0.3291],
        [0.4321, 0.3962, 0.3643],
        [0.4317, 0.4240, 0.3472],
        [0.4739, 0.3798, 0.3221],
        [0.4603, 0.3843, 0.3682],
        [0.4260, 0.4231, 0.3212],
        [0.4765, 0.3959, 0.3024],
        [0.4432, 0.3895, 0.3618],
        [0.4563, 0.3671, 0.3522],
        [0.4477, 0.3753, 0.3668],
        [0.4406, 0.3878, 0.3648],
        [0.4369, 0.3976, 0.3631],
        [0.4623, 0.3674, 0.3609],
        [0.4254, 0.4230, 0.3417],
        [0.4495, 0.3344, 0.3279],
        [0.4512, 0.4007, 0.3151],
        [0.5541, 0.3650, 0.2577],
        [0.4417, 0.3914, 0.3829],
        [0.4256, 0.4084, 0.3646],
        [0.4524, 0.3796, 0.3612],
        [0.4549, 0.3795, 0.3686],
        [0.4600, 0.3833, 0.3511],
        [0.4604, 0.3659, 0.3473],
        [0.4798, 0.3895, 0.3309],
        [0.4782, 0.3843, 0.3053],
        [0.4570, 0.3746, 0.3556],
        [0.4962, 0.3994, 0.3004],
        [0.4962, 0.3913, 0.2884],
        [0.4476, 0.3806, 0.3685],
        [0.4344, 0.4071, 0.3541],
        [0.4855, 0.4092, 0.3275],
        [0.4190, 0.4145, 0.3597],
        [0.4495, 0.3864, 0.3724],
        [0.4655, 0.3831, 0.3184],
        [0.4464, 0.3704, 0.3645],
        [0.4909, 0.3902, 0.2980],
        [0.4650, 0.3800, 0.3501],
        [0.4493, 0.3774, 0.3771],
        [0.4604, 0.4089, 0.3120],
        [0.4269, 0.4153, 0.3397],
        [0.4575, 0.3947, 0.3057],
        [0.4459, 0.4219, 0.3211],
        [0.4306, 0.3950, 0.3436],
        [0.4373, 0.4158, 0.3215],
        [0.4536, 0.3721, 0.3555],
        [0.4607, 0.3768, 0.3526],
        [0.4469, 0.3836, 0.3567],
        [0.4472, 0.3808, 0.3772],
        [0.4242, 0.4125, 0.3515],
        [0.4370, 0.3950, 0.3760],
        [0.4557, 0.3736, 0.3724],
        [0.4308, 0.4046, 0.3338],
        [0.4671, 0.3555, 0.3553],
        [0.4768, 0.3780, 0.3275],
        [0.4344, 0.3897, 0.3710],
        [0.4352, 0.4201, 0.3438],
        [0.4594, 0.3911, 0.3414],
        [0.5222, 0.3679, 0.2723],
        [0.4268, 0.4076, 0.3396],
        [0.4873, 0.3821, 0.3050],
        [0.4397, 0.3802, 0.3429],
        [0.4631, 0.3659, 0.3564],
        [0.4597, 0.3673, 0.3520],
        [0.4844, 0.4104, 0.3056],
        [0.4361, 0.3849, 0.3664],
        [0.4871, 0.3809, 0.3217],
        [0.4633, 0.3810, 0.3512],
        [0.4540, 0.3790, 0.3583],
        [0.4428, 0.3929, 0.3737],
        [0.4523, 0.3700, 0.3699],
        [0.4653, 0.4000, 0.3466],
        [0.4440, 0.3836, 0.3713],
        [0.4939, 0.3973, 0.2822],
        [0.4870, 0.3900, 0.2960],
        [0.4483, 0.3816, 0.3712],
        [0.4708, 0.3646, 0.3488],
        [0.4606, 0.3695, 0.3574],
        [0.4776, 0.3893, 0.3185],
        [0.4780, 0.3792, 0.3179],
        [0.4531, 0.3983, 0.3571],
        [0.4667, 0.3654, 0.3449],
        [0.4494, 0.3701, 0.3685]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6,  8],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  8],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 62: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17808.0, Mean: 1890.722412109375, Std: 1540.94970703125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 177928.984375, Mean: 18890.3125, Std: 15396.513671875
[DEBUG] Top-3 class probabilities:
tensor([[0.4465, 0.3882, 0.3684],
        [0.4715, 0.3931, 0.3408],
        [0.4506, 0.3846, 0.3495],
        [0.4673, 0.4127, 0.3081],
        [0.5017, 0.3771, 0.2871],
        [0.5713, 0.3515, 0.2454],
        [0.5499, 0.3659, 0.2644],
        [0.4379, 0.3788, 0.3470],
        [0.4670, 0.3760, 0.3158],
        [0.4748, 0.3806, 0.3211],
        [0.4288, 0.4044, 0.3447],
        [0.4371, 0.4133, 0.3177],
        [0.4335, 0.3993, 0.3367],
        [0.4833, 0.3995, 0.3082],
        [0.4454, 0.3826, 0.3454],
        [0.4751, 0.3693, 0.3326],
        [0.4295, 0.4265, 0.3260],
        [0.4779, 0.3980, 0.3183],
        [0.4588, 0.4144, 0.3246],
        [0.5026, 0.3663, 0.2749],
        [0.4669, 0.3495, 0.3286],
        [0.4336, 0.3894, 0.3505],
        [0.4613, 0.3696, 0.3578],
        [0.5031, 0.3763, 0.2653],
        [0.4707, 0.4031, 0.3099],
        [0.4669, 0.3612, 0.3513],
        [0.4395, 0.4045, 0.3501],
        [0.4715, 0.3786, 0.3426],
        [0.4309, 0.4049, 0.3494],
        [0.4601, 0.3718, 0.3495],
        [0.5020, 0.3959, 0.2636],
        [0.4361, 0.3943, 0.3543],
        [0.4601, 0.3793, 0.3595],
        [0.4614, 0.3636, 0.3633],
        [0.4208, 0.4052, 0.3412],
        [0.4824, 0.3923, 0.3193],
        [0.4558, 0.3657, 0.3638],
        [0.5035, 0.4056, 0.2758],
        [0.4539, 0.3883, 0.3802],
        [0.4480, 0.3693, 0.3613],
        [0.4417, 0.3744, 0.3607],
        [0.4418, 0.3943, 0.3505],
        [0.4429, 0.3877, 0.3471],
        [0.4587, 0.3737, 0.3685],
        [0.4353, 0.4186, 0.3480],
        [0.4311, 0.4063, 0.3494],
        [0.4295, 0.4167, 0.3371],
        [0.4350, 0.4102, 0.3595],
        [0.5146, 0.3673, 0.2762],
        [0.5444, 0.3703, 0.2593],
        [0.4403, 0.4159, 0.3288],
        [0.4448, 0.3977, 0.3453],
        [0.4449, 0.3843, 0.3653],
        [0.4457, 0.3679, 0.3649],
        [0.4480, 0.3691, 0.3521],
        [0.5274, 0.3813, 0.2772],
        [0.4363, 0.4127, 0.3431],
        [0.4624, 0.4027, 0.2981],
        [0.4426, 0.3778, 0.3647],
        [0.4450, 0.3869, 0.3612],
        [0.4466, 0.3690, 0.3522],
        [0.4434, 0.3801, 0.3234],
        [0.4491, 0.4046, 0.3032],
        [0.4864, 0.4027, 0.3037],
        [0.4575, 0.3763, 0.3561],
        [0.4752, 0.3845, 0.3242],
        [0.4643, 0.3978, 0.3383],
        [0.4567, 0.3716, 0.3587],
        [0.4608, 0.3799, 0.3416],
        [0.4719, 0.3729, 0.3459],
        [0.4615, 0.4030, 0.3591],
        [0.4349, 0.4203, 0.3299],
        [0.4611, 0.3841, 0.3382],
        [0.4538, 0.3823, 0.3422],
        [0.4772, 0.3924, 0.3138],
        [0.4451, 0.3921, 0.3586],
        [0.4546, 0.3722, 0.3599],
        [0.4478, 0.3731, 0.3522],
        [0.5480, 0.3515, 0.2500],
        [0.5921, 0.3537, 0.2414],
        [0.5644, 0.3577, 0.2524],
        [0.7230, 0.2703, 0.2149],
        [0.7195, 0.2872, 0.2148],
        [0.7156, 0.2883, 0.2175],
        [0.7166, 0.2863, 0.2178],
        [0.7175, 0.2844, 0.2175],
        [0.7232, 0.2810, 0.2130],
        [0.5647, 0.3372, 0.2485],
        [0.5336, 0.3470, 0.2393],
        [0.7305, 0.2502, 0.2085],
        [0.7306, 0.2574, 0.2046],
        [0.7334, 0.2451, 0.2080],
        [0.4763, 0.3923, 0.2984],
        [0.4559, 0.4126, 0.3101],
        [0.4295, 0.3939, 0.3412],
        [0.4593, 0.3732, 0.3609],
        [0.4349, 0.4160, 0.3411],
        [0.4661, 0.3777, 0.3528],
        [0.4327, 0.3865, 0.3489],
        [0.4785, 0.3853, 0.3241],
        [0.4739, 0.3818, 0.3423],
        [0.4684, 0.3535, 0.3237],
        [0.5031, 0.4235, 0.2732],
        [0.4724, 0.3728, 0.3327],
        [0.4481, 0.3741, 0.3682],
        [0.4936, 0.3845, 0.2953],
        [0.4888, 0.3922, 0.2821],
        [0.4634, 0.3681, 0.3142],
        [0.4404, 0.4142, 0.3511],
        [0.4577, 0.4208, 0.3317],
        [0.4460, 0.3631, 0.3624],
        [0.4987, 0.3841, 0.2984],
        [0.5369, 0.3704, 0.2707],
        [0.5352, 0.3693, 0.2520],
        [0.5692, 0.3666, 0.2586],
        [0.6080, 0.3187, 0.2301],
        [0.5360, 0.3369, 0.3019],
        [0.6546, 0.2815, 0.2094],
        [0.5262, 0.3418, 0.2895],
        [0.5594, 0.3323, 0.2642],
        [0.6709, 0.2704, 0.2228],
        [0.5970, 0.3240, 0.2369],
        [0.5494, 0.3379, 0.2779],
        [0.6444, 0.2810, 0.2172],
        [0.5286, 0.3633, 0.3049],
        [0.5870, 0.3122, 0.2467],
        [0.6001, 0.2998, 0.2336],
        [0.6581, 0.2682, 0.2246]])
[DEBUG] Top-3 class indices:
tensor([[17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6,  8],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 63: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 44.0, Max: 15146.0, Mean: 1791.29296875, Std: 1332.639404296875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 438.64898681640625, Max: 151331.40625, Mean: 17896.861328125, Std: 13315.1650390625
[DEBUG] Top-3 class probabilities:
tensor([[0.6658, 0.2677, 0.2260],
        [0.5807, 0.3110, 0.2325],
        [0.6284, 0.2729, 0.2059],
        [0.5109, 0.3452, 0.2989],
        [0.5719, 0.3243, 0.2560],
        [0.5009, 0.3648, 0.2995],
        [0.5951, 0.3194, 0.2394],
        [0.5821, 0.3200, 0.2512],
        [0.5420, 0.3289, 0.2824],
        [0.5106, 0.3605, 0.3041],
        [0.5765, 0.3398, 0.2574],
        [0.5435, 0.3452, 0.2654],
        [0.5992, 0.3083, 0.2292],
        [0.6594, 0.2743, 0.2198],
        [0.5200, 0.3431, 0.2912],
        [0.5951, 0.3222, 0.2386],
        [0.5780, 0.3194, 0.2412],
        [0.5939, 0.3091, 0.2415],
        [0.5324, 0.3449, 0.2829],
        [0.4963, 0.3593, 0.3081],
        [0.5189, 0.3434, 0.2839],
        [0.6068, 0.3082, 0.2304],
        [0.6039, 0.3123, 0.2332],
        [0.5383, 0.3526, 0.2808],
        [0.5363, 0.3467, 0.2878],
        [0.4598, 0.3760, 0.3587],
        [0.5469, 0.3307, 0.2748],
        [0.5754, 0.3232, 0.2600],
        [0.5663, 0.3271, 0.2668],
        [0.4549, 0.4198, 0.3291],
        [0.4721, 0.3866, 0.2986],
        [0.5438, 0.3583, 0.2714],
        [0.6649, 0.2617, 0.2155],
        [0.6270, 0.2728, 0.2124],
        [0.6503, 0.2821, 0.2091],
        [0.5328, 0.3653, 0.2755],
        [0.5309, 0.3447, 0.2901],
        [0.5279, 0.3478, 0.2888],
        [0.5176, 0.3571, 0.2891],
        [0.5612, 0.3300, 0.2600],
        [0.5461, 0.3387, 0.2678],
        [0.4973, 0.3638, 0.3254],
        [0.6124, 0.2992, 0.2311],
        [0.5556, 0.3392, 0.2431],
        [0.5670, 0.3276, 0.2577],
        [0.5463, 0.3370, 0.2826],
        [0.5092, 0.3561, 0.3246],
        [0.5733, 0.3224, 0.2588],
        [0.5588, 0.3384, 0.2689],
        [0.4418, 0.4198, 0.3418],
        [0.4233, 0.4231, 0.3215],
        [0.4550, 0.3979, 0.3302],
        [0.6563, 0.2669, 0.2010],
        [0.6658, 0.2623, 0.2078],
        [0.6072, 0.3086, 0.2219],
        [0.5359, 0.3494, 0.2896],
        [0.5050, 0.3686, 0.2945],
        [0.4716, 0.3860, 0.3289],
        [0.4978, 0.4121, 0.2835],
        [0.4926, 0.3852, 0.2860],
        [0.5035, 0.3617, 0.3001],
        [0.5115, 0.3555, 0.3007],
        [0.5316, 0.3358, 0.2641],
        [0.5070, 0.3429, 0.2857],
        [0.5300, 0.3730, 0.2860],
        [0.5340, 0.3715, 0.2942],
        [0.5531, 0.3371, 0.2852],
        [0.5548, 0.3296, 0.2717],
        [0.5817, 0.3179, 0.2513],
        [0.4901, 0.3625, 0.3261],
        [0.4613, 0.3930, 0.3325],
        [0.4734, 0.3829, 0.2771],
        [0.4972, 0.3873, 0.2878],
        [0.6019, 0.3221, 0.2318],
        [0.7042, 0.2453, 0.2191],
        [0.6316, 0.2836, 0.2186],
        [0.5454, 0.3479, 0.2608],
        [0.5075, 0.3653, 0.2872],
        [0.5302, 0.3637, 0.2620],
        [0.4539, 0.3567, 0.3510],
        [0.4309, 0.4147, 0.3289],
        [0.4798, 0.3894, 0.3124],
        [0.4906, 0.3936, 0.2893],
        [0.4583, 0.3847, 0.2776],
        [0.5606, 0.3422, 0.2760],
        [0.5448, 0.3451, 0.2779],
        [0.5123, 0.3483, 0.3122],
        [0.6484, 0.2802, 0.2261],
        [0.6185, 0.2959, 0.2287],
        [0.5895, 0.3170, 0.2683],
        [0.5055, 0.3898, 0.2961],
        [0.5436, 0.3672, 0.2567],
        [0.5264, 0.3602, 0.2759],
        [0.5761, 0.3382, 0.2539],
        [0.4886, 0.3705, 0.3225],
        [0.4941, 0.3786, 0.3262],
        [0.5082, 0.3731, 0.2910],
        [0.5465, 0.3486, 0.2474],
        [0.5600, 0.3466, 0.2501],
        [0.5343, 0.3600, 0.2610],
        [0.5644, 0.3481, 0.2397],
        [0.5796, 0.3362, 0.2389],
        [0.5041, 0.3762, 0.2724],
        [0.5335, 0.3595, 0.2619],
        [0.4268, 0.4131, 0.3203],
        [0.4846, 0.3944, 0.2879],
        [0.5077, 0.4274, 0.2483],
        [0.5120, 0.4432, 0.2438],
        [0.4994, 0.3870, 0.2932],
        [0.5916, 0.3238, 0.2324],
        [0.5549, 0.3561, 0.2719],
        [0.5491, 0.3465, 0.2678],
        [0.5640, 0.3186, 0.2656],
        [0.6143, 0.3081, 0.2404],
        [0.5832, 0.2994, 0.2315],
        [0.6423, 0.2835, 0.2269],
        [0.6280, 0.2885, 0.2372],
        [0.5229, 0.3470, 0.2804],
        [0.6063, 0.3147, 0.2332],
        [0.4572, 0.4059, 0.3154],
        [0.5442, 0.3490, 0.2642],
        [0.6688, 0.2752, 0.2242],
        [0.5026, 0.3778, 0.3107],
        [0.4620, 0.3927, 0.3185],
        [0.5229, 0.3683, 0.2909],
        [0.4790, 0.3918, 0.3215],
        [0.4822, 0.3887, 0.3152],
        [0.4864, 0.3940, 0.3051]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 64: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 38.0, Max: 15136.0, Mean: 1926.298828125, Std: 1394.548095703125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 378.6995544433594, Max: 151231.5, Mean: 19245.77734375, Std: 13933.7314453125
[DEBUG] Top-3 class probabilities:
tensor([[0.5438, 0.3676, 0.2683],
        [0.5354, 0.3628, 0.2775],
        [0.4979, 0.3847, 0.3068],
        [0.5469, 0.3592, 0.2601],
        [0.5729, 0.3368, 0.2443],
        [0.5736, 0.3443, 0.2453],
        [0.5546, 0.3563, 0.2568],
        [0.5639, 0.3376, 0.2416],
        [0.5061, 0.3695, 0.2631],
        [0.5797, 0.3352, 0.2293],
        [0.5117, 0.3679, 0.2766],
        [0.4761, 0.3848, 0.2881],
        [0.4606, 0.3983, 0.2952],
        [0.4593, 0.3907, 0.3393],
        [0.4986, 0.3812, 0.2947],
        [0.5949, 0.3171, 0.2328],
        [0.7084, 0.2279, 0.2124],
        [0.5001, 0.3660, 0.2952],
        [0.6834, 0.2628, 0.2217],
        [0.5941, 0.3135, 0.2298],
        [0.5239, 0.3549, 0.2700],
        [0.5162, 0.3721, 0.2975],
        [0.4601, 0.4074, 0.3293],
        [0.6259, 0.2922, 0.2245],
        [0.5858, 0.3069, 0.2442],
        [0.4278, 0.4037, 0.3479],
        [0.4450, 0.3955, 0.2992],
        [0.5136, 0.3567, 0.2854],
        [0.6158, 0.2982, 0.2116],
        [0.4789, 0.3826, 0.3178],
        [0.4364, 0.4064, 0.3439],
        [0.4875, 0.3910, 0.3145],
        [0.5021, 0.3660, 0.3071],
        [0.5518, 0.3527, 0.2551],
        [0.5473, 0.3583, 0.2689],
        [0.4846, 0.3838, 0.3043],
        [0.5206, 0.3714, 0.2761],
        [0.4755, 0.3881, 0.3073],
        [0.5281, 0.3617, 0.2620],
        [0.4938, 0.3702, 0.2847],
        [0.5447, 0.3617, 0.2815],
        [0.5056, 0.3725, 0.2946],
        [0.5387, 0.3548, 0.2765],
        [0.5435, 0.3464, 0.2633],
        [0.4848, 0.3740, 0.2804],
        [0.5265, 0.3684, 0.2845],
        [0.4911, 0.3795, 0.2689],
        [0.4526, 0.3984, 0.3060],
        [0.4179, 0.4063, 0.3618],
        [0.4665, 0.4021, 0.3073],
        [0.6196, 0.2974, 0.2139],
        [0.5630, 0.3492, 0.2619],
        [0.6566, 0.2842, 0.2144],
        [0.4878, 0.3878, 0.2990],
        [0.4802, 0.3831, 0.2958],
        [0.5271, 0.3628, 0.2782],
        [0.5015, 0.3729, 0.2817],
        [0.4770, 0.3884, 0.3151],
        [0.6854, 0.2505, 0.2315],
        [0.6624, 0.2663, 0.2279],
        [0.5158, 0.3754, 0.2827],
        [0.4562, 0.3562, 0.3370],
        [0.5126, 0.3750, 0.2905],
        [0.4759, 0.3935, 0.3100],
        [0.6172, 0.2900, 0.2185],
        [0.5442, 0.3590, 0.2597],
        [0.5174, 0.3670, 0.2867],
        [0.5202, 0.3702, 0.2805],
        [0.5011, 0.3736, 0.2943],
        [0.5218, 0.3733, 0.2840],
        [0.4947, 0.3784, 0.3002],
        [0.4968, 0.3792, 0.3017],
        [0.5310, 0.3600, 0.2605],
        [0.5147, 0.3722, 0.2649],
        [0.4667, 0.3676, 0.3227],
        [0.4859, 0.3729, 0.3055],
        [0.4765, 0.3861, 0.3137],
        [0.4908, 0.3825, 0.3112],
        [0.5248, 0.3662, 0.2733],
        [0.4910, 0.3926, 0.2798],
        [0.5508, 0.3462, 0.2481],
        [0.5242, 0.3694, 0.2707],
        [0.4209, 0.4166, 0.3345],
        [0.4663, 0.3774, 0.3328],
        [0.4562, 0.3948, 0.3317],
        [0.5003, 0.3942, 0.3043],
        [0.5426, 0.3368, 0.2679],
        [0.6355, 0.2935, 0.2375],
        [0.5835, 0.2998, 0.2460],
        [0.6226, 0.2818, 0.2193],
        [0.6026, 0.3152, 0.2205],
        [0.5305, 0.3550, 0.2797],
        [0.5290, 0.3574, 0.2743],
        [0.4846, 0.3799, 0.3002],
        [0.5256, 0.3611, 0.2694],
        [0.5612, 0.3413, 0.2480],
        [0.6280, 0.3048, 0.2164],
        [0.5313, 0.3675, 0.2632],
        [0.5484, 0.3502, 0.2503],
        [0.5279, 0.3613, 0.2776],
        [0.4673, 0.3761, 0.3205],
        [0.4726, 0.3832, 0.3076],
        [0.4847, 0.3929, 0.3329],
        [0.4464, 0.4070, 0.3440],
        [0.4283, 0.4050, 0.3606],
        [0.5149, 0.3717, 0.2831],
        [0.4862, 0.3805, 0.3091],
        [0.4979, 0.3692, 0.2881],
        [0.5075, 0.3826, 0.3069],
        [0.4019, 0.4013, 0.3824],
        [0.4962, 0.3643, 0.2962],
        [0.5788, 0.3188, 0.2347],
        [0.4757, 0.3966, 0.3227],
        [0.5184, 0.3555, 0.2908],
        [0.4977, 0.3899, 0.3066],
        [0.5290, 0.3746, 0.2851],
        [0.5488, 0.3583, 0.2696],
        [0.5317, 0.3650, 0.2563],
        [0.5295, 0.3501, 0.2657],
        [0.5378, 0.3459, 0.2736],
        [0.5334, 0.3579, 0.2759],
        [0.5084, 0.3741, 0.2898],
        [0.4227, 0.4199, 0.3602],
        [0.4268, 0.4218, 0.3295],
        [0.4379, 0.4079, 0.3422],
        [0.5708, 0.3279, 0.2626],
        [0.6398, 0.2836, 0.2177],
        [0.5805, 0.3248, 0.2729]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 65: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15744.0, Mean: 1890.1395263671875, Std: 1338.928466796875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 157306.375, Mean: 18884.490234375, Std: 13378.00390625
[DEBUG] Top-3 class probabilities:
tensor([[0.6578, 0.2757, 0.2247],
        [0.5680, 0.3325, 0.2674],
        [0.5411, 0.3577, 0.2696],
        [0.5038, 0.3834, 0.3013],
        [0.4686, 0.3915, 0.2980],
        [0.4656, 0.3938, 0.2998],
        [0.4186, 0.4184, 0.3210],
        [0.5182, 0.3588, 0.2792],
        [0.5348, 0.3538, 0.2642],
        [0.5401, 0.3608, 0.2649],
        [0.4958, 0.3801, 0.2847],
        [0.5013, 0.3625, 0.2955],
        [0.4965, 0.3849, 0.3226],
        [0.6268, 0.2864, 0.2250],
        [0.5086, 0.3874, 0.2984],
        [0.5371, 0.3658, 0.2930],
        [0.4906, 0.3783, 0.3072],
        [0.4825, 0.3857, 0.3219],
        [0.4953, 0.3784, 0.3095],
        [0.4749, 0.3783, 0.3167],
        [0.4517, 0.3884, 0.3405],
        [0.4794, 0.3770, 0.3132],
        [0.5363, 0.3486, 0.2487],
        [0.5462, 0.3595, 0.2647],
        [0.4592, 0.3835, 0.3389],
        [0.5075, 0.3691, 0.2910],
        [0.5568, 0.3478, 0.2608],
        [0.5472, 0.3508, 0.2642],
        [0.5538, 0.3430, 0.2403],
        [0.5326, 0.3488, 0.2670],
        [0.5280, 0.3621, 0.2782],
        [0.5365, 0.3564, 0.2756],
        [0.5259, 0.3636, 0.2781],
        [0.4720, 0.4067, 0.3106],
        [0.4524, 0.3939, 0.3246],
        [0.5973, 0.3192, 0.2298],
        [0.5170, 0.3511, 0.2903],
        [0.5829, 0.3007, 0.2480],
        [0.6157, 0.3086, 0.2430],
        [0.6781, 0.2777, 0.2208],
        [0.5155, 0.3652, 0.2995],
        [0.4993, 0.3762, 0.2786],
        [0.4819, 0.3919, 0.2937],
        [0.5422, 0.3463, 0.2596],
        [0.4515, 0.3739, 0.3619],
        [0.5205, 0.3543, 0.2767],
        [0.4719, 0.3977, 0.3165],
        [0.4385, 0.4106, 0.3185],
        [0.5070, 0.3787, 0.2881],
        [0.5197, 0.3742, 0.2797],
        [0.4869, 0.3851, 0.3088],
        [0.4326, 0.4172, 0.3184],
        [0.6772, 0.2471, 0.2179],
        [0.5147, 0.3718, 0.2967],
        [0.5083, 0.3760, 0.2939],
        [0.5008, 0.3753, 0.3080],
        [0.4291, 0.4051, 0.3523],
        [0.4403, 0.3746, 0.3081],
        [0.4349, 0.4011, 0.3398],
        [0.4109, 0.3918, 0.3900],
        [0.4720, 0.3758, 0.3212],
        [0.5851, 0.3180, 0.2264],
        [0.5198, 0.3623, 0.2923],
        [0.4992, 0.3754, 0.3027],
        [0.4994, 0.3718, 0.2942],
        [0.5538, 0.3593, 0.2535],
        [0.5869, 0.3357, 0.2365],
        [0.4901, 0.3789, 0.3015],
        [0.5491, 0.3556, 0.2611],
        [0.5192, 0.3665, 0.2818],
        [0.4955, 0.3828, 0.3005],
        [0.5450, 0.3556, 0.2632],
        [0.4932, 0.3796, 0.3015],
        [0.4806, 0.3886, 0.3081],
        [0.4449, 0.3986, 0.3380],
        [0.5218, 0.3526, 0.3022],
        [0.5403, 0.3391, 0.2797],
        [0.6740, 0.2707, 0.2225],
        [0.6037, 0.3043, 0.2443],
        [0.6755, 0.2868, 0.2167],
        [0.4498, 0.3710, 0.3443],
        [0.5374, 0.3640, 0.2738],
        [0.5127, 0.3721, 0.2786],
        [0.5239, 0.3619, 0.2889],
        [0.5276, 0.3555, 0.2853],
        [0.5278, 0.3566, 0.2844],
        [0.5113, 0.3647, 0.2949],
        [0.5139, 0.3836, 0.2832],
        [0.5245, 0.3670, 0.2860],
        [0.5468, 0.3419, 0.2680],
        [0.5213, 0.3531, 0.2940],
        [0.5305, 0.3516, 0.2846],
        [0.6340, 0.2837, 0.2170],
        [0.4710, 0.3899, 0.3015],
        [0.5310, 0.3505, 0.2742],
        [0.5053, 0.3729, 0.2870],
        [0.4997, 0.3804, 0.2987],
        [0.5504, 0.3519, 0.2603],
        [0.5356, 0.3529, 0.2756],
        [0.5239, 0.3665, 0.3062],
        [0.5166, 0.3607, 0.2909],
        [0.4397, 0.4000, 0.3543],
        [0.5012, 0.3806, 0.3124],
        [0.6031, 0.3241, 0.2256],
        [0.4872, 0.3773, 0.3053],
        [0.4983, 0.3862, 0.2967],
        [0.5866, 0.3279, 0.2369],
        [0.6089, 0.3145, 0.2290],
        [0.5869, 0.3200, 0.2434],
        [0.5485, 0.3582, 0.2710],
        [0.5337, 0.3610, 0.2846],
        [0.4775, 0.3798, 0.3008],
        [0.4839, 0.3930, 0.3231],
        [0.4478, 0.4110, 0.3158],
        [0.4797, 0.3908, 0.2945],
        [0.4772, 0.3921, 0.3161],
        [0.5068, 0.3703, 0.2858],
        [0.6049, 0.3021, 0.2253],
        [0.6543, 0.2914, 0.2251],
        [0.5990, 0.3159, 0.2357],
        [0.6650, 0.2586, 0.2202],
        [0.5413, 0.3541, 0.2752],
        [0.4823, 0.3972, 0.3121],
        [0.4595, 0.3862, 0.3415],
        [0.5447, 0.3412, 0.2615],
        [0.4970, 0.3802, 0.3032],
        [0.5191, 0.3578, 0.2925],
        [0.5427, 0.3491, 0.2648]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 66: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18736.0, Mean: 1889.1884765625, Std: 1257.7919921875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 187201.15625, Mean: 18874.98046875, Std: 12567.3212890625
[DEBUG] Top-3 class probabilities:
tensor([[0.5129, 0.3746, 0.2882],
        [0.5300, 0.3749, 0.2778],
        [0.5379, 0.3622, 0.2786],
        [0.5239, 0.3623, 0.2852],
        [0.4696, 0.3807, 0.3355],
        [0.5477, 0.3365, 0.2738],
        [0.5854, 0.3252, 0.2575],
        [0.6256, 0.2907, 0.2280],
        [0.6156, 0.2881, 0.2455],
        [0.6572, 0.2586, 0.2429],
        [0.6308, 0.2873, 0.2411],
        [0.5210, 0.3735, 0.2825],
        [0.5771, 0.3347, 0.2402],
        [0.5366, 0.3597, 0.2806],
        [0.5299, 0.3655, 0.2711],
        [0.5352, 0.3607, 0.2730],
        [0.5271, 0.3574, 0.2670],
        [0.5198, 0.3635, 0.2929],
        [0.5141, 0.3594, 0.2932],
        [0.5507, 0.4678, 0.2362],
        [0.4166, 0.4100, 0.3463],
        [0.4558, 0.3975, 0.3331],
        [0.4573, 0.3856, 0.3445],
        [0.4755, 0.3794, 0.3024],
        [0.4800, 0.3761, 0.3024],
        [0.6025, 0.2993, 0.2140],
        [0.6709, 0.2664, 0.2038],
        [0.5502, 0.3278, 0.2720],
        [0.5184, 0.3639, 0.2841],
        [0.3962, 0.3633, 0.2555],
        [0.5168, 0.3615, 0.2824],
        [0.4845, 0.3860, 0.2916],
        [0.4626, 0.3893, 0.3007],
        [0.4286, 0.3979, 0.3431],
        [0.5161, 0.3698, 0.3020],
        [0.5546, 0.3561, 0.2763],
        [0.5821, 0.3258, 0.2404],
        [0.5977, 0.3279, 0.2243],
        [0.4624, 0.3880, 0.2940],
        [0.6045, 0.3314, 0.2331],
        [0.5255, 0.3678, 0.2674],
        [0.5240, 0.3581, 0.2881],
        [0.5048, 0.3662, 0.3114],
        [0.5519, 0.3517, 0.2870],
        [0.4989, 0.3730, 0.3042],
        [0.5147, 0.3751, 0.2840],
        [0.5480, 0.3580, 0.2618],
        [0.4715, 0.3811, 0.3241],
        [0.5380, 0.3623, 0.2736],
        [0.5570, 0.3474, 0.2642],
        [0.5070, 0.3718, 0.3021],
        [0.4775, 0.3928, 0.3168],
        [0.4668, 0.4002, 0.3198],
        [0.4452, 0.3952, 0.3536],
        [0.5205, 0.3456, 0.3022],
        [0.5068, 0.3596, 0.3212],
        [0.5803, 0.3161, 0.2451],
        [0.6097, 0.3370, 0.2304],
        [0.5822, 0.3295, 0.2457],
        [0.6069, 0.3332, 0.2314],
        [0.5096, 0.3621, 0.2904],
        [0.5423, 0.3726, 0.2695],
        [0.5177, 0.3684, 0.3000],
        [0.4872, 0.3748, 0.3124],
        [0.4928, 0.3812, 0.3290],
        [0.5432, 0.4344, 0.2311],
        [0.5230, 0.4231, 0.2330],
        [0.4755, 0.3704, 0.3387],
        [0.4890, 0.3709, 0.3311],
        [0.4919, 0.3679, 0.3135],
        [0.4290, 0.4002, 0.3618],
        [0.4391, 0.3958, 0.3459],
        [0.5726, 0.3340, 0.2383],
        [0.5261, 0.3646, 0.2796],
        [0.4382, 0.3967, 0.3441],
        [0.4255, 0.3963, 0.3606],
        [0.5780, 0.3225, 0.2426],
        [0.4507, 0.3960, 0.3544],
        [0.5357, 0.3503, 0.2830],
        [0.5103, 0.3725, 0.2895],
        [0.4910, 0.3679, 0.3094],
        [0.5595, 0.3523, 0.2640],
        [0.5465, 0.3526, 0.2645],
        [0.5165, 0.3838, 0.2948],
        [0.5705, 0.3300, 0.2438],
        [0.5429, 0.3572, 0.2637],
        [0.4139, 0.4049, 0.3768],
        [0.4598, 0.3925, 0.3264],
        [0.4563, 0.3898, 0.3384],
        [0.5793, 0.3280, 0.2497],
        [0.5387, 0.3430, 0.2836],
        [0.4701, 0.3846, 0.3117],
        [0.5434, 0.3546, 0.2601],
        [0.5023, 0.3712, 0.3159],
        [0.5099, 0.3636, 0.2940],
        [0.5772, 0.3326, 0.2521],
        [0.5176, 0.3611, 0.2809],
        [0.5160, 0.3663, 0.2760],
        [0.5021, 0.3670, 0.2914],
        [0.4874, 0.3789, 0.3107],
        [0.5476, 0.3520, 0.2597],
        [0.4713, 0.3882, 0.3460],
        [0.4575, 0.3786, 0.3678],
        [0.5257, 0.3703, 0.2589],
        [0.5761, 0.3453, 0.2468],
        [0.4997, 0.3791, 0.2830],
        [0.5064, 0.3807, 0.2891],
        [0.4611, 0.4074, 0.3387],
        [0.4647, 0.3797, 0.3405],
        [0.4860, 0.3718, 0.3137],
        [0.4216, 0.4192, 0.3557],
        [0.5237, 0.4267, 0.2321],
        [0.5015, 0.3538, 0.3013],
        [0.4558, 0.3937, 0.3386],
        [0.4984, 0.3582, 0.3009],
        [0.4861, 0.3730, 0.3299],
        [0.5206, 0.3650, 0.3012],
        [0.4677, 0.3716, 0.3476],
        [0.4203, 0.4126, 0.3843],
        [0.5277, 0.3561, 0.2594],
        [0.4859, 0.3868, 0.3085],
        [0.4439, 0.3924, 0.3410],
        [0.4633, 0.3937, 0.3258],
        [0.4925, 0.3595, 0.3020],
        [0.5170, 0.3618, 0.2908],
        [0.5008, 0.3811, 0.2956],
        [0.5743, 0.3304, 0.2280],
        [0.5745, 0.3212, 0.2297]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6,  9],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6,  9],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 67: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 107.0, Max: 18768.0, Mean: 1944.5286865234375, Std: 1180.6514892578125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 1068.1181640625, Max: 187520.890625, Mean: 19427.923828125, Std: 11796.56640625
[DEBUG] Top-3 class probabilities:
tensor([[0.5077, 0.3789, 0.3164],
        [0.5190, 0.3655, 0.2957],
        [0.5348, 0.3617, 0.2774],
        [0.4963, 0.3630, 0.2986],
        [0.4708, 0.3681, 0.3343],
        [0.4956, 0.3775, 0.3174],
        [0.5811, 0.3228, 0.2527],
        [0.5220, 0.3653, 0.2877],
        [0.5397, 0.3603, 0.2637],
        [0.5686, 0.3329, 0.2413],
        [0.5506, 0.3458, 0.2599],
        [0.4745, 0.3766, 0.3323],
        [0.4786, 0.3703, 0.3309],
        [0.5565, 0.3255, 0.2299],
        [0.5070, 0.3722, 0.2901],
        [0.4805, 0.3887, 0.3148],
        [0.4956, 0.3761, 0.2797],
        [0.5226, 0.3536, 0.2575],
        [0.5462, 0.3360, 0.2536],
        [0.5011, 0.3779, 0.3029],
        [0.5094, 0.3721, 0.2933],
        [0.5015, 0.3902, 0.2919],
        [0.4926, 0.3890, 0.3163],
        [0.4867, 0.3753, 0.3078],
        [0.4390, 0.3968, 0.3419],
        [0.3946, 0.2934, 0.2930],
        [0.5152, 0.3638, 0.2985],
        [0.4847, 0.3673, 0.3141],
        [0.4243, 0.4086, 0.3545],
        [0.5003, 0.3979, 0.2902],
        [0.4231, 0.4178, 0.3312],
        [0.4540, 0.3974, 0.3135],
        [0.4804, 0.3782, 0.3051],
        [0.4517, 0.3889, 0.3586],
        [0.4207, 0.3955, 0.3735],
        [0.4312, 0.3848, 0.3768],
        [0.4546, 0.3983, 0.3305],
        [0.5140, 0.3649, 0.2852],
        [0.5283, 0.3752, 0.2838],
        [0.5837, 0.3253, 0.2457],
        [0.5114, 0.3630, 0.2888],
        [0.5163, 0.3728, 0.2640],
        [0.4882, 0.3764, 0.3006],
        [0.4695, 0.3894, 0.3123],
        [0.5413, 0.3580, 0.2581],
        [0.5084, 0.3645, 0.2783],
        [0.4792, 0.3746, 0.3292],
        [0.5390, 0.3667, 0.2757],
        [0.5218, 0.3583, 0.2888],
        [0.4855, 0.3628, 0.3303],
        [0.4509, 0.3710, 0.3488],
        [0.5135, 0.3685, 0.3039],
        [0.5071, 0.3675, 0.2926],
        [0.4890, 0.3662, 0.3031],
        [0.4993, 0.3599, 0.2862],
        [0.4666, 0.3708, 0.3307],
        [0.4810, 0.3743, 0.3235],
        [0.5009, 0.3625, 0.3031],
        [0.5021, 0.3625, 0.2998],
        [0.4768, 0.3833, 0.3241],
        [0.4447, 0.3941, 0.3495],
        [0.5307, 0.3612, 0.2815],
        [0.5048, 0.3544, 0.3089],
        [0.4622, 0.3791, 0.3362],
        [0.5053, 0.3723, 0.3055],
        [0.4552, 0.3858, 0.3522],
        [0.5087, 0.3761, 0.2965],
        [0.4520, 0.3969, 0.3275],
        [0.5512, 0.3511, 0.2637],
        [0.5011, 0.3820, 0.3019],
        [0.4762, 0.3904, 0.3055],
        [0.5162, 0.3665, 0.2864],
        [0.4949, 0.3626, 0.2916],
        [0.4758, 0.3767, 0.3113],
        [0.5012, 0.3790, 0.3041],
        [0.4705, 0.3890, 0.3110],
        [0.4261, 0.4159, 0.3459],
        [0.4687, 0.3968, 0.3520],
        [0.4400, 0.4018, 0.3422],
        [0.4679, 0.3923, 0.3335],
        [0.4830, 0.3825, 0.3233],
        [0.4783, 0.3806, 0.3327],
        [0.4128, 0.4126, 0.3582],
        [0.5134, 0.3744, 0.2873],
        [0.5342, 0.3591, 0.2775],
        [0.6117, 0.3075, 0.2249],
        [0.5072, 0.3632, 0.2806],
        [0.5243, 0.3718, 0.2639],
        [0.5513, 0.3475, 0.2305],
        [0.4391, 0.3963, 0.3296],
        [0.5983, 0.3177, 0.2448],
        [0.5011, 0.3492, 0.2599],
        [0.5308, 0.3566, 0.2665],
        [0.5318, 0.3602, 0.2753],
        [0.5121, 0.3614, 0.2995],
        [0.4949, 0.3732, 0.3081],
        [0.5036, 0.3607, 0.3017],
        [0.5329, 0.3631, 0.2835],
        [0.5422, 0.3343, 0.2691],
        [0.4847, 0.3791, 0.3001],
        [0.4330, 0.3961, 0.3486],
        [0.4300, 0.4110, 0.3765],
        [0.4807, 0.3818, 0.3134],
        [0.5422, 0.3558, 0.2690],
        [0.5836, 0.3314, 0.2292],
        [0.4521, 0.4011, 0.3382],
        [0.5249, 0.3744, 0.2927],
        [0.5504, 0.3383, 0.2437],
        [0.4938, 0.3736, 0.3162],
        [0.5455, 0.3416, 0.2688],
        [0.5484, 0.3404, 0.2674],
        [0.4781, 0.3714, 0.3065],
        [0.4875, 0.3699, 0.3113],
        [0.4716, 0.3926, 0.3234],
        [0.5166, 0.3774, 0.2809],
        [0.4729, 0.3843, 0.3114],
        [0.5163, 0.3705, 0.2726],
        [0.4901, 0.3791, 0.3128],
        [0.5332, 0.3546, 0.2728],
        [0.4963, 0.3764, 0.2808],
        [0.4496, 0.3945, 0.3286],
        [0.4532, 0.4015, 0.3382],
        [0.4976, 0.3751, 0.3119],
        [0.5129, 0.3675, 0.2917],
        [0.5281, 0.3666, 0.2895],
        [0.4912, 0.3865, 0.3177],
        [0.4617, 0.3904, 0.3378],
        [0.5214, 0.3723, 0.2958]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 68: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13172.0, Mean: 1902.01953125, Std: 1186.809326171875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 131608.046875, Mean: 19003.19140625, Std: 11858.0927734375
[DEBUG] Top-3 class probabilities:
tensor([[0.4696, 0.3878, 0.3260],
        [0.4929, 0.3754, 0.3049],
        [0.5543, 0.3478, 0.2588],
        [0.5074, 0.3679, 0.2964],
        [0.5328, 0.3680, 0.2781],
        [0.5034, 0.3779, 0.3089],
        [0.5405, 0.3727, 0.2669],
        [0.5475, 0.3587, 0.2381],
        [0.5224, 0.3577, 0.2596],
        [0.4813, 0.3731, 0.2971],
        [0.5467, 0.3449, 0.2397],
        [0.6199, 0.3185, 0.2146],
        [0.5285, 0.3680, 0.3018],
        [0.5377, 0.3471, 0.2572],
        [0.4682, 0.3660, 0.3486],
        [0.4890, 0.3663, 0.3198],
        [0.4591, 0.3670, 0.3465],
        [0.4512, 0.3914, 0.3508],
        [0.4352, 0.3765, 0.3676],
        [0.5693, 0.3163, 0.2402],
        [0.5052, 0.3738, 0.2741],
        [0.5246, 0.3594, 0.2900],
        [0.5434, 0.3657, 0.2747],
        [0.5091, 0.3572, 0.2878],
        [0.5213, 0.3641, 0.2974],
        [0.5166, 0.3742, 0.2916],
        [0.5464, 0.3476, 0.2611],
        [0.5739, 0.3166, 0.2496],
        [0.5082, 0.3528, 0.2642],
        [0.4617, 0.3965, 0.3057],
        [0.5833, 0.3298, 0.2556],
        [0.4819, 0.3874, 0.3090],
        [0.4737, 0.3956, 0.3108],
        [0.4449, 0.3892, 0.3306],
        [0.5432, 0.3614, 0.2748],
        [0.4456, 0.3908, 0.3419],
        [0.4972, 0.3777, 0.3023],
        [0.4409, 0.3993, 0.2941],
        [0.5548, 0.3471, 0.2692],
        [0.4995, 0.3726, 0.3072],
        [0.5138, 0.3692, 0.2958],
        [0.4899, 0.3675, 0.3042],
        [0.5681, 0.3525, 0.2416],
        [0.5137, 0.3755, 0.2890],
        [0.4936, 0.3795, 0.3225],
        [0.5147, 0.3682, 0.3110],
        [0.5413, 0.3546, 0.2809],
        [0.5264, 0.3773, 0.2902],
        [0.4889, 0.3814, 0.2978],
        [0.4814, 0.3957, 0.3100],
        [0.4356, 0.3977, 0.3707],
        [0.4387, 0.3972, 0.3599],
        [0.5739, 0.3461, 0.2464],
        [0.5793, 0.3392, 0.2335],
        [0.5002, 0.3697, 0.3012],
        [0.5277, 0.3591, 0.3013],
        [0.4953, 0.3817, 0.3068],
        [0.5110, 0.3644, 0.3037],
        [0.5288, 0.3535, 0.2832],
        [0.4417, 0.3917, 0.3589],
        [0.4593, 0.3793, 0.3470],
        [0.4946, 0.3619, 0.3156],
        [0.4532, 0.3867, 0.3419],
        [0.4984, 0.3621, 0.3212],
        [0.4901, 0.3895, 0.2817],
        [0.6273, 0.2939, 0.2106],
        [0.5735, 0.3371, 0.2420],
        [0.5385, 0.3655, 0.2707],
        [0.5131, 0.3631, 0.2916],
        [0.4994, 0.3851, 0.3092],
        [0.5075, 0.3803, 0.2896],
        [0.5224, 0.3686, 0.2803],
        [0.4859, 0.3568, 0.3141],
        [0.5035, 0.3518, 0.2656],
        [0.4697, 0.3870, 0.3350],
        [0.4945, 0.3592, 0.3038],
        [0.4870, 0.3827, 0.3130],
        [0.5322, 0.3702, 0.2656],
        [0.5127, 0.3755, 0.2868],
        [0.4577, 0.3970, 0.3345],
        [0.4995, 0.3875, 0.3015],
        [0.5182, 0.3582, 0.2724],
        [0.5303, 0.3613, 0.2737],
        [0.5007, 0.3695, 0.2967],
        [0.4869, 0.3859, 0.2962],
        [0.5234, 0.3584, 0.2898],
        [0.4949, 0.3883, 0.3067],
        [0.4565, 0.4068, 0.3361],
        [0.4567, 0.4041, 0.3086],
        [0.5426, 0.3620, 0.2749],
        [0.4469, 0.4195, 0.3514],
        [0.4520, 0.4145, 0.3429],
        [0.5789, 0.3280, 0.2615],
        [0.5359, 0.3564, 0.2825],
        [0.5379, 0.3635, 0.2709],
        [0.5256, 0.3628, 0.2875],
        [0.5477, 0.3507, 0.2692],
        [0.5011, 0.3828, 0.3041],
        [0.5280, 0.3674, 0.2805],
        [0.5267, 0.3468, 0.2698],
        [0.5239, 0.3455, 0.2584],
        [0.5208, 0.3675, 0.2880],
        [0.4672, 0.3808, 0.3221],
        [0.4184, 0.3904, 0.3740],
        [0.4222, 0.3811, 0.3645],
        [0.4044, 0.3993, 0.3803],
        [0.4506, 0.3871, 0.3471],
        [0.5382, 0.3548, 0.2616],
        [0.5675, 0.3421, 0.2440],
        [0.5073, 0.3558, 0.2802],
        [0.5102, 0.3690, 0.2903],
        [0.5154, 0.3597, 0.2914],
        [0.4801, 0.3855, 0.3318],
        [0.5005, 0.3774, 0.3046],
        [0.5303, 0.3585, 0.2834],
        [0.4869, 0.3743, 0.3205],
        [0.5277, 0.3658, 0.2751],
        [0.4783, 0.3694, 0.3157],
        [0.4498, 0.3650, 0.3637],
        [0.4438, 0.3679, 0.3575],
        [0.5051, 0.3637, 0.3193],
        [0.5011, 0.3790, 0.2816],
        [0.5322, 0.3576, 0.2715],
        [0.5311, 0.3546, 0.2642],
        [0.4655, 0.3923, 0.3215],
        [0.4469, 0.4014, 0.3335],
        [0.4768, 0.3908, 0.3105],
        [0.5341, 0.3522, 0.2688]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 69: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 88.0, Max: 15139.0, Mean: 1911.621337890625, Std: 1266.391357421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 878.2783203125, Max: 151261.46875, Mean: 19099.126953125, Std: 12653.2431640625
[DEBUG] Top-3 class probabilities:
tensor([[0.5143, 0.3595, 0.2853],
        [0.5000, 0.3849, 0.3134],
        [0.5497, 0.3539, 0.2762],
        [0.5091, 0.3772, 0.3003],
        [0.5388, 0.3537, 0.2897],
        [0.5231, 0.3576, 0.2937],
        [0.4452, 0.4045, 0.3486],
        [0.5069, 0.3776, 0.2861],
        [0.5117, 0.3718, 0.2927],
        [0.5357, 0.3600, 0.2732],
        [0.5348, 0.3550, 0.2797],
        [0.5535, 0.3595, 0.2749],
        [0.5228, 0.3543, 0.2785],
        [0.5090, 0.3713, 0.2938],
        [0.5200, 0.3656, 0.2871],
        [0.5263, 0.3623, 0.2833],
        [0.5512, 0.3470, 0.2665],
        [0.4017, 0.4013, 0.3830],
        [0.4521, 0.3924, 0.3312],
        [0.4217, 0.4175, 0.3505],
        [0.4079, 0.4028, 0.3863],
        [0.4425, 0.3737, 0.3583],
        [0.4097, 0.3915, 0.3856],
        [0.4517, 0.3856, 0.3522],
        [0.4573, 0.3819, 0.3473],
        [0.4898, 0.3679, 0.3185],
        [0.4726, 0.3790, 0.3339],
        [0.5059, 0.3708, 0.2929],
        [0.5642, 0.3311, 0.2506],
        [0.4951, 0.3697, 0.3116],
        [0.5221, 0.3661, 0.2806],
        [0.5728, 0.3455, 0.2531],
        [0.5634, 0.3401, 0.2553],
        [0.5413, 0.3585, 0.2706],
        [0.5050, 0.3661, 0.2993],
        [0.4745, 0.3788, 0.3223],
        [0.4540, 0.3670, 0.3539],
        [0.4699, 0.3738, 0.3451],
        [0.4796, 0.3761, 0.3278],
        [0.4788, 0.3869, 0.3026],
        [0.5399, 0.3444, 0.2644],
        [0.4969, 0.3745, 0.2984],
        [0.4297, 0.3955, 0.3852],
        [0.4478, 0.4000, 0.3413],
        [0.4908, 0.3868, 0.2997],
        [0.5170, 0.3772, 0.2758],
        [0.4959, 0.3714, 0.3072],
        [0.4671, 0.3850, 0.3374],
        [0.5450, 0.3572, 0.2714],
        [0.5244, 0.3610, 0.2774],
        [0.4937, 0.3794, 0.3148],
        [0.5320, 0.3590, 0.2887],
        [0.5080, 0.3894, 0.2996],
        [0.4648, 0.4049, 0.3195],
        [0.5628, 0.3416, 0.2548],
        [0.5106, 0.3741, 0.2864],
        [0.5462, 0.3400, 0.2663],
        [0.5595, 0.3408, 0.2613],
        [0.5751, 0.3385, 0.2576],
        [0.5484, 0.3437, 0.2774],
        [0.5081, 0.3485, 0.3028],
        [0.5240, 0.3544, 0.2919],
        [0.4834, 0.3823, 0.3236],
        [0.4209, 0.4034, 0.3846],
        [0.4433, 0.3969, 0.3170],
        [0.4070, 0.3986, 0.3855],
        [0.4854, 0.3733, 0.3283],
        [0.4775, 0.3691, 0.3264],
        [0.4692, 0.3714, 0.3395],
        [0.4651, 0.3775, 0.3398],
        [0.4839, 0.3657, 0.3181],
        [0.5240, 0.3634, 0.2979],
        [0.5118, 0.3721, 0.3039],
        [0.4729, 0.3655, 0.3391],
        [0.5003, 0.3670, 0.3211],
        [0.5141, 0.3470, 0.2997],
        [0.5518, 0.3410, 0.2820],
        [0.4973, 0.3788, 0.2916],
        [0.5047, 0.3813, 0.3014],
        [0.4830, 0.3794, 0.2882],
        [0.4641, 0.3672, 0.3403],
        [0.4596, 0.3584, 0.3544],
        [0.5194, 0.3495, 0.3014],
        [0.4625, 0.3974, 0.3166],
        [0.4783, 0.3855, 0.3095],
        [0.4641, 0.3970, 0.3017],
        [0.4991, 0.3681, 0.2922],
        [0.4798, 0.3728, 0.3249],
        [0.4373, 0.4023, 0.3502],
        [0.4672, 0.3809, 0.3242],
        [0.4660, 0.4010, 0.3311],
        [0.5116, 0.3659, 0.2931],
        [0.4708, 0.3958, 0.3300],
        [0.5313, 0.3557, 0.2607],
        [0.4559, 0.4031, 0.3193],
        [0.4421, 0.3787, 0.3781],
        [0.4643, 0.4008, 0.3321],
        [0.4855, 0.3785, 0.3159],
        [0.5084, 0.3747, 0.3015],
        [0.4355, 0.4136, 0.3522],
        [0.5285, 0.3508, 0.2877],
        [0.5457, 0.3452, 0.2735],
        [0.5537, 0.3488, 0.2699],
        [0.5614, 0.3515, 0.2518],
        [0.5398, 0.3500, 0.2770],
        [0.5483, 0.3382, 0.2672],
        [0.5120, 0.3592, 0.2721],
        [0.6089, 0.3103, 0.2211],
        [0.3993, 0.3953, 0.3892],
        [0.5040, 0.3488, 0.2594],
        [0.5323, 0.3443, 0.2349],
        [0.4880, 0.3688, 0.3091],
        [0.4770, 0.3744, 0.3182],
        [0.6237, 0.3138, 0.2262],
        [0.5078, 0.3598, 0.3011],
        [0.4421, 0.3853, 0.3562],
        [0.4828, 0.3647, 0.3196],
        [0.5157, 0.3439, 0.2967],
        [0.5131, 0.3589, 0.3036],
        [0.4700, 0.3622, 0.3354],
        [0.4834, 0.3637, 0.3260],
        [0.4755, 0.3927, 0.3375],
        [0.5170, 0.3742, 0.3084],
        [0.4428, 0.3890, 0.3303],
        [0.4240, 0.4212, 0.3579],
        [0.4745, 0.3833, 0.3183],
        [0.4555, 0.3685, 0.3584],
        [0.4961, 0.3500, 0.3238]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 70: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 152.0, Max: 12777.0, Mean: 1950.764404296875, Std: 1268.210205078125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 1517.739013671875, Max: 127661.3671875, Mean: 19490.2265625, Std: 12671.4169921875
[DEBUG] Top-3 class probabilities:
tensor([[0.4723, 0.3745, 0.3158],
        [0.4427, 0.3862, 0.3262],
        [0.4885, 0.3751, 0.3159],
        [0.4724, 0.3796, 0.3294],
        [0.5405, 0.3664, 0.2694],
        [0.5113, 0.3712, 0.2799],
        [0.5317, 0.3572, 0.2698],
        [0.5636, 0.3428, 0.2461],
        [0.4490, 0.3882, 0.3405],
        [0.5132, 0.3621, 0.2955],
        [0.5071, 0.3665, 0.3000],
        [0.5197, 0.3649, 0.2933],
        [0.4636, 0.3934, 0.3363],
        [0.4408, 0.3582, 0.3569],
        [0.5038, 0.3692, 0.2625],
        [0.4366, 0.3934, 0.3520],
        [0.5308, 0.3671, 0.2832],
        [0.4723, 0.3904, 0.3387],
        [0.4274, 0.4148, 0.3543],
        [0.5089, 0.3566, 0.3007],
        [0.5288, 0.3566, 0.2814],
        [0.5453, 0.3484, 0.2565],
        [0.5597, 0.3385, 0.2611],
        [0.5379, 0.3466, 0.2811],
        [0.4675, 0.3833, 0.3257],
        [0.5302, 0.3615, 0.2747],
        [0.4283, 0.3935, 0.3540],
        [0.4936, 0.3597, 0.2629],
        [0.5591, 0.3230, 0.2423],
        [0.5011, 0.3547, 0.2575],
        [0.4747, 0.3833, 0.3017],
        [0.4790, 0.3679, 0.3240],
        [0.4961, 0.3611, 0.3250],
        [0.5477, 0.3500, 0.2645],
        [0.5216, 0.3619, 0.2808],
        [0.5538, 0.3384, 0.2630],
        [0.5170, 0.3638, 0.2981],
        [0.5229, 0.3410, 0.2896],
        [0.5211, 0.3413, 0.2975],
        [0.5272, 0.3404, 0.2862],
        [0.4892, 0.3732, 0.3153],
        [0.4291, 0.3945, 0.3434],
        [0.4097, 0.4082, 0.3804],
        [0.4722, 0.3900, 0.3364],
        [0.4498, 0.3970, 0.3308],
        [0.4276, 0.4047, 0.3489],
        [0.4353, 0.3916, 0.3604],
        [0.4762, 0.3675, 0.3333],
        [0.4632, 0.3706, 0.3373],
        [0.4903, 0.3660, 0.3155],
        [0.4464, 0.3996, 0.3284],
        [0.5169, 0.3640, 0.2789],
        [0.5717, 0.3423, 0.2469],
        [0.5609, 0.3439, 0.2565],
        [0.5264, 0.3592, 0.2901],
        [0.4345, 0.4225, 0.3498],
        [0.4460, 0.4091, 0.3413],
        [0.4798, 0.3857, 0.3221],
        [0.4987, 0.3553, 0.3198],
        [0.4752, 0.3897, 0.3342],
        [0.4440, 0.4049, 0.3085],
        [0.5107, 0.3730, 0.3041],
        [0.5119, 0.3693, 0.2952],
        [0.4732, 0.3920, 0.3060],
        [0.4467, 0.4061, 0.3146],
        [0.4812, 0.3856, 0.3093],
        [0.4731, 0.3771, 0.3372],
        [0.5440, 0.3430, 0.2638],
        [0.5393, 0.3579, 0.2721],
        [0.5950, 0.3247, 0.2459],
        [0.5052, 0.3690, 0.2824],
        [0.5148, 0.3684, 0.2847],
        [0.4612, 0.3751, 0.3198],
        [0.5100, 0.3767, 0.2830],
        [0.5032, 0.3820, 0.2982],
        [0.4925, 0.3794, 0.3059],
        [0.4717, 0.3800, 0.3298],
        [0.4381, 0.3814, 0.3661],
        [0.4552, 0.3968, 0.3376],
        [0.4429, 0.4013, 0.3257],
        [0.5713, 0.3308, 0.2583],
        [0.5512, 0.3383, 0.2814],
        [0.5560, 0.3343, 0.2685],
        [0.5482, 0.3346, 0.2762],
        [0.5584, 0.3312, 0.2685],
        [0.5335, 0.3505, 0.2811],
        [0.5700, 0.3417, 0.2406],
        [0.4615, 0.3899, 0.3406],
        [0.4334, 0.3928, 0.3651],
        [0.5326, 0.3601, 0.2701],
        [0.4285, 0.3938, 0.3670],
        [0.4255, 0.3968, 0.3688],
        [0.4699, 0.3753, 0.3268],
        [0.4542, 0.3798, 0.3436],
        [0.5391, 0.3500, 0.2834],
        [0.4255, 0.3922, 0.3562],
        [0.4812, 0.3516, 0.2959],
        [0.4539, 0.3944, 0.3447],
        [0.4597, 0.3859, 0.3243],
        [0.5057, 0.3857, 0.3080],
        [0.4805, 0.3955, 0.3167],
        [0.4773, 0.3932, 0.3118],
        [0.5103, 0.3525, 0.3014],
        [0.4519, 0.3899, 0.3509],
        [0.4838, 0.3960, 0.2889],
        [0.4692, 0.3907, 0.3371],
        [0.4177, 0.4121, 0.3355],
        [0.4397, 0.3981, 0.3653],
        [0.4949, 0.3803, 0.2949],
        [0.4604, 0.3981, 0.3074],
        [0.4342, 0.4184, 0.3208],
        [0.4513, 0.3883, 0.3368],
        [0.4894, 0.3845, 0.3261],
        [0.4982, 0.3670, 0.3038],
        [0.5398, 0.3405, 0.2688],
        [0.5221, 0.3491, 0.2947],
        [0.4585, 0.3668, 0.3593],
        [0.4776, 0.3717, 0.3277],
        [0.4020, 0.4009, 0.3865],
        [0.4548, 0.3994, 0.3259],
        [0.4722, 0.3829, 0.3160],
        [0.4224, 0.3841, 0.3508],
        [0.4966, 0.3522, 0.3268],
        [0.5116, 0.3601, 0.2986],
        [0.5130, 0.3616, 0.2767],
        [0.5750, 0.3424, 0.2463],
        [0.5653, 0.3215, 0.2330],
        [0.5162, 0.3610, 0.2898]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 71: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 115.0, Max: 6756.0, Mean: 1963.5338134765625, Std: 1293.35888671875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 1148.05078125, Max: 67502.09375, Mean: 19617.8125, Std: 12922.6904296875
[DEBUG] Top-3 class probabilities:
tensor([[0.5176, 0.3518, 0.2895],
        [0.5244, 0.3497, 0.2842],
        [0.5140, 0.3503, 0.2833],
        [0.5057, 0.3638, 0.3009],
        [0.5832, 0.3290, 0.2487],
        [0.4892, 0.3523, 0.3158],
        [0.4258, 0.3875, 0.3568],
        [0.4510, 0.3832, 0.3431],
        [0.4554, 0.3834, 0.3424],
        [0.5049, 0.3644, 0.3002],
        [0.4728, 0.3677, 0.3224],
        [0.4882, 0.3686, 0.3245],
        [0.5152, 0.3573, 0.3004],
        [0.5302, 0.3425, 0.2618],
        [0.5356, 0.3247, 0.2769],
        [0.5040, 0.3626, 0.2862],
        [0.4580, 0.3884, 0.3409],
        [0.5111, 0.3680, 0.2954],
        [0.4808, 0.3836, 0.3161],
        [0.5046, 0.3744, 0.2928],
        [0.5272, 0.3574, 0.2850],
        [0.4507, 0.4161, 0.3430],
        [0.4888, 0.4404, 0.2847],
        [0.4724, 0.3972, 0.3285],
        [0.4931, 0.3779, 0.2979],
        [0.4443, 0.4047, 0.3197],
        [0.5269, 0.3638, 0.2583],
        [0.4985, 0.3822, 0.3165],
        [0.4214, 0.4150, 0.3532],
        [0.4848, 0.3905, 0.3037],
        [0.5172, 0.3577, 0.2762],
        [0.5333, 0.3516, 0.2671],
        [0.5018, 0.3761, 0.3054],
        [0.5159, 0.3465, 0.2957],
        [0.5065, 0.3606, 0.3118],
        [0.4557, 0.3786, 0.3434],
        [0.5185, 0.3418, 0.2745],
        [0.5690, 0.3336, 0.2396],
        [0.4440, 0.3920, 0.3457],
        [0.4068, 0.4059, 0.3902],
        [0.4151, 0.3941, 0.3844],
        [0.4471, 0.3837, 0.3559],
        [0.5100, 0.3662, 0.2983],
        [0.4401, 0.3894, 0.3448],
        [0.4231, 0.4218, 0.3096],
        [0.4931, 0.3920, 0.3041],
        [0.5484, 0.3517, 0.2700],
        [0.5210, 0.3634, 0.2970],
        [0.4730, 0.3644, 0.3263],
        [0.5269, 0.3479, 0.3010],
        [0.5379, 0.3367, 0.2722],
        [0.4991, 0.3523, 0.3260],
        [0.4320, 0.3959, 0.3690],
        [0.4293, 0.3875, 0.3667],
        [0.5099, 0.3803, 0.3039],
        [0.5685, 0.3390, 0.2410],
        [0.4876, 0.3690, 0.3075],
        [0.4970, 0.3632, 0.3141],
        [0.5325, 0.3466, 0.2853],
        [0.5545, 0.3473, 0.2695],
        [0.5220, 0.3558, 0.2641],
        [0.5527, 0.3291, 0.2703],
        [0.5382, 0.3542, 0.2742],
        [0.4983, 0.3740, 0.2975],
        [0.4569, 0.3986, 0.3265],
        [0.4557, 0.4017, 0.3195],
        [0.4195, 0.4148, 0.3457],
        [0.4756, 0.4098, 0.3333],
        [0.4695, 0.4260, 0.3265],
        [0.4884, 0.3904, 0.3068],
        [0.4456, 0.3828, 0.3767],
        [0.4968, 0.3868, 0.3174],
        [0.5343, 0.3514, 0.2618],
        [0.4784, 0.3797, 0.3226],
        [0.4830, 0.3644, 0.3258],
        [0.4497, 0.4016, 0.3414],
        [0.5770, 0.3394, 0.2445],
        [0.5113, 0.3664, 0.3041],
        [0.5688, 0.3302, 0.2539],
        [0.5836, 0.3293, 0.2522],
        [0.5353, 0.3525, 0.2949],
        [0.4969, 0.3616, 0.3073],
        [0.4823, 0.3705, 0.3193],
        [0.4943, 0.3671, 0.3019],
        [0.4739, 0.3811, 0.3159],
        [0.4230, 0.3876, 0.3846],
        [0.4401, 0.3931, 0.3642],
        [0.5137, 0.3705, 0.3027],
        [0.4871, 0.3691, 0.3232],
        [0.4447, 0.3941, 0.3207],
        [0.4887, 0.3912, 0.2830],
        [0.4457, 0.4133, 0.2976],
        [0.5588, 0.3549, 0.2582],
        [0.5054, 0.3583, 0.2964],
        [0.5490, 0.3318, 0.2769],
        [0.5487, 0.3454, 0.2763],
        [0.5387, 0.3416, 0.2699],
        [0.5458, 0.3283, 0.2737],
        [0.4308, 0.4214, 0.3567],
        [0.5058, 0.3642, 0.2895],
        [0.4188, 0.3857, 0.3667],
        [0.4184, 0.3916, 0.3664],
        [0.4572, 0.3769, 0.3472],
        [0.5206, 0.3539, 0.2899],
        [0.5242, 0.3649, 0.2720],
        [0.5652, 0.3497, 0.2640],
        [0.5536, 0.3443, 0.2674],
        [0.4391, 0.3931, 0.3599],
        [0.4380, 0.4032, 0.3611],
        [0.5092, 0.3651, 0.2759],
        [0.4419, 0.4060, 0.3078],
        [0.4283, 0.4181, 0.3319],
        [0.5152, 0.4639, 0.2352],
        [0.4707, 0.3997, 0.3042],
        [0.4676, 0.3959, 0.3191],
        [0.5229, 0.3543, 0.2761],
        [0.5130, 0.3645, 0.2875],
        [0.4754, 0.3995, 0.3300],
        [0.4652, 0.3938, 0.3335],
        [0.5260, 0.3589, 0.2901],
        [0.4870, 0.3718, 0.3338],
        [0.4609, 0.4383, 0.3269],
        [0.4507, 0.4040, 0.3208],
        [0.5042, 0.3764, 0.3000],
        [0.5369, 0.3616, 0.2876],
        [0.5440, 0.3510, 0.2741],
        [0.5666, 0.3407, 0.2614],
        [0.5427, 0.3572, 0.2675]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 72: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 74.0, Max: 15880.0, Mean: 1940.9813232421875, Std: 1265.4361572265625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 738.396240234375, Max: 158665.21875, Mean: 19392.48046875, Std: 12643.7001953125
[DEBUG] Top-3 class probabilities:
tensor([[0.4839, 0.3730, 0.3156],
        [0.4033, 0.4021, 0.3834],
        [0.4557, 0.3911, 0.3495],
        [0.4864, 0.3629, 0.3243],
        [0.4884, 0.3738, 0.3141],
        [0.5194, 0.3732, 0.3039],
        [0.5236, 0.3557, 0.2586],
        [0.4699, 0.3866, 0.2950],
        [0.5875, 0.3295, 0.2463],
        [0.5291, 0.3531, 0.2828],
        [0.5096, 0.3628, 0.2940],
        [0.4770, 0.3714, 0.3438],
        [0.5377, 0.3388, 0.2832],
        [0.5386, 0.3356, 0.2840],
        [0.5496, 0.3342, 0.2735],
        [0.5222, 0.3557, 0.2853],
        [0.5187, 0.3576, 0.2852],
        [0.5253, 0.3556, 0.2762],
        [0.4889, 0.3793, 0.3188],
        [0.4586, 0.3954, 0.3390],
        [0.5012, 0.3617, 0.3182],
        [0.4708, 0.3782, 0.3313],
        [0.5039, 0.3648, 0.3030],
        [0.5472, 0.3436, 0.2710],
        [0.5375, 0.3430, 0.2777],
        [0.5103, 0.3640, 0.2929],
        [0.4901, 0.3780, 0.2997],
        [0.4886, 0.3809, 0.3052],
        [0.4181, 0.3876, 0.3600],
        [0.4328, 0.3921, 0.3699],
        [0.5156, 0.3696, 0.2870],
        [0.5348, 0.3623, 0.2748],
        [0.4665, 0.3934, 0.3186],
        [0.4791, 0.3817, 0.3036],
        [0.4918, 0.3878, 0.3243],
        [0.4302, 0.4051, 0.3484],
        [0.4876, 0.3740, 0.3224],
        [0.4879, 0.3692, 0.3174],
        [0.5414, 0.3449, 0.2856],
        [0.4447, 0.3800, 0.3760],
        [0.4387, 0.3931, 0.3597],
        [0.5242, 0.3681, 0.2818],
        [0.5806, 0.3382, 0.2514],
        [0.5425, 0.3510, 0.2846],
        [0.5414, 0.3641, 0.2849],
        [0.4944, 0.3726, 0.3037],
        [0.5363, 0.3546, 0.2632],
        [0.5158, 0.3740, 0.2839],
        [0.5116, 0.3625, 0.3038],
        [0.4898, 0.3705, 0.3121],
        [0.5420, 0.3565, 0.2706],
        [0.4398, 0.3866, 0.3557],
        [0.4487, 0.3910, 0.3536],
        [0.4548, 0.3957, 0.3151],
        [0.5060, 0.3635, 0.2775],
        [0.5487, 0.3399, 0.2630],
        [0.4474, 0.3832, 0.3550],
        [0.4546, 0.3870, 0.3460],
        [0.4891, 0.3558, 0.3149],
        [0.4606, 0.3624, 0.3447],
        [0.5383, 0.3388, 0.2784],
        [0.5492, 0.3566, 0.2747],
        [0.5670, 0.3385, 0.2647],
        [0.5471, 0.3494, 0.2715],
        [0.5122, 0.3692, 0.3037],
        [0.5504, 0.3604, 0.2785],
        [0.4791, 0.3741, 0.3260],
        [0.4719, 0.3820, 0.3225],
        [0.5469, 0.3442, 0.2708],
        [0.5251, 0.3634, 0.2883],
        [0.4373, 0.3935, 0.3497],
        [0.5500, 0.3559, 0.2652],
        [0.4468, 0.3897, 0.3397],
        [0.4106, 0.4002, 0.3809],
        [0.4293, 0.3998, 0.3757],
        [0.4182, 0.4156, 0.3640],
        [0.5259, 0.3663, 0.2617],
        [0.5248, 0.3704, 0.2835],
        [0.5562, 0.3444, 0.2526],
        [0.6016, 0.3210, 0.2247],
        [0.5225, 0.3626, 0.2670],
        [0.5167, 0.3712, 0.2902],
        [0.6081, 0.3139, 0.2269],
        [0.5797, 0.3304, 0.2515],
        [0.4897, 0.3686, 0.3027],
        [0.5264, 0.3599, 0.2817],
        [0.4866, 0.3698, 0.3228],
        [0.5063, 0.3611, 0.2941],
        [0.5132, 0.3800, 0.2932],
        [0.5270, 0.3485, 0.2816],
        [0.5433, 0.3568, 0.2772],
        [0.5412, 0.3516, 0.2880],
        [0.4868, 0.3913, 0.3246],
        [0.5846, 0.3358, 0.2448],
        [0.5292, 0.3648, 0.3054],
        [0.4349, 0.3937, 0.3721],
        [0.4621, 0.3676, 0.3341],
        [0.4492, 0.3863, 0.3601],
        [0.4538, 0.3804, 0.3605],
        [0.5725, 0.3279, 0.2579],
        [0.5526, 0.3484, 0.2785],
        [0.5122, 0.3627, 0.2867],
        [0.5337, 0.3457, 0.2909],
        [0.4752, 0.3769, 0.3337],
        [0.4402, 0.3823, 0.3624],
        [0.4316, 0.3923, 0.3602],
        [0.4887, 0.3695, 0.3319],
        [0.5528, 0.3494, 0.2699],
        [0.5178, 0.3670, 0.2940],
        [0.5403, 0.3438, 0.2721],
        [0.5823, 0.3279, 0.2461],
        [0.5034, 0.3583, 0.3058],
        [0.4444, 0.4247, 0.3434],
        [0.4292, 0.4141, 0.3572],
        [0.4144, 0.3918, 0.3872],
        [0.4458, 0.3972, 0.3493],
        [0.5759, 0.3372, 0.2536],
        [0.4909, 0.3753, 0.3088],
        [0.5159, 0.3599, 0.2880],
        [0.4366, 0.3842, 0.3449],
        [0.4132, 0.3966, 0.3925],
        [0.5061, 0.3750, 0.2924],
        [0.4800, 0.3741, 0.2858],
        [0.5388, 0.3610, 0.2558],
        [0.5809, 0.3354, 0.2310],
        [0.5541, 0.3508, 0.2533],
        [0.5560, 0.3575, 0.2596],
        [0.5272, 0.3687, 0.2775]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 73: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8600.0, Mean: 1924.918701171875, Std: 1277.0806884765625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 85926.5625, Mean: 19231.98828125, Std: 12760.0458984375
[DEBUG] Top-3 class probabilities:
tensor([[0.5433, 0.3566, 0.2659],
        [0.5358, 0.3571, 0.2827],
        [0.4918, 0.3708, 0.2955],
        [0.5835, 0.3270, 0.2416],
        [0.4258, 0.4077, 0.3573],
        [0.4235, 0.3942, 0.3489],
        [0.4742, 0.3787, 0.3163],
        [0.5082, 0.3870, 0.3092],
        [0.5288, 0.3650, 0.2709],
        [0.5468, 0.3626, 0.2635],
        [0.5320, 0.3573, 0.2866],
        [0.6251, 0.2917, 0.2140],
        [0.4843, 0.3829, 0.3256],
        [0.4946, 0.3862, 0.3162],
        [0.4711, 0.3797, 0.3277],
        [0.4770, 0.3938, 0.3365],
        [0.4626, 0.3760, 0.3550],
        [0.5558, 0.3328, 0.2708],
        [0.5878, 0.3251, 0.2539],
        [0.5428, 0.3382, 0.2807],
        [0.4674, 0.3775, 0.3469],
        [0.5141, 0.3568, 0.3066],
        [0.4962, 0.3727, 0.3074],
        [0.4611, 0.3798, 0.3365],
        [0.5246, 0.3617, 0.2802],
        [0.5197, 0.3560, 0.2967],
        [0.5320, 0.3616, 0.2919],
        [0.5565, 0.3436, 0.2689],
        [0.5219, 0.3623, 0.2914],
        [0.4390, 0.4326, 0.3353],
        [0.4223, 0.3931, 0.3813],
        [0.4435, 0.4419, 0.3286],
        [0.4636, 0.3947, 0.3300],
        [0.4444, 0.3913, 0.3412],
        [0.5305, 0.3459, 0.2440],
        [0.4534, 0.4053, 0.3379],
        [0.4301, 0.3999, 0.3549],
        [0.4757, 0.3833, 0.3136],
        [0.4233, 0.4140, 0.3573],
        [0.4811, 0.3810, 0.3078],
        [0.5435, 0.3558, 0.2645],
        [0.6006, 0.2894, 0.2335],
        [0.5392, 0.3533, 0.2540],
        [0.4985, 0.3808, 0.2871],
        [0.4950, 0.3866, 0.2811],
        [0.5167, 0.3695, 0.2885],
        [0.5697, 0.3442, 0.2551],
        [0.5308, 0.3604, 0.2828],
        [0.5017, 0.3641, 0.2924],
        [0.5600, 0.3346, 0.2599],
        [0.4947, 0.3640, 0.3151],
        [0.5409, 0.3325, 0.2849],
        [0.5175, 0.3664, 0.2941],
        [0.5415, 0.3516, 0.2737],
        [0.5218, 0.3620, 0.2819],
        [0.5490, 0.3574, 0.2808],
        [0.6168, 0.3183, 0.2236],
        [0.5117, 0.3679, 0.3005],
        [0.5027, 0.3707, 0.3003],
        [0.4727, 0.3887, 0.3259],
        [0.4328, 0.3933, 0.3711],
        [0.5325, 0.3574, 0.2816],
        [0.5541, 0.3345, 0.2722],
        [0.5230, 0.3491, 0.3041],
        [0.4938, 0.3619, 0.3212],
        [0.5326, 0.3494, 0.2928],
        [0.4841, 0.3557, 0.3420],
        [0.5163, 0.3495, 0.2984],
        [0.5180, 0.3618, 0.2966],
        [0.4730, 0.3938, 0.3236],
        [0.5139, 0.3640, 0.2909],
        [0.4412, 0.3879, 0.3564],
        [0.4991, 0.3709, 0.3148],
        [0.5137, 0.3583, 0.2806],
        [0.5342, 0.3563, 0.2682],
        [0.4994, 0.3633, 0.3017],
        [0.4651, 0.3690, 0.2698],
        [0.4413, 0.3933, 0.3548],
        [0.4147, 0.4041, 0.3804],
        [0.4244, 0.4017, 0.3720],
        [0.5000, 0.3629, 0.3122],
        [0.4307, 0.4204, 0.3572],
        [0.4095, 0.4066, 0.3684],
        [0.4275, 0.4215, 0.3456],
        [0.4259, 0.4254, 0.3465],
        [0.4379, 0.4299, 0.3411],
        [0.5487, 0.3445, 0.2460],
        [0.5337, 0.3663, 0.2663],
        [0.5696, 0.3437, 0.2382],
        [0.4814, 0.3776, 0.3186],
        [0.4776, 0.3807, 0.3063],
        [0.4609, 0.4009, 0.3382],
        [0.5814, 0.3427, 0.2377],
        [0.5464, 0.3538, 0.2656],
        [0.4478, 0.3790, 0.3592],
        [0.4847, 0.3782, 0.3034],
        [0.4168, 0.3967, 0.3752],
        [0.5295, 0.3410, 0.2932],
        [0.4985, 0.3774, 0.3036],
        [0.5380, 0.3660, 0.2827],
        [0.4915, 0.3796, 0.3038],
        [0.5106, 0.3701, 0.2921],
        [0.5810, 0.3369, 0.2366],
        [0.5072, 0.3593, 0.3021],
        [0.5108, 0.3792, 0.3133],
        [0.4029, 0.3971, 0.3911],
        [0.4454, 0.3720, 0.3660],
        [0.4261, 0.4052, 0.3765],
        [0.4542, 0.3781, 0.3425],
        [0.5107, 0.3549, 0.3058],
        [0.5005, 0.3663, 0.3088],
        [0.4942, 0.3632, 0.3108],
        [0.5017, 0.3469, 0.3123],
        [0.5154, 0.3582, 0.3113],
        [0.5082, 0.3626, 0.3053],
        [0.4267, 0.4142, 0.3475],
        [0.5427, 0.3441, 0.2785],
        [0.4935, 0.3680, 0.3140],
        [0.5221, 0.3536, 0.2907],
        [0.4655, 0.3808, 0.3359],
        [0.5192, 0.3490, 0.2917],
        [0.5141, 0.3695, 0.2991],
        [0.4782, 0.3933, 0.2979],
        [0.4997, 0.3686, 0.2967],
        [0.5418, 0.3469, 0.2760],
        [0.4105, 0.3965, 0.3964],
        [0.4479, 0.3836, 0.3585],
        [0.4175, 0.3923, 0.3548]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [ 6, 17, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17, 10],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [ 6, 17, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 74: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15880.0, Mean: 1942.9759521484375, Std: 1213.6436767578125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 158665.21875, Mean: 19412.40625, Std: 12126.2119140625
[DEBUG] Top-3 class probabilities:
tensor([[0.4225, 0.4168, 0.3746],
        [0.4570, 0.4452, 0.3272],
        [0.4821, 0.4545, 0.2729],
        [0.4193, 0.3902, 0.3873],
        [0.4095, 0.3985, 0.3571],
        [0.5070, 0.3623, 0.2685],
        [0.5648, 0.3381, 0.2468],
        [0.5336, 0.3390, 0.2390],
        [0.4835, 0.3660, 0.2926],
        [0.4871, 0.3855, 0.3063],
        [0.4994, 0.3822, 0.2952],
        [0.5067, 0.3487, 0.2604],
        [0.5365, 0.3478, 0.2775],
        [0.4594, 0.3874, 0.3408],
        [0.4605, 0.3995, 0.3329],
        [0.4353, 0.4137, 0.3323],
        [0.4777, 0.3798, 0.3180],
        [0.5430, 0.3685, 0.2767],
        [0.5403, 0.3463, 0.2631],
        [0.4565, 0.4022, 0.3332],
        [0.5001, 0.3855, 0.2869],
        [0.5003, 0.3788, 0.3001],
        [0.4562, 0.3948, 0.3412],
        [0.5224, 0.3482, 0.2971],
        [0.5317, 0.3428, 0.2891],
        [0.4555, 0.4051, 0.3459],
        [0.4629, 0.3776, 0.3388],
        [0.4677, 0.3684, 0.3428],
        [0.5228, 0.3534, 0.2917],
        [0.5765, 0.3317, 0.2479],
        [0.5132, 0.3604, 0.3153],
        [0.4543, 0.3824, 0.3514],
        [0.5376, 0.3436, 0.2884],
        [0.4813, 0.3736, 0.3336],
        [0.4894, 0.3771, 0.3125],
        [0.5149, 0.3614, 0.3009],
        [0.4928, 0.3730, 0.3185],
        [0.4871, 0.3683, 0.3270],
        [0.4957, 0.3601, 0.3131],
        [0.5156, 0.3698, 0.2979],
        [0.5311, 0.3605, 0.2675],
        [0.4806, 0.3707, 0.3216],
        [0.4410, 0.3864, 0.3553],
        [0.4430, 0.3870, 0.3550],
        [0.4540, 0.3693, 0.3562],
        [0.4036, 0.3941, 0.3686],
        [0.4290, 0.4137, 0.3880],
        [0.4320, 0.4304, 0.3609],
        [0.4362, 0.4098, 0.3553],
        [0.4779, 0.3794, 0.3085],
        [0.4628, 0.3776, 0.3172],
        [0.5058, 0.3778, 0.2917],
        [0.4120, 0.3790, 0.3786],
        [0.4092, 0.4032, 0.3765],
        [0.5728, 0.3333, 0.2464],
        [0.5405, 0.3437, 0.2680],
        [0.5267, 0.3688, 0.2844],
        [0.5876, 0.3437, 0.2360],
        [0.5631, 0.3476, 0.2587],
        [0.4627, 0.3908, 0.3270],
        [0.4707, 0.3799, 0.3326],
        [0.4552, 0.3717, 0.3653],
        [0.4774, 0.3876, 0.3252],
        [0.4952, 0.3700, 0.3055],
        [0.4983, 0.3844, 0.3042],
        [0.5477, 0.3549, 0.2666],
        [0.5221, 0.3761, 0.2860],
        [0.4741, 0.3873, 0.3026],
        [0.4662, 0.3867, 0.3313],
        [0.4295, 0.4065, 0.3711],
        [0.5478, 0.3450, 0.2825],
        [0.4139, 0.3814, 0.3802],
        [0.4672, 0.3756, 0.3510],
        [0.4412, 0.3815, 0.3633],
        [0.5510, 0.3479, 0.2692],
        [0.5696, 0.3430, 0.2550],
        [0.5094, 0.3563, 0.3106],
        [0.4529, 0.3727, 0.3456],
        [0.4711, 0.3731, 0.3367],
        [0.4678, 0.3674, 0.3364],
        [0.4577, 0.3894, 0.3512],
        [0.5313, 0.3654, 0.2830],
        [0.5704, 0.3443, 0.2560],
        [0.4518, 0.3864, 0.3484],
        [0.4454, 0.3719, 0.3582],
        [0.4150, 0.3974, 0.3742],
        [0.4642, 0.3737, 0.3336],
        [0.4463, 0.3840, 0.3394],
        [0.4599, 0.3873, 0.3433],
        [0.4245, 0.3920, 0.3776],
        [0.4155, 0.3927, 0.3837],
        [0.4106, 0.3961, 0.3888],
        [0.4147, 0.3979, 0.3927],
        [0.4163, 0.3908, 0.3872],
        [0.5354, 0.3546, 0.2648],
        [0.5657, 0.3331, 0.2427],
        [0.4530, 0.3867, 0.3247],
        [0.4776, 0.3748, 0.3035],
        [0.4499, 0.3859, 0.3251],
        [0.4800, 0.3764, 0.3060],
        [0.4386, 0.3798, 0.3797],
        [0.4141, 0.4133, 0.3521],
        [0.4589, 0.3601, 0.3524],
        [0.5187, 0.3727, 0.2784],
        [0.4636, 0.4088, 0.3190],
        [0.4680, 0.3815, 0.3532],
        [0.4555, 0.3728, 0.3605],
        [0.4887, 0.3852, 0.3062],
        [0.4721, 0.4003, 0.3263],
        [0.5415, 0.3483, 0.2660],
        [0.5082, 0.3700, 0.2947],
        [0.4937, 0.3829, 0.2998],
        [0.5220, 0.3628, 0.2715],
        [0.4148, 0.4086, 0.3681],
        [0.4157, 0.3994, 0.3788],
        [0.4236, 0.3907, 0.3682],
        [0.4525, 0.3973, 0.3547],
        [0.4151, 0.3990, 0.3883],
        [0.4098, 0.4038, 0.3998],
        [0.4161, 0.3825, 0.3820],
        [0.4161, 0.3929, 0.3783],
        [0.4327, 0.3935, 0.3657],
        [0.5083, 0.3556, 0.3061],
        [0.4962, 0.3604, 0.3097],
        [0.4427, 0.3956, 0.3649],
        [0.4343, 0.3907, 0.3660],
        [0.5248, 0.3558, 0.3019],
        [0.4640, 0.3795, 0.3366]])
[DEBUG] Top-3 class indices:
tensor([[17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [ 6, 17, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [ 6, 17, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 75: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7404.0, Mean: 1677.6097412109375, Std: 1258.18798828125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 73976.6328125, Mean: 16760.984375, Std: 12571.2783203125
[DEBUG] Top-3 class probabilities:
tensor([[0.5135, 0.3646, 0.3080],
        [0.4467, 0.3782, 0.3525],
        [0.4726, 0.3594, 0.3449],
        [0.4286, 0.4058, 0.3606],
        [0.4690, 0.3688, 0.3415],
        [0.4082, 0.3919, 0.3913],
        [0.4172, 0.4037, 0.3722],
        [0.4352, 0.3817, 0.3630],
        [0.5662, 0.3326, 0.2516],
        [0.4167, 0.4008, 0.3694],
        [0.4192, 0.4036, 0.3612],
        [0.4218, 0.3998, 0.3630],
        [0.4113, 0.4027, 0.3826],
        [0.5691, 0.3333, 0.2335],
        [0.4142, 0.4140, 0.3621],
        [0.4540, 0.3764, 0.3145],
        [0.4680, 0.3823, 0.2975],
        [0.4303, 0.4042, 0.3544],
        [0.4904, 0.3753, 0.2955],
        [0.5730, 0.3275, 0.2792],
        [0.5042, 0.3624, 0.2780],
        [0.5196, 0.3731, 0.2917],
        [0.4969, 0.3852, 0.2876],
        [0.4495, 0.4057, 0.3198],
        [0.4250, 0.4147, 0.3461],
        [0.4887, 0.3763, 0.3109],
        [0.4264, 0.3942, 0.3655],
        [0.4880, 0.3750, 0.3102],
        [0.4860, 0.3811, 0.3279],
        [0.5251, 0.3650, 0.2902],
        [0.5118, 0.3618, 0.2907],
        [0.4711, 0.3807, 0.3033],
        [0.5031, 0.3592, 0.3014],
        [0.5456, 0.3482, 0.2702],
        [0.4173, 0.4013, 0.3796],
        [0.4506, 0.4020, 0.3753],
        [0.4032, 0.4028, 0.3998],
        [0.4412, 0.3906, 0.3643],
        [0.4051, 0.4004, 0.3804],
        [0.4565, 0.3780, 0.3519],
        [0.4908, 0.3641, 0.3258],
        [0.4810, 0.3611, 0.3293],
        [0.4745, 0.3734, 0.3366],
        [0.4700, 0.3802, 0.3392],
        [0.4938, 0.3687, 0.3114],
        [0.5133, 0.3570, 0.2968],
        [0.4116, 0.3980, 0.3776],
        [0.4986, 0.3674, 0.3238],
        [0.4441, 0.3995, 0.3395],
        [0.4442, 0.3911, 0.3534],
        [0.4597, 0.3856, 0.3460],
        [0.4006, 0.3969, 0.3945],
        [0.4342, 0.3922, 0.3568],
        [0.4367, 0.3825, 0.3566],
        [0.4049, 0.4002, 0.3892],
        [0.4182, 0.3962, 0.3662],
        [0.4367, 0.4054, 0.3761],
        [0.4519, 0.3890, 0.3461],
        [0.4544, 0.3685, 0.3536],
        [0.4758, 0.3911, 0.3329],
        [0.4426, 0.3906, 0.3656],
        [0.5081, 0.3487, 0.3184],
        [0.5241, 0.3599, 0.3140],
        [0.4986, 0.3644, 0.3214],
        [0.4972, 0.3561, 0.3255],
        [0.5022, 0.3793, 0.3008],
        [0.5346, 0.3523, 0.3137],
        [0.6049, 0.3011, 0.2850],
        [0.5550, 0.3484, 0.2677],
        [0.5257, 0.3598, 0.3109],
        [0.4731, 0.3817, 0.3528],
        [0.4662, 0.3872, 0.3581],
        [0.4816, 0.3736, 0.3248],
        [0.4834, 0.3841, 0.3117],
        [0.4805, 0.3715, 0.3313],
        [0.4670, 0.3962, 0.3237],
        [0.5192, 0.3611, 0.3063],
        [0.4943, 0.3731, 0.3239],
        [0.5127, 0.3657, 0.3251],
        [0.5081, 0.3759, 0.3157],
        [0.4520, 0.4147, 0.3434],
        [0.5230, 0.3474, 0.3035],
        [0.4840, 0.3692, 0.3280],
        [0.5241, 0.3729, 0.2911],
        [0.4972, 0.3606, 0.3437],
        [0.5075, 0.3668, 0.3198],
        [0.4534, 0.4057, 0.3485],
        [0.4967, 0.3573, 0.3317],
        [0.4521, 0.3921, 0.3742],
        [0.5336, 0.3534, 0.3021],
        [0.5116, 0.3704, 0.3164],
        [0.5612, 0.3379, 0.2614],
        [0.4557, 0.4007, 0.3398],
        [0.5377, 0.3393, 0.3015],
        [0.5087, 0.3542, 0.3260],
        [0.4471, 0.3898, 0.3656],
        [0.4897, 0.3550, 0.3368],
        [0.4616, 0.3697, 0.3606],
        [0.5208, 0.3684, 0.3042],
        [0.4586, 0.3758, 0.3661],
        [0.5067, 0.3584, 0.3197],
        [0.5251, 0.3618, 0.3055],
        [0.5450, 0.3461, 0.3029],
        [0.5355, 0.3439, 0.2768],
        [0.5952, 0.3060, 0.2882],
        [0.4753, 0.3659, 0.3562],
        [0.4904, 0.3662, 0.3381],
        [0.4861, 0.3843, 0.3358],
        [0.5170, 0.3803, 0.3079],
        [0.4325, 0.4279, 0.3441],
        [0.4715, 0.3704, 0.3535],
        [0.4830, 0.3846, 0.3298],
        [0.5149, 0.3556, 0.3209],
        [0.5043, 0.3785, 0.3163],
        [0.4796, 0.3807, 0.3379],
        [0.5529, 0.3385, 0.2677],
        [0.5380, 0.3537, 0.2724],
        [0.4938, 0.3628, 0.3282],
        [0.5032, 0.3672, 0.3278],
        [0.5019, 0.3760, 0.3126],
        [0.4838, 0.3748, 0.3556],
        [0.4593, 0.3799, 0.3734],
        [0.5399, 0.3428, 0.3099],
        [0.5232, 0.3626, 0.3158],
        [0.4765, 0.3953, 0.3385],
        [0.4942, 0.3789, 0.3248],
        [0.4794, 0.3738, 0.3313],
        [0.5217, 0.3594, 0.3137]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 76: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7248.0, Mean: 1468.49755859375, Std: 1197.722900390625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 72417.953125, Mean: 14671.6220703125, Std: 11967.1376953125
[DEBUG] Top-3 class probabilities:
tensor([[0.4959, 0.3782, 0.3368],
        [0.4997, 0.3670, 0.3048],
        [0.4571, 0.3738, 0.3642],
        [0.4821, 0.3684, 0.3490],
        [0.4536, 0.3773, 0.3642],
        [0.4914, 0.3682, 0.3468],
        [0.4724, 0.3742, 0.3539],
        [0.5193, 0.3646, 0.3261],
        [0.4896, 0.3736, 0.3392],
        [0.5074, 0.3648, 0.3174],
        [0.5326, 0.3492, 0.3032],
        [0.5119, 0.3606, 0.3120],
        [0.4900, 0.3652, 0.3352],
        [0.5630, 0.3429, 0.2737],
        [0.5191, 0.3513, 0.2817],
        [0.5228, 0.3669, 0.2936],
        [0.5021, 0.3645, 0.3313],
        [0.4531, 0.3740, 0.3650],
        [0.4594, 0.3824, 0.3513],
        [0.4455, 0.3921, 0.3739],
        [0.5081, 0.3601, 0.3355],
        [0.4603, 0.4052, 0.3189],
        [0.5345, 0.3521, 0.2947],
        [0.5002, 0.3651, 0.3126],
        [0.4455, 0.3841, 0.3730],
        [0.4845, 0.3774, 0.3452],
        [0.5751, 0.3433, 0.2644],
        [0.5157, 0.3636, 0.3043],
        [0.4682, 0.3855, 0.3522],
        [0.5209, 0.3568, 0.3160],
        [0.5126, 0.3636, 0.3196],
        [0.4853, 0.3881, 0.3385],
        [0.5606, 0.3491, 0.2847],
        [0.4906, 0.3515, 0.3393],
        [0.4783, 0.3858, 0.3137],
        [0.4542, 0.3828, 0.3573],
        [0.5340, 0.3518, 0.2909],
        [0.4953, 0.3670, 0.3395],
        [0.4571, 0.3846, 0.3719],
        [0.4578, 0.3834, 0.3630],
        [0.4666, 0.3801, 0.3604],
        [0.4903, 0.3601, 0.3382],
        [0.4749, 0.3773, 0.3366],
        [0.4953, 0.3636, 0.3372],
        [0.5097, 0.3566, 0.3276],
        [0.4727, 0.3797, 0.3394],
        [0.5080, 0.3616, 0.3216],
        [0.4104, 0.3968, 0.3853],
        [0.4932, 0.3641, 0.3559],
        [0.4877, 0.3652, 0.3435],
        [0.5082, 0.3579, 0.3272],
        [0.4561, 0.3736, 0.3606],
        [0.4466, 0.3751, 0.3613],
        [0.4808, 0.3624, 0.3509],
        [0.4733, 0.3702, 0.3506],
        [0.4641, 0.3704, 0.3585],
        [0.4769, 0.3874, 0.3386],
        [0.4811, 0.3704, 0.3332],
        [0.5068, 0.3760, 0.3127],
        [0.4482, 0.4114, 0.3704],
        [0.4595, 0.3663, 0.3538],
        [0.5605, 0.3296, 0.2830],
        [0.5163, 0.3600, 0.3175],
        [0.5103, 0.3826, 0.3159],
        [0.4912, 0.3731, 0.3098],
        [0.4916, 0.3676, 0.3353],
        [0.5145, 0.3719, 0.2895],
        [0.4896, 0.3779, 0.3398],
        [0.4282, 0.4084, 0.3739],
        [0.4752, 0.3777, 0.3409],
        [0.5137, 0.3655, 0.3161],
        [0.4979, 0.3852, 0.3450],
        [0.5057, 0.3735, 0.3406],
        [0.5052, 0.3739, 0.3270],
        [0.5136, 0.3509, 0.3139],
        [0.5116, 0.3557, 0.3190],
        [0.5017, 0.3550, 0.3432],
        [0.4694, 0.3708, 0.3506],
        [0.5279, 0.3397, 0.3204],
        [0.4944, 0.3691, 0.3211],
        [0.5474, 0.3533, 0.2990],
        [0.4522, 0.3904, 0.3625],
        [0.4523, 0.3704, 0.3656],
        [0.5327, 0.3452, 0.3112],
        [0.5039, 0.3746, 0.3267],
        [0.5201, 0.3575, 0.2953],
        [0.5328, 0.3428, 0.3007],
        [0.4654, 0.3795, 0.3463],
        [0.4319, 0.3891, 0.3598],
        [0.4869, 0.3780, 0.3387],
        [0.4390, 0.3797, 0.3729],
        [0.4685, 0.3645, 0.3621],
        [0.4686, 0.3737, 0.3647],
        [0.5085, 0.3581, 0.3183],
        [0.4974, 0.3556, 0.3380],
        [0.4551, 0.3667, 0.3639],
        [0.4823, 0.3653, 0.3430],
        [0.4233, 0.3872, 0.3752],
        [0.4859, 0.3591, 0.3471],
        [0.5122, 0.3608, 0.3020],
        [0.4953, 0.3629, 0.3411],
        [0.4588, 0.3697, 0.3590],
        [0.4434, 0.3640, 0.3632],
        [0.4569, 0.3669, 0.3535],
        [0.5099, 0.3594, 0.3456],
        [0.4741, 0.3658, 0.3563],
        [0.4608, 0.3649, 0.3594],
        [0.4861, 0.3609, 0.3470],
        [0.4389, 0.3876, 0.3859],
        [0.4664, 0.3694, 0.3464],
        [0.5159, 0.3588, 0.3147],
        [0.5310, 0.3595, 0.2890],
        [0.5032, 0.3599, 0.3293],
        [0.5131, 0.3490, 0.3271],
        [0.4759, 0.3614, 0.3589],
        [0.4715, 0.3777, 0.3449],
        [0.4855, 0.3790, 0.3224],
        [0.4983, 0.3682, 0.3488],
        [0.5382, 0.3450, 0.2942],
        [0.5117, 0.3615, 0.3313],
        [0.5046, 0.3622, 0.3261],
        [0.4756, 0.3709, 0.3469],
        [0.4767, 0.3724, 0.3444],
        [0.4659, 0.3742, 0.3635],
        [0.5274, 0.3643, 0.3174],
        [0.4600, 0.3863, 0.3586],
        [0.5031, 0.3610, 0.3166],
        [0.5149, 0.3556, 0.3283]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 77: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8060.0, Mean: 1375.6199951171875, Std: 1097.4630126953125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 80531.109375, Mean: 13743.6298828125, Std: 10965.3828125
[DEBUG] Top-3 class probabilities:
tensor([[0.4959, 0.3750, 0.3446],
        [0.4912, 0.3646, 0.3465],
        [0.5074, 0.3607, 0.3339],
        [0.4233, 0.3994, 0.3797],
        [0.5006, 0.3555, 0.3242],
        [0.4692, 0.3777, 0.3444],
        [0.4743, 0.3764, 0.3564],
        [0.4874, 0.3762, 0.2979],
        [0.5396, 0.3494, 0.2518],
        [0.4288, 0.3765, 0.3754],
        [0.5299, 0.3322, 0.3085],
        [0.4701, 0.3559, 0.3551],
        [0.5152, 0.3558, 0.3221],
        [0.4728, 0.3542, 0.3535],
        [0.4826, 0.3654, 0.3586],
        [0.4603, 0.3731, 0.3678],
        [0.4701, 0.3706, 0.3703],
        [0.4834, 0.3627, 0.3528],
        [0.4998, 0.3636, 0.3395],
        [0.4366, 0.3838, 0.3803],
        [0.4676, 0.3666, 0.3619],
        [0.5083, 0.3533, 0.3281],
        [0.5064, 0.3658, 0.3323],
        [0.5249, 0.3544, 0.3302],
        [0.5107, 0.3656, 0.3253],
        [0.5045, 0.3524, 0.3248],
        [0.4902, 0.3602, 0.3513],
        [0.5108, 0.3542, 0.3218],
        [0.5781, 0.3272, 0.2652],
        [0.5304, 0.3468, 0.3001],
        [0.4663, 0.3750, 0.3728],
        [0.5008, 0.3605, 0.3260],
        [0.4225, 0.4021, 0.3895],
        [0.4867, 0.3478, 0.3452],
        [0.4812, 0.3640, 0.3460],
        [0.4804, 0.3573, 0.3521],
        [0.4503, 0.3808, 0.3741],
        [0.4562, 0.3845, 0.3564],
        [0.4886, 0.3717, 0.3419],
        [0.4747, 0.3712, 0.3530],
        [0.4948, 0.3621, 0.3328],
        [0.4956, 0.3701, 0.3309],
        [0.5362, 0.3510, 0.2991],
        [0.5079, 0.3655, 0.3206],
        [0.4882, 0.3707, 0.3511],
        [0.4606, 0.3711, 0.3553],
        [0.4716, 0.3742, 0.3283],
        [0.4448, 0.3804, 0.3755],
        [0.4884, 0.3696, 0.3437],
        [0.4635, 0.3856, 0.3551],
        [0.4504, 0.3770, 0.3753],
        [0.4160, 0.3858, 0.3723],
        [0.4226, 0.3842, 0.3832],
        [0.4332, 0.3684, 0.3647],
        [0.4460, 0.3826, 0.3715],
        [0.5265, 0.3480, 0.3170],
        [0.4464, 0.3930, 0.3732],
        [0.4733, 0.3577, 0.3489],
        [0.4548, 0.3755, 0.3702],
        [0.4973, 0.3603, 0.3403],
        [0.4713, 0.3713, 0.3641],
        [0.4874, 0.3675, 0.3501],
        [0.4864, 0.3711, 0.3506],
        [0.4905, 0.3806, 0.3541],
        [0.4707, 0.3671, 0.3446],
        [0.4808, 0.3764, 0.3553],
        [0.4453, 0.3835, 0.3747],
        [0.5751, 0.3351, 0.2658],
        [0.4695, 0.3627, 0.3503],
        [0.5050, 0.3595, 0.3334],
        [0.5103, 0.3584, 0.3265],
        [0.5101, 0.3499, 0.3386],
        [0.5382, 0.3528, 0.3014],
        [0.5167, 0.3511, 0.3308],
        [0.4807, 0.3673, 0.3509],
        [0.4861, 0.3581, 0.3488],
        [0.4832, 0.3598, 0.3442],
        [0.4634, 0.3693, 0.3619],
        [0.5150, 0.3522, 0.3228],
        [0.6161, 0.2961, 0.2518],
        [0.5969, 0.3167, 0.2496],
        [0.5595, 0.3429, 0.2842],
        [0.4916, 0.3772, 0.3432],
        [0.4855, 0.3760, 0.3453],
        [0.4617, 0.3806, 0.3609],
        [0.5296, 0.3703, 0.2991],
        [0.4905, 0.3668, 0.3367],
        [0.5059, 0.3662, 0.3310],
        [0.5590, 0.3430, 0.2979],
        [0.5369, 0.3447, 0.2847],
        [0.4635, 0.3736, 0.3576],
        [0.4929, 0.3758, 0.3283],
        [0.4765, 0.3747, 0.3241],
        [0.5196, 0.3326, 0.3023],
        [0.5565, 0.3029, 0.2818],
        [0.4154, 0.3847, 0.3804],
        [0.4411, 0.3878, 0.3616],
        [0.4182, 0.3848, 0.3752],
        [0.4284, 0.3811, 0.3665],
        [0.4366, 0.3786, 0.3725],
        [0.4619, 0.3694, 0.3523],
        [0.5706, 0.3072, 0.2646],
        [0.4849, 0.3681, 0.3445],
        [0.4385, 0.3935, 0.3756],
        [0.4807, 0.3629, 0.3579],
        [0.4692, 0.3683, 0.3651],
        [0.4945, 0.3725, 0.3312],
        [0.4616, 0.3685, 0.3632],
        [0.5311, 0.3398, 0.3209],
        [0.4659, 0.3728, 0.3698],
        [0.5110, 0.3607, 0.3268],
        [0.4829, 0.3917, 0.3373],
        [0.4997, 0.3733, 0.3192],
        [0.4393, 0.3745, 0.3701],
        [0.5202, 0.3575, 0.3262],
        [0.5220, 0.3627, 0.3200],
        [0.4739, 0.3644, 0.3577],
        [0.5027, 0.3737, 0.3218],
        [0.4771, 0.3787, 0.3494],
        [0.5112, 0.3684, 0.3366],
        [0.5143, 0.3593, 0.3245],
        [0.4740, 0.3678, 0.3653],
        [0.5013, 0.3654, 0.3303],
        [0.5643, 0.3392, 0.2854],
        [0.5722, 0.3356, 0.2660],
        [0.5466, 0.3399, 0.2830],
        [0.5024, 0.3555, 0.3262],
        [0.5140, 0.3645, 0.3143]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 78: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8528.0, Mean: 1397.259033203125, Std: 1163.9755859375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 85207.171875, Mean: 13959.8349609375, Std: 11629.9482421875
[DEBUG] Top-3 class probabilities:
tensor([[0.4996, 0.3621, 0.3415],
        [0.5315, 0.3423, 0.3083],
        [0.4931, 0.3736, 0.3329],
        [0.5624, 0.3526, 0.2808],
        [0.4586, 0.3902, 0.3555],
        [0.4742, 0.3705, 0.3465],
        [0.4843, 0.3740, 0.3314],
        [0.4557, 0.3949, 0.3400],
        [0.4879, 0.3730, 0.3087],
        [0.5578, 0.3417, 0.2670],
        [0.5139, 0.3601, 0.3064],
        [0.6003, 0.2909, 0.2522],
        [0.5971, 0.3023, 0.2575],
        [0.6201, 0.2855, 0.2774],
        [0.5483, 0.3239, 0.2765],
        [0.5244, 0.3480, 0.3148],
        [0.5923, 0.3072, 0.2664],
        [0.4422, 0.3845, 0.3737],
        [0.4732, 0.3638, 0.3498],
        [0.4825, 0.3716, 0.3444],
        [0.4495, 0.3895, 0.3657],
        [0.4580, 0.3624, 0.3621],
        [0.4480, 0.3835, 0.3698],
        [0.4429, 0.3838, 0.3735],
        [0.4641, 0.3638, 0.3626],
        [0.4914, 0.3553, 0.3486],
        [0.6045, 0.3037, 0.2494],
        [0.4639, 0.3788, 0.3531],
        [0.5092, 0.3608, 0.3324],
        [0.5115, 0.3580, 0.3244],
        [0.4917, 0.3680, 0.3254],
        [0.4699, 0.3876, 0.3556],
        [0.5150, 0.3466, 0.3280],
        [0.4669, 0.3904, 0.3396],
        [0.5297, 0.3516, 0.2948],
        [0.5168, 0.3604, 0.3172],
        [0.5101, 0.3757, 0.3229],
        [0.5133, 0.3628, 0.3222],
        [0.5399, 0.3505, 0.2797],
        [0.4975, 0.3574, 0.3418],
        [0.5421, 0.3368, 0.3156],
        [0.5252, 0.3452, 0.2985],
        [0.4784, 0.3771, 0.3393],
        [0.4848, 0.3777, 0.3316],
        [0.4964, 0.3702, 0.3219],
        [0.5057, 0.3669, 0.3166],
        [0.5445, 0.3418, 0.2998],
        [0.5030, 0.3658, 0.3217],
        [0.4861, 0.3936, 0.3197],
        [0.6322, 0.3137, 0.2330],
        [0.4721, 0.3831, 0.3324],
        [0.4480, 0.3989, 0.3532],
        [0.4482, 0.3928, 0.3739],
        [0.5009, 0.3796, 0.3251],
        [0.5271, 0.3518, 0.3183],
        [0.5000, 0.3836, 0.3285],
        [0.5109, 0.3614, 0.3244],
        [0.6466, 0.2868, 0.2673],
        [0.6531, 0.2765, 0.2723],
        [0.4773, 0.3675, 0.3489],
        [0.5064, 0.3487, 0.3201],
        [0.4699, 0.3624, 0.3388],
        [0.4894, 0.3585, 0.3467],
        [0.4152, 0.4133, 0.3813],
        [0.4085, 0.4030, 0.3909],
        [0.4700, 0.3891, 0.3498],
        [0.4316, 0.3851, 0.3736],
        [0.4199, 0.3923, 0.3824],
        [0.4598, 0.3764, 0.3670],
        [0.4364, 0.3744, 0.3717],
        [0.4425, 0.3822, 0.3777],
        [0.5320, 0.3432, 0.3048],
        [0.5314, 0.3333, 0.3006],
        [0.5462, 0.3474, 0.2951],
        [0.6064, 0.3121, 0.2490],
        [0.4906, 0.3563, 0.3369],
        [0.4534, 0.3889, 0.3780],
        [0.5155, 0.3578, 0.3161],
        [0.4427, 0.3875, 0.3694],
        [0.4736, 0.3718, 0.3611],
        [0.5024, 0.3593, 0.3311],
        [0.5361, 0.3424, 0.3033],
        [0.4869, 0.3632, 0.3299],
        [0.4983, 0.3764, 0.3476],
        [0.5016, 0.3772, 0.3253],
        [0.5233, 0.3760, 0.3203],
        [0.5114, 0.3655, 0.3208],
        [0.4692, 0.3914, 0.3468],
        [0.5286, 0.3640, 0.2994],
        [0.5165, 0.3640, 0.3182],
        [0.4904, 0.3764, 0.3290],
        [0.5300, 0.3615, 0.3005],
        [0.5043, 0.3699, 0.3279],
        [0.5221, 0.3630, 0.3048],
        [0.4584, 0.4042, 0.3497],
        [0.5192, 0.3590, 0.2730],
        [0.5385, 0.3546, 0.2837],
        [0.4937, 0.3790, 0.3277],
        [0.4584, 0.3895, 0.3510],
        [0.4744, 0.3739, 0.3558],
        [0.5230, 0.3663, 0.3052],
        [0.5111, 0.3634, 0.3361],
        [0.5085, 0.3596, 0.3431],
        [0.5176, 0.3414, 0.3125],
        [0.4902, 0.3540, 0.3291],
        [0.4435, 0.3943, 0.3655],
        [0.4040, 0.3982, 0.3910],
        [0.4098, 0.3807, 0.3799],
        [0.4288, 0.3873, 0.3725],
        [0.4228, 0.3840, 0.3782],
        [0.4095, 0.3921, 0.3854],
        [0.4497, 0.3844, 0.3721],
        [0.4463, 0.3721, 0.3692],
        [0.4194, 0.3781, 0.3769],
        [0.4369, 0.3745, 0.3704],
        [0.4457, 0.3658, 0.3642],
        [0.4762, 0.3592, 0.3495],
        [0.4812, 0.3759, 0.3473],
        [0.4805, 0.3746, 0.3473],
        [0.5813, 0.3207, 0.2602],
        [0.5496, 0.3468, 0.2738],
        [0.5594, 0.3285, 0.2666],
        [0.5222, 0.3442, 0.3243],
        [0.4631, 0.3783, 0.3738],
        [0.4946, 0.3678, 0.3480],
        [0.4847, 0.3648, 0.3464],
        [0.4538, 0.3809, 0.3679],
        [0.5189, 0.3547, 0.3241]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [17,  6, 11],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 79: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6796.0, Mean: 1345.939697265625, Std: 1157.8111572265625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 67901.7578125, Mean: 13447.0751953125, Std: 11568.3564453125
[DEBUG] Top-3 class probabilities:
tensor([[0.5158, 0.3605, 0.3229],
        [0.4926, 0.3594, 0.3286],
        [0.5064, 0.3596, 0.3251],
        [0.5741, 0.3239, 0.2487],
        [0.6109, 0.3168, 0.2511],
        [0.6575, 0.2860, 0.2743],
        [0.5049, 0.3517, 0.3173],
        [0.5144, 0.3591, 0.3119],
        [0.4994, 0.3655, 0.3286],
        [0.5341, 0.3436, 0.3228],
        [0.5594, 0.3430, 0.3021],
        [0.5646, 0.3374, 0.2783],
        [0.4828, 0.3736, 0.3476],
        [0.5191, 0.3618, 0.3068],
        [0.4624, 0.4034, 0.3143],
        [0.4343, 0.4092, 0.3496],
        [0.5198, 0.3652, 0.2945],
        [0.4791, 0.3689, 0.3555],
        [0.5126, 0.3617, 0.3291],
        [0.5349, 0.3623, 0.3017],
        [0.5211, 0.3707, 0.3144],
        [0.4528, 0.3732, 0.3689],
        [0.4494, 0.3678, 0.3670],
        [0.4344, 0.3934, 0.3717],
        [0.4397, 0.3876, 0.3594],
        [0.4414, 0.3839, 0.3759],
        [0.4272, 0.3880, 0.3766],
        [0.4889, 0.3755, 0.3366],
        [0.4537, 0.3706, 0.3647],
        [0.4501, 0.3708, 0.3688],
        [0.4874, 0.3623, 0.3309],
        [0.4631, 0.3763, 0.3613],
        [0.4548, 0.3824, 0.3660],
        [0.4449, 0.3752, 0.3647],
        [0.5128, 0.3581, 0.3213],
        [0.4414, 0.3781, 0.3774],
        [0.4825, 0.3435, 0.3372],
        [0.4379, 0.3840, 0.3811],
        [0.4336, 0.3765, 0.3714],
        [0.6270, 0.2849, 0.2832],
        [0.5125, 0.3546, 0.3297],
        [0.4587, 0.3759, 0.3630],
        [0.5496, 0.3418, 0.3055],
        [0.6212, 0.3156, 0.2470],
        [0.5520, 0.3512, 0.3019],
        [0.4981, 0.3811, 0.3397],
        [0.5568, 0.3414, 0.2901],
        [0.5194, 0.3656, 0.3183],
        [0.5156, 0.3605, 0.3214],
        [0.5476, 0.3489, 0.2959],
        [0.6189, 0.3034, 0.2543],
        [0.6875, 0.2842, 0.2473],
        [0.4963, 0.3677, 0.3176],
        [0.5725, 0.3260, 0.2542],
        [0.4516, 0.4194, 0.3482],
        [0.5392, 0.3457, 0.2960],
        [0.5755, 0.3339, 0.2691],
        [0.5754, 0.3469, 0.2809],
        [0.5103, 0.3677, 0.3247],
        [0.5012, 0.3643, 0.3158],
        [0.4942, 0.3827, 0.3190],
        [0.4585, 0.3826, 0.3688],
        [0.5540, 0.3465, 0.2904],
        [0.5481, 0.3420, 0.3099],
        [0.4941, 0.3841, 0.3291],
        [0.4924, 0.3712, 0.3419],
        [0.5105, 0.3687, 0.3294],
        [0.4373, 0.3838, 0.3683],
        [0.4436, 0.3765, 0.3555],
        [0.4532, 0.3669, 0.3476],
        [0.4447, 0.3750, 0.3736],
        [0.4765, 0.3688, 0.3456],
        [0.4701, 0.3687, 0.3423],
        [0.4188, 0.3973, 0.3851],
        [0.4070, 0.3862, 0.3819],
        [0.4331, 0.3821, 0.3804],
        [0.4377, 0.3871, 0.3774],
        [0.4278, 0.3850, 0.3847],
        [0.4715, 0.3659, 0.3511],
        [0.5135, 0.3512, 0.3270],
        [0.4345, 0.3820, 0.3817],
        [0.5825, 0.3090, 0.2697],
        [0.6701, 0.3054, 0.2510],
        [0.6289, 0.2712, 0.2674],
        [0.4729, 0.3710, 0.3478],
        [0.5642, 0.3174, 0.2920],
        [0.5367, 0.3531, 0.3091],
        [0.5074, 0.3405, 0.3349],
        [0.5406, 0.3442, 0.2905],
        [0.6877, 0.2897, 0.2572],
        [0.6288, 0.3078, 0.2551],
        [0.5477, 0.3505, 0.2911],
        [0.4994, 0.3588, 0.3106],
        [0.5291, 0.3644, 0.3027],
        [0.5026, 0.3627, 0.3252],
        [0.5181, 0.3641, 0.3197],
        [0.4704, 0.4027, 0.3371],
        [0.5365, 0.3573, 0.2796],
        [0.5055, 0.3646, 0.3130],
        [0.5749, 0.3252, 0.2665],
        [0.6198, 0.3029, 0.2388],
        [0.5120, 0.3715, 0.3106],
        [0.5279, 0.3571, 0.3035],
        [0.5429, 0.3631, 0.3010],
        [0.5197, 0.3495, 0.3200],
        [0.5308, 0.3444, 0.3015],
        [0.4807, 0.3894, 0.3332],
        [0.5092, 0.3607, 0.3234],
        [0.6180, 0.3097, 0.2695],
        [0.5248, 0.3520, 0.3178],
        [0.5296, 0.3561, 0.3111],
        [0.4882, 0.3752, 0.3387],
        [0.5065, 0.3784, 0.3313],
        [0.5842, 0.3118, 0.2613],
        [0.5147, 0.3396, 0.3102],
        [0.4869, 0.3599, 0.3052],
        [0.4535, 0.3731, 0.3625],
        [0.4702, 0.3677, 0.3497],
        [0.4718, 0.3583, 0.3504],
        [0.4285, 0.3823, 0.3686],
        [0.4128, 0.3892, 0.3889],
        [0.4256, 0.3817, 0.3744],
        [0.4143, 0.4050, 0.3871],
        [0.4322, 0.3938, 0.3842],
        [0.4115, 0.4076, 0.4005],
        [0.4935, 0.3612, 0.3380],
        [0.4774, 0.3585, 0.3567],
        [0.4145, 0.4107, 0.3961]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 80: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7080.0, Mean: 1268.91796875, Std: 1194.2813720703125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 70739.3671875, Mean: 12677.509765625, Std: 11932.7509765625
[DEBUG] Top-3 class probabilities:
tensor([[0.4556, 0.3899, 0.3590],
        [0.6367, 0.2857, 0.2646],
        [0.5761, 0.3207, 0.2762],
        [0.4404, 0.3983, 0.3652],
        [0.5653, 0.3312, 0.2855],
        [0.5946, 0.3135, 0.2632],
        [0.6724, 0.2836, 0.2618],
        [0.7015, 0.3124, 0.2306],
        [0.6953, 0.2918, 0.2444],
        [0.7036, 0.3068, 0.2316],
        [0.6386, 0.3026, 0.2590],
        [0.5161, 0.3688, 0.3286],
        [0.5237, 0.3584, 0.3140],
        [0.4921, 0.3736, 0.3328],
        [0.5411, 0.3523, 0.2860],
        [0.5687, 0.3300, 0.2528],
        [0.4562, 0.3877, 0.3500],
        [0.5363, 0.3524, 0.2904],
        [0.5797, 0.3132, 0.2704],
        [0.6344, 0.2915, 0.2430],
        [0.5236, 0.3804, 0.2974],
        [0.5705, 0.3321, 0.2535],
        [0.5276, 0.3764, 0.3089],
        [0.5054, 0.3542, 0.3329],
        [0.4627, 0.3876, 0.3625],
        [0.4695, 0.3815, 0.3492],
        [0.4716, 0.3724, 0.3375],
        [0.4241, 0.4060, 0.3714],
        [0.5050, 0.3679, 0.3361],
        [0.5056, 0.3654, 0.3347],
        [0.5413, 0.3549, 0.2980],
        [0.4838, 0.3628, 0.3526],
        [0.4832, 0.3513, 0.3451],
        [0.5095, 0.3491, 0.3060],
        [0.4676, 0.3752, 0.3133],
        [0.4520, 0.3701, 0.3559],
        [0.4146, 0.4007, 0.3984],
        [0.4072, 0.4033, 0.3980],
        [0.4214, 0.3855, 0.3809],
        [0.4016, 0.3985, 0.3917],
        [0.4220, 0.3853, 0.3644],
        [0.4124, 0.4079, 0.3726],
        [0.4606, 0.3973, 0.3701],
        [0.4998, 0.3610, 0.3392],
        [0.5505, 0.3366, 0.2911],
        [0.5583, 0.3303, 0.2815],
        [0.4633, 0.3700, 0.3582],
        [0.5983, 0.3088, 0.2591],
        [0.6194, 0.2871, 0.2865],
        [0.5877, 0.3053, 0.2679],
        [0.4728, 0.3654, 0.3396],
        [0.4973, 0.3506, 0.3402],
        [0.6474, 0.2860, 0.2668],
        [0.6678, 0.2924, 0.2602],
        [0.6864, 0.2931, 0.2537],
        [0.5856, 0.2978, 0.2741],
        [0.5260, 0.3519, 0.3086],
        [0.5100, 0.3550, 0.3319],
        [0.4812, 0.3680, 0.3458],
        [0.5294, 0.3523, 0.3016],
        [0.5483, 0.3575, 0.2855],
        [0.4889, 0.3819, 0.3306],
        [0.4452, 0.4070, 0.3419],
        [0.5136, 0.3637, 0.3013],
        [0.5058, 0.3861, 0.3034],
        [0.5722, 0.3260, 0.2637],
        [0.6492, 0.2970, 0.2759],
        [0.5987, 0.3239, 0.2373],
        [0.6305, 0.3251, 0.2737],
        [0.5944, 0.3221, 0.2719],
        [0.5689, 0.3324, 0.2719],
        [0.4776, 0.3681, 0.3677],
        [0.5168, 0.3651, 0.3197],
        [0.4666, 0.3904, 0.3249],
        [0.5001, 0.3866, 0.3324],
        [0.4863, 0.3763, 0.3393],
        [0.5045, 0.3717, 0.3212],
        [0.4245, 0.3907, 0.3796],
        [0.4230, 0.3797, 0.3756],
        [0.5143, 0.3482, 0.3185],
        [0.4840, 0.3654, 0.3338],
        [0.4538, 0.3649, 0.3320],
        [0.4434, 0.3794, 0.3772],
        [0.3971, 0.3916, 0.3844],
        [0.4168, 0.4059, 0.3815],
        [0.4342, 0.3813, 0.3772],
        [0.4264, 0.4117, 0.3563],
        [0.4284, 0.3984, 0.3726],
        [0.4624, 0.3802, 0.3568],
        [0.4582, 0.3828, 0.3663],
        [0.4286, 0.3961, 0.3778],
        [0.4652, 0.3739, 0.3555],
        [0.4626, 0.3838, 0.3642],
        [0.4514, 0.3782, 0.3685],
        [0.4568, 0.3768, 0.3541],
        [0.4684, 0.3632, 0.3491],
        [0.5704, 0.3127, 0.2723],
        [0.5024, 0.3575, 0.3405],
        [0.5302, 0.3287, 0.3125],
        [0.5695, 0.3206, 0.2767],
        [0.5934, 0.3067, 0.2556],
        [0.5548, 0.3468, 0.2913],
        [0.5102, 0.3625, 0.3160],
        [0.5151, 0.3538, 0.3320],
        [0.4767, 0.3793, 0.3632],
        [0.5323, 0.3593, 0.3196],
        [0.5008, 0.3632, 0.3192],
        [0.5441, 0.3521, 0.3020],
        [0.4977, 0.3721, 0.3369],
        [0.4725, 0.3802, 0.3582],
        [0.5074, 0.3736, 0.3056],
        [0.5421, 0.3619, 0.2819],
        [0.5920, 0.3135, 0.2768],
        [0.6667, 0.2841, 0.2695],
        [0.5946, 0.3214, 0.2483],
        [0.4721, 0.3868, 0.3401],
        [0.4862, 0.3749, 0.3429],
        [0.4352, 0.3920, 0.3897],
        [0.4829, 0.3739, 0.3398],
        [0.5097, 0.3593, 0.3141],
        [0.4975, 0.3614, 0.3461],
        [0.5888, 0.3316, 0.2665],
        [0.5117, 0.3643, 0.3090],
        [0.5386, 0.3198, 0.2958],
        [0.6521, 0.2950, 0.2631],
        [0.5233, 0.3478, 0.3111],
        [0.6241, 0.2831, 0.2769],
        [0.6530, 0.2911, 0.2623]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  8],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11,  6, 17],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 81: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7672.0, Mean: 1283.4803466796875, Std: 1159.242431640625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 76654.375, Mean: 12823.0087890625, Std: 11582.6572265625
[DEBUG] Top-3 class probabilities:
tensor([[0.6233, 0.2940, 0.2662],
        [0.4152, 0.4073, 0.3796],
        [0.4640, 0.3777, 0.3512],
        [0.4377, 0.3856, 0.3847],
        [0.4375, 0.3891, 0.3697],
        [0.4550, 0.3787, 0.3666],
        [0.5343, 0.3331, 0.3194],
        [0.4631, 0.3784, 0.3485],
        [0.4721, 0.3777, 0.3459],
        [0.5726, 0.3265, 0.2792],
        [0.5060, 0.3617, 0.3260],
        [0.5017, 0.3609, 0.3220],
        [0.4858, 0.3743, 0.3420],
        [0.4719, 0.3670, 0.3578],
        [0.5354, 0.3540, 0.3063],
        [0.5254, 0.3528, 0.2989],
        [0.4791, 0.3554, 0.3448],
        [0.4843, 0.3733, 0.3341],
        [0.4862, 0.3654, 0.3401],
        [0.5584, 0.3533, 0.2990],
        [0.6150, 0.3169, 0.2499],
        [0.6800, 0.2991, 0.2488],
        [0.5061, 0.3658, 0.3365],
        [0.5322, 0.3422, 0.3119],
        [0.5265, 0.3571, 0.3145],
        [0.5515, 0.3430, 0.2837],
        [0.4951, 0.3724, 0.3280],
        [0.4398, 0.3996, 0.3775],
        [0.5734, 0.3316, 0.2807],
        [0.5401, 0.3732, 0.2787],
        [0.4886, 0.3772, 0.3324],
        [0.5785, 0.3345, 0.2526],
        [0.6011, 0.3154, 0.2742],
        [0.5144, 0.3735, 0.2994],
        [0.4880, 0.3853, 0.3226],
        [0.5836, 0.3321, 0.2593],
        [0.5065, 0.3700, 0.3155],
        [0.5613, 0.3409, 0.2735],
        [0.4401, 0.4243, 0.3622],
        [0.4690, 0.3924, 0.3174],
        [0.4892, 0.3741, 0.3184],
        [0.5053, 0.3516, 0.3319],
        [0.5211, 0.3421, 0.3046],
        [0.4628, 0.3674, 0.3543],
        [0.4729, 0.3629, 0.3294],
        [0.4416, 0.4018, 0.3471],
        [0.5065, 0.3375, 0.3021],
        [0.5868, 0.3023, 0.2651],
        [0.4944, 0.3647, 0.3364],
        [0.4879, 0.3712, 0.3434],
        [0.5025, 0.3577, 0.3339],
        [0.4636, 0.3717, 0.3611],
        [0.5091, 0.3533, 0.3357],
        [0.4843, 0.3594, 0.3461],
        [0.4810, 0.3661, 0.3456],
        [0.5143, 0.3462, 0.3285],
        [0.4809, 0.3826, 0.3454],
        [0.4912, 0.3768, 0.3350],
        [0.4756, 0.3817, 0.3442],
        [0.4940, 0.3637, 0.3443],
        [0.5391, 0.3506, 0.2931],
        [0.4835, 0.3825, 0.3449],
        [0.5201, 0.3688, 0.3237],
        [0.5436, 0.3636, 0.3077],
        [0.4954, 0.3677, 0.3441],
        [0.4724, 0.3748, 0.3551],
        [0.5402, 0.3432, 0.2902],
        [0.6201, 0.2946, 0.2555],
        [0.6380, 0.2936, 0.2849],
        [0.6124, 0.3204, 0.2504],
        [0.5687, 0.3437, 0.2813],
        [0.5668, 0.3328, 0.2701],
        [0.4959, 0.3722, 0.3223],
        [0.5025, 0.3766, 0.3219],
        [0.5318, 0.3684, 0.3006],
        [0.5363, 0.3682, 0.2913],
        [0.5336, 0.3471, 0.2885],
        [0.5587, 0.3402, 0.2735],
        [0.5149, 0.3680, 0.3076],
        [0.6169, 0.2910, 0.2796],
        [0.6575, 0.2921, 0.2688],
        [0.6918, 0.3082, 0.2529],
        [0.5863, 0.3263, 0.2843],
        [0.5029, 0.3684, 0.3526],
        [0.4988, 0.3727, 0.3360],
        [0.4852, 0.3817, 0.3398],
        [0.5003, 0.3619, 0.3193],
        [0.4405, 0.3910, 0.3777],
        [0.4601, 0.3632, 0.3627],
        [0.5552, 0.3258, 0.2948],
        [0.4511, 0.3876, 0.3674],
        [0.4575, 0.3688, 0.3577],
        [0.4100, 0.4074, 0.3840],
        [0.5304, 0.3340, 0.3019],
        [0.5394, 0.3469, 0.3051],
        [0.4887, 0.3642, 0.3441],
        [0.4782, 0.3669, 0.3598],
        [0.4498, 0.3777, 0.3748],
        [0.4903, 0.3672, 0.3447],
        [0.4765, 0.3664, 0.3513],
        [0.4900, 0.3611, 0.3499],
        [0.4706, 0.3760, 0.3560],
        [0.4805, 0.3763, 0.3483],
        [0.5000, 0.3497, 0.3273],
        [0.5262, 0.3456, 0.3195],
        [0.4618, 0.3945, 0.3714],
        [0.4896, 0.3695, 0.3427],
        [0.4563, 0.3888, 0.3522],
        [0.4832, 0.3843, 0.3425],
        [0.4988, 0.3655, 0.3255],
        [0.4976, 0.3732, 0.3479],
        [0.5006, 0.3601, 0.3371],
        [0.4885, 0.3644, 0.3504],
        [0.4699, 0.3790, 0.3718],
        [0.5036, 0.3638, 0.3096],
        [0.5450, 0.3421, 0.2679],
        [0.5461, 0.3494, 0.2833],
        [0.6071, 0.3079, 0.2697],
        [0.4309, 0.4079, 0.3354],
        [0.4859, 0.3954, 0.3207],
        [0.5612, 0.3454, 0.2798],
        [0.4539, 0.3917, 0.3800],
        [0.4809, 0.3635, 0.3455],
        [0.5612, 0.3352, 0.2932],
        [0.5299, 0.3623, 0.2958],
        [0.5855, 0.3125, 0.2519],
        [0.6434, 0.2921, 0.2655],
        [0.6883, 0.2922, 0.2508]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 82: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7572.0, Mean: 1282.776611328125, Std: 1042.8668212890625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 75655.21875, Mean: 12815.9794921875, Std: 10419.880859375
[DEBUG] Top-3 class probabilities:
tensor([[0.6098, 0.3222, 0.2602],
        [0.5112, 0.3656, 0.3260],
        [0.5006, 0.3673, 0.3417],
        [0.4908, 0.3774, 0.3425],
        [0.4403, 0.3851, 0.3820],
        [0.5074, 0.3523, 0.3268],
        [0.5020, 0.3547, 0.3296],
        [0.5209, 0.3275, 0.3144],
        [0.5860, 0.3023, 0.2719],
        [0.4713, 0.3711, 0.3603],
        [0.4934, 0.3603, 0.3256],
        [0.4815, 0.3596, 0.3566],
        [0.4904, 0.3598, 0.3439],
        [0.4936, 0.3605, 0.3311],
        [0.4784, 0.3755, 0.3552],
        [0.4585, 0.3771, 0.3766],
        [0.4678, 0.3706, 0.3446],
        [0.4694, 0.3670, 0.3425],
        [0.4705, 0.3680, 0.3647],
        [0.4505, 0.3898, 0.3558],
        [0.4350, 0.4060, 0.3563],
        [0.4573, 0.3825, 0.3504],
        [0.5369, 0.3413, 0.2960],
        [0.4661, 0.3676, 0.3133],
        [0.4232, 0.4067, 0.3566],
        [0.4199, 0.3974, 0.3851],
        [0.5104, 0.3598, 0.3114],
        [0.4902, 0.3610, 0.3475],
        [0.4952, 0.3610, 0.3432],
        [0.4874, 0.3750, 0.3408],
        [0.4638, 0.3788, 0.3594],
        [0.5188, 0.3440, 0.3088],
        [0.4640, 0.3884, 0.3553],
        [0.5169, 0.3685, 0.3256],
        [0.5185, 0.3483, 0.3132],
        [0.4440, 0.3911, 0.3491],
        [0.4925, 0.3829, 0.3162],
        [0.4237, 0.4059, 0.3695],
        [0.4711, 0.3751, 0.3492],
        [0.4434, 0.3909, 0.3704],
        [0.4722, 0.3801, 0.3706],
        [0.4366, 0.3881, 0.3632],
        [0.4754, 0.3829, 0.3325],
        [0.4669, 0.4027, 0.3354],
        [0.4724, 0.3883, 0.3403],
        [0.6303, 0.2938, 0.2828],
        [0.6974, 0.3061, 0.2319],
        [0.6521, 0.3182, 0.2749],
        [0.5874, 0.3182, 0.2704],
        [0.5581, 0.3648, 0.3089],
        [0.4308, 0.4020, 0.3705],
        [0.5465, 0.3279, 0.2859],
        [0.5368, 0.3335, 0.2992],
        [0.4281, 0.3943, 0.3775],
        [0.4450, 0.3829, 0.3593],
        [0.4691, 0.3632, 0.3573],
        [0.4903, 0.3732, 0.3370],
        [0.4674, 0.3723, 0.3710],
        [0.4670, 0.3736, 0.3710],
        [0.4598, 0.3762, 0.3653],
        [0.4781, 0.3671, 0.3467],
        [0.4867, 0.3692, 0.3566],
        [0.5361, 0.3417, 0.3219],
        [0.4828, 0.3737, 0.3366],
        [0.4956, 0.3662, 0.3341],
        [0.4897, 0.3648, 0.3453],
        [0.5064, 0.3655, 0.3336],
        [0.4196, 0.4111, 0.3750],
        [0.4817, 0.3906, 0.3435],
        [0.4999, 0.3541, 0.3354],
        [0.4791, 0.3776, 0.3365],
        [0.4928, 0.3675, 0.3396],
        [0.4889, 0.3745, 0.3460],
        [0.5053, 0.3584, 0.3258],
        [0.5042, 0.3630, 0.3299],
        [0.4788, 0.3739, 0.3409],
        [0.5005, 0.3740, 0.3386],
        [0.4621, 0.3860, 0.3565],
        [0.5127, 0.3524, 0.3290],
        [0.4591, 0.3836, 0.3650],
        [0.4303, 0.3985, 0.3667],
        [0.4426, 0.3954, 0.3619],
        [0.4240, 0.4160, 0.3749],
        [0.4384, 0.3860, 0.3719],
        [0.4713, 0.3674, 0.3645],
        [0.4823, 0.3747, 0.3522],
        [0.4571, 0.3809, 0.3670],
        [0.5112, 0.3667, 0.3401],
        [0.5073, 0.3626, 0.3382],
        [0.5093, 0.3600, 0.3415],
        [0.5246, 0.3636, 0.2976],
        [0.6215, 0.2993, 0.2819],
        [0.6091, 0.3125, 0.2564],
        [0.6765, 0.3071, 0.2593],
        [0.4183, 0.4158, 0.3737],
        [0.4683, 0.3770, 0.3609],
        [0.5568, 0.3188, 0.2757],
        [0.4471, 0.3835, 0.3399],
        [0.4449, 0.3922, 0.3677],
        [0.4525, 0.3781, 0.3624],
        [0.4464, 0.3909, 0.3693],
        [0.4869, 0.3734, 0.3470],
        [0.4673, 0.3650, 0.3562],
        [0.5103, 0.3421, 0.3266],
        [0.4511, 0.3757, 0.3737],
        [0.4830, 0.3672, 0.3586],
        [0.4885, 0.3641, 0.3507],
        [0.4703, 0.3647, 0.3618],
        [0.5074, 0.3603, 0.3245],
        [0.4727, 0.3764, 0.3450],
        [0.5105, 0.3761, 0.3154],
        [0.5217, 0.3583, 0.3016],
        [0.5405, 0.3503, 0.3107],
        [0.4988, 0.3674, 0.3411],
        [0.4833, 0.3685, 0.3490],
        [0.5171, 0.3584, 0.3105],
        [0.4767, 0.3712, 0.3565],
        [0.4987, 0.3619, 0.3317],
        [0.4707, 0.3764, 0.3369],
        [0.4724, 0.3766, 0.3665],
        [0.4946, 0.3654, 0.3452],
        [0.5172, 0.3545, 0.3244],
        [0.4639, 0.3864, 0.3521],
        [0.4786, 0.3801, 0.3340],
        [0.4597, 0.3969, 0.3430],
        [0.4718, 0.3841, 0.3329],
        [0.5376, 0.3426, 0.2871],
        [0.4531, 0.3840, 0.3539]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 83: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8552.0, Mean: 1184.3211669921875, Std: 1030.86865234375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 85446.96875, Mean: 11832.2529296875, Std: 10300.0009765625
[DEBUG] Top-3 class probabilities:
tensor([[0.4773, 0.3750, 0.3587],
        [0.5262, 0.3566, 0.3219],
        [0.4982, 0.3679, 0.3322],
        [0.5593, 0.3469, 0.2944],
        [0.5777, 0.3307, 0.2767],
        [0.5268, 0.3662, 0.3203],
        [0.5055, 0.3660, 0.3420],
        [0.4874, 0.3781, 0.3297],
        [0.5795, 0.3221, 0.2810],
        [0.6928, 0.3088, 0.2432],
        [0.6901, 0.3198, 0.2405],
        [0.6650, 0.3002, 0.2696],
        [0.4365, 0.3977, 0.3805],
        [0.4403, 0.3914, 0.3734],
        [0.4660, 0.3791, 0.3553],
        [0.4623, 0.3691, 0.3633],
        [0.5026, 0.3592, 0.3306],
        [0.4995, 0.3559, 0.3388],
        [0.4601, 0.3776, 0.3580],
        [0.4761, 0.3721, 0.3684],
        [0.4914, 0.3772, 0.3394],
        [0.4565, 0.3903, 0.3662],
        [0.4631, 0.3743, 0.3611],
        [0.4793, 0.3690, 0.3530],
        [0.4792, 0.3626, 0.3514],
        [0.4613, 0.3789, 0.3625],
        [0.5018, 0.3497, 0.3400],
        [0.4526, 0.3754, 0.3571],
        [0.4891, 0.3620, 0.3356],
        [0.4715, 0.3868, 0.3413],
        [0.5316, 0.3411, 0.2899],
        [0.4847, 0.3807, 0.3400],
        [0.5052, 0.3643, 0.3231],
        [0.5829, 0.3128, 0.2724],
        [0.5137, 0.3536, 0.3128],
        [0.5929, 0.2942, 0.2687],
        [0.6726, 0.2994, 0.2473],
        [0.5370, 0.3296, 0.2815],
        [0.4379, 0.3980, 0.3646],
        [0.5238, 0.3492, 0.3139],
        [0.4536, 0.3837, 0.3621],
        [0.4449, 0.3897, 0.3778],
        [0.5047, 0.3497, 0.3011],
        [0.4745, 0.3691, 0.3210],
        [0.4853, 0.3799, 0.3348],
        [0.5071, 0.3548, 0.3047],
        [0.6269, 0.2786, 0.2739],
        [0.5017, 0.3634, 0.3438],
        [0.4892, 0.3753, 0.3431],
        [0.4961, 0.3655, 0.3545],
        [0.5270, 0.3631, 0.3200],
        [0.4966, 0.3793, 0.3525],
        [0.5156, 0.3465, 0.3460],
        [0.5256, 0.3561, 0.3089],
        [0.4753, 0.3853, 0.3497],
        [0.5751, 0.3211, 0.2833],
        [0.6961, 0.3168, 0.2292],
        [0.6879, 0.2812, 0.2522],
        [0.4588, 0.3682, 0.3626],
        [0.4906, 0.3623, 0.3455],
        [0.4741, 0.3780, 0.3406],
        [0.4815, 0.3617, 0.3547],
        [0.4671, 0.3786, 0.3705],
        [0.5251, 0.3528, 0.3242],
        [0.4935, 0.3746, 0.3498],
        [0.5402, 0.3406, 0.2956],
        [0.4772, 0.3723, 0.3514],
        [0.4700, 0.3921, 0.3578],
        [0.4607, 0.3657, 0.3624],
        [0.4779, 0.3814, 0.3490],
        [0.4630, 0.3790, 0.3591],
        [0.4512, 0.3814, 0.3754],
        [0.4266, 0.4010, 0.3791],
        [0.4294, 0.3977, 0.3752],
        [0.4572, 0.3822, 0.3717],
        [0.4635, 0.3727, 0.3598],
        [0.4751, 0.3693, 0.3494],
        [0.4551, 0.3784, 0.3595],
        [0.4633, 0.3768, 0.3521],
        [0.4887, 0.3597, 0.3247],
        [0.5285, 0.3453, 0.2910],
        [0.5862, 0.3159, 0.2616],
        [0.5826, 0.3153, 0.2657],
        [0.6717, 0.2841, 0.2510],
        [0.6535, 0.2808, 0.2750],
        [0.6677, 0.2794, 0.2637],
        [0.6078, 0.2942, 0.2537],
        [0.5284, 0.3546, 0.3088],
        [0.5116, 0.3599, 0.3215],
        [0.4752, 0.3914, 0.3301],
        [0.4812, 0.3804, 0.3350],
        [0.4616, 0.3902, 0.3499],
        [0.4679, 0.3832, 0.3525],
        [0.5253, 0.3549, 0.3239],
        [0.5032, 0.3575, 0.3438],
        [0.5123, 0.3670, 0.3355],
        [0.5174, 0.3551, 0.3156],
        [0.5926, 0.3207, 0.2686],
        [0.5221, 0.3542, 0.3350],
        [0.5049, 0.3777, 0.3217],
        [0.5107, 0.3602, 0.3138],
        [0.5700, 0.3202, 0.2632],
        [0.6519, 0.3167, 0.2569],
        [0.4457, 0.3818, 0.3704],
        [0.4612, 0.3747, 0.3689],
        [0.5087, 0.3550, 0.3186],
        [0.4886, 0.3701, 0.3409],
        [0.5368, 0.3368, 0.3004],
        [0.6337, 0.2812, 0.2622],
        [0.5468, 0.3435, 0.3135],
        [0.5863, 0.3119, 0.2635],
        [0.5101, 0.3554, 0.3254],
        [0.4652, 0.3777, 0.3588],
        [0.4826, 0.3634, 0.3568],
        [0.4813, 0.3644, 0.3642],
        [0.4841, 0.3567, 0.3565],
        [0.4652, 0.3667, 0.3615],
        [0.4510, 0.3766, 0.3593],
        [0.4465, 0.3981, 0.3631],
        [0.4798, 0.3693, 0.3406],
        [0.4586, 0.3735, 0.3684],
        [0.4708, 0.3556, 0.3405],
        [0.4872, 0.3690, 0.3403],
        [0.4694, 0.3678, 0.3608],
        [0.5210, 0.3488, 0.3189],
        [0.4620, 0.3734, 0.3720],
        [0.5198, 0.3593, 0.3322],
        [0.4645, 0.3751, 0.3691]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 84: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7880.0, Mean: 1281.75927734375, Std: 1108.153076171875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 78732.625, Mean: 12805.8115234375, Std: 11072.193359375
[DEBUG] Top-3 class probabilities:
tensor([[0.5671, 0.3229, 0.2780],
        [0.5477, 0.3322, 0.2532],
        [0.5865, 0.3004, 0.2902],
        [0.6836, 0.2966, 0.2441],
        [0.5491, 0.3500, 0.2873],
        [0.5368, 0.3455, 0.3077],
        [0.4960, 0.3716, 0.3278],
        [0.4532, 0.3870, 0.3345],
        [0.4454, 0.3870, 0.3543],
        [0.4717, 0.3936, 0.3252],
        [0.5320, 0.3551, 0.2887],
        [0.5177, 0.3684, 0.3089],
        [0.4930, 0.3648, 0.3467],
        [0.5210, 0.3567, 0.3185],
        [0.5194, 0.3513, 0.3385],
        [0.5444, 0.3446, 0.3118],
        [0.5121, 0.3625, 0.3314],
        [0.5048, 0.3729, 0.3201],
        [0.4941, 0.3608, 0.3508],
        [0.4725, 0.3803, 0.3522],
        [0.4997, 0.3746, 0.3136],
        [0.4363, 0.3839, 0.3613],
        [0.4652, 0.3779, 0.3539],
        [0.4483, 0.3850, 0.3673],
        [0.4757, 0.3678, 0.3630],
        [0.4596, 0.3874, 0.3557],
        [0.6117, 0.2869, 0.2680],
        [0.4948, 0.3560, 0.3347],
        [0.4696, 0.3876, 0.3499],
        [0.4679, 0.3816, 0.3437],
        [0.4719, 0.3709, 0.3572],
        [0.4947, 0.3711, 0.3425],
        [0.4837, 0.3638, 0.3633],
        [0.4558, 0.3861, 0.3646],
        [0.4931, 0.3601, 0.3130],
        [0.4679, 0.3584, 0.3326],
        [0.4402, 0.3808, 0.3734],
        [0.4884, 0.3617, 0.3463],
        [0.4406, 0.3856, 0.3754],
        [0.4293, 0.3904, 0.3726],
        [0.4787, 0.3742, 0.3366],
        [0.4668, 0.3917, 0.3648],
        [0.4621, 0.3627, 0.3551],
        [0.4236, 0.3956, 0.3793],
        [0.4173, 0.4097, 0.3903],
        [0.4893, 0.3634, 0.3286],
        [0.5382, 0.3460, 0.3056],
        [0.5044, 0.3633, 0.3254],
        [0.4172, 0.4086, 0.3881],
        [0.4574, 0.4224, 0.3188],
        [0.4454, 0.4002, 0.3452],
        [0.4759, 0.3838, 0.3579],
        [0.5705, 0.3411, 0.2901],
        [0.5327, 0.3416, 0.3058],
        [0.4564, 0.3680, 0.3603],
        [0.4253, 0.4010, 0.3802],
        [0.5634, 0.3435, 0.2719],
        [0.6350, 0.3023, 0.2566],
        [0.6444, 0.2987, 0.2793],
        [0.6036, 0.3070, 0.2671],
        [0.5159, 0.3623, 0.3302],
        [0.4992, 0.3602, 0.3365],
        [0.4733, 0.3878, 0.3343],
        [0.4558, 0.4131, 0.3446],
        [0.4771, 0.3822, 0.3287],
        [0.5114, 0.3634, 0.3101],
        [0.4847, 0.3793, 0.3426],
        [0.6069, 0.2927, 0.2645],
        [0.4652, 0.3735, 0.3388],
        [0.4238, 0.4038, 0.3777],
        [0.6037, 0.2975, 0.2603],
        [0.4226, 0.4065, 0.3837],
        [0.5303, 0.3447, 0.3091],
        [0.5774, 0.3091, 0.2705],
        [0.4604, 0.3781, 0.3648],
        [0.4607, 0.3816, 0.3642],
        [0.4826, 0.3648, 0.3531],
        [0.4823, 0.3747, 0.3586],
        [0.4660, 0.3707, 0.3626],
        [0.4520, 0.3725, 0.3721],
        [0.4168, 0.3885, 0.3813],
        [0.4492, 0.3918, 0.3677],
        [0.4786, 0.3807, 0.3408],
        [0.4513, 0.3808, 0.3622],
        [0.4826, 0.3787, 0.3526],
        [0.4722, 0.3788, 0.3495],
        [0.4799, 0.3658, 0.3521],
        [0.4627, 0.3898, 0.3651],
        [0.4528, 0.3827, 0.3635],
        [0.4635, 0.3858, 0.3497],
        [0.5147, 0.3633, 0.3300],
        [0.5473, 0.3379, 0.2916],
        [0.5283, 0.3567, 0.3025],
        [0.5010, 0.3716, 0.3366],
        [0.4836, 0.3827, 0.3546],
        [0.4122, 0.4057, 0.3857],
        [0.5728, 0.3127, 0.2661],
        [0.5181, 0.3472, 0.3099],
        [0.4411, 0.3798, 0.3749],
        [0.5108, 0.3646, 0.3472],
        [0.4484, 0.3837, 0.3713],
        [0.4795, 0.3732, 0.3549],
        [0.6381, 0.2913, 0.2733],
        [0.6953, 0.3071, 0.2387],
        [0.6579, 0.2839, 0.2723],
        [0.6387, 0.2950, 0.2691],
        [0.5162, 0.3599, 0.3143],
        [0.4964, 0.3613, 0.3414],
        [0.5665, 0.3258, 0.2780],
        [0.5683, 0.3284, 0.2736],
        [0.4704, 0.3910, 0.3175],
        [0.5591, 0.3406, 0.2677],
        [0.5418, 0.3603, 0.2907],
        [0.4752, 0.3642, 0.3575],
        [0.6478, 0.2852, 0.2676],
        [0.4903, 0.3528, 0.3439],
        [0.6058, 0.2886, 0.2711],
        [0.6522, 0.2758, 0.2587],
        [0.4862, 0.3514, 0.3421],
        [0.4796, 0.3625, 0.3503],
        [0.4466, 0.3920, 0.3767],
        [0.4728, 0.3690, 0.3617],
        [0.4204, 0.3936, 0.3853],
        [0.4449, 0.4053, 0.3671],
        [0.5071, 0.3523, 0.3522],
        [0.4647, 0.3714, 0.3584],
        [0.4258, 0.3934, 0.3782],
        [0.4215, 0.4008, 0.3788]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 85: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7000.0, Mean: 1290.795166015625, Std: 1096.6427001953125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 69940.0390625, Mean: 12896.09375, Std: 10957.1865234375
[DEBUG] Top-3 class probabilities:
tensor([[0.5006, 0.3713, 0.3385],
        [0.4741, 0.3704, 0.3451],
        [0.4824, 0.3801, 0.3484],
        [0.4762, 0.3737, 0.3513],
        [0.4854, 0.3635, 0.3486],
        [0.4657, 0.3854, 0.3658],
        [0.4886, 0.3793, 0.3425],
        [0.5766, 0.3371, 0.2947],
        [0.4841, 0.3783, 0.3513],
        [0.4749, 0.3642, 0.3624],
        [0.4993, 0.3725, 0.3401],
        [0.4903, 0.3839, 0.3242],
        [0.5262, 0.3677, 0.3118],
        [0.5699, 0.3147, 0.2911],
        [0.5591, 0.3238, 0.2690],
        [0.5491, 0.3359, 0.2904],
        [0.4909, 0.3694, 0.3478],
        [0.4697, 0.3776, 0.3655],
        [0.5043, 0.3577, 0.3226],
        [0.4591, 0.3803, 0.3665],
        [0.4515, 0.3993, 0.3465],
        [0.5102, 0.3655, 0.3163],
        [0.6409, 0.3148, 0.2813],
        [0.5767, 0.3393, 0.2800],
        [0.4724, 0.3714, 0.3697],
        [0.4955, 0.3617, 0.3490],
        [0.4921, 0.3730, 0.3528],
        [0.5587, 0.3277, 0.2759],
        [0.5415, 0.3403, 0.2896],
        [0.5249, 0.3451, 0.3066],
        [0.4929, 0.3654, 0.3317],
        [0.4323, 0.3937, 0.3727],
        [0.4948, 0.3640, 0.3281],
        [0.5387, 0.3506, 0.3068],
        [0.4694, 0.3563, 0.3441],
        [0.6024, 0.2853, 0.2804],
        [0.6471, 0.2898, 0.2647],
        [0.4515, 0.3859, 0.3584],
        [0.4717, 0.3810, 0.3514],
        [0.5300, 0.3601, 0.3066],
        [0.5393, 0.3467, 0.2903],
        [0.4310, 0.4109, 0.3694],
        [0.4228, 0.4109, 0.3559],
        [0.4506, 0.3891, 0.3687],
        [0.4223, 0.4213, 0.3806],
        [0.4224, 0.4080, 0.3648],
        [0.5079, 0.3606, 0.3357],
        [0.4746, 0.3690, 0.3586],
        [0.4752, 0.3676, 0.3455],
        [0.4898, 0.3550, 0.3356],
        [0.5054, 0.3676, 0.3381],
        [0.5072, 0.3517, 0.3378],
        [0.4976, 0.3633, 0.3449],
        [0.4696, 0.3796, 0.3487],
        [0.4670, 0.3667, 0.3591],
        [0.4800, 0.3596, 0.3509],
        [0.5063, 0.3540, 0.3262],
        [0.5373, 0.3432, 0.3003],
        [0.5294, 0.3485, 0.3243],
        [0.4661, 0.3803, 0.3696],
        [0.4779, 0.3867, 0.3434],
        [0.4649, 0.3843, 0.3586],
        [0.5072, 0.3591, 0.3323],
        [0.4791, 0.3751, 0.3557],
        [0.4633, 0.3747, 0.3511],
        [0.5446, 0.3404, 0.2878],
        [0.4940, 0.3638, 0.3419],
        [0.4564, 0.3971, 0.3468],
        [0.4748, 0.3951, 0.3192],
        [0.4988, 0.3692, 0.3454],
        [0.5107, 0.3578, 0.3408],
        [0.5292, 0.3418, 0.3120],
        [0.4985, 0.3560, 0.3429],
        [0.5025, 0.3563, 0.3329],
        [0.4750, 0.3755, 0.3733],
        [0.5341, 0.3601, 0.3215],
        [0.5494, 0.3404, 0.2890],
        [0.4390, 0.3889, 0.3765],
        [0.5115, 0.3616, 0.3269],
        [0.4663, 0.3818, 0.3486],
        [0.4493, 0.3806, 0.3643],
        [0.4650, 0.3704, 0.3678],
        [0.5200, 0.3411, 0.3093],
        [0.4457, 0.3883, 0.3657],
        [0.4768, 0.3750, 0.3483],
        [0.4525, 0.3782, 0.3658],
        [0.5955, 0.3166, 0.2603],
        [0.6889, 0.2979, 0.2512],
        [0.4478, 0.3804, 0.3277],
        [0.4333, 0.3945, 0.3595],
        [0.4564, 0.3801, 0.3632],
        [0.5077, 0.3732, 0.3097],
        [0.4404, 0.3941, 0.3527],
        [0.4758, 0.3839, 0.3702],
        [0.4979, 0.3599, 0.3467],
        [0.4757, 0.3751, 0.3612],
        [0.4576, 0.3800, 0.3645],
        [0.4981, 0.3591, 0.3453],
        [0.4852, 0.3830, 0.3591],
        [0.4698, 0.3646, 0.3560],
        [0.4451, 0.3852, 0.3669],
        [0.4979, 0.3594, 0.3374],
        [0.5818, 0.3216, 0.2606],
        [0.6766, 0.3015, 0.2441],
        [0.6892, 0.3245, 0.2463],
        [0.6096, 0.3040, 0.2911],
        [0.5205, 0.3521, 0.3211],
        [0.4874, 0.3741, 0.3208],
        [0.4603, 0.3767, 0.3535],
        [0.5012, 0.3555, 0.3361],
        [0.5453, 0.3423, 0.2968],
        [0.4925, 0.3697, 0.3401],
        [0.4489, 0.3766, 0.3646],
        [0.4841, 0.3796, 0.3320],
        [0.4262, 0.3984, 0.3845],
        [0.5305, 0.3442, 0.3190],
        [0.5423, 0.3399, 0.2928],
        [0.5281, 0.3464, 0.3238],
        [0.4654, 0.3813, 0.3673],
        [0.5008, 0.3572, 0.3435],
        [0.4831, 0.3706, 0.3585],
        [0.4968, 0.3558, 0.3480],
        [0.5215, 0.3782, 0.3283],
        [0.4292, 0.3835, 0.3829],
        [0.4175, 0.4021, 0.3787],
        [0.4337, 0.4009, 0.3729],
        [0.4757, 0.3696, 0.3406],
        [0.4929, 0.3573, 0.3490]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 86: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7368.0, Mean: 1317.1690673828125, Std: 1119.527099609375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 73616.9375, Mean: 13159.611328125, Std: 11185.837890625
[DEBUG] Top-3 class probabilities:
tensor([[0.4412, 0.3861, 0.3690],
        [0.4429, 0.3834, 0.3771],
        [0.4291, 0.3865, 0.3729],
        [0.4211, 0.4000, 0.3885],
        [0.4670, 0.3862, 0.3459],
        [0.5572, 0.3205, 0.2692],
        [0.6572, 0.3098, 0.2653],
        [0.4512, 0.4145, 0.3359],
        [0.4077, 0.3979, 0.3804],
        [0.4963, 0.3647, 0.3358],
        [0.4453, 0.3914, 0.3611],
        [0.4722, 0.3817, 0.3499],
        [0.5357, 0.3429, 0.3145],
        [0.4679, 0.3706, 0.3616],
        [0.4357, 0.3978, 0.3758],
        [0.4653, 0.3779, 0.3675],
        [0.5020, 0.3734, 0.3316],
        [0.4388, 0.3891, 0.3651],
        [0.4526, 0.3712, 0.3698],
        [0.4776, 0.3694, 0.3512],
        [0.5416, 0.3598, 0.2965],
        [0.5524, 0.3399, 0.2752],
        [0.6316, 0.2901, 0.2755],
        [0.6936, 0.3160, 0.2451],
        [0.6882, 0.3101, 0.2382],
        [0.6361, 0.2928, 0.2708],
        [0.4967, 0.3647, 0.3376],
        [0.4594, 0.3892, 0.3600],
        [0.4728, 0.3713, 0.3474],
        [0.4593, 0.3853, 0.3652],
        [0.4619, 0.3785, 0.3594],
        [0.4632, 0.3800, 0.3649],
        [0.4523, 0.3849, 0.3674],
        [0.5231, 0.3652, 0.3382],
        [0.5419, 0.3541, 0.2998],
        [0.4851, 0.3705, 0.3507],
        [0.4983, 0.3565, 0.3344],
        [0.4888, 0.3714, 0.3481],
        [0.4976, 0.3646, 0.3402],
        [0.4886, 0.3626, 0.3433],
        [0.4989, 0.3792, 0.3320],
        [0.4248, 0.4016, 0.3871],
        [0.5074, 0.3601, 0.3207],
        [0.4378, 0.3766, 0.3751],
        [0.4325, 0.3853, 0.3781],
        [0.4623, 0.3690, 0.3669],
        [0.4655, 0.3740, 0.3735],
        [0.4418, 0.3816, 0.3736],
        [0.4767, 0.3688, 0.3620],
        [0.4340, 0.3828, 0.3800],
        [0.4662, 0.4414, 0.2995],
        [0.4082, 0.4012, 0.3912],
        [0.4485, 0.3885, 0.3320],
        [0.5076, 0.3719, 0.3204],
        [0.4731, 0.3752, 0.3245],
        [0.4741, 0.3741, 0.3572],
        [0.4752, 0.3809, 0.3569],
        [0.4188, 0.3965, 0.3943],
        [0.4482, 0.3934, 0.3749],
        [0.4625, 0.3777, 0.3741],
        [0.4684, 0.3701, 0.3609],
        [0.4523, 0.3841, 0.3807],
        [0.4594, 0.3729, 0.3724],
        [0.4767, 0.3659, 0.3624],
        [0.4690, 0.3790, 0.3537],
        [0.4533, 0.4001, 0.3572],
        [0.4445, 0.3898, 0.3687],
        [0.5109, 0.3654, 0.3274],
        [0.4867, 0.3767, 0.3448],
        [0.6081, 0.2895, 0.2863],
        [0.6690, 0.3097, 0.2502],
        [0.6968, 0.3170, 0.2387],
        [0.6543, 0.2904, 0.2729],
        [0.5511, 0.3403, 0.2871],
        [0.4767, 0.3643, 0.3349],
        [0.5184, 0.3549, 0.3192],
        [0.4915, 0.3566, 0.3366],
        [0.4784, 0.3685, 0.3633],
        [0.5053, 0.3817, 0.3362],
        [0.5263, 0.3514, 0.3170],
        [0.5158, 0.3614, 0.3237],
        [0.5393, 0.3444, 0.2888],
        [0.5719, 0.3361, 0.2820],
        [0.4804, 0.3742, 0.3514],
        [0.4619, 0.4017, 0.3580],
        [0.5342, 0.3539, 0.3193],
        [0.4574, 0.3939, 0.3425],
        [0.4623, 0.3706, 0.3694],
        [0.4759, 0.3746, 0.3481],
        [0.4488, 0.3785, 0.3772],
        [0.4152, 0.4001, 0.3921],
        [0.4584, 0.3857, 0.3696],
        [0.4237, 0.3885, 0.3812],
        [0.4536, 0.3920, 0.3804],
        [0.4487, 0.3762, 0.3635],
        [0.5003, 0.3631, 0.3296],
        [0.4106, 0.4075, 0.3875],
        [0.4924, 0.4317, 0.2787],
        [0.4898, 0.4098, 0.3103],
        [0.4596, 0.3985, 0.3449],
        [0.4663, 0.3914, 0.3499],
        [0.4271, 0.3864, 0.3822],
        [0.4980, 0.3614, 0.3402],
        [0.4782, 0.3674, 0.3510],
        [0.4349, 0.3889, 0.3844],
        [0.4663, 0.3941, 0.3510],
        [0.4902, 0.3675, 0.3445],
        [0.4980, 0.3674, 0.3398],
        [0.4668, 0.3742, 0.3645],
        [0.5165, 0.3500, 0.3150],
        [0.5458, 0.3203, 0.2845],
        [0.4474, 0.3830, 0.3729],
        [0.4228, 0.3961, 0.3887],
        [0.4219, 0.3901, 0.3840],
        [0.4464, 0.3971, 0.3425],
        [0.4562, 0.3734, 0.3657],
        [0.4346, 0.3917, 0.3632],
        [0.5374, 0.3412, 0.2955],
        [0.5224, 0.3474, 0.2803],
        [0.6511, 0.2855, 0.2791],
        [0.6202, 0.3008, 0.2999],
        [0.5672, 0.3441, 0.2619],
        [0.4877, 0.3786, 0.3234],
        [0.4799, 0.3880, 0.3548],
        [0.4490, 0.3906, 0.3788],
        [0.5285, 0.3600, 0.3266],
        [0.5490, 0.3433, 0.2968],
        [0.4873, 0.3831, 0.3415]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [ 6, 17, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 87: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7196.0, Mean: 1379.3939208984375, Std: 1063.655029296875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 71898.390625, Mean: 13781.3369140625, Std: 10627.5888671875
[DEBUG] Top-3 class probabilities:
tensor([[0.5294, 0.3629, 0.3192],
        [0.4824, 0.3781, 0.3612],
        [0.5010, 0.3690, 0.3213],
        [0.5170, 0.3697, 0.3160],
        [0.4748, 0.3962, 0.3460],
        [0.4560, 0.3780, 0.3769],
        [0.4162, 0.4020, 0.3922],
        [0.4576, 0.3706, 0.3596],
        [0.4696, 0.3763, 0.3574],
        [0.4420, 0.3927, 0.3798],
        [0.4739, 0.3773, 0.3480],
        [0.4473, 0.3918, 0.3639],
        [0.4729, 0.3825, 0.3476],
        [0.5010, 0.3632, 0.3296],
        [0.4528, 0.3963, 0.3672],
        [0.4539, 0.3990, 0.3561],
        [0.4213, 0.3999, 0.3792],
        [0.4263, 0.4040, 0.3780],
        [0.4077, 0.3979, 0.3896],
        [0.4317, 0.3863, 0.3639],
        [0.4418, 0.3938, 0.3839],
        [0.4986, 0.3586, 0.3503],
        [0.4794, 0.3762, 0.3611],
        [0.4530, 0.3830, 0.3643],
        [0.4792, 0.3714, 0.3551],
        [0.5045, 0.3581, 0.3399],
        [0.4153, 0.3971, 0.3894],
        [0.4762, 0.3688, 0.3393],
        [0.6134, 0.2905, 0.2828],
        [0.6029, 0.3056, 0.2685],
        [0.4460, 0.3951, 0.3659],
        [0.4086, 0.4079, 0.3695],
        [0.4658, 0.3837, 0.3574],
        [0.4759, 0.3921, 0.3259],
        [0.4677, 0.3773, 0.3623],
        [0.4680, 0.3872, 0.3527],
        [0.4785, 0.3845, 0.3283],
        [0.4214, 0.3999, 0.3885],
        [0.5312, 0.3472, 0.2985],
        [0.6085, 0.3006, 0.2516],
        [0.5608, 0.3538, 0.2798],
        [0.5511, 0.3306, 0.2804],
        [0.5118, 0.3624, 0.3094],
        [0.4860, 0.3680, 0.3301],
        [0.4652, 0.3767, 0.3595],
        [0.5179, 0.3598, 0.3220],
        [0.4773, 0.3711, 0.3509],
        [0.4432, 0.4036, 0.3733],
        [0.4846, 0.3793, 0.3609],
        [0.4403, 0.4036, 0.3584],
        [0.4876, 0.3905, 0.3086],
        [0.4172, 0.4080, 0.3855],
        [0.4448, 0.3888, 0.3677],
        [0.4196, 0.4037, 0.3914],
        [0.4585, 0.3719, 0.3626],
        [0.4509, 0.3846, 0.3746],
        [0.4267, 0.4032, 0.3800],
        [0.4674, 0.3726, 0.3671],
        [0.4723, 0.3682, 0.3626],
        [0.4867, 0.3657, 0.3603],
        [0.4660, 0.3708, 0.3652],
        [0.4550, 0.3900, 0.3668],
        [0.4301, 0.4182, 0.3487],
        [0.4641, 0.3833, 0.3530],
        [0.4028, 0.3947, 0.3947],
        [0.4134, 0.4042, 0.3957],
        [0.4300, 0.4007, 0.3928],
        [0.4422, 0.3839, 0.3743],
        [0.4316, 0.3835, 0.3781],
        [0.4724, 0.3596, 0.3485],
        [0.4592, 0.3688, 0.3553],
        [0.4827, 0.3708, 0.3479],
        [0.4255, 0.3860, 0.3848],
        [0.4346, 0.3803, 0.3779],
        [0.4425, 0.3760, 0.3692],
        [0.4616, 0.3699, 0.3558],
        [0.4502, 0.3795, 0.3644],
        [0.4588, 0.3662, 0.3623],
        [0.3976, 0.3937, 0.3841],
        [0.4266, 0.3927, 0.3726],
        [0.5031, 0.3716, 0.3316],
        [0.4729, 0.3777, 0.3348],
        [0.4559, 0.3788, 0.3700],
        [0.4723, 0.3668, 0.3471],
        [0.4890, 0.3571, 0.3485],
        [0.4577, 0.3690, 0.3662],
        [0.4491, 0.3741, 0.3715],
        [0.4906, 0.3630, 0.3276],
        [0.5283, 0.3640, 0.3137],
        [0.4381, 0.3808, 0.3406],
        [0.4792, 0.3657, 0.3418],
        [0.4660, 0.3686, 0.3589],
        [0.4618, 0.3699, 0.3491],
        [0.4929, 0.3650, 0.3405],
        [0.4996, 0.3677, 0.3379],
        [0.5179, 0.3520, 0.3376],
        [0.4975, 0.3805, 0.3261],
        [0.4570, 0.3773, 0.3769],
        [0.4478, 0.3951, 0.3648],
        [0.4174, 0.3997, 0.3927],
        [0.4479, 0.3881, 0.3610],
        [0.4906, 0.3537, 0.3535],
        [0.4873, 0.3696, 0.3466],
        [0.4924, 0.3659, 0.3449],
        [0.4930, 0.3534, 0.3314],
        [0.4798, 0.3610, 0.3459],
        [0.4744, 0.3773, 0.3615],
        [0.5031, 0.3500, 0.3470],
        [0.4242, 0.4046, 0.3755],
        [0.4193, 0.4109, 0.3821],
        [0.4120, 0.4059, 0.3932],
        [0.4146, 0.4038, 0.3851],
        [0.4752, 0.3718, 0.3655],
        [0.4460, 0.3836, 0.3755],
        [0.4582, 0.3706, 0.3594],
        [0.4652, 0.3824, 0.3369],
        [0.4836, 0.3612, 0.3441],
        [0.4625, 0.3664, 0.3627],
        [0.4378, 0.3749, 0.3725],
        [0.4616, 0.3736, 0.3629],
        [0.4220, 0.4032, 0.3906],
        [0.4704, 0.3578, 0.3536],
        [0.4689, 0.3756, 0.3568],
        [0.4578, 0.3820, 0.3654],
        [0.4530, 0.3831, 0.3652],
        [0.4074, 0.4029, 0.3990],
        [0.4257, 0.3960, 0.3871],
        [0.4916, 0.3736, 0.3587]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [ 6, 11, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 88: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9696.0, Mean: 1374.9632568359375, Std: 1048.119873046875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 96877.328125, Mean: 13737.06640625, Std: 10472.3671875
[DEBUG] Top-3 class probabilities:
tensor([[0.4725, 0.3862, 0.3501],
        [0.4659, 0.3726, 0.3603],
        [0.5032, 0.3566, 0.3395],
        [0.4730, 0.3676, 0.3587],
        [0.4620, 0.3647, 0.3609],
        [0.4800, 0.3696, 0.3433],
        [0.5041, 0.3641, 0.3149],
        [0.4508, 0.3844, 0.3619],
        [0.4763, 0.3880, 0.3314],
        [0.4750, 0.3558, 0.3540],
        [0.4362, 0.4020, 0.3537],
        [0.4728, 0.3831, 0.3336],
        [0.4293, 0.3941, 0.3706],
        [0.4694, 0.3773, 0.3518],
        [0.5163, 0.3640, 0.3251],
        [0.4554, 0.3960, 0.3646],
        [0.4757, 0.3770, 0.3604],
        [0.4855, 0.3658, 0.3608],
        [0.4745, 0.3690, 0.3621],
        [0.4558, 0.3793, 0.3716],
        [0.4930, 0.3702, 0.3422],
        [0.4718, 0.3649, 0.3618],
        [0.4466, 0.3989, 0.3715],
        [0.5113, 0.3491, 0.3320],
        [0.4684, 0.3755, 0.3451],
        [0.4813, 0.3682, 0.3518],
        [0.4438, 0.3817, 0.3758],
        [0.5076, 0.3672, 0.3419],
        [0.5089, 0.3463, 0.3277],
        [0.4780, 0.3581, 0.3502],
        [0.4330, 0.3891, 0.3876],
        [0.4286, 0.4043, 0.3638],
        [0.4474, 0.3900, 0.3652],
        [0.4483, 0.3993, 0.3672],
        [0.4790, 0.3699, 0.3383],
        [0.4608, 0.3761, 0.3530],
        [0.4579, 0.3843, 0.3691],
        [0.4355, 0.3834, 0.3796],
        [0.4066, 0.3942, 0.3898],
        [0.4683, 0.3667, 0.3664],
        [0.4535, 0.3717, 0.3601],
        [0.4647, 0.3794, 0.3687],
        [0.4817, 0.3560, 0.3444],
        [0.4948, 0.3673, 0.3357],
        [0.4517, 0.3809, 0.3632],
        [0.4837, 0.3680, 0.3524],
        [0.4876, 0.3819, 0.3366],
        [0.4372, 0.3924, 0.3757],
        [0.4622, 0.3754, 0.3608],
        [0.4808, 0.3697, 0.3500],
        [0.4750, 0.3665, 0.3266],
        [0.5068, 0.3593, 0.3305],
        [0.4911, 0.3571, 0.3336],
        [0.4583, 0.3769, 0.3430],
        [0.4340, 0.3860, 0.3540],
        [0.4864, 0.3652, 0.3437],
        [0.4429, 0.4010, 0.3511],
        [0.4579, 0.3938, 0.3362],
        [0.4475, 0.3935, 0.3531],
        [0.4902, 0.3812, 0.3338],
        [0.4797, 0.3737, 0.3412],
        [0.4480, 0.3748, 0.3695],
        [0.4932, 0.3641, 0.3396],
        [0.4592, 0.3795, 0.3711],
        [0.4740, 0.3700, 0.3585],
        [0.4850, 0.3714, 0.3640],
        [0.4695, 0.3685, 0.3652],
        [0.4939, 0.3556, 0.3554],
        [0.4856, 0.3468, 0.3466],
        [0.4683, 0.3658, 0.3652],
        [0.4797, 0.3813, 0.3405],
        [0.4494, 0.3719, 0.3718],
        [0.4537, 0.3763, 0.3673],
        [0.5205, 0.3663, 0.3361],
        [0.5251, 0.3592, 0.3338],
        [0.4678, 0.3879, 0.3577],
        [0.5011, 0.3603, 0.3390],
        [0.5204, 0.3552, 0.3179],
        [0.4458, 0.3808, 0.3696],
        [0.4487, 0.3957, 0.3750],
        [0.4639, 0.3762, 0.3593],
        [0.4802, 0.3747, 0.3495],
        [0.5107, 0.3600, 0.3268],
        [0.4615, 0.3790, 0.3561],
        [0.4884, 0.3572, 0.3433],
        [0.4288, 0.3928, 0.3661],
        [0.4593, 0.3850, 0.3563],
        [0.4847, 0.3667, 0.3452],
        [0.4896, 0.3633, 0.3538],
        [0.4956, 0.3721, 0.3269],
        [0.4546, 0.3920, 0.3547],
        [0.4448, 0.3937, 0.3561],
        [0.4195, 0.3950, 0.3908],
        [0.4260, 0.4091, 0.3677],
        [0.4419, 0.4009, 0.3674],
        [0.4650, 0.3770, 0.3642],
        [0.4609, 0.3872, 0.3713],
        [0.4692, 0.3782, 0.3575],
        [0.5267, 0.3650, 0.3181],
        [0.5264, 0.3507, 0.3071],
        [0.5237, 0.3490, 0.3069],
        [0.4958, 0.3722, 0.3401],
        [0.4644, 0.3821, 0.3644],
        [0.4349, 0.3962, 0.3632],
        [0.5064, 0.3752, 0.3140],
        [0.4728, 0.3948, 0.3420],
        [0.4877, 0.3728, 0.3398],
        [0.5065, 0.3583, 0.3260],
        [0.4502, 0.3737, 0.3657],
        [0.4521, 0.3875, 0.3801],
        [0.4697, 0.3800, 0.3774],
        [0.4724, 0.3729, 0.3718],
        [0.4855, 0.3611, 0.3351],
        [0.5165, 0.3576, 0.3239],
        [0.5016, 0.3631, 0.3307],
        [0.5831, 0.3194, 0.2670],
        [0.4590, 0.3886, 0.3799],
        [0.4781, 0.3704, 0.3574],
        [0.4484, 0.3849, 0.3625],
        [0.4626, 0.3722, 0.3623],
        [0.4770, 0.3594, 0.3545],
        [0.5144, 0.3648, 0.3287],
        [0.4866, 0.3633, 0.3471],
        [0.4895, 0.3627, 0.3539],
        [0.4342, 0.3841, 0.3757],
        [0.4784, 0.3858, 0.3499],
        [0.4853, 0.3664, 0.3449],
        [0.4697, 0.3754, 0.3589]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 89: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7316.0, Mean: 1293.625732421875, Std: 1015.144775390625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 73097.375, Mean: 12924.3779296875, Std: 10142.89453125
[DEBUG] Top-3 class probabilities:
tensor([[0.4843, 0.3671, 0.3471],
        [0.4663, 0.3754, 0.3634],
        [0.4520, 0.3619, 0.3617],
        [0.4551, 0.3926, 0.3679],
        [0.4829, 0.3818, 0.3416],
        [0.4759, 0.3770, 0.3516],
        [0.4790, 0.3684, 0.3434],
        [0.5337, 0.3548, 0.2991],
        [0.5110, 0.3578, 0.3335],
        [0.4258, 0.3977, 0.3778],
        [0.4169, 0.4117, 0.3858],
        [0.5046, 0.3526, 0.3161],
        [0.4586, 0.3872, 0.3586],
        [0.4389, 0.3743, 0.3644],
        [0.4376, 0.3801, 0.3782],
        [0.4596, 0.3821, 0.3413],
        [0.4405, 0.3856, 0.3711],
        [0.5616, 0.3394, 0.2879],
        [0.4748, 0.3767, 0.3471],
        [0.4583, 0.3815, 0.3733],
        [0.4874, 0.3599, 0.3300],
        [0.4753, 0.3696, 0.3407],
        [0.4526, 0.3772, 0.3543],
        [0.4505, 0.3913, 0.3480],
        [0.4824, 0.3791, 0.3485],
        [0.4721, 0.3634, 0.3618],
        [0.4510, 0.3817, 0.3757],
        [0.4097, 0.4042, 0.3999],
        [0.4738, 0.3707, 0.3555],
        [0.4705, 0.3704, 0.3507],
        [0.4992, 0.3697, 0.3344],
        [0.5207, 0.3513, 0.3235],
        [0.4902, 0.3606, 0.3490],
        [0.4929, 0.3475, 0.3332],
        [0.6365, 0.2816, 0.2811],
        [0.4504, 0.3759, 0.3639],
        [0.4839, 0.3518, 0.3442],
        [0.4794, 0.3737, 0.3518],
        [0.4828, 0.3774, 0.3599],
        [0.4878, 0.3791, 0.3559],
        [0.4836, 0.3791, 0.3458],
        [0.5081, 0.3623, 0.3298],
        [0.4062, 0.4042, 0.3928],
        [0.4045, 0.4036, 0.3947],
        [0.4402, 0.3781, 0.3599],
        [0.4117, 0.3875, 0.3815],
        [0.6268, 0.2838, 0.2789],
        [0.5662, 0.3241, 0.2791],
        [0.4367, 0.3801, 0.3743],
        [0.4823, 0.3700, 0.3397],
        [0.4939, 0.3634, 0.3506],
        [0.4640, 0.3762, 0.3610],
        [0.4362, 0.3982, 0.3751],
        [0.4134, 0.4017, 0.3930],
        [0.4146, 0.3973, 0.3897],
        [0.4523, 0.3723, 0.3650],
        [0.4302, 0.3990, 0.3671],
        [0.3999, 0.3963, 0.3830],
        [0.4769, 0.3600, 0.3529],
        [0.4600, 0.3776, 0.3619],
        [0.4329, 0.3854, 0.3787],
        [0.4450, 0.3769, 0.3640],
        [0.4916, 0.3656, 0.3351],
        [0.4948, 0.3624, 0.3255],
        [0.4296, 0.4010, 0.3757],
        [0.4791, 0.3725, 0.3383],
        [0.4860, 0.3738, 0.3324],
        [0.4449, 0.3826, 0.3368],
        [0.4742, 0.3716, 0.3346],
        [0.4606, 0.3829, 0.3729],
        [0.5965, 0.3049, 0.2746],
        [0.4719, 0.3664, 0.3588],
        [0.4580, 0.3663, 0.3568],
        [0.4513, 0.3793, 0.3749],
        [0.4448, 0.3787, 0.3766],
        [0.4750, 0.3663, 0.3652],
        [0.5073, 0.3497, 0.3351],
        [0.4937, 0.3653, 0.3521],
        [0.4934, 0.3602, 0.3601],
        [0.4452, 0.3800, 0.3727],
        [0.4984, 0.3374, 0.3204],
        [0.4989, 0.3656, 0.3432],
        [0.4678, 0.3895, 0.3677],
        [0.4650, 0.3756, 0.3722],
        [0.4681, 0.3718, 0.3567],
        [0.4689, 0.3710, 0.3460],
        [0.4472, 0.3889, 0.3608],
        [0.5081, 0.3606, 0.3277],
        [0.6195, 0.2971, 0.2829],
        [0.4943, 0.3651, 0.3346],
        [0.5618, 0.3265, 0.2864],
        [0.4974, 0.3540, 0.3258],
        [0.4685, 0.3698, 0.3535],
        [0.4673, 0.3770, 0.3466],
        [0.4501, 0.3873, 0.3668],
        [0.4730, 0.3689, 0.3628],
        [0.5097, 0.3649, 0.3304],
        [0.4640, 0.3670, 0.3475],
        [0.4373, 0.3806, 0.3754],
        [0.5310, 0.3465, 0.3075],
        [0.4277, 0.3928, 0.3811],
        [0.4089, 0.4010, 0.3972],
        [0.4845, 0.3618, 0.3430],
        [0.4720, 0.3810, 0.3501],
        [0.5072, 0.3509, 0.3358],
        [0.4404, 0.3815, 0.3781],
        [0.4522, 0.3843, 0.3694],
        [0.4823, 0.3625, 0.3536],
        [0.4669, 0.3711, 0.3665],
        [0.4950, 0.3636, 0.3351],
        [0.4795, 0.3629, 0.3481],
        [0.4494, 0.3762, 0.3553],
        [0.4160, 0.3972, 0.3712],
        [0.4586, 0.3827, 0.3412],
        [0.4997, 0.3524, 0.2927],
        [0.5209, 0.3514, 0.3016],
        [0.5684, 0.3224, 0.2692],
        [0.4833, 0.3622, 0.3557],
        [0.4911, 0.3599, 0.3308],
        [0.4432, 0.3801, 0.3778],
        [0.4595, 0.3722, 0.3665],
        [0.4854, 0.3752, 0.3615],
        [0.4795, 0.3750, 0.3632],
        [0.4877, 0.3688, 0.3452],
        [0.5390, 0.3351, 0.3008],
        [0.4905, 0.3632, 0.3473],
        [0.4890, 0.3611, 0.3327],
        [0.5174, 0.3486, 0.3439]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [ 6, 11, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 90: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7324.0, Mean: 1323.2586669921875, Std: 1020.1575927734375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 73177.3125, Mean: 13220.458984375, Std: 10192.98046875
[DEBUG] Top-3 class probabilities:
tensor([[0.4716, 0.3745, 0.3683],
        [0.4542, 0.3788, 0.3622],
        [0.4490, 0.3720, 0.3678],
        [0.4669, 0.3767, 0.3676],
        [0.4661, 0.3782, 0.3499],
        [0.4283, 0.3869, 0.3758],
        [0.6281, 0.2844, 0.2740],
        [0.4674, 0.3646, 0.3624],
        [0.4574, 0.3701, 0.3602],
        [0.4844, 0.3638, 0.3524],
        [0.4565, 0.3788, 0.3553],
        [0.4795, 0.3748, 0.3617],
        [0.5024, 0.3467, 0.3416],
        [0.5340, 0.3481, 0.3274],
        [0.4963, 0.3524, 0.3411],
        [0.4971, 0.3586, 0.3280],
        [0.4856, 0.3706, 0.3444],
        [0.4346, 0.3814, 0.3717],
        [0.4518, 0.3783, 0.3636],
        [0.4670, 0.3788, 0.3714],
        [0.4246, 0.3948, 0.3825],
        [0.4810, 0.3647, 0.3450],
        [0.4534, 0.3734, 0.3669],
        [0.4278, 0.3825, 0.3794],
        [0.4319, 0.3854, 0.3831],
        [0.4290, 0.3817, 0.3806],
        [0.4650, 0.3820, 0.3567],
        [0.4599, 0.3763, 0.3710],
        [0.4780, 0.3752, 0.3658],
        [0.4500, 0.3694, 0.3668],
        [0.4351, 0.3865, 0.3559],
        [0.4818, 0.3865, 0.3347],
        [0.4337, 0.3963, 0.3698],
        [0.5317, 0.3338, 0.2812],
        [0.4855, 0.3612, 0.3352],
        [0.4563, 0.3876, 0.3475],
        [0.4863, 0.3699, 0.3634],
        [0.4694, 0.3686, 0.3680],
        [0.4355, 0.3862, 0.3810],
        [0.4798, 0.3741, 0.3609],
        [0.4719, 0.3848, 0.3646],
        [0.4801, 0.3719, 0.3428],
        [0.5802, 0.3140, 0.2684],
        [0.5541, 0.3251, 0.2871],
        [0.4602, 0.3794, 0.3566],
        [0.4759, 0.3741, 0.3650],
        [0.5040, 0.3638, 0.3394],
        [0.5001, 0.3710, 0.3432],
        [0.4642, 0.3706, 0.3539],
        [0.4791, 0.3737, 0.3536],
        [0.4763, 0.3726, 0.3524],
        [0.4580, 0.3788, 0.3714],
        [0.5698, 0.3088, 0.2679],
        [0.6309, 0.2884, 0.2683],
        [0.4645, 0.3770, 0.3437],
        [0.5145, 0.3471, 0.3164],
        [0.4451, 0.3981, 0.3714],
        [0.4270, 0.4164, 0.3694],
        [0.5401, 0.3466, 0.3152],
        [0.5246, 0.3552, 0.3263],
        [0.4991, 0.3578, 0.3375],
        [0.4670, 0.3779, 0.3732],
        [0.4498, 0.3816, 0.3773],
        [0.4318, 0.3904, 0.3886],
        [0.4863, 0.3594, 0.3396],
        [0.4613, 0.3758, 0.3585],
        [0.4553, 0.3780, 0.3737],
        [0.4489, 0.3774, 0.3671],
        [0.4762, 0.3693, 0.3562],
        [0.4117, 0.4059, 0.3844],
        [0.4278, 0.3909, 0.3835],
        [0.4354, 0.3908, 0.3877],
        [0.4771, 0.3643, 0.3367],
        [0.4779, 0.3860, 0.3434],
        [0.4631, 0.3776, 0.3585],
        [0.4997, 0.3798, 0.2899],
        [0.4875, 0.3810, 0.3233],
        [0.4953, 0.3595, 0.3319],
        [0.4564, 0.3885, 0.3649],
        [0.4449, 0.3832, 0.3747],
        [0.4761, 0.3719, 0.3509],
        [0.4198, 0.4043, 0.3510],
        [0.4766, 0.3890, 0.3156],
        [0.4927, 0.3738, 0.3419],
        [0.4566, 0.3732, 0.3712],
        [0.4466, 0.3869, 0.3672],
        [0.5136, 0.3613, 0.3210],
        [0.4966, 0.3797, 0.3293],
        [0.4714, 0.3701, 0.3479],
        [0.4957, 0.3508, 0.3331],
        [0.4877, 0.3569, 0.3525],
        [0.4645, 0.3723, 0.3617],
        [0.4777, 0.3686, 0.3610],
        [0.4913, 0.3641, 0.3600],
        [0.4722, 0.3739, 0.3690],
        [0.4720, 0.3693, 0.3559],
        [0.4916, 0.3552, 0.3419],
        [0.4940, 0.3737, 0.3466],
        [0.4779, 0.3673, 0.3551],
        [0.5749, 0.3267, 0.2688],
        [0.4450, 0.3862, 0.3615],
        [0.4764, 0.3575, 0.3447],
        [0.4816, 0.3629, 0.3534],
        [0.4632, 0.3854, 0.3516],
        [0.4927, 0.3493, 0.3197],
        [0.5307, 0.3433, 0.3119],
        [0.4983, 0.3611, 0.3464],
        [0.4615, 0.3715, 0.3684],
        [0.4483, 0.3862, 0.3776],
        [0.4955, 0.3567, 0.3354],
        [0.5423, 0.3224, 0.2836],
        [0.4871, 0.3639, 0.3575],
        [0.4915, 0.3643, 0.3584],
        [0.4707, 0.3791, 0.3528],
        [0.4609, 0.3682, 0.3589],
        [0.4800, 0.3654, 0.3415],
        [0.4806, 0.3582, 0.3495],
        [0.4592, 0.3786, 0.3585],
        [0.4543, 0.3802, 0.3630],
        [0.4570, 0.3904, 0.3522],
        [0.4557, 0.3890, 0.3632],
        [0.4662, 0.3632, 0.3519],
        [0.4804, 0.3755, 0.3411],
        [0.4899, 0.3713, 0.3469],
        [0.4488, 0.3839, 0.3713],
        [0.4816, 0.3602, 0.3348],
        [0.5061, 0.3680, 0.3315],
        [0.4606, 0.3842, 0.3512]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 91: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7284.0, Mean: 1071.3143310546875, Std: 1056.2630615234375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 72777.6484375, Mean: 10703.13671875, Std: 10553.73046875
[DEBUG] Top-3 class probabilities:
tensor([[0.4521, 0.3858, 0.3631],
        [0.4589, 0.3969, 0.3403],
        [0.5102, 0.3483, 0.3331],
        [0.4776, 0.3771, 0.3476],
        [0.5228, 0.3443, 0.3131],
        [0.6854, 0.3057, 0.2463],
        [0.5430, 0.3490, 0.2881],
        [0.5045, 0.3643, 0.3448],
        [0.4656, 0.3756, 0.3529],
        [0.4894, 0.3750, 0.3427],
        [0.4518, 0.3917, 0.3676],
        [0.4954, 0.3706, 0.3478],
        [0.4888, 0.3614, 0.3601],
        [0.4829, 0.3674, 0.3520],
        [0.5148, 0.3563, 0.3218],
        [0.4658, 0.3660, 0.3654],
        [0.4733, 0.3736, 0.3573],
        [0.4504, 0.3757, 0.3698],
        [0.5640, 0.3373, 0.2868],
        [0.4729, 0.3604, 0.3601],
        [0.4526, 0.3725, 0.3681],
        [0.4408, 0.3958, 0.3802],
        [0.4536, 0.3788, 0.3787],
        [0.4655, 0.3787, 0.3670],
        [0.4875, 0.3597, 0.3482],
        [0.4385, 0.3823, 0.3693],
        [0.4640, 0.3639, 0.3632],
        [0.4708, 0.3603, 0.3474],
        [0.4441, 0.3907, 0.3748],
        [0.4854, 0.3708, 0.3531],
        [0.4848, 0.3641, 0.3401],
        [0.4732, 0.3730, 0.3549],
        [0.4962, 0.3764, 0.3382],
        [0.5005, 0.3720, 0.3439],
        [0.4426, 0.3839, 0.3834],
        [0.4348, 0.3920, 0.3876],
        [0.4656, 0.3821, 0.3684],
        [0.4669, 0.3751, 0.3509],
        [0.4798, 0.3656, 0.3398],
        [0.4657, 0.3745, 0.3566],
        [0.4422, 0.3860, 0.3778],
        [0.4046, 0.3998, 0.3894],
        [0.4594, 0.3821, 0.3653],
        [0.4924, 0.3679, 0.3311],
        [0.4719, 0.3808, 0.3503],
        [0.5053, 0.3771, 0.3207],
        [0.4802, 0.4010, 0.3189],
        [0.4333, 0.4298, 0.3632],
        [0.4286, 0.4176, 0.3617],
        [0.4779, 0.3621, 0.3475],
        [0.4837, 0.3731, 0.3395],
        [0.5924, 0.3027, 0.2748],
        [0.6497, 0.2849, 0.2746],
        [0.4760, 0.3706, 0.3662],
        [0.4682, 0.3746, 0.3571],
        [0.4993, 0.3647, 0.3276],
        [0.4697, 0.3838, 0.3595],
        [0.4813, 0.3671, 0.3645],
        [0.4671, 0.3833, 0.3727],
        [0.5435, 0.3392, 0.3099],
        [0.5063, 0.3671, 0.3298],
        [0.4446, 0.3890, 0.3701],
        [0.4988, 0.3588, 0.3473],
        [0.4739, 0.3609, 0.3599],
        [0.4916, 0.3611, 0.3414],
        [0.4899, 0.3716, 0.3349],
        [0.4601, 0.3697, 0.3665],
        [0.4511, 0.3717, 0.3585],
        [0.4500, 0.3768, 0.3613],
        [0.4436, 0.3828, 0.3718],
        [0.4400, 0.3762, 0.3723],
        [0.4352, 0.3851, 0.3789],
        [0.5104, 0.3585, 0.3361],
        [0.4619, 0.3692, 0.3643],
        [0.4488, 0.3794, 0.3674],
        [0.4858, 0.3645, 0.3503],
        [0.5154, 0.3586, 0.3255],
        [0.4591, 0.3902, 0.3671],
        [0.4599, 0.3873, 0.3474],
        [0.6208, 0.2900, 0.2818],
        [0.5590, 0.3203, 0.2810],
        [0.4284, 0.3990, 0.3798],
        [0.4612, 0.3677, 0.3614],
        [0.4817, 0.3732, 0.3456],
        [0.4742, 0.3793, 0.3511],
        [0.4622, 0.3796, 0.3618],
        [0.4662, 0.3778, 0.3593],
        [0.4749, 0.3704, 0.3552],
        [0.4523, 0.3853, 0.3563],
        [0.4893, 0.3701, 0.3384],
        [0.4864, 0.3681, 0.3453],
        [0.6981, 0.2837, 0.2211],
        [0.6994, 0.2825, 0.2208],
        [0.7002, 0.2844, 0.2213],
        [0.7007, 0.2836, 0.2207],
        [0.6996, 0.2846, 0.2209],
        [0.7009, 0.2835, 0.2206],
        [0.7018, 0.2840, 0.2203],
        [0.7011, 0.2847, 0.2211],
        [0.7009, 0.2847, 0.2211],
        [0.7017, 0.2843, 0.2207],
        [0.7009, 0.2843, 0.2206],
        [0.7009, 0.2864, 0.2214],
        [0.7013, 0.2871, 0.2213],
        [0.7014, 0.2861, 0.2211],
        [0.7000, 0.2859, 0.2214],
        [0.7001, 0.2866, 0.2220],
        [0.7002, 0.2871, 0.2220],
        [0.6997, 0.2859, 0.2219],
        [0.6990, 0.2824, 0.2215],
        [0.6990, 0.2808, 0.2205],
        [0.6998, 0.2827, 0.2211],
        [0.7022, 0.2823, 0.2203],
        [0.7015, 0.2836, 0.2204],
        [0.7006, 0.2838, 0.2206],
        [0.7001, 0.2831, 0.2210],
        [0.6421, 0.2676, 0.2408],
        [0.4077, 0.3918, 0.3812],
        [0.4275, 0.3845, 0.3703],
        [0.4616, 0.4044, 0.3391],
        [0.4534, 0.3689, 0.3664],
        [0.6192, 0.3005, 0.2572],
        [0.7002, 0.2825, 0.2207],
        [0.7002, 0.2832, 0.2210],
        [0.6996, 0.2840, 0.2217],
        [0.7008, 0.2836, 0.2207],
        [0.7008, 0.2848, 0.2209],
        [0.7015, 0.2843, 0.2207]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 92: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 4.0, Max: 7724.0, Mean: 879.8303833007812, Std: 1202.492919921875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 38.98603057861328, Max: 77173.9375, Mean: 8789.91015625, Std: 12014.7978515625
[DEBUG] Top-3 class probabilities:
tensor([[0.7012, 0.2839, 0.2205],
        [0.7009, 0.2843, 0.2207],
        [0.7008, 0.2842, 0.2208],
        [0.7012, 0.2852, 0.2209],
        [0.7008, 0.2868, 0.2215],
        [0.7021, 0.2861, 0.2211],
        [0.7009, 0.2869, 0.2216],
        [0.7001, 0.2822, 0.2205],
        [0.6979, 0.2802, 0.2212],
        [0.6973, 0.2809, 0.2215],
        [0.6980, 0.2821, 0.2218],
        [0.6987, 0.2798, 0.2202],
        [0.6983, 0.2796, 0.2206],
        [0.6981, 0.2808, 0.2213],
        [0.7007, 0.2804, 0.2201],
        [0.7012, 0.2826, 0.2209],
        [0.7000, 0.2827, 0.2212],
        [0.7022, 0.2815, 0.2197],
        [0.6652, 0.2581, 0.2545],
        [0.4021, 0.3921, 0.3879],
        [0.4457, 0.3754, 0.3458],
        [0.4673, 0.3906, 0.3296],
        [0.4271, 0.3932, 0.3460],
        [0.6791, 0.2818, 0.2365],
        [0.7006, 0.2837, 0.2209],
        [0.6997, 0.2836, 0.2208],
        [0.7002, 0.2847, 0.2213],
        [0.7006, 0.2856, 0.2214],
        [0.7012, 0.2856, 0.2215],
        [0.7014, 0.2848, 0.2207],
        [0.7015, 0.2852, 0.2206],
        [0.7012, 0.2845, 0.2209],
        [0.7005, 0.2854, 0.2211],
        [0.7003, 0.2856, 0.2213],
        [0.6997, 0.2852, 0.2218],
        [0.6997, 0.2828, 0.2214],
        [0.6979, 0.2805, 0.2210],
        [0.6972, 0.2781, 0.2200],
        [0.6970, 0.2781, 0.2208],
        [0.6974, 0.2803, 0.2218],
        [0.6975, 0.2803, 0.2210],
        [0.6974, 0.2819, 0.2220],
        [0.6978, 0.2789, 0.2206],
        [0.6985, 0.2805, 0.2209],
        [0.6997, 0.2808, 0.2208],
        [0.6999, 0.2826, 0.2210],
        [0.6998, 0.2804, 0.2206],
        [0.6998, 0.2788, 0.2204],
        [0.6890, 0.2766, 0.2317],
        [0.4909, 0.3609, 0.3032],
        [0.4403, 0.3862, 0.3840],
        [0.4589, 0.3746, 0.3699],
        [0.6062, 0.2896, 0.2690],
        [0.7004, 0.2838, 0.2214],
        [0.7005, 0.2844, 0.2209],
        [0.7004, 0.2855, 0.2209],
        [0.7020, 0.2854, 0.2210],
        [0.7014, 0.2859, 0.2213],
        [0.7010, 0.2847, 0.2215],
        [0.6987, 0.2809, 0.2200],
        [0.6982, 0.2780, 0.2198],
        [0.6971, 0.2800, 0.2210],
        [0.6971, 0.2817, 0.2212],
        [0.6971, 0.2812, 0.2216],
        [0.6980, 0.2800, 0.2209],
        [0.6978, 0.2799, 0.2205],
        [0.6974, 0.2775, 0.2198],
        [0.6973, 0.2788, 0.2209],
        [0.6968, 0.2817, 0.2216],
        [0.6968, 0.2825, 0.2224],
        [0.6990, 0.2794, 0.2206],
        [0.6975, 0.2799, 0.2209],
        [0.6979, 0.2777, 0.2201],
        [0.6990, 0.2818, 0.2208],
        [0.6999, 0.2808, 0.2206],
        [0.6990, 0.2750, 0.2179],
        [0.6996, 0.2765, 0.2183],
        [0.6833, 0.2802, 0.2304],
        [0.5312, 0.3435, 0.2654],
        [0.6188, 0.2802, 0.2749],
        [0.7012, 0.2832, 0.2205],
        [0.7005, 0.2837, 0.2210],
        [0.6969, 0.2804, 0.2197],
        [0.6966, 0.2780, 0.2197],
        [0.6957, 0.2727, 0.2186],
        [0.6952, 0.2747, 0.2190],
        [0.4207, 0.4051, 0.3735],
        [0.4385, 0.3823, 0.3615],
        [0.4282, 0.4043, 0.3550],
        [0.5090, 0.3701, 0.2928],
        [0.5288, 0.3494, 0.2697],
        [0.5169, 0.3733, 0.2708],
        [0.4845, 0.3877, 0.2951],
        [0.4813, 0.3905, 0.3008],
        [0.4820, 0.3741, 0.3154],
        [0.5663, 0.3369, 0.2436],
        [0.4823, 0.3765, 0.2996],
        [0.4806, 0.3853, 0.2982],
        [0.4933, 0.3679, 0.2895],
        [0.4633, 0.3879, 0.3161],
        [0.4963, 0.3627, 0.2906],
        [0.5175, 0.3611, 0.2804],
        [0.5350, 0.3508, 0.2692],
        [0.5820, 0.3241, 0.2466],
        [0.5165, 0.3682, 0.2881],
        [0.4370, 0.4094, 0.3403],
        [0.4295, 0.4073, 0.3495],
        [0.4564, 0.3764, 0.3546],
        [0.4030, 0.3986, 0.3970],
        [0.4896, 0.3547, 0.3176],
        [0.4579, 0.3760, 0.3479],
        [0.5082, 0.3559, 0.2945],
        [0.5146, 0.3557, 0.2880],
        [0.4554, 0.3892, 0.3309],
        [0.5449, 0.3569, 0.2550],
        [0.5308, 0.3611, 0.2556],
        [0.5724, 0.3306, 0.2386],
        [0.5292, 0.3547, 0.2656],
        [0.4573, 0.4034, 0.3130],
        [0.5063, 0.3752, 0.2933],
        [0.5434, 0.3499, 0.2548],
        [0.5093, 0.3715, 0.2830],
        [0.5394, 0.3568, 0.2626],
        [0.5463, 0.3511, 0.2670],
        [0.5582, 0.3357, 0.2499],
        [0.4661, 0.4016, 0.3278],
        [0.5087, 0.3847, 0.2874],
        [0.4217, 0.4199, 0.3321]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [17,  6, 11],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 93: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6568.0, Mean: 1812.6142578125, Std: 1286.5950927734375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 65623.6796875, Mean: 18109.890625, Std: 12855.1103515625
[DEBUG] Top-3 class probabilities:
tensor([[0.4581, 0.3920, 0.3398],
        [0.4756, 0.3866, 0.3364],
        [0.4447, 0.3746, 0.3444],
        [0.5807, 0.3308, 0.2321],
        [0.5533, 0.3395, 0.2507],
        [0.4256, 0.3933, 0.3655],
        [0.5489, 0.3412, 0.2704],
        [0.5778, 0.3312, 0.2377],
        [0.5773, 0.3221, 0.2330],
        [0.5806, 0.3247, 0.2356],
        [0.5195, 0.3556, 0.2784],
        [0.4248, 0.4126, 0.3317],
        [0.5264, 0.3588, 0.2680],
        [0.5451, 0.3484, 0.2599],
        [0.5336, 0.3479, 0.2625],
        [0.5200, 0.3539, 0.3037],
        [0.5314, 0.3535, 0.2788],
        [0.5270, 0.3564, 0.2732],
        [0.5158, 0.3660, 0.2833],
        [0.5024, 0.3612, 0.2933],
        [0.4925, 0.3883, 0.3077],
        [0.4681, 0.3734, 0.3378],
        [0.4870, 0.3620, 0.3089],
        [0.5053, 0.3517, 0.3040],
        [0.5223, 0.3491, 0.2939],
        [0.5245, 0.3439, 0.2793],
        [0.5700, 0.3322, 0.2486],
        [0.5550, 0.3317, 0.2529],
        [0.5517, 0.3363, 0.2587],
        [0.5550, 0.3431, 0.2530],
        [0.4687, 0.3943, 0.3043],
        [0.4942, 0.3778, 0.3011],
        [0.5043, 0.3670, 0.3011],
        [0.5378, 0.3454, 0.2778],
        [0.5255, 0.3510, 0.2866],
        [0.5046, 0.3670, 0.3012],
        [0.5485, 0.3432, 0.2548],
        [0.5131, 0.3635, 0.2765],
        [0.4576, 0.3931, 0.3373],
        [0.4881, 0.3818, 0.3062],
        [0.5313, 0.3427, 0.2751],
        [0.5431, 0.3430, 0.2804],
        [0.4921, 0.3742, 0.3132],
        [0.5075, 0.3731, 0.2948],
        [0.5101, 0.3574, 0.2944],
        [0.5105, 0.3593, 0.3020],
        [0.4972, 0.3664, 0.3070],
        [0.5130, 0.3666, 0.2922],
        [0.5314, 0.3536, 0.2802],
        [0.5727, 0.3389, 0.2663],
        [0.5759, 0.3346, 0.2540],
        [0.5407, 0.3372, 0.2648],
        [0.5084, 0.3604, 0.2960],
        [0.5370, 0.3488, 0.2708],
        [0.4989, 0.3694, 0.2975],
        [0.5041, 0.3684, 0.2837],
        [0.5426, 0.3468, 0.2591],
        [0.5377, 0.3489, 0.2625],
        [0.4578, 0.3810, 0.3404],
        [0.4904, 0.3793, 0.3031],
        [0.4594, 0.3848, 0.3322],
        [0.4500, 0.3891, 0.3326],
        [0.5322, 0.3486, 0.2859],
        [0.5388, 0.3408, 0.2663],
        [0.5519, 0.3416, 0.2658],
        [0.5266, 0.3521, 0.2801],
        [0.4746, 0.3737, 0.3252],
        [0.4718, 0.3700, 0.3323],
        [0.5424, 0.3355, 0.2748],
        [0.5853, 0.3201, 0.2394],
        [0.5460, 0.3406, 0.2651],
        [0.4835, 0.3743, 0.3046],
        [0.5305, 0.3495, 0.2748],
        [0.5373, 0.3460, 0.2678],
        [0.5224, 0.3502, 0.2765],
        [0.5373, 0.3501, 0.2725],
        [0.5688, 0.3322, 0.2436],
        [0.5594, 0.3363, 0.2511],
        [0.5692, 0.3377, 0.2380],
        [0.5390, 0.3613, 0.2609],
        [0.4619, 0.4024, 0.3089],
        [0.5016, 0.3704, 0.2874],
        [0.4978, 0.3799, 0.3192],
        [0.5504, 0.3431, 0.2725],
        [0.5168, 0.3522, 0.3037],
        [0.4979, 0.3594, 0.3143],
        [0.5324, 0.3482, 0.2870],
        [0.5501, 0.3262, 0.2679],
        [0.4429, 0.3866, 0.3653],
        [0.5271, 0.3523, 0.2819],
        [0.5434, 0.3396, 0.2703],
        [0.5246, 0.3565, 0.2764],
        [0.4944, 0.3771, 0.2924],
        [0.5232, 0.3648, 0.2754],
        [0.4416, 0.3859, 0.3536],
        [0.5040, 0.3589, 0.3066],
        [0.5370, 0.3459, 0.2668],
        [0.5841, 0.3254, 0.2377],
        [0.5207, 0.3658, 0.2802],
        [0.5248, 0.3700, 0.2806],
        [0.4983, 0.3762, 0.2897],
        [0.5081, 0.3669, 0.3082],
        [0.5268, 0.3369, 0.2858],
        [0.4623, 0.3746, 0.3371],
        [0.5761, 0.3149, 0.2443],
        [0.5312, 0.3454, 0.2759],
        [0.4732, 0.3760, 0.3339],
        [0.5267, 0.3386, 0.2844],
        [0.4603, 0.3751, 0.3506],
        [0.5634, 0.3191, 0.2599],
        [0.5554, 0.3289, 0.2594],
        [0.5036, 0.3641, 0.2922],
        [0.4962, 0.3770, 0.2978],
        [0.5201, 0.3507, 0.2743],
        [0.4903, 0.3661, 0.3163],
        [0.5217, 0.3587, 0.2851],
        [0.5012, 0.3589, 0.2936],
        [0.5114, 0.3595, 0.2821],
        [0.5463, 0.3441, 0.2601],
        [0.4669, 0.3796, 0.3254],
        [0.4446, 0.3946, 0.3453],
        [0.4884, 0.3776, 0.3137],
        [0.4281, 0.3945, 0.3552],
        [0.4580, 0.3834, 0.3370],
        [0.4983, 0.3575, 0.3151],
        [0.5484, 0.3357, 0.2727],
        [0.5112, 0.3546, 0.2974],
        [0.4710, 0.3725, 0.3141]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 94: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9984.0, Mean: 1747.9462890625, Std: 1200.638427734375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 99754.90625, Mean: 17463.75390625, Std: 11996.267578125
[DEBUG] Top-3 class probabilities:
tensor([[0.5043, 0.3515, 0.3032],
        [0.5064, 0.3593, 0.2961],
        [0.4810, 0.3689, 0.3311],
        [0.5475, 0.3264, 0.2609],
        [0.5439, 0.3415, 0.2746],
        [0.5196, 0.3487, 0.2856],
        [0.5717, 0.3383, 0.2418],
        [0.4929, 0.3659, 0.3022],
        [0.4057, 0.4044, 0.3746],
        [0.4707, 0.3752, 0.3272],
        [0.5403, 0.3451, 0.2744],
        [0.5702, 0.3316, 0.2618],
        [0.4351, 0.3802, 0.3701],
        [0.4767, 0.3791, 0.3245],
        [0.4291, 0.3864, 0.3642],
        [0.4956, 0.3641, 0.3086],
        [0.4835, 0.3808, 0.3146],
        [0.4451, 0.3958, 0.3577],
        [0.4714, 0.3783, 0.3347],
        [0.5677, 0.3228, 0.2561],
        [0.5251, 0.3554, 0.2968],
        [0.4674, 0.3682, 0.3477],
        [0.5535, 0.3270, 0.2587],
        [0.4572, 0.3896, 0.3541],
        [0.4838, 0.3596, 0.3303],
        [0.5372, 0.3431, 0.2702],
        [0.5128, 0.3567, 0.2949],
        [0.5251, 0.3536, 0.2895],
        [0.5405, 0.3454, 0.2706],
        [0.4958, 0.3676, 0.3045],
        [0.4459, 0.3740, 0.3594],
        [0.4565, 0.3860, 0.3357],
        [0.4320, 0.3846, 0.3677],
        [0.4197, 0.3943, 0.3720],
        [0.4817, 0.3699, 0.3235],
        [0.5157, 0.3608, 0.3122],
        [0.4813, 0.3734, 0.3353],
        [0.4793, 0.3761, 0.3298],
        [0.4892, 0.3749, 0.3090],
        [0.4878, 0.3699, 0.3175],
        [0.5734, 0.3182, 0.2439],
        [0.5273, 0.3442, 0.2899],
        [0.5427, 0.3383, 0.2732],
        [0.4496, 0.3915, 0.3429],
        [0.5232, 0.3556, 0.2985],
        [0.4740, 0.3653, 0.3285],
        [0.4942, 0.3693, 0.3111],
        [0.5714, 0.3303, 0.2373],
        [0.4929, 0.3787, 0.3063],
        [0.5041, 0.3650, 0.2992],
        [0.4731, 0.3736, 0.3341],
        [0.4339, 0.4051, 0.3725],
        [0.4278, 0.3923, 0.3662],
        [0.4391, 0.3828, 0.3640],
        [0.4439, 0.3846, 0.3576],
        [0.4883, 0.3582, 0.3194],
        [0.4730, 0.3706, 0.3394],
        [0.4175, 0.3917, 0.3875],
        [0.5074, 0.3699, 0.2992],
        [0.5193, 0.3659, 0.2956],
        [0.5666, 0.3267, 0.2553],
        [0.5857, 0.3179, 0.2399],
        [0.5609, 0.3239, 0.2550],
        [0.4615, 0.3790, 0.3334],
        [0.5092, 0.3532, 0.2977],
        [0.5176, 0.3414, 0.2980],
        [0.5118, 0.3500, 0.3018],
        [0.5717, 0.3308, 0.2483],
        [0.5405, 0.3465, 0.2608],
        [0.5756, 0.3220, 0.2472],
        [0.5359, 0.3457, 0.2855],
        [0.4349, 0.4175, 0.3471],
        [0.4698, 0.3716, 0.3327],
        [0.4814, 0.3577, 0.3248],
        [0.4600, 0.3748, 0.3541],
        [0.4549, 0.3906, 0.3611],
        [0.4723, 0.3720, 0.3419],
        [0.4720, 0.3645, 0.3385],
        [0.4876, 0.3796, 0.3130],
        [0.5476, 0.3559, 0.2606],
        [0.5257, 0.3474, 0.2917],
        [0.5708, 0.3334, 0.2518],
        [0.5357, 0.3466, 0.2966],
        [0.5773, 0.3247, 0.2482],
        [0.4974, 0.3527, 0.2959],
        [0.5356, 0.3349, 0.2796],
        [0.5325, 0.3409, 0.2799],
        [0.5281, 0.3476, 0.2791],
        [0.5404, 0.3447, 0.2717],
        [0.5642, 0.3367, 0.2495],
        [0.5528, 0.3456, 0.2552],
        [0.4670, 0.3792, 0.3322],
        [0.4081, 0.3962, 0.3785],
        [0.4696, 0.3757, 0.3347],
        [0.4819, 0.3640, 0.3348],
        [0.4758, 0.3711, 0.3416],
        [0.4781, 0.3812, 0.3326],
        [0.4925, 0.3736, 0.3295],
        [0.5204, 0.3676, 0.2786],
        [0.5316, 0.3514, 0.2908],
        [0.5008, 0.3523, 0.3164],
        [0.4737, 0.3701, 0.3512],
        [0.5026, 0.3675, 0.3189],
        [0.5184, 0.3508, 0.3016],
        [0.5319, 0.3401, 0.2722],
        [0.4916, 0.3558, 0.3166],
        [0.4687, 0.3794, 0.3454],
        [0.5566, 0.3260, 0.2623],
        [0.5694, 0.3280, 0.2479],
        [0.5058, 0.3580, 0.3075],
        [0.4623, 0.3859, 0.3391],
        [0.4607, 0.3876, 0.3471],
        [0.4762, 0.3686, 0.3351],
        [0.4839, 0.3751, 0.3390],
        [0.4555, 0.3939, 0.3454],
        [0.5881, 0.3326, 0.2253],
        [0.5014, 0.3651, 0.3053],
        [0.4719, 0.3733, 0.3360],
        [0.5355, 0.3464, 0.2811],
        [0.5165, 0.3533, 0.2993],
        [0.5405, 0.3323, 0.2651],
        [0.5280, 0.3530, 0.2839],
        [0.5242, 0.3517, 0.2957],
        [0.4569, 0.3983, 0.3429],
        [0.4665, 0.3730, 0.3376],
        [0.4289, 0.4011, 0.3344],
        [0.4389, 0.3936, 0.3628],
        [0.4508, 0.3905, 0.3549]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 95: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6268.0, Mean: 988.779541015625, Std: 1181.9801025390625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 62626.20703125, Mean: 9878.482421875, Std: 11809.8427734375
[DEBUG] Top-3 class probabilities:
tensor([[0.4906, 0.3758, 0.3301],
        [0.4648, 0.3879, 0.3599],
        [0.4999, 0.3739, 0.3052],
        [0.5790, 0.3383, 0.2418],
        [0.4824, 0.3572, 0.3215],
        [0.5225, 0.3423, 0.2865],
        [0.4680, 0.3690, 0.3470],
        [0.5362, 0.3401, 0.2831],
        [0.5490, 0.3340, 0.2720],
        [0.4829, 0.3827, 0.3051],
        [0.4764, 0.3687, 0.3361],
        [0.4946, 0.3596, 0.3235],
        [0.5231, 0.3533, 0.2932],
        [0.4670, 0.3789, 0.3423],
        [0.4969, 0.3633, 0.3207],
        [0.4836, 0.3739, 0.3161],
        [0.5304, 0.3794, 0.2766],
        [0.5135, 0.3650, 0.2785],
        [0.5486, 0.3482, 0.2763],
        [0.4397, 0.4080, 0.3462],
        [0.4494, 0.3889, 0.3305],
        [0.4945, 0.3662, 0.3167],
        [0.5145, 0.3754, 0.2852],
        [0.5573, 0.3685, 0.2599],
        [0.4780, 0.3785, 0.3265],
        [0.5272, 0.3677, 0.2899],
        [0.5402, 0.3616, 0.2665],
        [0.5492, 0.3428, 0.2841],
        [0.4511, 0.3934, 0.3294],
        [0.5426, 0.3613, 0.2676],
        [0.5184, 0.3634, 0.3056],
        [0.4771, 0.3837, 0.3281],
        [0.4708, 0.3857, 0.3312],
        [0.5320, 0.3519, 0.2717],
        [0.4997, 0.3847, 0.3023],
        [0.4635, 0.3975, 0.3364],
        [0.5319, 0.3658, 0.2888],
        [0.4755, 0.4089, 0.3145],
        [0.4782, 0.3882, 0.3097],
        [0.5553, 0.3544, 0.2583],
        [0.5101, 0.3563, 0.2978],
        [0.4563, 0.3871, 0.3424],
        [0.4414, 0.3836, 0.3521],
        [0.4540, 0.3876, 0.3409],
        [0.4795, 0.3780, 0.3445],
        [0.4243, 0.4157, 0.3492],
        [0.4640, 0.4023, 0.3241],
        [0.4897, 0.3795, 0.3056],
        [0.4658, 0.3777, 0.3540],
        [0.4868, 0.3623, 0.3223],
        [0.4145, 0.3977, 0.3877],
        [0.4154, 0.4000, 0.3903],
        [0.4427, 0.3949, 0.3444],
        [0.4195, 0.4152, 0.3548],
        [0.4619, 0.3844, 0.3285],
        [0.4185, 0.4073, 0.3739],
        [0.4516, 0.3934, 0.3510],
        [0.5319, 0.3365, 0.2781],
        [0.4518, 0.4045, 0.3298],
        [0.4554, 0.3919, 0.3240],
        [0.4583, 0.3888, 0.3328],
        [0.4404, 0.3926, 0.3611],
        [0.4205, 0.4055, 0.3707],
        [0.4794, 0.3693, 0.3410],
        [0.4854, 0.3705, 0.3168],
        [0.4308, 0.3954, 0.3556],
        [0.4099, 0.4001, 0.3769],
        [0.4199, 0.4126, 0.3728],
        [0.7065, 0.2992, 0.2230],
        [0.7068, 0.2991, 0.2228],
        [0.7065, 0.2987, 0.2231],
        [0.7064, 0.2991, 0.2230],
        [0.7063, 0.2990, 0.2232],
        [0.7064, 0.2991, 0.2230],
        [0.7059, 0.2996, 0.2236],
        [0.7060, 0.2990, 0.2232],
        [0.7066, 0.2993, 0.2230],
        [0.7067, 0.2994, 0.2231],
        [0.7070, 0.2989, 0.2230],
        [0.7065, 0.2999, 0.2234],
        [0.7067, 0.2997, 0.2234],
        [0.7069, 0.3001, 0.2233],
        [0.7061, 0.3010, 0.2237],
        [0.7066, 0.3006, 0.2235],
        [0.7061, 0.3011, 0.2238],
        [0.7057, 0.3010, 0.2237],
        [0.7060, 0.3009, 0.2238],
        [0.7062, 0.3006, 0.2237],
        [0.7067, 0.3005, 0.2233],
        [0.7067, 0.3001, 0.2230],
        [0.7068, 0.3007, 0.2233],
        [0.7064, 0.3006, 0.2235],
        [0.7061, 0.3010, 0.2237],
        [0.7046, 0.3029, 0.2247],
        [0.7044, 0.3034, 0.2249],
        [0.7045, 0.3038, 0.2250],
        [0.7040, 0.3038, 0.2252],
        [0.7035, 0.3045, 0.2255],
        [0.7096, 0.2980, 0.2217],
        [0.7006, 0.2996, 0.2349],
        [0.7009, 0.2929, 0.2300],
        [0.6995, 0.2820, 0.2402],
        [0.6840, 0.2749, 0.2631],
        [0.5728, 0.3380, 0.2866],
        [0.6578, 0.2854, 0.2713],
        [0.7200, 0.2872, 0.2154],
        [0.7194, 0.2881, 0.2171],
        [0.7070, 0.2990, 0.2229],
        [0.7067, 0.2991, 0.2232],
        [0.7065, 0.2990, 0.2231],
        [0.7061, 0.2993, 0.2232],
        [0.7063, 0.2991, 0.2233],
        [0.7065, 0.2989, 0.2230],
        [0.7068, 0.2987, 0.2230],
        [0.7062, 0.2992, 0.2231],
        [0.7062, 0.2992, 0.2232],
        [0.7066, 0.2992, 0.2231],
        [0.7065, 0.2992, 0.2230],
        [0.7068, 0.2993, 0.2231],
        [0.7068, 0.2998, 0.2232],
        [0.7065, 0.3002, 0.2234],
        [0.7057, 0.3014, 0.2243],
        [0.7053, 0.3014, 0.2242],
        [0.7056, 0.3011, 0.2239],
        [0.7053, 0.3013, 0.2242],
        [0.7055, 0.3015, 0.2241],
        [0.7057, 0.3011, 0.2240],
        [0.7062, 0.3009, 0.2236]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 96: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8424.0, Mean: 134.8587646484375, Std: 436.7115478515625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 84168.046875, Mean: 1346.47119140625, Std: 4363.43603515625
[DEBUG] Top-3 class probabilities:
tensor([[0.7064, 0.3008, 0.2235],
        [0.7073, 0.3001, 0.2229],
        [0.7078, 0.2994, 0.2227],
        [0.7066, 0.3007, 0.2234],
        [0.7055, 0.3017, 0.2240],
        [0.7051, 0.3027, 0.2243],
        [0.7062, 0.3015, 0.2237],
        [0.7040, 0.3039, 0.2253],
        [0.7050, 0.3029, 0.2245],
        [0.7073, 0.3002, 0.2229],
        [0.7112, 0.2959, 0.2204],
        [0.7096, 0.2985, 0.2205],
        [0.7193, 0.2983, 0.2289],
        [0.7237, 0.2895, 0.2163],
        [0.7137, 0.3046, 0.2320],
        [0.6851, 0.2856, 0.2551],
        [0.6871, 0.2654, 0.2615],
        [0.6892, 0.3045, 0.2616],
        [0.7163, 0.2947, 0.2271],
        [0.7074, 0.2987, 0.2228],
        [0.7070, 0.2991, 0.2228],
        [0.7066, 0.2988, 0.2229],
        [0.7064, 0.2989, 0.2231],
        [0.7064, 0.2988, 0.2230],
        [0.7070, 0.2987, 0.2227],
        [0.7069, 0.2984, 0.2229],
        [0.7065, 0.2990, 0.2231],
        [0.7062, 0.2991, 0.2232],
        [0.7062, 0.2990, 0.2231],
        [0.7071, 0.2988, 0.2230],
        [0.7077, 0.2987, 0.2227],
        [0.7075, 0.2988, 0.2229],
        [0.7069, 0.2998, 0.2231],
        [0.7057, 0.3012, 0.2238],
        [0.7055, 0.3013, 0.2240],
        [0.7047, 0.3023, 0.2244],
        [0.7048, 0.3021, 0.2245],
        [0.7050, 0.3018, 0.2243],
        [0.7055, 0.3016, 0.2242],
        [0.7056, 0.3016, 0.2241],
        [0.7060, 0.3010, 0.2237],
        [0.7060, 0.3009, 0.2237],
        [0.7060, 0.3011, 0.2240],
        [0.7067, 0.3002, 0.2233],
        [0.7076, 0.2993, 0.2227],
        [0.7067, 0.3003, 0.2232],
        [0.7061, 0.3009, 0.2236],
        [0.7066, 0.3011, 0.2236],
        [0.7075, 0.3003, 0.2230],
        [0.7052, 0.3022, 0.2243],
        [0.7068, 0.3010, 0.2233],
        [0.7084, 0.2989, 0.2223],
        [0.7103, 0.2970, 0.2211],
        [0.7106, 0.2966, 0.2209],
        [0.7108, 0.3061, 0.2318],
        [0.6782, 0.2841, 0.2725],
        [0.6786, 0.3028, 0.2542],
        [0.7120, 0.3066, 0.2242],
        [0.7104, 0.2825, 0.2453],
        [0.6623, 0.2998, 0.2790],
        [0.6216, 0.2926, 0.2401],
        [0.7069, 0.2991, 0.2229],
        [0.7071, 0.2990, 0.2229],
        [0.7069, 0.2989, 0.2229],
        [0.7071, 0.2989, 0.2228],
        [0.7068, 0.2983, 0.2226],
        [0.7065, 0.2988, 0.2230],
        [0.7067, 0.2984, 0.2228],
        [0.7066, 0.2985, 0.2229],
        [0.7068, 0.2983, 0.2230],
        [0.7073, 0.2981, 0.2226],
        [0.7072, 0.2987, 0.2229],
        [0.7071, 0.2992, 0.2230],
        [0.7064, 0.2997, 0.2235],
        [0.7063, 0.3004, 0.2236],
        [0.7058, 0.3009, 0.2239],
        [0.7048, 0.3018, 0.2245],
        [0.7047, 0.3020, 0.2247],
        [0.7046, 0.3025, 0.2247],
        [0.7043, 0.3030, 0.2250],
        [0.7045, 0.3021, 0.2247],
        [0.7054, 0.3016, 0.2243],
        [0.7053, 0.3016, 0.2243],
        [0.7057, 0.3010, 0.2239],
        [0.7057, 0.3011, 0.2239],
        [0.7053, 0.3015, 0.2243],
        [0.7056, 0.3011, 0.2241],
        [0.7071, 0.2997, 0.2230],
        [0.7067, 0.3005, 0.2236],
        [0.7060, 0.3014, 0.2239],
        [0.7064, 0.3009, 0.2236],
        [0.7079, 0.2996, 0.2227],
        [0.7074, 0.2999, 0.2227],
        [0.7080, 0.2992, 0.2223],
        [0.7084, 0.2987, 0.2222],
        [0.7079, 0.2992, 0.2227],
        [0.7072, 0.2994, 0.2232],
        [0.5030, 0.3650, 0.3002],
        [0.5927, 0.3301, 0.2683],
        [0.6509, 0.2744, 0.2688],
        [0.6445, 0.2820, 0.2675],
        [0.5229, 0.3653, 0.3260],
        [0.5497, 0.3580, 0.3013],
        [0.7072, 0.2993, 0.2228],
        [0.7075, 0.2988, 0.2227],
        [0.7071, 0.2987, 0.2228],
        [0.7073, 0.2989, 0.2227],
        [0.7068, 0.2990, 0.2229],
        [0.7064, 0.2987, 0.2230],
        [0.7066, 0.2984, 0.2228],
        [0.7075, 0.2979, 0.2225],
        [0.7069, 0.2975, 0.2227],
        [0.7066, 0.2977, 0.2229],
        [0.7063, 0.2979, 0.2229],
        [0.7066, 0.2986, 0.2231],
        [0.7060, 0.2995, 0.2233],
        [0.7058, 0.3004, 0.2237],
        [0.7054, 0.3011, 0.2241],
        [0.7045, 0.3021, 0.2245],
        [0.7041, 0.3029, 0.2250],
        [0.7041, 0.3030, 0.2252],
        [0.7041, 0.3030, 0.2251],
        [0.7041, 0.3030, 0.2252],
        [0.7045, 0.3023, 0.2246],
        [0.7049, 0.3022, 0.2245],
        [0.7049, 0.3018, 0.2245],
        [0.7052, 0.3015, 0.2243],
        [0.7049, 0.3017, 0.2246]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 97: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6676.0, Mean: 244.3748016357422, Std: 653.528076171875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 66702.7734375, Mean: 2440.708984375, Std: 6529.77392578125
[DEBUG] Top-3 class probabilities:
tensor([[0.7047, 0.3020, 0.2247],
        [0.7056, 0.3015, 0.2241],
        [0.7078, 0.2989, 0.2226],
        [0.7069, 0.3003, 0.2233],
        [0.7064, 0.3005, 0.2234],
        [0.7067, 0.3007, 0.2234],
        [0.7071, 0.3002, 0.2232],
        [0.7077, 0.2995, 0.2227],
        [0.7072, 0.2998, 0.2229],
        [0.7067, 0.3000, 0.2233],
        [0.7068, 0.3002, 0.2233],
        [0.7062, 0.3001, 0.2237],
        [0.7133, 0.2867, 0.2241],
        [0.5638, 0.3438, 0.2819],
        [0.5157, 0.3707, 0.3316],
        [0.5164, 0.3785, 0.3258],
        [0.4521, 0.4066, 0.3326],
        [0.5047, 0.3790, 0.3331],
        [0.5383, 0.3537, 0.3080],
        [0.7067, 0.2938, 0.2221],
        [0.7025, 0.2990, 0.2235],
        [0.7077, 0.2988, 0.2224],
        [0.7079, 0.2989, 0.2226],
        [0.7068, 0.2987, 0.2229],
        [0.7070, 0.2984, 0.2226],
        [0.7078, 0.2980, 0.2225],
        [0.7080, 0.2977, 0.2222],
        [0.7073, 0.2978, 0.2225],
        [0.7050, 0.2965, 0.2232],
        [0.7045, 0.2963, 0.2233],
        [0.7057, 0.2972, 0.2232],
        [0.7060, 0.2979, 0.2231],
        [0.7058, 0.2993, 0.2236],
        [0.7059, 0.2999, 0.2237],
        [0.7052, 0.3007, 0.2243],
        [0.7043, 0.3016, 0.2246],
        [0.7039, 0.3024, 0.2252],
        [0.7037, 0.3028, 0.2251],
        [0.7032, 0.3036, 0.2257],
        [0.7032, 0.3037, 0.2256],
        [0.7037, 0.3034, 0.2253],
        [0.7039, 0.3032, 0.2251],
        [0.7042, 0.3026, 0.2249],
        [0.7044, 0.3022, 0.2249],
        [0.7047, 0.3017, 0.2246],
        [0.7047, 0.3015, 0.2245],
        [0.7050, 0.3017, 0.2243],
        [0.7080, 0.2988, 0.2225],
        [0.7091, 0.2978, 0.2217],
        [0.7103, 0.2964, 0.2211],
        [0.7092, 0.2980, 0.2217],
        [0.7084, 0.2990, 0.2224],
        [0.7083, 0.2988, 0.2223],
        [0.7075, 0.2997, 0.2227],
        [0.7065, 0.3002, 0.2235],
        [0.7074, 0.2993, 0.2229],
        [0.7086, 0.2975, 0.2218],
        [0.7114, 0.2957, 0.2202],
        [0.6695, 0.2550, 0.2443],
        [0.5301, 0.3688, 0.3161],
        [0.4943, 0.3785, 0.3355],
        [0.4996, 0.3677, 0.3298],
        [0.5134, 0.3681, 0.3136],
        [0.4790, 0.3731, 0.3333],
        [0.5342, 0.3690, 0.3141],
        [0.7070, 0.2986, 0.2229],
        [0.7052, 0.2979, 0.2232],
        [0.7071, 0.2983, 0.2225],
        [0.7083, 0.2981, 0.2222],
        [0.7087, 0.2978, 0.2220],
        [0.7085, 0.2973, 0.2222],
        [0.7081, 0.2979, 0.2223],
        [0.7069, 0.2977, 0.2227],
        [0.7063, 0.2962, 0.2228],
        [0.7061, 0.2972, 0.2230],
        [0.7064, 0.2975, 0.2232],
        [0.7063, 0.2972, 0.2233],
        [0.7057, 0.2985, 0.2236],
        [0.7054, 0.2994, 0.2238],
        [0.7052, 0.3002, 0.2242],
        [0.7048, 0.3010, 0.2244],
        [0.7048, 0.3008, 0.2245],
        [0.7036, 0.3027, 0.2253],
        [0.7033, 0.3032, 0.2255],
        [0.7031, 0.3033, 0.2257],
        [0.7028, 0.3035, 0.2258],
        [0.7030, 0.3037, 0.2257],
        [0.7034, 0.3031, 0.2253],
        [0.7036, 0.3022, 0.2251],
        [0.7044, 0.3004, 0.2246],
        [0.7042, 0.2998, 0.2246],
        [0.7044, 0.3004, 0.2245],
        [0.7063, 0.2985, 0.2232],
        [0.7085, 0.2963, 0.2217],
        [0.7096, 0.2957, 0.2211],
        [0.7108, 0.2946, 0.2205],
        [0.7110, 0.2952, 0.2206],
        [0.7175, 0.2894, 0.2170],
        [0.7154, 0.2909, 0.2180],
        [0.7169, 0.2897, 0.2173],
        [0.7119, 0.2944, 0.2199],
        [0.7171, 0.2908, 0.2165],
        [0.7277, 0.2813, 0.2104],
        [0.4639, 0.3861, 0.3411],
        [0.5246, 0.3696, 0.3198],
        [0.4923, 0.3787, 0.3457],
        [0.4208, 0.4027, 0.3530],
        [0.5047, 0.3655, 0.3303],
        [0.5000, 0.3826, 0.3465],
        [0.7069, 0.2989, 0.2230],
        [0.7081, 0.2966, 0.2219],
        [0.7085, 0.2975, 0.2220],
        [0.7083, 0.2983, 0.2222],
        [0.7078, 0.2985, 0.2225],
        [0.7077, 0.2986, 0.2226],
        [0.7074, 0.2979, 0.2227],
        [0.7069, 0.2977, 0.2228],
        [0.7068, 0.2974, 0.2229],
        [0.7062, 0.2990, 0.2234],
        [0.7059, 0.2996, 0.2237],
        [0.7060, 0.3000, 0.2236],
        [0.7052, 0.2997, 0.2241],
        [0.7051, 0.3003, 0.2242],
        [0.7048, 0.3007, 0.2243],
        [0.7043, 0.3013, 0.2247],
        [0.7046, 0.3011, 0.2244],
        [0.7036, 0.3024, 0.2250],
        [0.7032, 0.3029, 0.2254]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 98: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7872.0, Mean: 357.9537048339844, Std: 701.8460693359375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 78652.6953125, Mean: 3575.54052734375, Std: 7012.54736328125
[DEBUG] Top-3 class probabilities:
tensor([[0.7033, 0.3027, 0.2253],
        [0.7031, 0.3026, 0.2254],
        [0.7033, 0.3021, 0.2254],
        [0.7039, 0.3007, 0.2250],
        [0.7041, 0.2996, 0.2248],
        [0.7039, 0.3000, 0.2248],
        [0.7036, 0.3001, 0.2249],
        [0.7039, 0.2994, 0.2249],
        [0.7056, 0.2980, 0.2236],
        [0.7070, 0.2965, 0.2227],
        [0.7084, 0.2952, 0.2217],
        [0.7103, 0.2938, 0.2207],
        [0.7123, 0.2922, 0.2196],
        [0.7258, 0.2804, 0.2115],
        [0.7338, 0.2758, 0.2064],
        [0.7334, 0.2748, 0.2062],
        [0.6619, 0.2643, 0.2608],
        [0.7286, 0.2617, 0.2168],
        [0.5897, 0.3199, 0.2452],
        [0.4912, 0.3800, 0.3225],
        [0.4423, 0.3879, 0.3568],
        [0.4682, 0.3700, 0.3326],
        [0.4870, 0.3849, 0.3383],
        [0.4566, 0.3900, 0.3600],
        [0.4889, 0.3745, 0.3352],
        [0.7084, 0.2987, 0.2223],
        [0.7079, 0.2981, 0.2225],
        [0.7081, 0.2974, 0.2222],
        [0.7081, 0.2984, 0.2224],
        [0.7078, 0.2986, 0.2227],
        [0.7074, 0.2990, 0.2229],
        [0.7071, 0.2988, 0.2230],
        [0.7071, 0.2986, 0.2229],
        [0.7064, 0.2995, 0.2234],
        [0.7057, 0.2999, 0.2237],
        [0.7055, 0.3004, 0.2239],
        [0.7054, 0.3006, 0.2241],
        [0.7048, 0.3012, 0.2244],
        [0.7043, 0.3016, 0.2247],
        [0.7040, 0.3019, 0.2250],
        [0.7041, 0.3025, 0.2251],
        [0.7045, 0.3017, 0.2246],
        [0.7046, 0.3014, 0.2246],
        [0.7040, 0.3012, 0.2249],
        [0.7035, 0.3009, 0.2251],
        [0.7030, 0.3017, 0.2257],
        [0.7033, 0.3012, 0.2254],
        [0.7034, 0.3006, 0.2253],
        [0.7037, 0.3002, 0.2250],
        [0.7031, 0.3002, 0.2250],
        [0.7042, 0.2996, 0.2246],
        [0.7059, 0.2974, 0.2233],
        [0.7069, 0.2963, 0.2227],
        [0.7077, 0.2956, 0.2223],
        [0.7074, 0.2963, 0.2224],
        [0.7089, 0.2946, 0.2216],
        [0.7102, 0.2934, 0.2206],
        [0.7164, 0.2855, 0.2171],
        [0.7098, 0.2503, 0.2368],
        [0.5921, 0.3105, 0.2434],
        [0.5965, 0.3211, 0.2618],
        [0.5232, 0.3536, 0.3034],
        [0.5117, 0.3615, 0.3212],
        [0.4830, 0.3852, 0.3411],
        [0.4906, 0.3632, 0.3384],
        [0.5024, 0.3585, 0.3359],
        [0.5111, 0.3595, 0.3174],
        [0.4187, 0.3951, 0.3549],
        [0.4312, 0.4012, 0.3615],
        [0.4816, 0.3747, 0.3373],
        [0.7074, 0.2998, 0.2229],
        [0.7073, 0.2997, 0.2230],
        [0.7074, 0.2981, 0.2228],
        [0.7070, 0.2981, 0.2225],
        [0.7075, 0.2983, 0.2225],
        [0.7074, 0.2986, 0.2229],
        [0.7072, 0.2988, 0.2229],
        [0.7066, 0.2990, 0.2231],
        [0.7064, 0.2999, 0.2234],
        [0.7062, 0.2996, 0.2235],
        [0.7055, 0.3003, 0.2238],
        [0.7051, 0.3009, 0.2242],
        [0.7049, 0.3013, 0.2246],
        [0.7040, 0.3025, 0.2252],
        [0.7039, 0.3028, 0.2251],
        [0.7036, 0.3026, 0.2253],
        [0.7041, 0.3011, 0.2247],
        [0.7042, 0.3011, 0.2248],
        [0.7041, 0.3005, 0.2248],
        [0.7033, 0.3013, 0.2251],
        [0.7036, 0.3012, 0.2251],
        [0.7031, 0.3017, 0.2255],
        [0.7029, 0.3016, 0.2258],
        [0.7032, 0.3010, 0.2255],
        [0.7050, 0.2987, 0.2241],
        [0.7056, 0.2979, 0.2235],
        [0.7068, 0.2963, 0.2228],
        [0.7081, 0.2947, 0.2221],
        [0.7083, 0.2944, 0.2217],
        [0.7097, 0.2934, 0.2208],
        [0.7107, 0.2927, 0.2200],
        [0.7160, 0.2882, 0.2170],
        [0.7183, 0.2849, 0.2157],
        [0.7297, 0.2536, 0.2238],
        [0.5056, 0.3620, 0.3357],
        [0.5028, 0.3671, 0.3384],
        [0.5104, 0.3576, 0.3336],
        [0.5795, 0.3323, 0.2589],
        [0.5105, 0.3596, 0.3267],
        [0.4971, 0.3649, 0.3301],
        [0.5025, 0.3581, 0.3562],
        [0.5103, 0.3811, 0.3270],
        [0.4993, 0.3602, 0.3405],
        [0.4826, 0.3821, 0.3116],
        [0.4697, 0.3791, 0.3479],
        [0.4375, 0.3949, 0.3497],
        [0.7074, 0.2997, 0.2229],
        [0.7075, 0.2996, 0.2228],
        [0.7073, 0.2993, 0.2228],
        [0.7074, 0.2979, 0.2226],
        [0.7073, 0.2982, 0.2227],
        [0.7074, 0.2985, 0.2226],
        [0.7079, 0.2981, 0.2225],
        [0.7063, 0.2992, 0.2232],
        [0.7060, 0.2998, 0.2236],
        [0.7060, 0.2996, 0.2235],
        [0.7061, 0.2997, 0.2236],
        [0.7058, 0.3002, 0.2238]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 99: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8240.0, Mean: 567.2452392578125, Std: 993.942138671875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 82329.59375, Mean: 5666.6923828125, Std: 9931.046875
[DEBUG] Top-3 class probabilities:
tensor([[0.7046, 0.3015, 0.2244],
        [0.7039, 0.3022, 0.2250],
        [0.7037, 0.3026, 0.2253],
        [0.7036, 0.3017, 0.2250],
        [0.7042, 0.3010, 0.2247],
        [0.7052, 0.3002, 0.2240],
        [0.7043, 0.3006, 0.2247],
        [0.7035, 0.3009, 0.2250],
        [0.7034, 0.3006, 0.2253],
        [0.7032, 0.3006, 0.2252],
        [0.7047, 0.2988, 0.2242],
        [0.7061, 0.2977, 0.2233],
        [0.7063, 0.2968, 0.2232],
        [0.7058, 0.2969, 0.2233],
        [0.7077, 0.2951, 0.2222],
        [0.7117, 0.2917, 0.2198],
        [0.7151, 0.2876, 0.2173],
        [0.7161, 0.2847, 0.2161],
        [0.7176, 0.2837, 0.2152],
        [0.7183, 0.2818, 0.2147],
        [0.7180, 0.2826, 0.2142],
        [0.6959, 0.2387, 0.2354],
        [0.5106, 0.3657, 0.3328],
        [0.5130, 0.3664, 0.3147],
        [0.4459, 0.3999, 0.3576],
        [0.5731, 0.3469, 0.2725],
        [0.5414, 0.3617, 0.3121],
        [0.4708, 0.3894, 0.3477],
        [0.5029, 0.3760, 0.3551],
        [0.4998, 0.3638, 0.3466],
        [0.5091, 0.3618, 0.3320],
        [0.5464, 0.3574, 0.3083],
        [0.4869, 0.3687, 0.3294],
        [0.4714, 0.3901, 0.3160],
        [0.7075, 0.2995, 0.2228],
        [0.7072, 0.2998, 0.2230],
        [0.7072, 0.2993, 0.2229],
        [0.7074, 0.2990, 0.2229],
        [0.7076, 0.2980, 0.2226],
        [0.7073, 0.2984, 0.2227],
        [0.7075, 0.2984, 0.2227],
        [0.7081, 0.2973, 0.2221],
        [0.7086, 0.2972, 0.2219],
        [0.7067, 0.2990, 0.2232],
        [0.7066, 0.2993, 0.2231],
        [0.7057, 0.3001, 0.2237],
        [0.7053, 0.3005, 0.2240],
        [0.7046, 0.3015, 0.2245],
        [0.7047, 0.3006, 0.2244],
        [0.7042, 0.3016, 0.2248],
        [0.7042, 0.3017, 0.2249],
        [0.7053, 0.3005, 0.2241],
        [0.7052, 0.3006, 0.2243],
        [0.7036, 0.3017, 0.2254],
        [0.7029, 0.3022, 0.2257],
        [0.7051, 0.2994, 0.2243],
        [0.7060, 0.2971, 0.2231],
        [0.7060, 0.2970, 0.2233],
        [0.7060, 0.2961, 0.2233],
        [0.7069, 0.2950, 0.2224],
        [0.7086, 0.2935, 0.2214],
        [0.7125, 0.2916, 0.2197],
        [0.7138, 0.2718, 0.2254],
        [0.5863, 0.3200, 0.2643],
        [0.5527, 0.3343, 0.2927],
        [0.5465, 0.3305, 0.3081],
        [0.5514, 0.3259, 0.2696],
        [0.5514, 0.3328, 0.2916],
        [0.5749, 0.3381, 0.2664],
        [0.5322, 0.3623, 0.2967],
        [0.4668, 0.3998, 0.3388],
        [0.4448, 0.4117, 0.3436],
        [0.5406, 0.3525, 0.2962],
        [0.4798, 0.3889, 0.3337],
        [0.5155, 0.3676, 0.3305],
        [0.5300, 0.3536, 0.3295],
        [0.5210, 0.3577, 0.3148],
        [0.4470, 0.3984, 0.3658],
        [0.4668, 0.4021, 0.3455],
        [0.4306, 0.4167, 0.3578],
        [0.7071, 0.2995, 0.2231],
        [0.7067, 0.2998, 0.2232],
        [0.7072, 0.2994, 0.2231],
        [0.7072, 0.2989, 0.2228],
        [0.7074, 0.2988, 0.2228],
        [0.7076, 0.2987, 0.2228],
        [0.7089, 0.2971, 0.2219],
        [0.7095, 0.2961, 0.2213],
        [0.7120, 0.2933, 0.2200],
        [0.7199, 0.2866, 0.2151],
        [0.7250, 0.2860, 0.2104],
        [0.7120, 0.2961, 0.2199],
        [0.7057, 0.3002, 0.2238],
        [0.7057, 0.3001, 0.2238],
        [0.7039, 0.3018, 0.2249],
        [0.7036, 0.3026, 0.2254],
        [0.7041, 0.3022, 0.2249],
        [0.7051, 0.3012, 0.2242],
        [0.7045, 0.3018, 0.2246],
        [0.7041, 0.3022, 0.2250],
        [0.7041, 0.3018, 0.2249],
        [0.7053, 0.2996, 0.2240],
        [0.7060, 0.2979, 0.2236],
        [0.7075, 0.2944, 0.2226],
        [0.7080, 0.2935, 0.2217],
        [0.7088, 0.2914, 0.2209],
        [0.7111, 0.2886, 0.2193],
        [0.7059, 0.2857, 0.2255],
        [0.6300, 0.2992, 0.2286],
        [0.5048, 0.3697, 0.3436],
        [0.5108, 0.3644, 0.3586],
        [0.5723, 0.3469, 0.2706],
        [0.5629, 0.3488, 0.2763],
        [0.5528, 0.3536, 0.2972],
        [0.5270, 0.3621, 0.3074],
        [0.5367, 0.3703, 0.2829],
        [0.5708, 0.3515, 0.2754],
        [0.4212, 0.4149, 0.3534],
        [0.5316, 0.3574, 0.3163],
        [0.5329, 0.3620, 0.3064],
        [0.5026, 0.3724, 0.3331],
        [0.4879, 0.3690, 0.3589],
        [0.4705, 0.3695, 0.3674],
        [0.4249, 0.4166, 0.3482],
        [0.5036, 0.3910, 0.3021],
        [0.4993, 0.3806, 0.3293],
        [0.7068, 0.2997, 0.2232],
        [0.7074, 0.2996, 0.2228]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 100: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7018.0, Mean: 667.6825561523438, Std: 1051.568115234375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 70119.890625, Mean: 6670.2197265625, Std: 10506.8212890625
[DEBUG] Top-3 class probabilities:
tensor([[0.7074, 0.2993, 0.2229],
        [0.7076, 0.2992, 0.2226],
        [0.7074, 0.2993, 0.2229],
        [0.7091, 0.2971, 0.2217],
        [0.7100, 0.2955, 0.2210],
        [0.7115, 0.2944, 0.2201],
        [0.7102, 0.2956, 0.2209],
        [0.7108, 0.2950, 0.2205],
        [0.7108, 0.2940, 0.2207],
        [0.7151, 0.2905, 0.2178],
        [0.7058, 0.3001, 0.2238],
        [0.7054, 0.3007, 0.2241],
        [0.7034, 0.3028, 0.2254],
        [0.7040, 0.3020, 0.2249],
        [0.7036, 0.3025, 0.2252],
        [0.7049, 0.3014, 0.2243],
        [0.7043, 0.3022, 0.2247],
        [0.7055, 0.3008, 0.2240],
        [0.7055, 0.3005, 0.2242],
        [0.7055, 0.2995, 0.2237],
        [0.7080, 0.2956, 0.2222],
        [0.7107, 0.2918, 0.2197],
        [0.7175, 0.2885, 0.2157],
        [0.6974, 0.2742, 0.2342],
        [0.6901, 0.2808, 0.2490],
        [0.6231, 0.3111, 0.2457],
        [0.5009, 0.3761, 0.3261],
        [0.4221, 0.4128, 0.3862],
        [0.5288, 0.3614, 0.3162],
        [0.5157, 0.3508, 0.3343],
        [0.5420, 0.3401, 0.3230],
        [0.5354, 0.3413, 0.3189],
        [0.5436, 0.3495, 0.2977],
        [0.5479, 0.3595, 0.2912],
        [0.5344, 0.3694, 0.2926],
        [0.5273, 0.3729, 0.2995],
        [0.4569, 0.3998, 0.3361],
        [0.4881, 0.3803, 0.3418],
        [0.4907, 0.3846, 0.3369],
        [0.5173, 0.3711, 0.3050],
        [0.4946, 0.3741, 0.3321],
        [0.4579, 0.4023, 0.3540],
        [0.5206, 0.3538, 0.3296],
        [0.5252, 0.3580, 0.3134],
        [0.7072, 0.2996, 0.2229],
        [0.7073, 0.2999, 0.2231],
        [0.7071, 0.2996, 0.2232],
        [0.7068, 0.2991, 0.2230],
        [0.7076, 0.2989, 0.2228],
        [0.7079, 0.2981, 0.2225],
        [0.7089, 0.2969, 0.2217],
        [0.7080, 0.2973, 0.2222],
        [0.7074, 0.2979, 0.2227],
        [0.7089, 0.2965, 0.2218],
        [0.7094, 0.2962, 0.2216],
        [0.7070, 0.2988, 0.2229],
        [0.7058, 0.2996, 0.2238],
        [0.7046, 0.3015, 0.2246],
        [0.7037, 0.3022, 0.2251],
        [0.7047, 0.3014, 0.2243],
        [0.7040, 0.3024, 0.2251],
        [0.7054, 0.3008, 0.2240],
        [0.7046, 0.3014, 0.2244],
        [0.7085, 0.2975, 0.2218],
        [0.7097, 0.2951, 0.2210],
        [0.7140, 0.2935, 0.2185],
        [0.7144, 0.2887, 0.2200],
        [0.6531, 0.2681, 0.2663],
        [0.5735, 0.3125, 0.2608],
        [0.5164, 0.3605, 0.3406],
        [0.5234, 0.3625, 0.3239],
        [0.5183, 0.3630, 0.3085],
        [0.5358, 0.3593, 0.3007],
        [0.4962, 0.3833, 0.3276],
        [0.5325, 0.3536, 0.3155],
        [0.5019, 0.3582, 0.3445],
        [0.4802, 0.3656, 0.3558],
        [0.5063, 0.3600, 0.3456],
        [0.5323, 0.3513, 0.3226],
        [0.5167, 0.3805, 0.3095],
        [0.4353, 0.4205, 0.3475],
        [0.4768, 0.4042, 0.3082],
        [0.4448, 0.4128, 0.3384],
        [0.5004, 0.3821, 0.3128],
        [0.4966, 0.3905, 0.3361],
        [0.5363, 0.3565, 0.3237],
        [0.4926, 0.3610, 0.3546],
        [0.5515, 0.3469, 0.3135],
        [0.5152, 0.3559, 0.3370],
        [0.5175, 0.3576, 0.3438],
        [0.7068, 0.3002, 0.2233],
        [0.7068, 0.2998, 0.2232],
        [0.7070, 0.2993, 0.2230],
        [0.7088, 0.2972, 0.2218],
        [0.7072, 0.2990, 0.2229],
        [0.7093, 0.2967, 0.2216],
        [0.7085, 0.2974, 0.2220],
        [0.7071, 0.2982, 0.2228],
        [0.7078, 0.2977, 0.2225],
        [0.7090, 0.2968, 0.2217],
        [0.7076, 0.2984, 0.2227],
        [0.7071, 0.2987, 0.2228],
        [0.7070, 0.2988, 0.2229],
        [0.7052, 0.3008, 0.2241],
        [0.7050, 0.3016, 0.2244],
        [0.7064, 0.3000, 0.2235],
        [0.7052, 0.3010, 0.2242],
        [0.7072, 0.2987, 0.2227],
        [0.7089, 0.2970, 0.2219],
        [0.7106, 0.2946, 0.2207],
        [0.7150, 0.2890, 0.2178],
        [0.6914, 0.2679, 0.2524],
        [0.6030, 0.3121, 0.2565],
        [0.5429, 0.3652, 0.3180],
        [0.5534, 0.3469, 0.3051],
        [0.5036, 0.3638, 0.3534],
        [0.5388, 0.3502, 0.3067],
        [0.5585, 0.3411, 0.2900],
        [0.5035, 0.3620, 0.3432],
        [0.4581, 0.3728, 0.3686],
        [0.5403, 0.3543, 0.3253],
        [0.4956, 0.3736, 0.3343],
        [0.4314, 0.4071, 0.3845],
        [0.5011, 0.3590, 0.3489],
        [0.5254, 0.3602, 0.3346],
        [0.5310, 0.3610, 0.3108],
        [0.4218, 0.4188, 0.3429],
        [0.4319, 0.4229, 0.3366]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 0.,  ..., 0., 1., 0.]])
[DEBUG] Batch 101: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8600.0, Mean: 780.9866943359375, Std: 1141.0732421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 85926.5625, Mean: 7802.3056640625, Std: 11401.1181640625
[DEBUG] Top-3 class probabilities:
tensor([[0.5087, 0.3821, 0.2797],
        [0.4966, 0.3926, 0.3162],
        [0.4562, 0.4168, 0.3349],
        [0.4338, 0.4209, 0.3566],
        [0.5175, 0.3554, 0.3298],
        [0.4888, 0.3703, 0.3637],
        [0.5320, 0.3627, 0.3295],
        [0.4938, 0.3651, 0.3495],
        [0.7072, 0.2995, 0.2231],
        [0.7073, 0.2993, 0.2230],
        [0.7080, 0.2983, 0.2224],
        [0.7108, 0.2948, 0.2205],
        [0.7090, 0.2975, 0.2218],
        [0.7084, 0.2974, 0.2220],
        [0.7077, 0.2980, 0.2224],
        [0.7068, 0.2986, 0.2231],
        [0.7070, 0.2986, 0.2228],
        [0.7091, 0.2964, 0.2216],
        [0.7082, 0.2973, 0.2221],
        [0.7074, 0.2983, 0.2226],
        [0.7076, 0.2984, 0.2227],
        [0.7077, 0.2984, 0.2225],
        [0.7064, 0.2995, 0.2235],
        [0.7075, 0.2989, 0.2225],
        [0.7070, 0.2990, 0.2230],
        [0.7085, 0.2969, 0.2219],
        [0.7115, 0.2944, 0.2200],
        [0.7128, 0.2927, 0.2192],
        [0.7211, 0.2918, 0.2186],
        [0.6944, 0.2607, 0.2425],
        [0.5377, 0.3572, 0.3054],
        [0.5036, 0.3691, 0.3162],
        [0.5143, 0.3678, 0.3140],
        [0.5025, 0.3679, 0.3519],
        [0.5042, 0.3681, 0.3394],
        [0.5097, 0.3590, 0.3394],
        [0.5208, 0.3564, 0.3089],
        [0.5176, 0.3573, 0.3346],
        [0.4808, 0.3619, 0.3574],
        [0.5273, 0.3601, 0.3379],
        [0.5248, 0.3550, 0.3436],
        [0.5382, 0.3513, 0.3300],
        [0.5195, 0.3690, 0.3285],
        [0.5170, 0.3781, 0.3304],
        [0.4741, 0.4005, 0.3215],
        [0.4916, 0.3951, 0.3023],
        [0.4506, 0.3634, 0.3546],
        [0.4490, 0.4086, 0.3304],
        [0.5193, 0.3852, 0.2976],
        [0.4619, 0.4151, 0.3226],
        [0.4867, 0.3970, 0.3280],
        [0.5244, 0.3588, 0.3192],
        [0.5326, 0.3484, 0.3211],
        [0.4959, 0.3642, 0.3502],
        [0.7070, 0.2999, 0.2232],
        [0.7085, 0.2980, 0.2222],
        [0.7109, 0.2948, 0.2204],
        [0.7102, 0.2956, 0.2211],
        [0.7085, 0.2973, 0.2220],
        [0.7099, 0.2959, 0.2212],
        [0.7077, 0.2977, 0.2223],
        [0.7066, 0.2994, 0.2232],
        [0.7070, 0.2986, 0.2229],
        [0.7082, 0.2971, 0.2221],
        [0.7080, 0.2979, 0.2225],
        [0.7084, 0.2976, 0.2220],
        [0.7081, 0.2975, 0.2223],
        [0.7081, 0.2977, 0.2222],
        [0.7063, 0.2993, 0.2234],
        [0.7090, 0.2964, 0.2214],
        [0.7086, 0.2966, 0.2217],
        [0.7139, 0.2914, 0.2185],
        [0.7191, 0.2809, 0.2210],
        [0.6516, 0.2707, 0.2685],
        [0.7178, 0.2853, 0.2157],
        [0.6615, 0.2776, 0.2371],
        [0.5063, 0.3765, 0.3214],
        [0.4653, 0.3955, 0.3387],
        [0.5423, 0.3647, 0.3100],
        [0.5037, 0.3682, 0.3452],
        [0.4918, 0.3556, 0.3499],
        [0.4699, 0.3705, 0.3645],
        [0.4812, 0.3886, 0.3361],
        [0.4219, 0.4084, 0.3789],
        [0.5186, 0.3540, 0.3339],
        [0.5138, 0.3597, 0.3287],
        [0.4861, 0.3544, 0.3421],
        [0.5333, 0.3568, 0.3278],
        [0.5238, 0.3563, 0.3257],
        [0.5332, 0.3578, 0.3271],
        [0.5384, 0.3648, 0.3201],
        [0.4772, 0.3876, 0.3253],
        [0.4785, 0.3977, 0.3523],
        [0.4927, 0.3825, 0.3239],
        [0.4962, 0.3836, 0.3094],
        [0.5178, 0.3854, 0.3080],
        [0.4867, 0.3877, 0.3211],
        [0.4695, 0.3933, 0.3284],
        [0.5056, 0.3678, 0.3333],
        [0.4872, 0.3563, 0.3553],
        [0.7078, 0.2989, 0.2226],
        [0.7098, 0.2959, 0.2212],
        [0.7090, 0.2967, 0.2219],
        [0.7102, 0.2955, 0.2209],
        [0.7107, 0.2949, 0.2206],
        [0.7090, 0.2965, 0.2216],
        [0.7069, 0.2987, 0.2231],
        [0.7069, 0.2988, 0.2231],
        [0.7076, 0.2980, 0.2225],
        [0.7084, 0.2973, 0.2221],
        [0.7085, 0.2983, 0.2219],
        [0.7086, 0.2967, 0.2220],
        [0.7086, 0.2971, 0.2220],
        [0.7085, 0.2967, 0.2219],
        [0.7093, 0.2965, 0.2215],
        [0.7105, 0.2953, 0.2206],
        [0.7118, 0.2934, 0.2198],
        [0.7208, 0.2854, 0.2149],
        [0.6849, 0.2696, 0.2488],
        [0.4797, 0.3683, 0.3591],
        [0.5242, 0.3462, 0.3165],
        [0.5409, 0.3518, 0.2974],
        [0.5080, 0.3755, 0.3110],
        [0.4321, 0.4225, 0.3641],
        [0.4999, 0.3727, 0.3142],
        [0.5201, 0.3519, 0.3233],
        [0.4789, 0.3695, 0.3487],
        [0.5144, 0.3614, 0.3296]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 102: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6944.0, Mean: 867.8179931640625, Std: 1127.1632080078125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 69380.515625, Mean: 8669.8876953125, Std: 11262.134765625
[DEBUG] Top-3 class probabilities:
tensor([[0.4713, 0.3729, 0.3572],
        [0.4530, 0.3915, 0.3539],
        [0.4766, 0.3743, 0.3449],
        [0.4067, 0.4009, 0.3758],
        [0.4681, 0.3775, 0.3713],
        [0.4468, 0.3921, 0.3782],
        [0.4607, 0.3874, 0.3667],
        [0.4861, 0.3623, 0.3585],
        [0.5363, 0.3588, 0.3342],
        [0.5218, 0.3676, 0.3226],
        [0.5168, 0.3761, 0.3227],
        [0.5223, 0.3563, 0.3192],
        [0.5002, 0.3895, 0.3035],
        [0.5184, 0.3709, 0.3101],
        [0.4991, 0.3758, 0.3380],
        [0.4722, 0.4013, 0.3369],
        [0.5099, 0.3728, 0.3233],
        [0.5066, 0.3730, 0.3348],
        [0.7079, 0.2984, 0.2225],
        [0.7082, 0.2978, 0.2223],
        [0.7076, 0.2980, 0.2226],
        [0.7086, 0.2970, 0.2220],
        [0.7077, 0.2976, 0.2225],
        [0.7076, 0.2985, 0.2224],
        [0.7093, 0.2966, 0.2214],
        [0.7081, 0.2976, 0.2222],
        [0.7078, 0.2973, 0.2223],
        [0.7086, 0.2971, 0.2217],
        [0.7092, 0.2966, 0.2215],
        [0.7097, 0.2960, 0.2213],
        [0.7107, 0.2949, 0.2204],
        [0.7117, 0.2939, 0.2198],
        [0.7115, 0.2939, 0.2202],
        [0.7149, 0.2904, 0.2179],
        [0.7182, 0.2874, 0.2160],
        [0.6852, 0.2652, 0.2512],
        [0.5065, 0.3733, 0.3336],
        [0.5382, 0.3587, 0.3056],
        [0.5282, 0.3651, 0.3269],
        [0.5299, 0.3475, 0.3138],
        [0.4846, 0.3668, 0.3577],
        [0.5465, 0.3443, 0.3176],
        [0.5421, 0.3499, 0.3135],
        [0.5206, 0.3561, 0.3280],
        [0.5359, 0.3494, 0.3177],
        [0.4618, 0.3829, 0.3591],
        [0.4474, 0.3817, 0.3672],
        [0.5049, 0.3555, 0.3430],
        [0.5111, 0.3593, 0.3379],
        [0.4760, 0.3691, 0.3577],
        [0.4573, 0.3741, 0.3704],
        [0.4599, 0.4416, 0.3038],
        [0.4927, 0.4615, 0.2638],
        [0.4739, 0.3794, 0.3621],
        [0.5300, 0.3613, 0.3371],
        [0.5087, 0.3619, 0.3255],
        [0.4847, 0.3874, 0.3337],
        [0.4908, 0.3946, 0.3303],
        [0.4541, 0.4090, 0.3393],
        [0.5072, 0.3671, 0.3178],
        [0.4826, 0.3790, 0.3404],
        [0.5360, 0.3520, 0.3221],
        [0.5431, 0.3381, 0.3159],
        [0.4758, 0.3750, 0.3300],
        [0.7071, 0.2992, 0.2230],
        [0.7071, 0.2992, 0.2230],
        [0.7069, 0.2990, 0.2232],
        [0.7073, 0.2987, 0.2229],
        [0.7104, 0.2947, 0.2208],
        [0.7116, 0.2940, 0.2199],
        [0.7104, 0.2952, 0.2207],
        [0.7090, 0.2965, 0.2216],
        [0.7093, 0.2962, 0.2213],
        [0.7101, 0.2954, 0.2207],
        [0.7104, 0.2950, 0.2207],
        [0.7119, 0.2936, 0.2199],
        [0.7161, 0.2894, 0.2174],
        [0.7182, 0.2874, 0.2158],
        [0.7228, 0.2831, 0.2130],
        [0.7214, 0.2841, 0.2142],
        [0.7291, 0.2796, 0.2100],
        [0.6040, 0.3078, 0.2503],
        [0.5122, 0.3658, 0.3209],
        [0.4673, 0.3878, 0.3447],
        [0.4670, 0.3681, 0.3616],
        [0.4563, 0.3778, 0.3694],
        [0.4946, 0.3609, 0.3422],
        [0.4879, 0.3609, 0.3509],
        [0.5356, 0.3548, 0.3215],
        [0.5079, 0.3626, 0.3155],
        [0.4724, 0.4036, 0.3440],
        [0.4315, 0.4078, 0.3635],
        [0.4540, 0.3959, 0.3464],
        [0.4593, 0.3914, 0.3477],
        [0.4550, 0.3831, 0.3572],
        [0.4619, 0.3822, 0.3659],
        [0.4786, 0.3713, 0.3593],
        [0.4711, 0.3883, 0.3514],
        [0.4153, 0.4030, 0.4001],
        [0.4569, 0.3745, 0.3736],
        [0.5112, 0.3604, 0.3451],
        [0.5419, 0.3517, 0.3132],
        [0.5431, 0.3462, 0.3229],
        [0.4985, 0.3748, 0.3362],
        [0.4293, 0.4173, 0.3541],
        [0.5049, 0.3801, 0.3259],
        [0.4584, 0.3836, 0.3789],
        [0.4904, 0.3588, 0.3404],
        [0.5311, 0.3584, 0.3346],
        [0.5164, 0.3561, 0.3438],
        [0.7073, 0.2988, 0.2227],
        [0.7087, 0.2974, 0.2220],
        [0.7071, 0.2986, 0.2230],
        [0.7080, 0.2978, 0.2225],
        [0.7107, 0.2945, 0.2206],
        [0.7121, 0.2932, 0.2198],
        [0.7120, 0.2931, 0.2196],
        [0.7099, 0.2954, 0.2208],
        [0.7104, 0.2946, 0.2206],
        [0.7131, 0.2924, 0.2190],
        [0.7128, 0.2929, 0.2192],
        [0.7157, 0.2897, 0.2175],
        [0.7207, 0.2844, 0.2147],
        [0.7255, 0.2801, 0.2118],
        [0.7294, 0.2725, 0.2095],
        [0.6766, 0.2653, 0.2485],
        [0.5991, 0.3196, 0.2556],
        [0.5194, 0.3641, 0.3578]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [17,  6, 11],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 103: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7052.0, Mean: 1040.3382568359375, Std: 1174.653076171875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 70459.6015625, Mean: 10393.6357421875, Std: 11736.6328125
[DEBUG] Top-3 class probabilities:
tensor([[0.5066, 0.3776, 0.3224],
        [0.5011, 0.3662, 0.3450],
        [0.4976, 0.3617, 0.3393],
        [0.4401, 0.4193, 0.3465],
        [0.4922, 0.3746, 0.3429],
        [0.4557, 0.3806, 0.3771],
        [0.4971, 0.3654, 0.3413],
        [0.4958, 0.3581, 0.3445],
        [0.4822, 0.3749, 0.3568],
        [0.5075, 0.3681, 0.3203],
        [0.4442, 0.4078, 0.3484],
        [0.4468, 0.4122, 0.3572],
        [0.4689, 0.3805, 0.3574],
        [0.4650, 0.3651, 0.3589],
        [0.4706, 0.3734, 0.3497],
        [0.4479, 0.3906, 0.3680],
        [0.4698, 0.3724, 0.3681],
        [0.4461, 0.3692, 0.3680],
        [0.4916, 0.3639, 0.3506],
        [0.4913, 0.3526, 0.3339],
        [0.5030, 0.3763, 0.3205],
        [0.5534, 0.3439, 0.2996],
        [0.4309, 0.4269, 0.3535],
        [0.5274, 0.3667, 0.2883],
        [0.4970, 0.3845, 0.3375],
        [0.5270, 0.3602, 0.3015],
        [0.4447, 0.3910, 0.3875],
        [0.5346, 0.3477, 0.3406],
        [0.7079, 0.2979, 0.2224],
        [0.7111, 0.2945, 0.2203],
        [0.7104, 0.2953, 0.2206],
        [0.7088, 0.2972, 0.2218],
        [0.7128, 0.2927, 0.2193],
        [0.7133, 0.2920, 0.2190],
        [0.7112, 0.2937, 0.2200],
        [0.7141, 0.2918, 0.2184],
        [0.7147, 0.2909, 0.2181],
        [0.7243, 0.2825, 0.2106],
        [0.7251, 0.2803, 0.2107],
        [0.7266, 0.2782, 0.2106],
        [0.7292, 0.2745, 0.2086],
        [0.7164, 0.2719, 0.2205],
        [0.6547, 0.2761, 0.2681],
        [0.5835, 0.3339, 0.2681],
        [0.5231, 0.3747, 0.3004],
        [0.5014, 0.3656, 0.3259],
        [0.5129, 0.3644, 0.3355],
        [0.5136, 0.3567, 0.3467],
        [0.5003, 0.3684, 0.3336],
        [0.4647, 0.3909, 0.3555],
        [0.5070, 0.3694, 0.3390],
        [0.4806, 0.3883, 0.3434],
        [0.4968, 0.3779, 0.3331],
        [0.4438, 0.3944, 0.3862],
        [0.4798, 0.3624, 0.3611],
        [0.4692, 0.3630, 0.3586],
        [0.4902, 0.3640, 0.3468],
        [0.4788, 0.3828, 0.3575],
        [0.4398, 0.4187, 0.3402],
        [0.4884, 0.3891, 0.3327],
        [0.4818, 0.3880, 0.3430],
        [0.5296, 0.3495, 0.3170],
        [0.5085, 0.3670, 0.3303],
        [0.5077, 0.3570, 0.3460],
        [0.5108, 0.3614, 0.3373],
        [0.5089, 0.3472, 0.3391],
        [0.5387, 0.3635, 0.3088],
        [0.5303, 0.3604, 0.3237],
        [0.4747, 0.3837, 0.3479],
        [0.5157, 0.3848, 0.3136],
        [0.5026, 0.3808, 0.3088],
        [0.4807, 0.4130, 0.3278],
        [0.4560, 0.4031, 0.3556],
        [0.5329, 0.3504, 0.3398],
        [0.7109, 0.2948, 0.2204],
        [0.7131, 0.2920, 0.2190],
        [0.7143, 0.2914, 0.2182],
        [0.7130, 0.2926, 0.2191],
        [0.7159, 0.2897, 0.2174],
        [0.7154, 0.2902, 0.2176],
        [0.7151, 0.2902, 0.2178],
        [0.7235, 0.2816, 0.2128],
        [0.7266, 0.2790, 0.2104],
        [0.7429, 0.2322, 0.1881],
        [0.7181, 0.2404, 0.2277],
        [0.7332, 0.2698, 0.2038],
        [0.7266, 0.2713, 0.2155],
        [0.5488, 0.3353, 0.3126],
        [0.4944, 0.3731, 0.3532],
        [0.5456, 0.3535, 0.2839],
        [0.5548, 0.3638, 0.2808],
        [0.5605, 0.3542, 0.2701],
        [0.4899, 0.3829, 0.3504],
        [0.5054, 0.3684, 0.3516],
        [0.4983, 0.3666, 0.3321],
        [0.5191, 0.3626, 0.3464],
        [0.4896, 0.3640, 0.3627],
        [0.4860, 0.3636, 0.3428],
        [0.5353, 0.3500, 0.3252],
        [0.5106, 0.3561, 0.3477],
        [0.5111, 0.3604, 0.3428],
        [0.4889, 0.3633, 0.3466],
        [0.4471, 0.3926, 0.3785],
        [0.4574, 0.3701, 0.3672],
        [0.4911, 0.3620, 0.3544],
        [0.4935, 0.3591, 0.3584],
        [0.4458, 0.4145, 0.3435],
        [0.4521, 0.4098, 0.3293],
        [0.4413, 0.4042, 0.3511],
        [0.4992, 0.3529, 0.3506],
        [0.4856, 0.3653, 0.3396],
        [0.4842, 0.3927, 0.3460],
        [0.5216, 0.3615, 0.3247],
        [0.5434, 0.3594, 0.3058],
        [0.4857, 0.3860, 0.3527],
        [0.5693, 0.3307, 0.2959],
        [0.5595, 0.3570, 0.2738],
        [0.4284, 0.4211, 0.3636],
        [0.5292, 0.3632, 0.3302],
        [0.5332, 0.3461, 0.3381],
        [0.7162, 0.2897, 0.2171],
        [0.7173, 0.2881, 0.2163],
        [0.7195, 0.2861, 0.2150],
        [0.7243, 0.2827, 0.2122],
        [0.7206, 0.2850, 0.2145],
        [0.7201, 0.2851, 0.2147],
        [0.7209, 0.2852, 0.2142],
        [0.7235, 0.2814, 0.2129]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 104: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6976.0, Mean: 1083.56982421875, Std: 1094.548828125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 69700.2421875, Mean: 10825.587890625, Std: 10936.2666015625
[DEBUG] Top-3 class probabilities:
tensor([[0.7417, 0.2582, 0.1978],
        [0.7303, 0.2231, 0.2056],
        [0.6269, 0.2881, 0.2415],
        [0.6711, 0.2707, 0.2579],
        [0.5198, 0.3520, 0.3393],
        [0.5133, 0.3550, 0.3469],
        [0.5753, 0.3434, 0.2798],
        [0.5597, 0.3551, 0.2890],
        [0.5505, 0.3679, 0.3017],
        [0.5435, 0.3519, 0.3265],
        [0.5275, 0.3648, 0.3484],
        [0.4797, 0.3704, 0.3639],
        [0.4824, 0.3774, 0.3582],
        [0.5341, 0.3510, 0.3266],
        [0.5175, 0.3556, 0.3421],
        [0.5287, 0.3435, 0.3376],
        [0.5187, 0.3601, 0.3432],
        [0.5290, 0.3495, 0.3364],
        [0.5071, 0.3591, 0.3506],
        [0.4853, 0.3708, 0.3485],
        [0.4559, 0.3732, 0.3700],
        [0.5336, 0.3483, 0.3319],
        [0.5603, 0.3444, 0.3097],
        [0.5417, 0.3465, 0.2713],
        [0.4711, 0.3763, 0.3474],
        [0.4873, 0.3740, 0.3334],
        [0.5198, 0.3771, 0.3196],
        [0.5513, 0.3514, 0.3068],
        [0.5062, 0.3721, 0.3182],
        [0.4516, 0.3904, 0.3867],
        [0.5592, 0.3498, 0.3077],
        [0.5585, 0.3401, 0.3098],
        [0.5447, 0.3514, 0.3123],
        [0.5745, 0.3426, 0.2787],
        [0.5187, 0.3621, 0.3384],
        [0.5238, 0.3502, 0.3396],
        [0.5422, 0.3499, 0.3194],
        [0.7313, 0.2759, 0.2080],
        [0.7296, 0.2774, 0.2088],
        [0.7325, 0.2727, 0.2064],
        [0.7390, 0.2690, 0.2007],
        [0.7326, 0.2760, 0.2066],
        [0.7251, 0.2801, 0.2116],
        [0.7370, 0.2698, 0.2033],
        [0.7380, 0.2699, 0.2022],
        [0.7434, 0.2618, 0.1976],
        [0.7607, 0.2113, 0.1686],
        [0.7010, 0.2465, 0.2384],
        [0.4979, 0.3660, 0.3476],
        [0.5394, 0.3508, 0.3056],
        [0.5266, 0.3606, 0.3348],
        [0.5245, 0.3617, 0.3274],
        [0.5317, 0.3483, 0.3152],
        [0.5044, 0.3667, 0.3265],
        [0.5991, 0.3254, 0.2585],
        [0.5133, 0.3597, 0.3246],
        [0.4816, 0.3861, 0.3431],
        [0.5311, 0.3500, 0.3042],
        [0.4791, 0.3961, 0.3328],
        [0.5368, 0.3598, 0.3198],
        [0.5222, 0.3497, 0.3362],
        [0.5090, 0.3604, 0.3532],
        [0.5337, 0.3637, 0.3314],
        [0.5252, 0.3502, 0.3213],
        [0.5254, 0.3493, 0.3290],
        [0.4868, 0.3637, 0.3514],
        [0.4889, 0.3684, 0.3475],
        [0.5773, 0.3443, 0.2887],
        [0.5308, 0.3672, 0.3270],
        [0.5252, 0.3598, 0.3020],
        [0.5574, 0.3445, 0.3093],
        [0.5114, 0.3600, 0.3240],
        [0.4775, 0.3951, 0.3239],
        [0.4430, 0.4107, 0.3559],
        [0.4876, 0.3864, 0.3309],
        [0.5527, 0.3498, 0.3124],
        [0.5401, 0.3513, 0.3215],
        [0.5799, 0.3460, 0.2839],
        [0.5466, 0.3598, 0.2920],
        [0.5268, 0.3715, 0.2985],
        [0.5064, 0.3794, 0.3203],
        [0.5107, 0.3659, 0.3337],
        [0.5178, 0.3565, 0.3378],
        [0.7414, 0.2670, 0.1970],
        [0.7403, 0.2685, 0.1984],
        [0.7369, 0.2707, 0.2016],
        [0.7418, 0.2620, 0.1961],
        [0.7453, 0.2528, 0.1857],
        [0.7415, 0.2659, 0.1979],
        [0.7450, 0.2559, 0.1900],
        [0.7441, 0.2565, 0.1925],
        [0.7478, 0.2426, 0.1818],
        [0.7463, 0.2377, 0.1771],
        [0.5949, 0.3157, 0.2625],
        [0.5738, 0.3344, 0.2840],
        [0.5236, 0.3655, 0.2885],
        [0.5133, 0.3783, 0.3387],
        [0.4862, 0.3760, 0.3686],
        [0.5226, 0.3551, 0.3302],
        [0.5469, 0.3562, 0.3113],
        [0.5193, 0.3802, 0.3182],
        [0.5183, 0.3813, 0.3126],
        [0.5924, 0.3431, 0.2698],
        [0.5817, 0.3384, 0.2767],
        [0.4891, 0.3708, 0.3511],
        [0.5075, 0.3620, 0.3502],
        [0.5198, 0.3580, 0.3444],
        [0.5442, 0.3339, 0.3178],
        [0.4680, 0.3643, 0.3634],
        [0.4956, 0.3534, 0.3458],
        [0.5053, 0.3545, 0.3392],
        [0.5423, 0.3575, 0.3143],
        [0.4491, 0.4024, 0.3579],
        [0.5268, 0.3686, 0.3126],
        [0.5594, 0.3421, 0.3183],
        [0.5371, 0.3701, 0.3148],
        [0.4377, 0.4188, 0.3537],
        [0.4618, 0.3881, 0.3375],
        [0.4647, 0.3985, 0.3268],
        [0.5349, 0.3673, 0.2967],
        [0.4817, 0.3871, 0.3373],
        [0.4595, 0.3817, 0.3535],
        [0.5146, 0.3670, 0.3331],
        [0.5307, 0.3606, 0.3390],
        [0.5152, 0.3755, 0.3217],
        [0.4873, 0.3877, 0.3303],
        [0.5159, 0.3704, 0.3367],
        [0.5280, 0.3604, 0.3199]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 105: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7044.0, Mean: 1182.711669921875, Std: 1109.44091796875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 70379.671875, Mean: 11816.1728515625, Std: 11085.0615234375
[DEBUG] Top-3 class probabilities:
tensor([[0.7451, 0.2516, 0.1856],
        [0.7464, 0.2539, 0.1851],
        [0.7460, 0.2527, 0.1848],
        [0.7235, 0.2614, 0.2074],
        [0.7474, 0.2454, 0.1869],
        [0.7454, 0.2412, 0.1806],
        [0.7256, 0.2453, 0.2063],
        [0.7436, 0.2468, 0.1822],
        [0.7478, 0.2417, 0.1802],
        [0.6613, 0.2463, 0.2324],
        [0.4729, 0.3684, 0.3605],
        [0.5319, 0.3505, 0.3163],
        [0.5281, 0.3612, 0.3056],
        [0.4993, 0.3796, 0.3130],
        [0.5148, 0.3709, 0.3154],
        [0.5002, 0.3720, 0.3575],
        [0.5529, 0.3492, 0.3135],
        [0.5059, 0.3628, 0.3436],
        [0.5440, 0.3522, 0.3073],
        [0.5423, 0.3640, 0.3188],
        [0.5366, 0.3517, 0.3120],
        [0.5516, 0.3522, 0.2927],
        [0.5516, 0.3546, 0.3076],
        [0.5626, 0.3556, 0.2992],
        [0.5363, 0.3581, 0.3287],
        [0.4727, 0.3824, 0.3624],
        [0.5066, 0.3503, 0.3410],
        [0.4752, 0.3715, 0.3533],
        [0.4915, 0.3624, 0.3536],
        [0.5029, 0.3570, 0.3532],
        [0.5210, 0.3714, 0.3309],
        [0.5577, 0.3478, 0.3001],
        [0.5294, 0.3576, 0.3190],
        [0.5430, 0.3628, 0.3072],
        [0.5402, 0.3643, 0.3166],
        [0.5078, 0.3541, 0.3453],
        [0.5386, 0.3621, 0.3129],
        [0.5214, 0.3638, 0.3024],
        [0.4389, 0.3955, 0.3736],
        [0.4757, 0.3750, 0.3483],
        [0.4448, 0.3950, 0.3789],
        [0.4954, 0.3741, 0.3316],
        [0.5020, 0.3792, 0.3193],
        [0.5938, 0.3528, 0.2801],
        [0.5377, 0.3561, 0.3082],
        [0.7226, 0.2571, 0.2217],
        [0.7293, 0.2442, 0.2047],
        [0.6919, 0.2573, 0.2434],
        [0.6532, 0.2914, 0.2347],
        [0.6089, 0.3065, 0.2380],
        [0.6536, 0.2810, 0.2216],
        [0.5494, 0.3423, 0.2946],
        [0.5993, 0.3066, 0.2285],
        [0.6455, 0.2825, 0.2198],
        [0.5742, 0.3350, 0.2777],
        [0.5496, 0.3568, 0.3058],
        [0.5611, 0.3449, 0.3109],
        [0.5400, 0.3575, 0.3158],
        [0.5857, 0.3351, 0.2822],
        [0.4958, 0.3761, 0.3371],
        [0.4804, 0.3872, 0.3543],
        [0.5271, 0.3445, 0.3304],
        [0.4783, 0.3722, 0.3702],
        [0.5151, 0.3652, 0.3413],
        [0.4820, 0.3860, 0.3533],
        [0.6017, 0.3177, 0.2744],
        [0.5390, 0.3478, 0.3076],
        [0.4563, 0.4017, 0.3479],
        [0.5325, 0.3706, 0.3105],
        [0.5196, 0.3724, 0.3340],
        [0.5812, 0.3316, 0.2874],
        [0.5109, 0.3640, 0.3533],
        [0.5281, 0.3516, 0.3183],
        [0.5363, 0.3650, 0.3115],
        [0.4682, 0.3766, 0.3683],
        [0.5651, 0.3491, 0.3051],
        [0.5623, 0.3520, 0.3001],
        [0.5300, 0.3503, 0.3288],
        [0.5254, 0.3532, 0.3295],
        [0.5190, 0.3672, 0.3426],
        [0.5378, 0.3543, 0.3228],
        [0.5348, 0.3503, 0.3306],
        [0.5465, 0.3591, 0.2998],
        [0.4808, 0.3787, 0.3434],
        [0.4916, 0.3647, 0.3635],
        [0.4701, 0.3752, 0.3392],
        [0.4553, 0.4022, 0.3461],
        [0.5065, 0.3817, 0.3346],
        [0.5497, 0.3546, 0.2935],
        [0.5137, 0.3852, 0.3121],
        [0.5410, 0.3656, 0.3151],
        [0.7002, 0.2530, 0.2468],
        [0.6094, 0.3271, 0.2647],
        [0.5833, 0.3358, 0.2640],
        [0.5805, 0.3336, 0.2734],
        [0.5446, 0.3577, 0.3040],
        [0.5682, 0.3478, 0.2886],
        [0.5294, 0.3582, 0.3224],
        [0.5485, 0.3476, 0.3090],
        [0.5253, 0.3635, 0.3347],
        [0.5320, 0.3664, 0.3263],
        [0.5204, 0.3774, 0.3289],
        [0.5585, 0.3525, 0.3124],
        [0.5830, 0.3370, 0.2957],
        [0.5335, 0.3459, 0.3230],
        [0.5131, 0.3739, 0.3421],
        [0.5279, 0.3646, 0.3240],
        [0.5289, 0.3556, 0.3368],
        [0.5103, 0.3598, 0.3486],
        [0.4938, 0.3726, 0.3494],
        [0.4969, 0.3727, 0.3413],
        [0.5169, 0.3656, 0.3452],
        [0.5174, 0.3590, 0.3446],
        [0.4960, 0.3730, 0.3402],
        [0.5473, 0.3563, 0.2963],
        [0.4815, 0.3762, 0.3591],
        [0.5397, 0.3550, 0.3091],
        [0.5353, 0.3592, 0.3191],
        [0.4774, 0.3715, 0.3698],
        [0.5304, 0.3581, 0.3221],
        [0.5381, 0.3514, 0.3190],
        [0.5581, 0.3446, 0.3100],
        [0.5273, 0.3678, 0.3279],
        [0.4983, 0.3637, 0.3341],
        [0.4972, 0.3520, 0.3411],
        [0.4671, 0.3735, 0.3686],
        [0.5383, 0.3447, 0.3146],
        [0.5386, 0.3631, 0.3052]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 106: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15100.0, Mean: 1291.1943359375, Std: 1110.6905517578125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 150871.796875, Mean: 12900.0849609375, Std: 11097.546875
[DEBUG] Top-3 class probabilities:
tensor([[0.5651, 0.3483, 0.2834],
        [0.5364, 0.3602, 0.3013],
        [0.5357, 0.3564, 0.3270],
        [0.5018, 0.3766, 0.3253],
        [0.4542, 0.3979, 0.3322],
        [0.4891, 0.3862, 0.3348],
        [0.5282, 0.3779, 0.3056],
        [0.4953, 0.3847, 0.3207],
        [0.4876, 0.3855, 0.3336],
        [0.5916, 0.3309, 0.2655],
        [0.5625, 0.3461, 0.2942],
        [0.5405, 0.3504, 0.3074],
        [0.5507, 0.3705, 0.2937],
        [0.4988, 0.3839, 0.3337],
        [0.5185, 0.3536, 0.3279],
        [0.5478, 0.3596, 0.3047],
        [0.5239, 0.3478, 0.3214],
        [0.5252, 0.3571, 0.3284],
        [0.5401, 0.3541, 0.3169],
        [0.5376, 0.3643, 0.3213],
        [0.5787, 0.3445, 0.3111],
        [0.5277, 0.3486, 0.3316],
        [0.5346, 0.3562, 0.3302],
        [0.5506, 0.3491, 0.3209],
        [0.5524, 0.3510, 0.3175],
        [0.4961, 0.3734, 0.3648],
        [0.5352, 0.3618, 0.3371],
        [0.5391, 0.3610, 0.3317],
        [0.5456, 0.3534, 0.2968],
        [0.5097, 0.3576, 0.3540],
        [0.5307, 0.3534, 0.3268],
        [0.5195, 0.3574, 0.3326],
        [0.5327, 0.3603, 0.3164],
        [0.4980, 0.3620, 0.3446],
        [0.5494, 0.3461, 0.2999],
        [0.5361, 0.3573, 0.3100],
        [0.5253, 0.3483, 0.3162],
        [0.5120, 0.3617, 0.3364],
        [0.5198, 0.3806, 0.3288],
        [0.5411, 0.3512, 0.3029],
        [0.5047, 0.3798, 0.3238],
        [0.5310, 0.3607, 0.2986],
        [0.5360, 0.3570, 0.3127],
        [0.5176, 0.3584, 0.3237],
        [0.5471, 0.3512, 0.2955],
        [0.4822, 0.3897, 0.3325],
        [0.5378, 0.3722, 0.3064],
        [0.5394, 0.3698, 0.3059],
        [0.5038, 0.3768, 0.3489],
        [0.5110, 0.3528, 0.3504],
        [0.4696, 0.3837, 0.3412],
        [0.5134, 0.3668, 0.3329],
        [0.5464, 0.3541, 0.2988],
        [0.5014, 0.3788, 0.3172],
        [0.4966, 0.3888, 0.3127],
        [0.5684, 0.3490, 0.2940],
        [0.5696, 0.3374, 0.2935],
        [0.5418, 0.3488, 0.3331],
        [0.5182, 0.3768, 0.3161],
        [0.4630, 0.3760, 0.3536],
        [0.5112, 0.3481, 0.3473],
        [0.5242, 0.3511, 0.3280],
        [0.4633, 0.3855, 0.3500],
        [0.5151, 0.3643, 0.3344],
        [0.5291, 0.3583, 0.3302],
        [0.5279, 0.3379, 0.3345],
        [0.5350, 0.3571, 0.3303],
        [0.5535, 0.3487, 0.3087],
        [0.5252, 0.3597, 0.3468],
        [0.5281, 0.3500, 0.3406],
        [0.5871, 0.3331, 0.2952],
        [0.5133, 0.3558, 0.3434],
        [0.5101, 0.3619, 0.3345],
        [0.5326, 0.3501, 0.3270],
        [0.5154, 0.3556, 0.3442],
        [0.5162, 0.3646, 0.3423],
        [0.4781, 0.3788, 0.3588],
        [0.5044, 0.3543, 0.3519],
        [0.5324, 0.3654, 0.3175],
        [0.5083, 0.3661, 0.3340],
        [0.4718, 0.3852, 0.3515],
        [0.5668, 0.3344, 0.2877],
        [0.5477, 0.3539, 0.3024],
        [0.5153, 0.3624, 0.3214],
        [0.5344, 0.3463, 0.3170],
        [0.5549, 0.3481, 0.2761],
        [0.4829, 0.3914, 0.3234],
        [0.5804, 0.3495, 0.2733],
        [0.5213, 0.3715, 0.3210],
        [0.5324, 0.3590, 0.3136],
        [0.4957, 0.3661, 0.3381],
        [0.5574, 0.3485, 0.3064],
        [0.5102, 0.3648, 0.3426],
        [0.5324, 0.3675, 0.3216],
        [0.5544, 0.3506, 0.3230],
        [0.5214, 0.3664, 0.3297],
        [0.5395, 0.3645, 0.2878],
        [0.4692, 0.3944, 0.3501],
        [0.5413, 0.3556, 0.2971],
        [0.4481, 0.4098, 0.3344],
        [0.4278, 0.4266, 0.3480],
        [0.5552, 0.3534, 0.3168],
        [0.5194, 0.3580, 0.3376],
        [0.5229, 0.3673, 0.3304],
        [0.5364, 0.3629, 0.3069],
        [0.4613, 0.3695, 0.3620],
        [0.4820, 0.3685, 0.3626],
        [0.5078, 0.3535, 0.3525],
        [0.5336, 0.3525, 0.3180],
        [0.5223, 0.3563, 0.3232],
        [0.5544, 0.3492, 0.2937],
        [0.4909, 0.3661, 0.3434],
        [0.5259, 0.3650, 0.3346],
        [0.5124, 0.3498, 0.3472],
        [0.5196, 0.3683, 0.3468],
        [0.5207, 0.3463, 0.3394],
        [0.4688, 0.3779, 0.3637],
        [0.4736, 0.3799, 0.3634],
        [0.5119, 0.3562, 0.3475],
        [0.5315, 0.3515, 0.3277],
        [0.5237, 0.3543, 0.3459],
        [0.5230, 0.3573, 0.3429],
        [0.5073, 0.3655, 0.3452],
        [0.5355, 0.3511, 0.3309],
        [0.5095, 0.3592, 0.3274],
        [0.5091, 0.3707, 0.3192],
        [0.5163, 0.3596, 0.3386],
        [0.4915, 0.3550, 0.3455]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 107: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13080.0, Mean: 1250.5723876953125, Std: 1076.9464111328125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 130688.8203125, Mean: 12494.2080078125, Std: 10760.390625
[DEBUG] Top-3 class probabilities:
tensor([[0.5260, 0.3575, 0.3102],
        [0.5620, 0.3452, 0.2972],
        [0.5209, 0.3622, 0.3111],
        [0.5424, 0.3581, 0.3014],
        [0.5264, 0.3729, 0.3005],
        [0.5261, 0.3664, 0.3153],
        [0.5238, 0.3536, 0.2923],
        [0.5098, 0.3654, 0.3430],
        [0.5202, 0.3680, 0.3339],
        [0.5438, 0.3543, 0.3060],
        [0.5100, 0.3586, 0.3389],
        [0.5351, 0.3450, 0.3169],
        [0.4987, 0.3589, 0.3561],
        [0.5607, 0.3512, 0.3067],
        [0.5749, 0.3428, 0.2821],
        [0.4582, 0.3940, 0.3323],
        [0.5312, 0.3661, 0.2980],
        [0.5263, 0.3492, 0.3054],
        [0.5064, 0.3745, 0.3329],
        [0.7148, 0.3014, 0.2271],
        [0.6618, 0.2817, 0.2682],
        [0.5908, 0.3117, 0.2572],
        [0.5030, 0.3668, 0.3365],
        [0.4473, 0.3972, 0.3696],
        [0.4630, 0.3809, 0.3723],
        [0.4841, 0.3582, 0.3486],
        [0.5105, 0.3593, 0.3285],
        [0.5113, 0.3672, 0.3517],
        [0.5031, 0.3538, 0.3415],
        [0.4387, 0.3881, 0.3627],
        [0.5309, 0.3520, 0.3293],
        [0.5068, 0.3601, 0.3517],
        [0.5240, 0.3558, 0.3370],
        [0.5229, 0.3546, 0.3369],
        [0.5063, 0.3501, 0.3432],
        [0.4602, 0.3774, 0.3733],
        [0.4990, 0.3541, 0.3375],
        [0.5040, 0.3587, 0.3451],
        [0.4989, 0.3627, 0.3557],
        [0.5243, 0.3554, 0.3316],
        [0.5255, 0.3602, 0.3302],
        [0.5490, 0.3437, 0.3201],
        [0.5581, 0.3479, 0.3068],
        [0.5386, 0.3519, 0.3219],
        [0.5035, 0.3696, 0.3423],
        [0.5028, 0.3557, 0.3494],
        [0.5894, 0.3406, 0.2833],
        [0.5491, 0.3487, 0.3038],
        [0.5329, 0.3701, 0.3098],
        [0.4513, 0.4059, 0.3245],
        [0.4728, 0.4009, 0.3242],
        [0.5151, 0.3689, 0.3117],
        [0.5845, 0.3265, 0.2669],
        [0.5261, 0.3596, 0.3420],
        [0.5131, 0.3583, 0.3374],
        [0.5167, 0.3617, 0.3316],
        [0.4542, 0.3779, 0.3688],
        [0.5167, 0.3599, 0.3278],
        [0.4670, 0.3731, 0.3630],
        [0.5724, 0.3497, 0.2828],
        [0.5431, 0.3584, 0.2994],
        [0.5186, 0.3687, 0.3187],
        [0.5189, 0.3736, 0.3088],
        [0.5034, 0.3557, 0.3538],
        [0.4984, 0.3602, 0.3595],
        [0.6478, 0.2807, 0.2653],
        [0.4795, 0.3706, 0.3474],
        [0.5223, 0.3666, 0.3466],
        [0.5283, 0.3594, 0.3338],
        [0.4916, 0.3764, 0.3450],
        [0.5132, 0.3611, 0.3455],
        [0.4969, 0.3637, 0.3528],
        [0.5039, 0.3556, 0.3419],
        [0.5305, 0.3544, 0.3268],
        [0.5170, 0.3680, 0.3262],
        [0.4762, 0.3785, 0.3434],
        [0.5018, 0.3645, 0.3439],
        [0.4630, 0.3700, 0.3532],
        [0.5120, 0.3507, 0.3398],
        [0.4790, 0.3673, 0.3561],
        [0.4649, 0.3680, 0.3621],
        [0.4635, 0.3722, 0.3675],
        [0.5422, 0.3556, 0.3155],
        [0.4943, 0.3620, 0.3564],
        [0.4598, 0.3948, 0.3719],
        [0.4883, 0.3795, 0.3648],
        [0.5056, 0.3495, 0.3449],
        [0.5561, 0.3499, 0.3173],
        [0.5568, 0.3451, 0.3112],
        [0.5316, 0.3562, 0.3301],
        [0.5726, 0.3332, 0.2966],
        [0.4967, 0.3519, 0.3472],
        [0.5303, 0.3474, 0.3201],
        [0.5377, 0.3601, 0.3237],
        [0.5420, 0.3561, 0.2981],
        [0.4946, 0.3908, 0.3321],
        [0.5025, 0.3709, 0.3155],
        [0.5597, 0.3488, 0.3031],
        [0.5595, 0.3525, 0.2898],
        [0.5075, 0.3703, 0.3281],
        [0.5502, 0.3473, 0.3082],
        [0.5355, 0.3463, 0.3177],
        [0.5276, 0.3608, 0.3334],
        [0.5443, 0.3588, 0.3127],
        [0.4965, 0.3865, 0.3338],
        [0.5195, 0.3612, 0.3174],
        [0.5283, 0.3748, 0.3021],
        [0.5546, 0.3607, 0.3088],
        [0.5710, 0.3392, 0.2896],
        [0.5150, 0.3670, 0.3053],
        [0.5064, 0.3661, 0.3348],
        [0.4872, 0.3711, 0.3620],
        [0.5142, 0.3631, 0.3554],
        [0.4972, 0.3595, 0.3573],
        [0.4831, 0.3736, 0.3637],
        [0.5071, 0.3541, 0.3517],
        [0.5240, 0.3609, 0.3451],
        [0.5015, 0.3599, 0.3567],
        [0.4873, 0.3707, 0.3689],
        [0.5134, 0.3570, 0.3304],
        [0.5361, 0.3607, 0.3245],
        [0.5202, 0.3567, 0.3258],
        [0.5287, 0.3598, 0.3220],
        [0.5213, 0.3643, 0.3405],
        [0.4935, 0.3642, 0.3480],
        [0.5006, 0.3555, 0.3479],
        [0.4667, 0.3645, 0.3564],
        [0.5068, 0.3535, 0.3452]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 108: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10304.0, Mean: 1235.8028564453125, Std: 1052.029541015625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 102952.203125, Mean: 12346.63671875, Std: 10511.4306640625
[DEBUG] Top-3 class probabilities:
tensor([[0.4514, 0.3793, 0.3725],
        [0.5131, 0.3543, 0.3318],
        [0.4831, 0.3554, 0.3544],
        [0.4973, 0.3564, 0.3555],
        [0.4918, 0.3727, 0.3665],
        [0.5348, 0.3511, 0.3272],
        [0.4899, 0.3607, 0.3581],
        [0.5128, 0.3612, 0.3439],
        [0.5310, 0.3596, 0.3261],
        [0.5012, 0.3641, 0.3318],
        [0.5611, 0.3448, 0.2859],
        [0.4797, 0.3879, 0.3611],
        [0.5840, 0.3430, 0.2827],
        [0.5320, 0.3649, 0.3148],
        [0.5381, 0.3552, 0.3196],
        [0.5393, 0.3559, 0.3273],
        [0.5300, 0.3683, 0.3250],
        [0.4962, 0.3716, 0.3491],
        [0.5115, 0.3670, 0.3414],
        [0.4681, 0.3798, 0.3750],
        [0.5304, 0.3578, 0.3395],
        [0.5441, 0.3534, 0.3249],
        [0.5614, 0.3352, 0.3063],
        [0.5743, 0.3449, 0.2849],
        [0.5149, 0.3941, 0.3281],
        [0.4976, 0.3867, 0.3323],
        [0.5165, 0.3642, 0.3283],
        [0.5157, 0.3540, 0.3262],
        [0.5161, 0.3616, 0.3360],
        [0.5223, 0.3662, 0.3175],
        [0.4771, 0.3773, 0.3757],
        [0.5154, 0.3539, 0.3441],
        [0.5143, 0.3617, 0.3414],
        [0.5226, 0.3580, 0.3378],
        [0.5128, 0.3598, 0.3501],
        [0.5042, 0.3683, 0.3589],
        [0.4546, 0.3804, 0.3652],
        [0.5130, 0.3568, 0.3374],
        [0.5174, 0.3505, 0.3420],
        [0.5221, 0.3598, 0.3291],
        [0.5106, 0.3585, 0.3340],
        [0.5060, 0.3591, 0.3488],
        [0.5223, 0.3498, 0.3317],
        [0.5052, 0.3572, 0.3494],
        [0.4936, 0.3621, 0.3554],
        [0.5032, 0.3573, 0.3537],
        [0.5031, 0.3618, 0.3423],
        [0.5063, 0.3615, 0.3346],
        [0.4995, 0.3648, 0.3513],
        [0.5208, 0.3587, 0.3374],
        [0.4933, 0.3638, 0.3564],
        [0.5054, 0.3678, 0.3447],
        [0.5613, 0.3552, 0.3147],
        [0.5517, 0.3546, 0.3095],
        [0.5379, 0.3607, 0.3225],
        [0.5175, 0.3766, 0.3071],
        [0.5760, 0.3415, 0.2872],
        [0.5691, 0.3415, 0.2848],
        [0.5989, 0.3387, 0.2667],
        [0.5735, 0.3489, 0.2854],
        [0.5518, 0.3504, 0.3217],
        [0.5543, 0.3611, 0.3130],
        [0.5302, 0.3571, 0.3283],
        [0.5480, 0.3493, 0.3157],
        [0.4190, 0.3920, 0.3716],
        [0.4491, 0.3887, 0.3779],
        [0.4948, 0.3801, 0.3489],
        [0.5332, 0.3527, 0.3188],
        [0.5219, 0.3496, 0.3337],
        [0.4861, 0.3848, 0.3376],
        [0.5068, 0.3772, 0.3141],
        [0.5538, 0.3447, 0.3139],
        [0.5495, 0.3461, 0.3019],
        [0.5484, 0.3516, 0.3034],
        [0.5639, 0.3370, 0.2871],
        [0.4990, 0.3837, 0.3445],
        [0.5198, 0.3578, 0.3298],
        [0.5457, 0.3565, 0.3101],
        [0.5469, 0.3607, 0.2869],
        [0.5360, 0.3550, 0.3374],
        [0.5247, 0.3581, 0.3291],
        [0.4959, 0.3604, 0.3579],
        [0.5089, 0.3614, 0.3569],
        [0.5189, 0.3512, 0.3330],
        [0.5256, 0.3454, 0.3315],
        [0.5134, 0.3747, 0.3436],
        [0.5205, 0.3517, 0.3514],
        [0.5463, 0.3484, 0.3318],
        [0.4610, 0.3777, 0.3638],
        [0.5120, 0.3581, 0.3484],
        [0.5219, 0.3599, 0.3352],
        [0.4802, 0.3616, 0.3591],
        [0.4932, 0.3592, 0.3522],
        [0.5329, 0.3485, 0.3324],
        [0.5544, 0.3425, 0.3077],
        [0.5420, 0.3511, 0.3177],
        [0.5252, 0.3676, 0.3190],
        [0.5661, 0.3492, 0.3007],
        [0.5712, 0.3440, 0.2979],
        [0.5728, 0.3466, 0.2970],
        [0.5737, 0.3451, 0.3043],
        [0.5517, 0.3423, 0.3089],
        [0.5498, 0.3599, 0.3036],
        [0.5314, 0.3674, 0.3135],
        [0.5063, 0.3746, 0.3247],
        [0.4904, 0.3905, 0.3268],
        [0.5040, 0.3845, 0.3160],
        [0.5612, 0.3505, 0.3001],
        [0.5070, 0.3679, 0.3421],
        [0.4750, 0.3699, 0.3652],
        [0.4832, 0.3588, 0.3558],
        [0.4851, 0.3682, 0.3608],
        [0.5285, 0.3466, 0.3365],
        [0.5545, 0.3526, 0.3069],
        [0.5446, 0.3589, 0.2905],
        [0.5003, 0.3638, 0.3484],
        [0.5121, 0.3565, 0.3296],
        [0.5664, 0.3331, 0.2956],
        [0.5148, 0.3605, 0.3189],
        [0.4821, 0.3676, 0.3472],
        [0.5421, 0.3461, 0.3177],
        [0.5013, 0.3697, 0.3581],
        [0.5575, 0.3460, 0.2958],
        [0.5210, 0.3629, 0.3365],
        [0.5696, 0.3298, 0.2902],
        [0.5000, 0.3681, 0.3597],
        [0.4656, 0.3729, 0.3653],
        [0.4966, 0.3572, 0.3531]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 109: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8800.0, Mean: 1257.060546875, Std: 1041.7628173828125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 87924.875, Mean: 12559.033203125, Std: 10408.8505859375
[DEBUG] Top-3 class probabilities:
tensor([[0.5135, 0.3586, 0.3433],
        [0.5192, 0.3520, 0.3310],
        [0.4885, 0.3600, 0.3597],
        [0.4835, 0.3700, 0.3454],
        [0.5087, 0.3528, 0.3515],
        [0.5363, 0.3427, 0.3373],
        [0.5334, 0.3444, 0.3253],
        [0.5119, 0.3587, 0.3486],
        [0.5087, 0.3615, 0.3442],
        [0.5168, 0.3607, 0.3343],
        [0.5412, 0.3432, 0.3259],
        [0.5565, 0.3424, 0.3183],
        [0.5106, 0.3525, 0.3421],
        [0.5383, 0.3509, 0.3182],
        [0.5526, 0.3531, 0.3211],
        [0.5851, 0.3302, 0.2842],
        [0.5499, 0.3436, 0.3181],
        [0.5573, 0.3473, 0.3039],
        [0.5528, 0.3539, 0.3115],
        [0.5522, 0.3477, 0.3196],
        [0.5300, 0.3688, 0.3314],
        [0.5720, 0.3450, 0.2866],
        [0.5366, 0.3640, 0.3120],
        [0.5704, 0.3431, 0.2891],
        [0.5114, 0.3859, 0.3231],
        [0.4961, 0.3776, 0.3297],
        [0.5075, 0.3706, 0.3102],
        [0.5057, 0.3694, 0.3372],
        [0.4981, 0.3632, 0.3238],
        [0.4807, 0.3658, 0.3599],
        [0.5039, 0.3544, 0.3489],
        [0.4820, 0.3563, 0.3562],
        [0.5157, 0.3598, 0.3314],
        [0.5254, 0.3494, 0.3400],
        [0.5485, 0.3441, 0.3092],
        [0.4983, 0.3548, 0.3493],
        [0.5372, 0.3624, 0.3188],
        [0.5017, 0.3437, 0.3420],
        [0.5392, 0.3437, 0.3131],
        [0.5007, 0.3663, 0.3428],
        [0.5348, 0.3505, 0.3309],
        [0.5424, 0.3502, 0.3132],
        [0.4976, 0.3648, 0.3417],
        [0.4570, 0.3752, 0.3709],
        [0.4584, 0.3799, 0.3752],
        [0.4881, 0.3564, 0.3561],
        [0.5252, 0.3484, 0.3280],
        [0.4808, 0.3646, 0.3446],
        [0.4060, 0.4043, 0.3869],
        [0.5358, 0.3653, 0.3212],
        [0.5133, 0.3504, 0.3450],
        [0.5026, 0.3565, 0.3534],
        [0.5407, 0.3496, 0.3228],
        [0.5375, 0.3570, 0.3275],
        [0.5060, 0.3608, 0.3507],
        [0.5106, 0.3526, 0.3339],
        [0.5363, 0.3539, 0.3124],
        [0.5169, 0.3582, 0.3437],
        [0.5410, 0.3575, 0.3248],
        [0.4942, 0.3576, 0.3527],
        [0.5110, 0.3606, 0.3387],
        [0.5355, 0.3572, 0.3292],
        [0.5613, 0.3413, 0.3080],
        [0.5621, 0.3405, 0.3056],
        [0.5386, 0.3578, 0.3299],
        [0.5309, 0.3560, 0.3298],
        [0.5323, 0.3549, 0.3303],
        [0.4937, 0.3588, 0.3376],
        [0.5170, 0.3574, 0.3407],
        [0.5409, 0.3455, 0.3144],
        [0.5297, 0.3629, 0.3026],
        [0.5293, 0.3594, 0.3156],
        [0.5204, 0.3599, 0.3186],
        [0.4925, 0.3613, 0.3367],
        [0.4654, 0.3854, 0.3578],
        [0.4901, 0.3725, 0.3443],
        [0.4795, 0.3677, 0.3675],
        [0.4884, 0.3557, 0.3517],
        [0.5467, 0.3500, 0.3204],
        [0.5578, 0.3543, 0.3117],
        [0.5379, 0.3536, 0.3187],
        [0.5265, 0.3674, 0.3370],
        [0.4992, 0.3712, 0.3267],
        [0.5226, 0.3653, 0.3311],
        [0.5483, 0.3491, 0.3132],
        [0.5089, 0.3710, 0.3290],
        [0.5321, 0.3478, 0.3125],
        [0.5155, 0.3598, 0.3223],
        [0.5117, 0.3625, 0.3237],
        [0.4717, 0.3701, 0.3485],
        [0.4680, 0.3765, 0.3670],
        [0.4794, 0.3650, 0.3632],
        [0.4860, 0.3659, 0.3557],
        [0.4766, 0.3680, 0.3482],
        [0.4306, 0.4189, 0.3788],
        [0.5080, 0.3607, 0.3302],
        [0.5417, 0.3537, 0.3273],
        [0.4766, 0.3609, 0.3574],
        [0.4875, 0.3661, 0.3624],
        [0.5000, 0.3570, 0.3497],
        [0.4875, 0.3627, 0.3587],
        [0.5101, 0.3516, 0.3458],
        [0.5088, 0.3580, 0.3415],
        [0.5137, 0.3436, 0.2728],
        [0.5602, 0.3460, 0.2845],
        [0.4838, 0.3681, 0.3631],
        [0.4707, 0.3899, 0.3695],
        [0.5361, 0.3472, 0.3220],
        [0.4844, 0.3684, 0.3667],
        [0.4983, 0.3706, 0.3603],
        [0.5251, 0.3631, 0.3307],
        [0.4834, 0.3632, 0.3493],
        [0.4895, 0.3668, 0.3536],
        [0.5417, 0.3489, 0.3208],
        [0.5148, 0.3644, 0.3446],
        [0.4876, 0.3710, 0.3491],
        [0.5455, 0.3460, 0.3013],
        [0.4922, 0.3748, 0.3186],
        [0.5099, 0.3676, 0.3274],
        [0.5109, 0.3657, 0.3291],
        [0.5249, 0.3504, 0.3394],
        [0.4826, 0.3646, 0.3602],
        [0.4864, 0.3666, 0.3643],
        [0.4898, 0.3663, 0.3485],
        [0.4836, 0.3671, 0.3554],
        [0.5272, 0.3496, 0.3254],
        [0.5083, 0.3505, 0.3282],
        [0.5067, 0.3642, 0.3494]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 110: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6484.0, Mean: 1270.900634765625, Std: 1027.55078125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 64784.38671875, Mean: 12697.3173828125, Std: 10266.849609375
[DEBUG] Top-3 class probabilities:
tensor([[0.5130, 0.3608, 0.3344],
        [0.4524, 0.3703, 0.3702],
        [0.5055, 0.3477, 0.3424],
        [0.5383, 0.3450, 0.3289],
        [0.5165, 0.3589, 0.3165],
        [0.5153, 0.3751, 0.3378],
        [0.5132, 0.3602, 0.3481],
        [0.4873, 0.3642, 0.3565],
        [0.4836, 0.3666, 0.3402],
        [0.4794, 0.3651, 0.3586],
        [0.4789, 0.3678, 0.3621],
        [0.4934, 0.3617, 0.3416],
        [0.5263, 0.3565, 0.3308],
        [0.5037, 0.3681, 0.3304],
        [0.4939, 0.3678, 0.3386],
        [0.4966, 0.3673, 0.3504],
        [0.4957, 0.3656, 0.3644],
        [0.4717, 0.3729, 0.3619],
        [0.4846, 0.3737, 0.3584],
        [0.4784, 0.3689, 0.3670],
        [0.5199, 0.3485, 0.3333],
        [0.5112, 0.3644, 0.3432],
        [0.5029, 0.3551, 0.3527],
        [0.5034, 0.3628, 0.3543],
        [0.5446, 0.3427, 0.3113],
        [0.5143, 0.3640, 0.3376],
        [0.5322, 0.3557, 0.3356],
        [0.4872, 0.3578, 0.3470],
        [0.5073, 0.3561, 0.3375],
        [0.4725, 0.3655, 0.3610],
        [0.4742, 0.3712, 0.3621],
        [0.5054, 0.3499, 0.3425],
        [0.4959, 0.3584, 0.3555],
        [0.5320, 0.3633, 0.3157],
        [0.5576, 0.3568, 0.2867],
        [0.5303, 0.3607, 0.3248],
        [0.5155, 0.3629, 0.3140],
        [0.4755, 0.3817, 0.3540],
        [0.4940, 0.3621, 0.3543],
        [0.4909, 0.3726, 0.3660],
        [0.4732, 0.3640, 0.3554],
        [0.4628, 0.3604, 0.3569],
        [0.4477, 0.3890, 0.3639],
        [0.4863, 0.3707, 0.3642],
        [0.4929, 0.3646, 0.3586],
        [0.5246, 0.3599, 0.3392],
        [0.4829, 0.3630, 0.3600],
        [0.4854, 0.3669, 0.3531],
        [0.4771, 0.3668, 0.3661],
        [0.5295, 0.3454, 0.3338],
        [0.4753, 0.3651, 0.3635],
        [0.5303, 0.3531, 0.3202],
        [0.5604, 0.3524, 0.3052],
        [0.4940, 0.3565, 0.3434],
        [0.4865, 0.3703, 0.3430],
        [0.4559, 0.3865, 0.3775],
        [0.5160, 0.3572, 0.3362],
        [0.4978, 0.3499, 0.3440],
        [0.5252, 0.3601, 0.3296],
        [0.4988, 0.3558, 0.3453],
        [0.4517, 0.3820, 0.3626],
        [0.4585, 0.3800, 0.3763],
        [0.4916, 0.3708, 0.3649],
        [0.5202, 0.3677, 0.3496],
        [0.5866, 0.3264, 0.2823],
        [0.5589, 0.3384, 0.3085],
        [0.5161, 0.3637, 0.3203],
        [0.4326, 0.3873, 0.3865],
        [0.5022, 0.3600, 0.3357],
        [0.4919, 0.3746, 0.3397],
        [0.5191, 0.3596, 0.3314],
        [0.5205, 0.3704, 0.3380],
        [0.5042, 0.3682, 0.3488],
        [0.5505, 0.3345, 0.2970],
        [0.6036, 0.3135, 0.2596],
        [0.4948, 0.3575, 0.3468],
        [0.5296, 0.3520, 0.3324],
        [0.5267, 0.3426, 0.3175],
        [0.5637, 0.3459, 0.2823],
        [0.5150, 0.3696, 0.3206],
        [0.4967, 0.3681, 0.3334],
        [0.4931, 0.3678, 0.3417],
        [0.5029, 0.3699, 0.3474],
        [0.5082, 0.3592, 0.3355],
        [0.5212, 0.3648, 0.3293],
        [0.4993, 0.3633, 0.3523],
        [0.4870, 0.3773, 0.3527],
        [0.4890, 0.3679, 0.3570],
        [0.4963, 0.3615, 0.3600],
        [0.5330, 0.3486, 0.3215],
        [0.5539, 0.3434, 0.3182],
        [0.4749, 0.3617, 0.3580],
        [0.4844, 0.3654, 0.3488],
        [0.5061, 0.3579, 0.3222],
        [0.4718, 0.3689, 0.3511],
        [0.4655, 0.3688, 0.3680],
        [0.5103, 0.3547, 0.3398],
        [0.5180, 0.3513, 0.3302],
        [0.4830, 0.3669, 0.3593],
        [0.4500, 0.3896, 0.3878],
        [0.4501, 0.3888, 0.3630],
        [0.4669, 0.3751, 0.3711],
        [0.5041, 0.3563, 0.3452],
        [0.4767, 0.3827, 0.3713],
        [0.5041, 0.3600, 0.3367],
        [0.5197, 0.3625, 0.3350],
        [0.4917, 0.3650, 0.3405],
        [0.5305, 0.3555, 0.3285],
        [0.4636, 0.3837, 0.3698],
        [0.4728, 0.3772, 0.3615],
        [0.5033, 0.3592, 0.3579],
        [0.5260, 0.3579, 0.3360],
        [0.4897, 0.3670, 0.3335],
        [0.5080, 0.3605, 0.3315],
        [0.5095, 0.3684, 0.3445],
        [0.5358, 0.3538, 0.3143],
        [0.4832, 0.3662, 0.3391],
        [0.5248, 0.3531, 0.3334],
        [0.5031, 0.3616, 0.3253],
        [0.5154, 0.3658, 0.3407],
        [0.5217, 0.3526, 0.3245],
        [0.5005, 0.3541, 0.3540],
        [0.5086, 0.3569, 0.3348],
        [0.4995, 0.3675, 0.3397],
        [0.5295, 0.3663, 0.3037],
        [0.5112, 0.3734, 0.3295],
        [0.4854, 0.3756, 0.3532],
        [0.5279, 0.3542, 0.3283]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 111: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11301.0, Mean: 1354.2347412109375, Std: 1041.985107421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 112913.8046875, Mean: 13529.9580078125, Std: 10411.072265625
[DEBUG] Top-3 class probabilities:
tensor([[0.5106, 0.3713, 0.3444],
        [0.5469, 0.3425, 0.3053],
        [0.5311, 0.3514, 0.3216],
        [0.4843, 0.3704, 0.3511],
        [0.4835, 0.3714, 0.3648],
        [0.4839, 0.3657, 0.3625],
        [0.5148, 0.3669, 0.3451],
        [0.5111, 0.3532, 0.3466],
        [0.5642, 0.3481, 0.3006],
        [0.5149, 0.3667, 0.3325],
        [0.5319, 0.3590, 0.3185],
        [0.5105, 0.3563, 0.3318],
        [0.4876, 0.3658, 0.3596],
        [0.5022, 0.3564, 0.3555],
        [0.5145, 0.3606, 0.3354],
        [0.4975, 0.3576, 0.3478],
        [0.4985, 0.3676, 0.3568],
        [0.4598, 0.3817, 0.3556],
        [0.4276, 0.4129, 0.3715],
        [0.4412, 0.3890, 0.3880],
        [0.5026, 0.3672, 0.3479],
        [0.5259, 0.3603, 0.3337],
        [0.5066, 0.3645, 0.3381],
        [0.5025, 0.3682, 0.3479],
        [0.5149, 0.3597, 0.3286],
        [0.4807, 0.3806, 0.3461],
        [0.4486, 0.4029, 0.3826],
        [0.4693, 0.3673, 0.3618],
        [0.4700, 0.3772, 0.3729],
        [0.4717, 0.3657, 0.3528],
        [0.5634, 0.3573, 0.3014],
        [0.5452, 0.3528, 0.3173],
        [0.5160, 0.3512, 0.3406],
        [0.4948, 0.3655, 0.3460],
        [0.4919, 0.3682, 0.3508],
        [0.5336, 0.3471, 0.3297],
        [0.5007, 0.3660, 0.3448],
        [0.5507, 0.3482, 0.3125],
        [0.5234, 0.3398, 0.3326],
        [0.4710, 0.3710, 0.3563],
        [0.4981, 0.3658, 0.3519],
        [0.4748, 0.3889, 0.3358],
        [0.4999, 0.3747, 0.3059],
        [0.5068, 0.3615, 0.3435],
        [0.5377, 0.3623, 0.3138],
        [0.5378, 0.3564, 0.3227],
        [0.5221, 0.3415, 0.3276],
        [0.5050, 0.3577, 0.3452],
        [0.5074, 0.3538, 0.3383],
        [0.5294, 0.3578, 0.3281],
        [0.4287, 0.3886, 0.3834],
        [0.4619, 0.3912, 0.3764],
        [0.5058, 0.3555, 0.3485],
        [0.4964, 0.3586, 0.3326],
        [0.5629, 0.3446, 0.2944],
        [0.4873, 0.3801, 0.3404],
        [0.4622, 0.3812, 0.3665],
        [0.4561, 0.3878, 0.3416],
        [0.4794, 0.3753, 0.3622],
        [0.4709, 0.3737, 0.3583],
        [0.4446, 0.3880, 0.3780],
        [0.5029, 0.3663, 0.3391],
        [0.5122, 0.3569, 0.3166],
        [0.5254, 0.3571, 0.3230],
        [0.4672, 0.3832, 0.3535],
        [0.4870, 0.3660, 0.3551],
        [0.4960, 0.3658, 0.3607],
        [0.5401, 0.3498, 0.3084],
        [0.4866, 0.3610, 0.3578],
        [0.4977, 0.3608, 0.3532],
        [0.4477, 0.3899, 0.3451],
        [0.5194, 0.3589, 0.3469],
        [0.5292, 0.3488, 0.3087],
        [0.4521, 0.3752, 0.3707],
        [0.4837, 0.3747, 0.3643],
        [0.5011, 0.3585, 0.3465],
        [0.5670, 0.3432, 0.2933],
        [0.5110, 0.3635, 0.3353],
        [0.4915, 0.3537, 0.3536],
        [0.4241, 0.3915, 0.3768],
        [0.4598, 0.3793, 0.3662],
        [0.4562, 0.3785, 0.3644],
        [0.4505, 0.3778, 0.3659],
        [0.4912, 0.3609, 0.3516],
        [0.5246, 0.3457, 0.3357],
        [0.4969, 0.3584, 0.3367],
        [0.5112, 0.3589, 0.3295],
        [0.5616, 0.3571, 0.2861],
        [0.5230, 0.3795, 0.2976],
        [0.5265, 0.3696, 0.3244],
        [0.5191, 0.3546, 0.3318],
        [0.4918, 0.3722, 0.3559],
        [0.4907, 0.3626, 0.3549],
        [0.4769, 0.3721, 0.3582],
        [0.4933, 0.3644, 0.3594],
        [0.4949, 0.3594, 0.3482],
        [0.4868, 0.3737, 0.3551],
        [0.4535, 0.3912, 0.3908],
        [0.4861, 0.3651, 0.3631],
        [0.5122, 0.3516, 0.3214],
        [0.5712, 0.3354, 0.2886],
        [0.5684, 0.3455, 0.2517],
        [0.5379, 0.3694, 0.2634],
        [0.5327, 0.3688, 0.2725],
        [0.4940, 0.3831, 0.3253],
        [0.5028, 0.3627, 0.3277],
        [0.5222, 0.3505, 0.3037],
        [0.5651, 0.3218, 0.2557],
        [0.7015, 0.2421, 0.2099],
        [0.5157, 0.3476, 0.3078],
        [0.4217, 0.3901, 0.3886],
        [0.4601, 0.3783, 0.3534],
        [0.4911, 0.3658, 0.3271],
        [0.6003, 0.3036, 0.2252],
        [0.5672, 0.3222, 0.2629],
        [0.4882, 0.3513, 0.3353],
        [0.4557, 0.3621, 0.3589],
        [0.5195, 0.3434, 0.3098],
        [0.4587, 0.3747, 0.3649],
        [0.4615, 0.3747, 0.3591],
        [0.5271, 0.3489, 0.3007],
        [0.4673, 0.3720, 0.3515],
        [0.4392, 0.3807, 0.3727],
        [0.4482, 0.3767, 0.3563],
        [0.4971, 0.3640, 0.2989],
        [0.5412, 0.3438, 0.2704],
        [0.5288, 0.3597, 0.2694],
        [0.4806, 0.3806, 0.3072]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 112: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 19.0, Max: 15036.0, Mean: 1643.5887451171875, Std: 967.735107421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 188.85964965820312, Max: 150232.34375, Mean: 16421.05859375, Std: 9669.197265625
[DEBUG] Top-3 class probabilities:
tensor([[0.5383, 0.3575, 0.2797],
        [0.4576, 0.3867, 0.3565],
        [0.5324, 0.3571, 0.2670],
        [0.4744, 0.3772, 0.3283],
        [0.5050, 0.3699, 0.2967],
        [0.5501, 0.3569, 0.2605],
        [0.4931, 0.3844, 0.3135],
        [0.4673, 0.3964, 0.3134],
        [0.5010, 0.3747, 0.2876],
        [0.4913, 0.3742, 0.3371],
        [0.4971, 0.3681, 0.3175],
        [0.4957, 0.3692, 0.3062],
        [0.5701, 0.3326, 0.2352],
        [0.5466, 0.3501, 0.2586],
        [0.5224, 0.3643, 0.2879],
        [0.5435, 0.3498, 0.2465],
        [0.4803, 0.3840, 0.3332],
        [0.5162, 0.3770, 0.3021],
        [0.5604, 0.3504, 0.2491],
        [0.5535, 0.3509, 0.2675],
        [0.5607, 0.3373, 0.2353],
        [0.5776, 0.3379, 0.2412],
        [0.5419, 0.3528, 0.2890],
        [0.6622, 0.2431, 0.2362],
        [0.4532, 0.3833, 0.3483],
        [0.4608, 0.3882, 0.3568],
        [0.4711, 0.3806, 0.3405],
        [0.4779, 0.3710, 0.3314],
        [0.5034, 0.3458, 0.3166],
        [0.5329, 0.3338, 0.2835],
        [0.4740, 0.3730, 0.3458],
        [0.4336, 0.3802, 0.3531],
        [0.4592, 0.3718, 0.3561],
        [0.4617, 0.3639, 0.3601],
        [0.5318, 0.3571, 0.2970],
        [0.4725, 0.3642, 0.3489],
        [0.4753, 0.3705, 0.3399],
        [0.4797, 0.3618, 0.3315],
        [0.4819, 0.3734, 0.3515],
        [0.4772, 0.3783, 0.3371],
        [0.4835, 0.3735, 0.3281],
        [0.4406, 0.3738, 0.3705],
        [0.5006, 0.3682, 0.2972],
        [0.4834, 0.3745, 0.3129],
        [0.4638, 0.3805, 0.3489],
        [0.5477, 0.3477, 0.2658],
        [0.4617, 0.3905, 0.3488],
        [0.4505, 0.3829, 0.3488],
        [0.5342, 0.3605, 0.2789],
        [0.5255, 0.3571, 0.2615],
        [0.5163, 0.3641, 0.2846],
        [0.4671, 0.3803, 0.3484],
        [0.5023, 0.3672, 0.3012],
        [0.5283, 0.3475, 0.2893],
        [0.4534, 0.3834, 0.3408],
        [0.4517, 0.3840, 0.3667],
        [0.4919, 0.3723, 0.3281],
        [0.4745, 0.3831, 0.3453],
        [0.4636, 0.3843, 0.3458],
        [0.5268, 0.3678, 0.2863],
        [0.5051, 0.3788, 0.2828],
        [0.5622, 0.3294, 0.2428],
        [0.5717, 0.3454, 0.2400],
        [0.5907, 0.3184, 0.2345],
        [0.5028, 0.3668, 0.3271],
        [0.6752, 0.2432, 0.2339],
        [0.6940, 0.2471, 0.2273],
        [0.5962, 0.2917, 0.2386],
        [0.6004, 0.3181, 0.2421],
        [0.4559, 0.3839, 0.3600],
        [0.4520, 0.3742, 0.3453],
        [0.5253, 0.3614, 0.2946],
        [0.4638, 0.3784, 0.3446],
        [0.4750, 0.3692, 0.3384],
        [0.4680, 0.3750, 0.3538],
        [0.4718, 0.3795, 0.3398],
        [0.4624, 0.3737, 0.3534],
        [0.4437, 0.3905, 0.3758],
        [0.4534, 0.3826, 0.3542],
        [0.4835, 0.3657, 0.3373],
        [0.4868, 0.3456, 0.3357],
        [0.4791, 0.3742, 0.3371],
        [0.4392, 0.3679, 0.3654],
        [0.4788, 0.3565, 0.3395],
        [0.4838, 0.3562, 0.3357],
        [0.4833, 0.3680, 0.3317],
        [0.4827, 0.3701, 0.3167],
        [0.4750, 0.3534, 0.3380],
        [0.5005, 0.3778, 0.3027],
        [0.5105, 0.3578, 0.2869],
        [0.4623, 0.3912, 0.3507],
        [0.4881, 0.3631, 0.3164],
        [0.4899, 0.3789, 0.3417],
        [0.4390, 0.3929, 0.3790],
        [0.4800, 0.3845, 0.3373],
        [0.4727, 0.3684, 0.3335],
        [0.4490, 0.3820, 0.3648],
        [0.4624, 0.3796, 0.3538],
        [0.4546, 0.3867, 0.3617],
        [0.4723, 0.3834, 0.3607],
        [0.4681, 0.3866, 0.3517],
        [0.4695, 0.3813, 0.3469],
        [0.5114, 0.3613, 0.2772],
        [0.5741, 0.3392, 0.2528],
        [0.5797, 0.3351, 0.2351],
        [0.7070, 0.2414, 0.2260],
        [0.5417, 0.3451, 0.2803],
        [0.5089, 0.3636, 0.3025],
        [0.5224, 0.3653, 0.2983],
        [0.4393, 0.3981, 0.3673],
        [0.4519, 0.3858, 0.3647],
        [0.4763, 0.3669, 0.3446],
        [0.4555, 0.3825, 0.3573],
        [0.4808, 0.3698, 0.3272],
        [0.4799, 0.3679, 0.3335],
        [0.4729, 0.3721, 0.3430],
        [0.4601, 0.3778, 0.3540],
        [0.4644, 0.3698, 0.3362],
        [0.4950, 0.3641, 0.3157],
        [0.4946, 0.3677, 0.3185],
        [0.4548, 0.3605, 0.3544],
        [0.4831, 0.3544, 0.3426],
        [0.4901, 0.3788, 0.3182],
        [0.4571, 0.3780, 0.3576],
        [0.4655, 0.3666, 0.3528],
        [0.5319, 0.3412, 0.2695],
        [0.5254, 0.3560, 0.3068],
        [0.4912, 0.3635, 0.3228]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 113: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 45.0, Max: 6396.0, Mean: 1653.372314453125, Std: 988.813232421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 448.64056396484375, Max: 63905.12890625, Mean: 16518.814453125, Std: 9879.80078125
[DEBUG] Top-3 class probabilities:
tensor([[0.4995, 0.3666, 0.3266],
        [0.4750, 0.3749, 0.3458],
        [0.4347, 0.3855, 0.3840],
        [0.4388, 0.3898, 0.3829],
        [0.4655, 0.3760, 0.3507],
        [0.5185, 0.3591, 0.3071],
        [0.4602, 0.3826, 0.3467],
        [0.4863, 0.3800, 0.3306],
        [0.4815, 0.3782, 0.3434],
        [0.4716, 0.3711, 0.3553],
        [0.4854, 0.3705, 0.3414],
        [0.6554, 0.3004, 0.2052],
        [0.4708, 0.3879, 0.3240],
        [0.4805, 0.3694, 0.3250],
        [0.6415, 0.2669, 0.2434],
        [0.4605, 0.3864, 0.3476],
        [0.4338, 0.4021, 0.3630],
        [0.5966, 0.3159, 0.2517],
        [0.4352, 0.3956, 0.3514],
        [0.4651, 0.3693, 0.3602],
        [0.4588, 0.3837, 0.3610],
        [0.4844, 0.3709, 0.3255],
        [0.4812, 0.3651, 0.3160],
        [0.4814, 0.3686, 0.3236],
        [0.5025, 0.3764, 0.3188],
        [0.5117, 0.3755, 0.3216],
        [0.4859, 0.3729, 0.3159],
        [0.4910, 0.3578, 0.3390],
        [0.4531, 0.3907, 0.3603],
        [0.4494, 0.3685, 0.3630],
        [0.4444, 0.3813, 0.3680],
        [0.5101, 0.3593, 0.3089],
        [0.4799, 0.3681, 0.3306],
        [0.4642, 0.3729, 0.3498],
        [0.5218, 0.3642, 0.2940],
        [0.4723, 0.3767, 0.3404],
        [0.4747, 0.3713, 0.3409],
        [0.5044, 0.3563, 0.3022],
        [0.5077, 0.3660, 0.3103],
        [0.4601, 0.3777, 0.3692],
        [0.4505, 0.3795, 0.3617],
        [0.5032, 0.3584, 0.3111],
        [0.4787, 0.3687, 0.3357],
        [0.4925, 0.3734, 0.3198],
        [0.4594, 0.3786, 0.3608],
        [0.4608, 0.3920, 0.3642],
        [0.4837, 0.3672, 0.3453],
        [0.4428, 0.3881, 0.3774],
        [0.6172, 0.3155, 0.2146],
        [0.5233, 0.3641, 0.2853],
        [0.5151, 0.3707, 0.3110],
        [0.6492, 0.2720, 0.2325],
        [0.5716, 0.3263, 0.2622],
        [0.5153, 0.3488, 0.3019],
        [0.5257, 0.3565, 0.2970],
        [0.4780, 0.3751, 0.3212],
        [0.4388, 0.3921, 0.3687],
        [0.4571, 0.3812, 0.3761],
        [0.4488, 0.3803, 0.3689],
        [0.4850, 0.3662, 0.3298],
        [0.5320, 0.3549, 0.2971],
        [0.4601, 0.3761, 0.3401],
        [0.4682, 0.3875, 0.3375],
        [0.5138, 0.3552, 0.3008],
        [0.5052, 0.3756, 0.3059],
        [0.4800, 0.3829, 0.3392],
        [0.4413, 0.3907, 0.3568],
        [0.4603, 0.3709, 0.3432],
        [0.4653, 0.3648, 0.3513],
        [0.5075, 0.3467, 0.3247],
        [0.5078, 0.3598, 0.2920],
        [0.4924, 0.3676, 0.3285],
        [0.4246, 0.3880, 0.3673],
        [0.4855, 0.3781, 0.3225],
        [0.4901, 0.3699, 0.3154],
        [0.4744, 0.3810, 0.3310],
        [0.4737, 0.3596, 0.3555],
        [0.5242, 0.3494, 0.2973],
        [0.4804, 0.3688, 0.3347],
        [0.5106, 0.3565, 0.3044],
        [0.5134, 0.3668, 0.3028],
        [0.4888, 0.3713, 0.3326],
        [0.4830, 0.3746, 0.3418],
        [0.4749, 0.3756, 0.3559],
        [0.4593, 0.3822, 0.3552],
        [0.5736, 0.3256, 0.2214],
        [0.5407, 0.3532, 0.2578],
        [0.5188, 0.3746, 0.2886],
        [0.6597, 0.2728, 0.2354],
        [0.5291, 0.3482, 0.3018],
        [0.4644, 0.3905, 0.3481],
        [0.4432, 0.3933, 0.3513],
        [0.4907, 0.3794, 0.3181],
        [0.4924, 0.3701, 0.3397],
        [0.4293, 0.3823, 0.3789],
        [0.4568, 0.3769, 0.3541],
        [0.4948, 0.3674, 0.3149],
        [0.4714, 0.3774, 0.3479],
        [0.5036, 0.3656, 0.3076],
        [0.4791, 0.3758, 0.3407],
        [0.4991, 0.3746, 0.3130],
        [0.4939, 0.3588, 0.3242],
        [0.4678, 0.3743, 0.3406],
        [0.4936, 0.3766, 0.3325],
        [0.4747, 0.3781, 0.3397],
        [0.4194, 0.3999, 0.3877],
        [0.4707, 0.3757, 0.3351],
        [0.4662, 0.3763, 0.3470],
        [0.4830, 0.3773, 0.3374],
        [0.4918, 0.3681, 0.3144],
        [0.4706, 0.3756, 0.3527],
        [0.6007, 0.3221, 0.2270],
        [0.4461, 0.3856, 0.3652],
        [0.4795, 0.3794, 0.3357],
        [0.5003, 0.3642, 0.3154],
        [0.5148, 0.3596, 0.3131],
        [0.4726, 0.3778, 0.3554],
        [0.5149, 0.3584, 0.3125],
        [0.4756, 0.3731, 0.3382],
        [0.5121, 0.3628, 0.3090],
        [0.4986, 0.3660, 0.3082],
        [0.4543, 0.3831, 0.3609],
        [0.4749, 0.3869, 0.3088],
        [0.5206, 0.3562, 0.2606],
        [0.5807, 0.3445, 0.2560],
        [0.5729, 0.3294, 0.2669],
        [0.5157, 0.3493, 0.3018],
        [0.4615, 0.3804, 0.3476]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17, 10],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 114: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15960.0, Mean: 1693.419677734375, Std: 991.4462890625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 159464.546875, Mean: 16918.94921875, Std: 9906.109375
[DEBUG] Top-3 class probabilities:
tensor([[0.4499, 0.3905, 0.3576],
        [0.5163, 0.3621, 0.3081],
        [0.4271, 0.3942, 0.3895],
        [0.4753, 0.3599, 0.3499],
        [0.4462, 0.3929, 0.3327],
        [0.4625, 0.3831, 0.3344],
        [0.4834, 0.3689, 0.3197],
        [0.4864, 0.3753, 0.3255],
        [0.4573, 0.3796, 0.3596],
        [0.4580, 0.3866, 0.3274],
        [0.4621, 0.3726, 0.3598],
        [0.4408, 0.3864, 0.3649],
        [0.4628, 0.3858, 0.3400],
        [0.5150, 0.3660, 0.3066],
        [0.4802, 0.3681, 0.3252],
        [0.5239, 0.3532, 0.2971],
        [0.4780, 0.3703, 0.3306],
        [0.4892, 0.3819, 0.3200],
        [0.4921, 0.3588, 0.3146],
        [0.5014, 0.3704, 0.3098],
        [0.5278, 0.3594, 0.2796],
        [0.4831, 0.3648, 0.3224],
        [0.5307, 0.3586, 0.2942],
        [0.5192, 0.3681, 0.3142],
        [0.5220, 0.3676, 0.3148],
        [0.4671, 0.3778, 0.3575],
        [0.5268, 0.3555, 0.2997],
        [0.4980, 0.3687, 0.3222],
        [0.4942, 0.3783, 0.3257],
        [0.4597, 0.3895, 0.3498],
        [0.4770, 0.3758, 0.3385],
        [0.4470, 0.3937, 0.3460],
        [0.5095, 0.3666, 0.2810],
        [0.6006, 0.3283, 0.2381],
        [0.6882, 0.2574, 0.2484],
        [0.5620, 0.3312, 0.2596],
        [0.4546, 0.3975, 0.3272],
        [0.4826, 0.3713, 0.3402],
        [0.4790, 0.3632, 0.3550],
        [0.5131, 0.3432, 0.3216],
        [0.5352, 0.3338, 0.2960],
        [0.4926, 0.3597, 0.3259],
        [0.4722, 0.3963, 0.3399],
        [0.4812, 0.3704, 0.3158],
        [0.5102, 0.3651, 0.2964],
        [0.4459, 0.3871, 0.3649],
        [0.4600, 0.3772, 0.3581],
        [0.4333, 0.3868, 0.3644],
        [0.4264, 0.3874, 0.3692],
        [0.4562, 0.3789, 0.3456],
        [0.5048, 0.3694, 0.2926],
        [0.5669, 0.3490, 0.2484],
        [0.5441, 0.3481, 0.2688],
        [0.5305, 0.3620, 0.2851],
        [0.4845, 0.3633, 0.3290],
        [0.4932, 0.3720, 0.3137],
        [0.4650, 0.3846, 0.3482],
        [0.4893, 0.3703, 0.3408],
        [0.5244, 0.3691, 0.2998],
        [0.4800, 0.3840, 0.3411],
        [0.4905, 0.3721, 0.3332],
        [0.5034, 0.3774, 0.3292],
        [0.4563, 0.3736, 0.3580],
        [0.4919, 0.3654, 0.3238],
        [0.4974, 0.3790, 0.3255],
        [0.4875, 0.3855, 0.3296],
        [0.4913, 0.3759, 0.3182],
        [0.4947, 0.3611, 0.3130],
        [0.5314, 0.3491, 0.2489],
        [0.5712, 0.3282, 0.2351],
        [0.5042, 0.3727, 0.3102],
        [0.7104, 0.2482, 0.2288],
        [0.4717, 0.3897, 0.3152],
        [0.4908, 0.3827, 0.3061],
        [0.4640, 0.3740, 0.3484],
        [0.5084, 0.3463, 0.3144],
        [0.5620, 0.3247, 0.2641],
        [0.5391, 0.3327, 0.2873],
        [0.5396, 0.3339, 0.2882],
        [0.5450, 0.3392, 0.2688],
        [0.5301, 0.3549, 0.2805],
        [0.4545, 0.3802, 0.3449],
        [0.4550, 0.3842, 0.3417],
        [0.4648, 0.3801, 0.3395],
        [0.4341, 0.4029, 0.3754],
        [0.5665, 0.3533, 0.2686],
        [0.4399, 0.3933, 0.3610],
        [0.4770, 0.3741, 0.3145],
        [0.5090, 0.3621, 0.2980],
        [0.4783, 0.3807, 0.3143],
        [0.4832, 0.3704, 0.3283],
        [0.4914, 0.3681, 0.3008],
        [0.4763, 0.3756, 0.3422],
        [0.5149, 0.3715, 0.3088],
        [0.4832, 0.3683, 0.3266],
        [0.4650, 0.3793, 0.3481],
        [0.4451, 0.4041, 0.3779],
        [0.4738, 0.3774, 0.3513],
        [0.4991, 0.3659, 0.3319],
        [0.5027, 0.3670, 0.3192],
        [0.5084, 0.3660, 0.3210],
        [0.4683, 0.3854, 0.3440],
        [0.4761, 0.3706, 0.3365],
        [0.4776, 0.3842, 0.3283],
        [0.4930, 0.3688, 0.3191],
        [0.4872, 0.3697, 0.2768],
        [0.4566, 0.3896, 0.3582],
        [0.4062, 0.3974, 0.3849],
        [0.7365, 0.2492, 0.2008],
        [0.5930, 0.3266, 0.2384],
        [0.4614, 0.4052, 0.3509],
        [0.4644, 0.3910, 0.3361],
        [0.4555, 0.4000, 0.3625],
        [0.5375, 0.3348, 0.2888],
        [0.4765, 0.3579, 0.3223],
        [0.5167, 0.3565, 0.2883],
        [0.4910, 0.3820, 0.3141],
        [0.4816, 0.3746, 0.3223],
        [0.4569, 0.3835, 0.3536],
        [0.4584, 0.3866, 0.3365],
        [0.4856, 0.3774, 0.3338],
        [0.4468, 0.3884, 0.3684],
        [0.4465, 0.3845, 0.3503],
        [0.4919, 0.3795, 0.3128],
        [0.5085, 0.3756, 0.2860],
        [0.5529, 0.3578, 0.2585],
        [0.4985, 0.3728, 0.3233],
        [0.4816, 0.3829, 0.3280]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 115: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 35.0, Max: 10520.0, Mean: 1655.0133056640625, Std: 1032.2938232421875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 348.7248229980469, Max: 105110.390625, Mean: 16535.208984375, Std: 10314.240234375
[DEBUG] Top-3 class probabilities:
tensor([[0.4791, 0.3683, 0.3339],
        [0.4461, 0.3868, 0.3741],
        [0.4268, 0.3848, 0.3768],
        [0.4734, 0.3656, 0.3364],
        [0.4758, 0.3697, 0.3339],
        [0.4392, 0.3776, 0.3745],
        [0.4535, 0.3943, 0.3578],
        [0.4564, 0.3769, 0.3478],
        [0.4802, 0.3657, 0.3318],
        [0.5101, 0.3507, 0.3098],
        [0.4491, 0.3809, 0.3528],
        [0.4617, 0.3847, 0.3525],
        [0.4671, 0.3810, 0.3536],
        [0.4513, 0.3888, 0.3663],
        [0.5111, 0.3621, 0.2731],
        [0.4949, 0.3675, 0.3062],
        [0.7264, 0.2394, 0.2240],
        [0.5624, 0.3421, 0.2580],
        [0.4647, 0.3706, 0.3415],
        [0.4747, 0.3870, 0.3248],
        [0.5073, 0.3697, 0.3083],
        [0.5109, 0.3574, 0.3148],
        [0.4635, 0.3794, 0.3453],
        [0.5444, 0.3483, 0.2735],
        [0.5038, 0.3622, 0.3172],
        [0.5410, 0.3381, 0.2925],
        [0.5091, 0.3518, 0.3134],
        [0.4885, 0.3695, 0.3317],
        [0.5060, 0.3604, 0.3179],
        [0.4570, 0.3964, 0.3653],
        [0.5216, 0.3649, 0.3103],
        [0.5114, 0.3597, 0.3117],
        [0.5262, 0.3670, 0.2967],
        [0.5119, 0.3778, 0.3023],
        [0.5648, 0.3501, 0.2616],
        [0.4556, 0.3905, 0.3410],
        [0.4687, 0.3800, 0.3504],
        [0.4333, 0.3936, 0.3868],
        [0.4620, 0.3782, 0.3485],
        [0.4793, 0.3722, 0.3401],
        [0.4481, 0.3847, 0.3817],
        [0.4684, 0.3638, 0.3548],
        [0.4682, 0.3652, 0.3453],
        [0.4567, 0.3743, 0.3697],
        [0.4868, 0.3679, 0.3339],
        [0.4999, 0.3708, 0.3317],
        [0.4389, 0.4010, 0.3657],
        [0.4706, 0.3751, 0.3383],
        [0.5154, 0.3695, 0.3129],
        [0.4552, 0.3797, 0.3446],
        [0.4567, 0.3916, 0.3426],
        [0.4587, 0.3981, 0.3385],
        [0.6945, 0.2518, 0.2356],
        [0.4772, 0.3730, 0.3460],
        [0.5411, 0.3363, 0.2782],
        [0.5138, 0.3537, 0.3093],
        [0.5529, 0.3358, 0.2731],
        [0.4987, 0.3610, 0.3084],
        [0.5490, 0.3535, 0.2799],
        [0.5183, 0.3903, 0.3104],
        [0.5578, 0.3203, 0.2719],
        [0.5396, 0.3272, 0.2785],
        [0.4804, 0.3786, 0.3312],
        [0.4848, 0.3639, 0.3281],
        [0.4527, 0.3838, 0.3417],
        [0.4719, 0.3612, 0.3362],
        [0.4693, 0.3721, 0.3545],
        [0.4563, 0.3666, 0.3566],
        [0.4761, 0.3752, 0.3319],
        [0.4870, 0.3759, 0.3374],
        [0.4862, 0.3832, 0.3284],
        [0.4696, 0.3862, 0.3375],
        [0.4140, 0.4060, 0.3958],
        [0.4074, 0.4027, 0.3977],
        [0.5146, 0.3667, 0.3085],
        [0.4455, 0.3776, 0.3747],
        [0.4374, 0.3843, 0.3834],
        [0.4438, 0.3768, 0.3692],
        [0.4926, 0.3484, 0.3427],
        [0.5220, 0.3452, 0.3077],
        [0.5083, 0.3535, 0.3183],
        [0.4534, 0.3789, 0.3553],
        [0.4555, 0.3801, 0.3563],
        [0.4461, 0.3942, 0.3635],
        [0.4940, 0.3764, 0.3289],
        [0.4756, 0.3636, 0.3452],
        [0.4913, 0.3601, 0.3083],
        [0.4209, 0.3861, 0.3775],
        [0.7201, 0.2530, 0.2175],
        [0.4871, 0.3633, 0.3154],
        [0.4595, 0.4038, 0.3373],
        [0.5129, 0.3690, 0.3185],
        [0.5218, 0.3579, 0.2908],
        [0.5394, 0.3460, 0.2789],
        [0.4935, 0.3647, 0.3116],
        [0.5051, 0.3568, 0.3175],
        [0.5589, 0.3289, 0.2692],
        [0.5565, 0.3330, 0.2638],
        [0.5093, 0.3596, 0.3014],
        [0.5046, 0.3595, 0.3114],
        [0.4908, 0.3655, 0.3291],
        [0.5044, 0.3545, 0.3149],
        [0.4796, 0.3590, 0.3407],
        [0.5016, 0.3473, 0.3266],
        [0.4090, 0.4031, 0.3918],
        [0.4872, 0.3741, 0.3282],
        [0.4916, 0.3849, 0.3163],
        [0.5119, 0.3566, 0.3050],
        [0.4573, 0.3922, 0.3584],
        [0.4168, 0.4104, 0.3917],
        [0.4091, 0.4023, 0.3961],
        [0.4446, 0.3964, 0.3862],
        [0.4321, 0.3954, 0.3814],
        [0.4707, 0.3702, 0.3569],
        [0.4870, 0.3551, 0.3328],
        [0.5114, 0.3601, 0.3178],
        [0.5143, 0.3487, 0.3052],
        [0.5019, 0.3577, 0.3278],
        [0.4964, 0.3575, 0.3220],
        [0.4935, 0.3621, 0.3316],
        [0.4793, 0.3709, 0.3441],
        [0.4912, 0.3548, 0.3288],
        [0.6240, 0.2977, 0.2267],
        [0.4493, 0.4037, 0.3339],
        [0.4747, 0.3899, 0.3178],
        [0.4318, 0.4106, 0.3547],
        [0.4375, 0.4122, 0.3576],
        [0.4772, 0.3858, 0.3442]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [17, 11,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17,  6, 11],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 116: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 31.0, Max: 7540.0, Mean: 1553.4066162109375, Std: 1097.5875244140625
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 308.758544921875, Max: 75335.4921875, Mean: 15519.998046875, Std: 10966.626953125
[DEBUG] Top-3 class probabilities:
tensor([[0.4752, 0.3718, 0.3516],
        [0.5133, 0.3481, 0.3059],
        [0.5674, 0.3174, 0.2507],
        [0.4848, 0.3690, 0.3274],
        [0.4852, 0.3720, 0.3352],
        [0.4835, 0.3621, 0.3327],
        [0.5311, 0.3403, 0.2978],
        [0.5317, 0.3289, 0.2941],
        [0.4810, 0.3803, 0.3376],
        [0.4659, 0.3756, 0.3606],
        [0.4440, 0.3818, 0.3771],
        [0.4598, 0.3803, 0.3437],
        [0.4763, 0.3891, 0.3438],
        [0.4492, 0.3890, 0.3560],
        [0.4296, 0.4000, 0.3720],
        [0.4255, 0.4015, 0.3900],
        [0.4562, 0.3747, 0.3619],
        [0.4878, 0.3543, 0.3341],
        [0.4895, 0.3580, 0.3396],
        [0.5464, 0.3328, 0.2825],
        [0.4938, 0.3722, 0.3211],
        [0.4953, 0.3607, 0.3208],
        [0.5463, 0.3389, 0.2762],
        [0.5768, 0.3220, 0.2614],
        [0.5129, 0.3531, 0.3141],
        [0.5353, 0.3275, 0.2882],
        [0.5152, 0.3482, 0.3051],
        [0.6833, 0.2443, 0.2385],
        [0.4900, 0.3646, 0.3173],
        [0.4820, 0.3869, 0.3203],
        [0.4615, 0.4075, 0.3410],
        [0.4297, 0.4040, 0.3740],
        [0.4562, 0.3749, 0.3459],
        [0.4561, 0.3892, 0.3505],
        [0.4583, 0.3819, 0.3612],
        [0.5373, 0.3304, 0.2793],
        [0.5226, 0.3497, 0.3101],
        [0.5012, 0.3714, 0.3167],
        [0.5078, 0.3584, 0.3032],
        [0.4607, 0.3699, 0.3569],
        [0.4667, 0.3816, 0.3292],
        [0.4139, 0.4090, 0.3716],
        [0.4473, 0.3916, 0.3521],
        [0.4292, 0.4029, 0.3835],
        [0.4199, 0.3970, 0.3941],
        [0.4646, 0.3843, 0.3413],
        [0.4866, 0.3717, 0.3412],
        [0.4657, 0.3699, 0.3573],
        [0.4562, 0.3753, 0.3645],
        [0.4710, 0.3612, 0.3426],
        [0.5128, 0.3465, 0.3109],
        [0.5135, 0.3519, 0.3118],
        [0.5331, 0.3416, 0.2982],
        [0.5338, 0.3360, 0.2839],
        [0.5898, 0.3144, 0.2409],
        [0.5398, 0.3411, 0.2853],
        [0.5505, 0.3304, 0.2876],
        [0.5489, 0.3392, 0.2771],
        [0.5710, 0.3210, 0.2488],
        [0.4491, 0.3805, 0.3789],
        [0.6875, 0.2655, 0.2280],
        [0.6016, 0.3002, 0.2456],
        [0.4858, 0.3789, 0.3306],
        [0.4839, 0.3653, 0.3193],
        [0.4987, 0.3820, 0.3199],
        [0.4549, 0.3824, 0.3541],
        [0.5096, 0.3636, 0.3263],
        [0.5195, 0.3535, 0.2944],
        [0.4947, 0.3629, 0.3243],
        [0.4841, 0.3726, 0.3455],
        [0.4439, 0.3787, 0.3573],
        [0.4687, 0.3792, 0.3492],
        [0.4456, 0.3926, 0.3515],
        [0.4195, 0.3968, 0.3841],
        [0.4179, 0.4011, 0.3685],
        [0.4025, 0.4021, 0.3900],
        [0.4177, 0.3914, 0.3827],
        [0.4453, 0.3789, 0.3769],
        [0.4365, 0.3897, 0.3855],
        [0.4551, 0.3867, 0.3557],
        [0.4542, 0.3842, 0.3636],
        [0.4711, 0.3772, 0.3635],
        [0.5107, 0.3376, 0.3166],
        [0.5168, 0.3475, 0.2985],
        [0.5621, 0.3270, 0.2589],
        [0.5697, 0.3181, 0.2471],
        [0.5727, 0.3281, 0.2577],
        [0.5161, 0.3506, 0.3008],
        [0.5385, 0.3413, 0.2956],
        [0.5539, 0.3358, 0.2786],
        [0.4883, 0.3631, 0.3334],
        [0.5595, 0.3217, 0.2695],
        [0.6803, 0.2574, 0.2381],
        [0.7039, 0.2577, 0.2148],
        [0.6900, 0.2625, 0.2356],
        [0.6558, 0.2575, 0.2550],
        [0.5599, 0.3221, 0.2713],
        [0.4376, 0.4071, 0.3654],
        [0.4856, 0.3765, 0.3153],
        [0.4692, 0.3810, 0.3447],
        [0.4326, 0.3900, 0.3829],
        [0.4241, 0.4003, 0.3915],
        [0.4611, 0.3738, 0.3624],
        [0.4380, 0.3855, 0.3798],
        [0.4286, 0.3878, 0.3848],
        [0.4701, 0.3677, 0.3612],
        [0.4671, 0.3744, 0.3638],
        [0.4755, 0.3636, 0.3532],
        [0.4906, 0.3589, 0.3472],
        [0.5002, 0.3543, 0.3294],
        [0.5254, 0.3359, 0.2971],
        [0.5285, 0.3382, 0.2766],
        [0.4831, 0.3687, 0.3428],
        [0.4641, 0.3732, 0.3375],
        [0.4862, 0.3696, 0.3380],
        [0.4928, 0.3537, 0.3266],
        [0.5275, 0.3466, 0.3035],
        [0.5598, 0.3352, 0.2763],
        [0.5286, 0.3500, 0.2975],
        [0.5748, 0.3236, 0.2431],
        [0.7220, 0.2543, 0.1997],
        [0.5772, 0.3342, 0.2581],
        [0.4414, 0.3862, 0.3641],
        [0.4436, 0.3965, 0.3695],
        [0.4686, 0.3625, 0.3533],
        [0.4582, 0.3721, 0.3717],
        [0.4577, 0.3688, 0.3684],
        [0.4622, 0.3681, 0.3644]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  8, 17],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [17, 11,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11,  6, 17],
        [11,  6, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 117: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 21.0, Max: 6311.0, Mean: 1583.325439453125, Std: 1138.0927734375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 208.84280395507812, Max: 63055.84375, Mean: 15818.93359375, Std: 11371.337890625
[DEBUG] Top-3 class probabilities:
tensor([[0.4470, 0.3812, 0.3799],
        [0.5240, 0.3388, 0.3102],
        [0.5272, 0.3444, 0.3041],
        [0.4848, 0.3610, 0.3402],
        [0.5197, 0.3482, 0.2998],
        [0.5260, 0.3381, 0.3027],
        [0.5829, 0.3219, 0.2467],
        [0.5070, 0.3746, 0.3178],
        [0.5098, 0.3607, 0.3183],
        [0.5081, 0.3519, 0.3115],
        [0.4632, 0.3722, 0.3455],
        [0.4912, 0.3704, 0.3267],
        [0.5187, 0.3501, 0.3090],
        [0.5827, 0.3200, 0.2535],
        [0.5589, 0.3229, 0.2642],
        [0.4697, 0.3860, 0.3437],
        [0.7326, 0.2341, 0.1884],
        [0.5540, 0.3248, 0.2899],
        [0.4567, 0.3819, 0.3723],
        [0.4781, 0.3643, 0.3492],
        [0.4834, 0.3530, 0.3344],
        [0.4777, 0.3567, 0.3376],
        [0.4887, 0.3577, 0.3416],
        [0.5079, 0.3493, 0.3134],
        [0.5052, 0.3558, 0.3164],
        [0.4943, 0.3673, 0.3083],
        [0.4836, 0.3747, 0.3635],
        [0.5083, 0.3504, 0.3063],
        [0.5180, 0.3461, 0.3057],
        [0.4900, 0.3712, 0.3404],
        [0.4458, 0.3821, 0.3569],
        [0.4684, 0.3757, 0.3594],
        [0.4941, 0.3681, 0.3352],
        [0.5146, 0.3540, 0.3136],
        [0.5053, 0.3533, 0.3239],
        [0.5191, 0.3495, 0.3174],
        [0.5074, 0.3547, 0.3141],
        [0.5006, 0.3493, 0.3117],
        [0.5562, 0.3397, 0.2725],
        [0.7190, 0.2359, 0.2069],
        [0.4777, 0.3500, 0.3481],
        [0.4640, 0.3618, 0.3536],
        [0.4243, 0.3946, 0.3874],
        [0.4802, 0.3581, 0.3384],
        [0.5008, 0.3566, 0.3293],
        [0.4549, 0.3919, 0.3496],
        [0.4735, 0.3644, 0.3361],
        [0.4617, 0.3616, 0.3426],
        [0.5420, 0.3338, 0.2690],
        [0.4932, 0.3663, 0.3159],
        [0.5196, 0.3445, 0.3064],
        [0.5006, 0.3615, 0.3227],
        [0.4497, 0.3845, 0.3731],
        [0.4303, 0.3886, 0.3805],
        [0.4723, 0.3717, 0.3528],
        [0.4613, 0.3676, 0.3562],
        [0.5245, 0.3512, 0.2984],
        [0.5001, 0.3662, 0.3116],
        [0.5179, 0.3542, 0.3174],
        [0.4989, 0.3555, 0.3326],
        [0.4829, 0.3675, 0.3368],
        [0.5590, 0.3321, 0.2627],
        [0.5635, 0.3277, 0.2620],
        [0.6752, 0.2496, 0.2397],
        [0.4692, 0.3673, 0.3483],
        [0.4657, 0.3723, 0.3627],
        [0.4507, 0.3871, 0.3651],
        [0.4611, 0.3736, 0.3471],
        [0.4979, 0.3657, 0.3108],
        [0.5234, 0.3550, 0.2978],
        [0.4615, 0.3777, 0.3628],
        [0.4203, 0.4009, 0.3915],
        [0.4842, 0.3669, 0.3277],
        [0.4835, 0.3798, 0.3320],
        [0.4381, 0.3904, 0.3812],
        [0.4523, 0.3868, 0.3717],
        [0.4544, 0.3773, 0.3745],
        [0.5111, 0.3518, 0.3158],
        [0.4878, 0.3595, 0.3306],
        [0.5117, 0.3493, 0.3108],
        [0.6089, 0.3234, 0.2302],
        [0.4769, 0.3800, 0.3302],
        [0.5306, 0.3394, 0.2941],
        [0.5261, 0.3421, 0.3005],
        [0.5108, 0.3565, 0.3117],
        [0.5244, 0.3520, 0.2911],
        [0.5351, 0.3476, 0.2898],
        [0.6222, 0.2724, 0.2337],
        [0.5003, 0.3480, 0.3332],
        [0.5012, 0.3524, 0.3309],
        [0.4913, 0.3513, 0.3212],
        [0.5262, 0.3450, 0.2803],
        [0.4929, 0.3590, 0.3192],
        [0.4577, 0.3801, 0.3488],
        [0.4677, 0.3673, 0.3442],
        [0.5127, 0.3464, 0.3034],
        [0.4498, 0.3798, 0.3694],
        [0.4394, 0.3948, 0.3821],
        [0.4552, 0.3838, 0.3683],
        [0.4258, 0.3928, 0.3868],
        [0.4869, 0.3675, 0.3428],
        [0.5138, 0.3551, 0.3058],
        [0.4979, 0.3599, 0.3195],
        [0.4560, 0.3869, 0.3555],
        [0.4314, 0.4058, 0.3618],
        [0.4863, 0.3744, 0.3131],
        [0.4805, 0.3723, 0.3499],
        [0.5380, 0.3360, 0.2939],
        [0.5293, 0.3550, 0.3082],
        [0.5470, 0.3559, 0.2887],
        [0.5285, 0.3497, 0.2953],
        [0.6032, 0.3082, 0.2322],
        [0.4612, 0.3720, 0.3626],
        [0.5048, 0.3479, 0.3215],
        [0.4961, 0.3592, 0.3245],
        [0.5367, 0.3391, 0.2764],
        [0.4793, 0.3611, 0.3485],
        [0.4917, 0.3525, 0.3122],
        [0.5504, 0.3335, 0.2675],
        [0.4499, 0.3857, 0.3636],
        [0.4399, 0.3855, 0.3833],
        [0.5341, 0.3437, 0.2924],
        [0.4585, 0.3759, 0.3461],
        [0.5281, 0.3385, 0.2867],
        [0.4862, 0.3609, 0.3316],
        [0.5010, 0.3571, 0.3203],
        [0.4824, 0.3580, 0.3331],
        [0.5131, 0.3475, 0.3151]])
[DEBUG] Top-3 class indices:
tensor([[11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 118: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 35.0, Max: 6464.0, Mean: 1606.150390625, Std: 1162.963623046875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 348.7248229980469, Max: 64584.5546875, Mean: 16046.9921875, Std: 11619.8369140625
[DEBUG] Top-3 class probabilities:
tensor([[0.4521, 0.3790, 0.3597],
        [0.4758, 0.3744, 0.3327],
        [0.5370, 0.3549, 0.2899],
        [0.4806, 0.3685, 0.3319],
        [0.5251, 0.3462, 0.3130],
        [0.5240, 0.3519, 0.3025],
        [0.4941, 0.3642, 0.3187],
        [0.6416, 0.2971, 0.2164],
        [0.4836, 0.3737, 0.3286],
        [0.4815, 0.3682, 0.3536],
        [0.4875, 0.3615, 0.3316],
        [0.4998, 0.3559, 0.3204],
        [0.4755, 0.3727, 0.3358],
        [0.5020, 0.3572, 0.3105],
        [0.5252, 0.3437, 0.2979],
        [0.5028, 0.3505, 0.3132],
        [0.5091, 0.3547, 0.3084],
        [0.4863, 0.3655, 0.3283],
        [0.5128, 0.3527, 0.2957],
        [0.5179, 0.3483, 0.2988],
        [0.4991, 0.3514, 0.3229],
        [0.4931, 0.3636, 0.3266],
        [0.5221, 0.3499, 0.3059],
        [0.5076, 0.3522, 0.3041],
        [0.4807, 0.3682, 0.3335],
        [0.4647, 0.3778, 0.3524],
        [0.5265, 0.3491, 0.3060],
        [0.5409, 0.3356, 0.2786],
        [0.5746, 0.3278, 0.2450],
        [0.5777, 0.3220, 0.2562],
        [0.5807, 0.3228, 0.2640],
        [0.4548, 0.3807, 0.3653],
        [0.5283, 0.3537, 0.3165],
        [0.4844, 0.3569, 0.3286],
        [0.5094, 0.3596, 0.3146],
        [0.5004, 0.3585, 0.3006],
        [0.5078, 0.3617, 0.3196],
        [0.5455, 0.3386, 0.2749],
        [0.5028, 0.3533, 0.3156],
        [0.5188, 0.3446, 0.2988],
        [0.5804, 0.3189, 0.2386],
        [0.5111, 0.3571, 0.3076],
        [0.5015, 0.3608, 0.3195],
        [0.4651, 0.3832, 0.3536],
        [0.4819, 0.3753, 0.3460],
        [0.4604, 0.3810, 0.3473],
        [0.4856, 0.3745, 0.3501],
        [0.5169, 0.3485, 0.2998],
        [0.5046, 0.3564, 0.3143],
        [0.5201, 0.3682, 0.3195],
        [0.5024, 0.3586, 0.3369],
        [0.4865, 0.3516, 0.3390],
        [0.4714, 0.3803, 0.3378],
        [0.5343, 0.3314, 0.3043],
        [0.5221, 0.3540, 0.3074],
        [0.5873, 0.3137, 0.2432],
        [0.5004, 0.3637, 0.3193],
        [0.4936, 0.3605, 0.3233],
        [0.5032, 0.3705, 0.3176],
        [0.5267, 0.3555, 0.3013],
        [0.5601, 0.3377, 0.2755],
        [0.4951, 0.3585, 0.3226],
        [0.4495, 0.3805, 0.3538],
        [0.5028, 0.3637, 0.3033],
        [0.5344, 0.3340, 0.2859],
        [0.5012, 0.3512, 0.3178],
        [0.4741, 0.3729, 0.3432],
        [0.4716, 0.3741, 0.3509],
        [0.4850, 0.3610, 0.3373],
        [0.4799, 0.3825, 0.3249],
        [0.4864, 0.3782, 0.3344],
        [0.4941, 0.3581, 0.3323],
        [0.4786, 0.3720, 0.3566],
        [0.5310, 0.3414, 0.2955],
        [0.5540, 0.3271, 0.2792],
        [0.5018, 0.3562, 0.3224],
        [0.6509, 0.2687, 0.2397],
        [0.5237, 0.3568, 0.2948],
        [0.4634, 0.3716, 0.3586],
        [0.5155, 0.3394, 0.2957],
        [0.4977, 0.3596, 0.3275],
        [0.5124, 0.3642, 0.3073],
        [0.5558, 0.3373, 0.2675],
        [0.4999, 0.3580, 0.3159],
        [0.5412, 0.3487, 0.2943],
        [0.4634, 0.3791, 0.3511],
        [0.4574, 0.3726, 0.3367],
        [0.5071, 0.3560, 0.3126],
        [0.4544, 0.3842, 0.3630],
        [0.5196, 0.3526, 0.3123],
        [0.5131, 0.3560, 0.3110],
        [0.5145, 0.3585, 0.3093],
        [0.4659, 0.3673, 0.3422],
        [0.4871, 0.3752, 0.3387],
        [0.4586, 0.3972, 0.3614],
        [0.5277, 0.3384, 0.3083],
        [0.5230, 0.3482, 0.2960],
        [0.5147, 0.3490, 0.3039],
        [0.5091, 0.3619, 0.3051],
        [0.5025, 0.3640, 0.3110],
        [0.7143, 0.2518, 0.2201],
        [0.4726, 0.3666, 0.3568],
        [0.5288, 0.3336, 0.2931],
        [0.5072, 0.3549, 0.3143],
        [0.5276, 0.3377, 0.2893],
        [0.4941, 0.3628, 0.3288],
        [0.4756, 0.3667, 0.3365],
        [0.5534, 0.3402, 0.2802],
        [0.5288, 0.3400, 0.2914],
        [0.4845, 0.3671, 0.3285],
        [0.5093, 0.3610, 0.3109],
        [0.5090, 0.3488, 0.3196],
        [0.4741, 0.3717, 0.3471],
        [0.4929, 0.3657, 0.3284],
        [0.4828, 0.3508, 0.3361],
        [0.4833, 0.3646, 0.3256],
        [0.4630, 0.3703, 0.3459],
        [0.4893, 0.3652, 0.3253],
        [0.5289, 0.3387, 0.3039],
        [0.5376, 0.3363, 0.2946],
        [0.5037, 0.3589, 0.3122],
        [0.5618, 0.3212, 0.2719],
        [0.5375, 0.3440, 0.2820],
        [0.5173, 0.3457, 0.2990],
        [0.7139, 0.2617, 0.2141],
        [0.4965, 0.3556, 0.3250],
        [0.4818, 0.3665, 0.3342],
        [0.4713, 0.3667, 0.3393]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 119: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 11.0, Max: 6000.0, Mean: 1583.024169921875, Std: 1268.4901123046875
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 108.92705535888672, Max: 59948.46484375, Mean: 15815.921875, Std: 12674.212890625
[DEBUG] Top-3 class probabilities:
tensor([[0.5019, 0.3559, 0.3176],
        [0.4805, 0.3682, 0.3291],
        [0.4882, 0.3710, 0.3328],
        [0.5210, 0.3440, 0.3072],
        [0.4796, 0.3877, 0.3445],
        [0.5168, 0.3561, 0.3051],
        [0.4886, 0.3633, 0.3373],
        [0.4944, 0.3657, 0.3318],
        [0.4776, 0.3677, 0.3345],
        [0.5234, 0.3421, 0.3023],
        [0.4991, 0.3486, 0.3208],
        [0.5055, 0.3498, 0.3265],
        [0.4757, 0.3772, 0.3427],
        [0.4917, 0.3621, 0.3429],
        [0.5255, 0.3473, 0.3020],
        [0.5219, 0.3429, 0.3011],
        [0.5438, 0.3399, 0.2817],
        [0.5070, 0.3642, 0.3359],
        [0.5065, 0.3574, 0.3147],
        [0.5006, 0.3603, 0.3135],
        [0.6682, 0.2532, 0.2504],
        [0.5034, 0.3610, 0.3200],
        [0.5411, 0.3375, 0.2835],
        [0.4898, 0.3654, 0.3206],
        [0.5361, 0.3345, 0.2748],
        [0.5020, 0.3657, 0.3072],
        [0.4817, 0.3588, 0.3253],
        [0.5290, 0.3498, 0.2727],
        [0.5123, 0.3532, 0.3079],
        [0.5405, 0.3414, 0.2749],
        [0.4762, 0.3733, 0.3325],
        [0.4998, 0.3527, 0.3308],
        [0.5327, 0.3438, 0.2901],
        [0.5069, 0.3584, 0.3284],
        [0.5389, 0.3370, 0.2737],
        [0.4873, 0.3546, 0.3343],
        [0.4905, 0.3622, 0.3426],
        [0.5264, 0.3433, 0.2992],
        [0.4608, 0.3790, 0.3602],
        [0.5008, 0.3573, 0.3198],
        [0.4827, 0.3594, 0.3322],
        [0.5050, 0.3500, 0.3125],
        [0.5419, 0.3406, 0.2885],
        [0.7295, 0.2556, 0.1996],
        [0.5509, 0.3342, 0.2891],
        [0.5550, 0.3391, 0.2656],
        [0.5020, 0.3580, 0.3146],
        [0.4827, 0.3692, 0.3318],
        [0.5104, 0.3550, 0.3207],
        [0.5154, 0.3544, 0.3205],
        [0.5698, 0.3423, 0.2711],
        [0.5689, 0.3382, 0.2596],
        [0.5605, 0.3410, 0.2563],
        [0.5520, 0.3393, 0.2715],
        [0.5245, 0.3426, 0.2883],
        [0.5048, 0.3573, 0.3104],
        [0.5300, 0.3437, 0.2958],
        [0.5240, 0.3557, 0.3043],
        [0.4947, 0.3576, 0.3290],
        [0.5278, 0.3402, 0.3037],
        [0.4972, 0.3584, 0.3222],
        [0.4886, 0.3695, 0.3378],
        [0.5331, 0.3384, 0.2799],
        [0.4841, 0.3568, 0.3313],
        [0.4823, 0.3637, 0.3377],
        [0.5265, 0.3430, 0.3072],
        [0.6902, 0.2484, 0.2395],
        [0.5066, 0.3552, 0.3271],
        [0.5017, 0.3544, 0.3322],
        [0.4906, 0.3647, 0.3331],
        [0.5459, 0.3343, 0.2773],
        [0.5391, 0.3503, 0.2911],
        [0.5689, 0.3305, 0.2626],
        [0.5629, 0.3346, 0.2570],
        [0.5343, 0.3538, 0.2773],
        [0.5613, 0.3385, 0.2672],
        [0.5842, 0.3191, 0.2435],
        [0.4880, 0.3666, 0.3235],
        [0.5977, 0.3089, 0.2343],
        [0.4825, 0.3652, 0.3416],
        [0.5121, 0.3542, 0.3121],
        [0.5719, 0.3260, 0.2589],
        [0.5059, 0.3521, 0.3231],
        [0.5073, 0.3580, 0.3171],
        [0.5337, 0.3368, 0.2846],
        [0.5639, 0.3329, 0.2558],
        [0.5089, 0.3585, 0.2977],
        [0.4837, 0.3676, 0.3251],
        [0.5058, 0.3558, 0.3134],
        [0.6933, 0.2444, 0.2326],
        [0.4834, 0.3654, 0.3406],
        [0.5060, 0.3487, 0.3129],
        [0.5357, 0.3478, 0.2929],
        [0.5923, 0.3224, 0.2363],
        [0.5235, 0.3536, 0.2983],
        [0.5180, 0.3571, 0.3098],
        [0.6023, 0.3117, 0.2301],
        [0.5076, 0.3644, 0.2894],
        [0.5480, 0.3452, 0.2710],
        [0.5642, 0.3276, 0.2568],
        [0.5369, 0.3416, 0.2740],
        [0.4834, 0.3688, 0.3411],
        [0.5156, 0.3518, 0.3125],
        [0.5474, 0.3436, 0.2677],
        [0.5015, 0.3680, 0.3131],
        [0.5430, 0.3410, 0.2883],
        [0.5754, 0.3298, 0.2603],
        [0.4798, 0.3659, 0.3317],
        [0.5216, 0.3523, 0.2972],
        [0.5554, 0.3345, 0.2730],
        [0.5440, 0.3406, 0.2712],
        [0.4929, 0.3680, 0.3216],
        [0.6961, 0.2515, 0.2340],
        [0.4719, 0.3718, 0.3469],
        [0.4840, 0.3702, 0.3532],
        [0.5388, 0.3461, 0.2892],
        [0.4979, 0.3658, 0.3149],
        [0.5470, 0.3422, 0.2819],
        [0.5334, 0.3533, 0.2877],
        [0.5707, 0.3312, 0.2617],
        [0.5974, 0.3140, 0.2368],
        [0.5045, 0.3670, 0.3136],
        [0.5405, 0.3397, 0.2723],
        [0.4911, 0.3644, 0.3270],
        [0.5008, 0.3593, 0.3292],
        [0.5297, 0.3600, 0.3054],
        [0.5464, 0.3431, 0.2770],
        [0.5146, 0.3662, 0.2979]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 120: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 23.0, Max: 5480.0, Mean: 1561.63916015625, Std: 1284.5245361328125
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 228.82595825195312, Max: 54752.84765625, Mean: 15602.2529296875, Std: 12834.4228515625
[DEBUG] Top-3 class probabilities:
tensor([[0.4883, 0.3778, 0.3311],
        [0.5149, 0.3590, 0.3053],
        [0.4988, 0.3657, 0.3218],
        [0.5300, 0.3537, 0.3014],
        [0.5320, 0.3469, 0.3010],
        [0.5309, 0.3441, 0.2935],
        [0.4888, 0.3702, 0.3376],
        [0.6996, 0.2556, 0.2240],
        [0.5068, 0.3495, 0.3183],
        [0.5047, 0.3510, 0.3163],
        [0.5316, 0.3373, 0.2753],
        [0.4927, 0.3704, 0.3219],
        [0.4999, 0.3629, 0.3316],
        [0.5207, 0.3570, 0.3195],
        [0.6018, 0.3093, 0.2322],
        [0.5127, 0.3668, 0.2987],
        [0.4955, 0.3665, 0.3127],
        [0.5365, 0.3449, 0.2913],
        [0.5762, 0.3258, 0.2581],
        [0.5568, 0.3382, 0.2504],
        [0.5550, 0.3343, 0.2739],
        [0.5670, 0.3352, 0.2598],
        [0.5558, 0.3361, 0.2570],
        [0.4793, 0.3749, 0.3337],
        [0.5261, 0.3560, 0.2963],
        [0.5611, 0.3370, 0.2650],
        [0.5372, 0.3468, 0.2981],
        [0.5605, 0.3377, 0.2567],
        [0.4781, 0.3826, 0.3225],
        [0.4986, 0.3664, 0.3202],
        [0.6280, 0.2706, 0.2336],
        [0.5771, 0.3155, 0.2712],
        [0.5617, 0.3293, 0.2584],
        [0.5082, 0.3573, 0.3107],
        [0.5082, 0.3505, 0.3048],
        [0.5004, 0.3606, 0.3133],
        [0.5541, 0.3338, 0.2632],
        [0.5950, 0.3161, 0.2366],
        [0.5146, 0.3571, 0.3013],
        [0.4835, 0.3780, 0.3330],
        [0.4984, 0.3748, 0.3134],
        [0.5251, 0.3441, 0.2916],
        [0.5596, 0.3298, 0.2676],
        [0.5437, 0.3478, 0.2887],
        [0.5380, 0.3515, 0.2808],
        [0.5595, 0.3345, 0.2649],
        [0.5220, 0.3512, 0.2988],
        [0.4820, 0.3756, 0.3363],
        [0.5317, 0.3519, 0.2989],
        [0.5282, 0.3641, 0.3010],
        [0.5356, 0.3563, 0.2852],
        [0.5386, 0.3430, 0.2916],
        [0.6283, 0.2759, 0.2391],
        [0.5548, 0.3269, 0.2863],
        [0.4771, 0.3635, 0.3467],
        [0.5106, 0.3446, 0.3155],
        [0.4641, 0.3669, 0.3603],
        [0.4876, 0.3695, 0.3407],
        [0.5032, 0.3605, 0.3222],
        [0.4976, 0.3624, 0.3178],
        [0.5588, 0.3348, 0.2682],
        [0.5308, 0.3554, 0.2911],
        [0.5646, 0.3419, 0.2584],
        [0.5210, 0.3664, 0.3010],
        [0.5026, 0.3717, 0.3217],
        [0.5426, 0.3468, 0.2813],
        [0.5490, 0.3378, 0.2665],
        [0.4923, 0.3628, 0.3260],
        [0.5384, 0.3387, 0.2778],
        [0.5012, 0.3744, 0.3339],
        [0.4817, 0.3705, 0.3368],
        [0.5234, 0.3555, 0.3115],
        [0.5609, 0.3309, 0.2698],
        [0.6307, 0.2770, 0.2301],
        [0.4941, 0.3606, 0.3300],
        [0.4856, 0.3684, 0.3252],
        [0.4722, 0.3639, 0.3539],
        [0.5025, 0.3588, 0.3129],
        [0.5307, 0.3411, 0.3001],
        [0.4757, 0.3714, 0.3324],
        [0.4902, 0.3553, 0.3302],
        [0.5501, 0.3356, 0.2942],
        [0.5890, 0.3205, 0.2399],
        [0.5401, 0.3511, 0.2731],
        [0.5522, 0.3354, 0.2864],
        [0.5326, 0.3463, 0.2829],
        [0.5270, 0.3500, 0.2980],
        [0.5398, 0.3497, 0.2864],
        [0.5267, 0.3461, 0.3049],
        [0.5542, 0.3399, 0.2858],
        [0.5191, 0.3679, 0.3263],
        [0.4481, 0.3883, 0.3766],
        [0.6753, 0.2523, 0.2411],
        [0.4727, 0.3615, 0.3563],
        [0.4771, 0.3577, 0.3525],
        [0.4736, 0.3609, 0.3452],
        [0.5047, 0.3563, 0.3210],
        [0.5020, 0.3659, 0.3320],
        [0.5148, 0.3532, 0.3203],
        [0.5076, 0.3547, 0.3079],
        [0.5189, 0.3593, 0.3178],
        [0.5710, 0.3318, 0.2589],
        [0.5414, 0.3501, 0.2763],
        [0.5709, 0.3338, 0.2613],
        [0.5980, 0.3178, 0.2368],
        [0.5494, 0.3425, 0.2803],
        [0.5168, 0.3588, 0.3042],
        [0.5359, 0.3479, 0.2877],
        [0.5430, 0.3507, 0.2927],
        [0.5911, 0.3239, 0.2646],
        [0.5460, 0.3304, 0.2577],
        [0.5552, 0.3408, 0.2751],
        [0.5699, 0.3422, 0.2620],
        [0.5718, 0.3243, 0.2787],
        [0.4897, 0.3643, 0.3395],
        [0.4865, 0.3629, 0.3243],
        [0.5050, 0.3534, 0.3251],
        [0.5331, 0.3460, 0.2907],
        [0.4777, 0.3687, 0.3354],
        [0.5388, 0.3507, 0.2953],
        [0.4868, 0.3633, 0.3364],
        [0.5799, 0.3269, 0.2426],
        [0.5078, 0.3591, 0.3119],
        [0.5705, 0.3233, 0.2586],
        [0.5285, 0.3515, 0.2873],
        [0.4922, 0.3582, 0.3335],
        [0.4880, 0.3590, 0.3421],
        [0.5218, 0.3559, 0.3141]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 121: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 5600.0, Mean: 1173.61083984375, Std: 1212.753662109375
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 55951.8359375, Mean: 11725.2412109375, Std: 12117.318359375
[DEBUG] Top-3 class probabilities:
tensor([[0.5283, 0.3576, 0.2994],
        [0.5047, 0.3635, 0.3309],
        [0.4650, 0.3779, 0.3644],
        [0.5012, 0.3588, 0.3390],
        [0.5103, 0.3565, 0.3158],
        [0.5842, 0.3195, 0.2483],
        [0.5322, 0.3435, 0.3142],
        [0.5207, 0.3501, 0.3173],
        [0.5213, 0.3435, 0.3129],
        [0.4787, 0.3601, 0.3464],
        [0.5153, 0.3482, 0.3110],
        [0.4857, 0.3726, 0.3376],
        [0.5610, 0.3426, 0.2694],
        [0.5231, 0.3584, 0.3050],
        [0.5942, 0.3196, 0.2501],
        [0.5648, 0.3356, 0.2540],
        [0.5521, 0.3349, 0.2854],
        [0.4988, 0.3628, 0.3445],
        [0.4815, 0.3607, 0.3434],
        [0.4784, 0.3651, 0.3513],
        [0.6239, 0.3039, 0.2312],
        [0.5650, 0.3434, 0.2771],
        [0.5494, 0.3443, 0.2894],
        [0.5289, 0.3536, 0.3102],
        [0.5441, 0.3464, 0.2865],
        [0.5694, 0.3310, 0.2643],
        [0.4764, 0.3685, 0.3420],
        [0.6322, 0.2823, 0.2282],
        [0.5343, 0.3466, 0.3050],
        [0.4951, 0.3671, 0.3376],
        [0.4693, 0.3676, 0.3565],
        [0.5150, 0.3465, 0.3050],
        [0.4903, 0.3696, 0.3314],
        [0.5466, 0.3336, 0.2768],
        [0.5364, 0.3396, 0.2978],
        [0.5204, 0.3601, 0.2970],
        [0.5021, 0.3532, 0.3079],
        [0.5094, 0.3472, 0.3219],
        [0.5401, 0.3339, 0.3028],
        [0.4905, 0.3676, 0.3351],
        [0.5202, 0.3520, 0.3097],
        [0.5432, 0.3410, 0.2783],
        [0.5812, 0.3311, 0.2590],
        [0.6011, 0.3156, 0.2347],
        [0.5652, 0.3389, 0.2668],
        [0.5134, 0.3547, 0.3009],
        [0.5155, 0.3478, 0.3088],
        [0.5103, 0.3492, 0.3256],
        [0.6009, 0.3015, 0.2456],
        [0.4601, 0.3804, 0.3701],
        [0.4766, 0.3605, 0.3497],
        [0.4728, 0.3713, 0.3379],
        [0.4552, 0.3799, 0.3562],
        [0.5388, 0.3436, 0.2842],
        [0.5191, 0.3429, 0.3114],
        [0.5007, 0.3591, 0.3194],
        [0.4966, 0.3516, 0.3255],
        [0.5160, 0.3526, 0.3178],
        [0.4860, 0.3594, 0.3355],
        [0.5089, 0.3485, 0.3225],
        [0.4889, 0.3717, 0.3460],
        [0.5344, 0.3533, 0.3086],
        [0.5935, 0.3189, 0.2361],
        [0.5294, 0.3548, 0.2925],
        [0.4659, 0.3742, 0.3512],
        [0.5165, 0.3455, 0.3243],
        [0.5054, 0.3556, 0.3210],
        [0.6362, 0.2772, 0.2568],
        [0.5801, 0.3054, 0.2552],
        [0.6033, 0.2918, 0.2410],
        [0.5377, 0.3512, 0.2938],
        [0.4434, 0.3997, 0.3671],
        [0.4998, 0.3601, 0.3238],
        [0.5356, 0.3364, 0.3032],
        [0.4569, 0.3872, 0.3647],
        [0.4997, 0.3551, 0.3367],
        [0.5639, 0.3255, 0.2701],
        [0.4705, 0.3617, 0.3608],
        [0.5132, 0.3525, 0.3261],
        [0.5291, 0.3485, 0.3051],
        [0.5026, 0.3630, 0.3278],
        [0.5075, 0.3703, 0.3225],
        [0.5278, 0.3553, 0.3058],
        [0.5130, 0.3536, 0.3057],
        [0.5225, 0.3518, 0.3108],
        [0.6181, 0.2905, 0.2601],
        [0.5938, 0.3108, 0.2476],
        [0.5002, 0.3737, 0.3169],
        [0.4772, 0.3625, 0.3344],
        [0.4884, 0.3641, 0.3415],
        [0.4792, 0.3685, 0.3452],
        [0.5175, 0.3372, 0.3128],
        [0.5464, 0.3298, 0.2777],
        [0.5344, 0.3366, 0.2966],
        [0.5538, 0.3369, 0.2876],
        [0.5375, 0.3463, 0.2954],
        [0.4730, 0.3713, 0.3547],
        [0.5358, 0.3440, 0.2928],
        [0.5134, 0.3597, 0.3031],
        [0.7138, 0.2917, 0.2185],
        [0.7135, 0.2918, 0.2185],
        [0.7138, 0.2915, 0.2183],
        [0.7131, 0.2924, 0.2187],
        [0.7131, 0.2921, 0.2187],
        [0.7136, 0.2919, 0.2185],
        [0.7137, 0.2916, 0.2183],
        [0.7129, 0.2926, 0.2188],
        [0.7137, 0.2914, 0.2185],
        [0.7139, 0.2915, 0.2185],
        [0.7136, 0.2917, 0.2187],
        [0.7138, 0.2917, 0.2185],
        [0.7140, 0.2913, 0.2182],
        [0.7132, 0.2924, 0.2188],
        [0.7135, 0.2919, 0.2186],
        [0.7135, 0.2917, 0.2185],
        [0.7140, 0.2915, 0.2182],
        [0.7134, 0.2918, 0.2186],
        [0.7141, 0.2917, 0.2182],
        [0.7149, 0.2908, 0.2178],
        [0.7134, 0.2922, 0.2185],
        [0.7137, 0.2917, 0.2185],
        [0.7139, 0.2911, 0.2183],
        [0.7141, 0.2915, 0.2183],
        [0.7139, 0.2915, 0.2183],
        [0.7138, 0.2916, 0.2183],
        [0.7139, 0.2913, 0.2183],
        [0.7138, 0.2911, 0.2184],
        [0.7138, 0.2917, 0.2183]])
[DEBUG] Top-3 class indices:
tensor([[11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  6, 17],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  8],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11, 17,  6],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] Batch 122: Data shape: torch.Size([104, 10, 120, 120]), Labels shape: torch.Size([104, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 2276.0, Mean: 63.9050178527832, Std: 47.6503791809082
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 22739.841796875, Mean: 637.5314331054688, Std: 476.102294921875
[DEBUG] Top-3 class probabilities:
tensor([[0.7139, 0.2913, 0.2183],
        [0.7140, 0.2916, 0.2184],
        [0.7137, 0.2915, 0.2183],
        [0.7135, 0.2919, 0.2185],
        [0.7136, 0.2918, 0.2183],
        [0.7141, 0.2913, 0.2182],
        [0.7136, 0.2921, 0.2185],
        [0.7135, 0.2923, 0.2184],
        [0.7138, 0.2918, 0.2182],
        [0.7145, 0.2910, 0.2181],
        [0.7137, 0.2917, 0.2185],
        [0.7137, 0.2917, 0.2186],
        [0.7135, 0.2917, 0.2185],
        [0.7137, 0.2916, 0.2184],
        [0.7135, 0.2924, 0.2185],
        [0.7141, 0.2912, 0.2182],
        [0.7136, 0.2917, 0.2185],
        [0.7139, 0.2918, 0.2183],
        [0.7136, 0.2917, 0.2183],
        [0.7144, 0.2905, 0.2179],
        [0.7137, 0.2913, 0.2185],
        [0.7136, 0.2919, 0.2186],
        [0.7132, 0.2920, 0.2185],
        [0.7135, 0.2917, 0.2185],
        [0.7141, 0.2910, 0.2184],
        [0.7134, 0.2919, 0.2187],
        [0.7136, 0.2916, 0.2182],
        [0.7140, 0.2917, 0.2183],
        [0.7135, 0.2921, 0.2185],
        [0.7140, 0.2911, 0.2183],
        [0.7144, 0.2907, 0.2180],
        [0.7144, 0.2905, 0.2178],
        [0.7141, 0.2911, 0.2184],
        [0.7135, 0.2915, 0.2185],
        [0.7138, 0.2918, 0.2189],
        [0.7132, 0.2915, 0.2185],
        [0.7137, 0.2914, 0.2185],
        [0.7136, 0.2919, 0.2185],
        [0.7133, 0.2919, 0.2186],
        [0.7139, 0.2915, 0.2182],
        [0.7133, 0.2927, 0.2186],
        [0.7136, 0.2919, 0.2184],
        [0.7139, 0.2908, 0.2182],
        [0.7140, 0.2911, 0.2184],
        [0.7140, 0.2910, 0.2183],
        [0.7141, 0.2907, 0.2182],
        [0.7142, 0.2910, 0.2182],
        [0.7134, 0.2916, 0.2185],
        [0.7138, 0.2911, 0.2185],
        [0.7141, 0.2913, 0.2182],
        [0.7130, 0.2925, 0.2187],
        [0.7134, 0.2920, 0.2185],
        [0.7140, 0.2917, 0.2182],
        [0.7143, 0.2914, 0.2181],
        [0.7133, 0.2924, 0.2186],
        [0.7139, 0.2919, 0.2185],
        [0.7144, 0.2899, 0.2182],
        [0.7136, 0.2908, 0.2185],
        [0.7140, 0.2912, 0.2185],
        [0.7137, 0.2910, 0.2185],
        [0.7141, 0.2910, 0.2183],
        [0.7144, 0.2907, 0.2182],
        [0.7139, 0.2909, 0.2184],
        [0.7136, 0.2916, 0.2184],
        [0.7133, 0.2921, 0.2186],
        [0.7132, 0.2924, 0.2186],
        [0.7134, 0.2922, 0.2185],
        [0.7139, 0.2917, 0.2183],
        [0.7139, 0.2915, 0.2183],
        [0.7146, 0.2912, 0.2183],
        [0.7139, 0.2916, 0.2183],
        [0.7143, 0.2896, 0.2179],
        [0.7140, 0.2903, 0.2183],
        [0.7140, 0.2907, 0.2181],
        [0.7138, 0.2912, 0.2185],
        [0.7141, 0.2907, 0.2183],
        [0.7139, 0.2911, 0.2184],
        [0.7139, 0.2914, 0.2184],
        [0.7138, 0.2916, 0.2183],
        [0.7130, 0.2921, 0.2187],
        [0.7133, 0.2923, 0.2186],
        [0.7138, 0.2919, 0.2183],
        [0.7140, 0.2914, 0.2183],
        [0.7146, 0.2907, 0.2179],
        [0.7141, 0.2913, 0.2182],
        [0.7141, 0.2912, 0.2179],
        [0.7143, 0.2908, 0.2181],
        [0.7142, 0.2897, 0.2181],
        [0.7142, 0.2900, 0.2181],
        [0.7141, 0.2904, 0.2181],
        [0.7142, 0.2905, 0.2183],
        [0.7139, 0.2909, 0.2184],
        [0.7139, 0.2912, 0.2181],
        [0.7139, 0.2912, 0.2183],
        [0.7139, 0.2913, 0.2183],
        [0.7142, 0.2913, 0.2183],
        [0.7140, 0.2916, 0.2183],
        [0.7145, 0.2912, 0.2181],
        [0.7145, 0.2911, 0.2179],
        [0.7145, 0.2907, 0.2179],
        [0.7149, 0.2904, 0.2177],
        [0.7147, 0.2904, 0.2178],
        [0.7139, 0.2908, 0.2181],
        [0.7141, 0.2912, 0.2181]])
[DEBUG] Top-3 class indices:
tensor([[11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17],
        [11,  8, 17]])
[DEBUG] Argmax prediction (from top-3 classes):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
[DEBUG] True labels sample: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]
[DEBUG] Predicted probabilities sample: [[3.64253909e-04 7.67257437e-02 1.73351134e-03 3.66829634e-02
  2.69565429e-03 6.29054010e-03 1.46297902e-01 3.17379273e-03
  2.84913629e-01 6.70750663e-02 1.15411215e-01 6.94556415e-01
  9.58568081e-02 1.08500654e-02 1.10778837e-02 2.14589424e-02
  1.50602963e-03 2.23666698e-01 2.66480353e-02]
 [3.62103194e-04 7.71396086e-02 1.73874677e-03 3.66772041e-02
  2.69509037e-03 6.29424630e-03 1.46086261e-01 3.18422960e-03
  2.84337074e-01 6.71226904e-02 1.15303576e-01 6.94845021e-01
  9.57057849e-02 1.08247427e-02 1.10920817e-02 2.14439034e-02
  1.50759146e-03 2.23224998e-01 2.66492702e-02]
 [3.62493622e-04 7.72074014e-02 1.73902395e-03 3.65940407e-02
  2.69486872e-03 6.28944626e-03 1.46316960e-01 3.18304775e-03
  2.83990324e-01 6.73156157e-02 1.15193956e-01 6.93972886e-01
  9.57245231e-02 1.08408788e-02 1.11230407e-02 2.14105397e-02
  1.50920590e-03 2.23515868e-01 2.66125984e-02]
 [3.62708117e-04 7.72494450e-02 1.73795398e-03 3.66301872e-02
  2.69223261e-03 6.28397427e-03 1.46658063e-01 3.17905610e-03
  2.84442157e-01 6.73193634e-02 1.15295686e-01 6.93755448e-01
  9.60211381e-02 1.08392565e-02 1.11045633e-02 2.14179624e-02
  1.51004386e-03 2.23709390e-01 2.65568234e-02]
 [3.62621853e-04 7.70713165e-02 1.74009393e-03 3.66552547e-02
  2.69638537e-03 6.28578709e-03 1.46536499e-01 3.18215857e-03
  2.84957379e-01 6.74948990e-02 1.15471512e-01 6.93593085e-01
  9.59138423e-02 1.08722672e-02 1.11288857e-02 2.14537419e-02
  1.50718517e-03 2.23730579e-01 2.66625844e-02]]
Bug fix for empty classification report.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.2938
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.2509

[INFO] Performing LRP Pruning in Round 1...
[INFO] Computing LRP pruning mask...
Erstelle DataLoader fr Land: Finland
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Targets shape: torch.Size([5])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([5, 19])
Relevance computation successful. Relevance shape: torch.Size([5, 10, 1, 5])
[INFO] Relevance maps computed for 53 layers.
[DEBUG] Layer: conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.0.downsample.0
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.1.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.1.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.1.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.2.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.2.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.2.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.5.0.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.0.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.0.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.0.downsample.0
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.1.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.1.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.1.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.2.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.2.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.2.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.3.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.3.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.3.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.6.0.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.0.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.0.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.0.downsample.0
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.1.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.1.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.1.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.2.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.2.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.2.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.3.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.3.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.3.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.4.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.4.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.4.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.5.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.5.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.5.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.7.0.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.0.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.0.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.0.downsample.0
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.1.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.1.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.1.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.2.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.2.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.2.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 1743
[INFO] Using device: cuda
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.1589e+17, 3.2068e+17, 5.2167e+17, 5.3578e+17, 6.9979e+17, 6.8546e+17,
        7.0052e+17, 3.9313e+17, 1.3485e+18, 7.6903e+17, 7.6743e+17, 2.1780e+17,
        5.3716e+17, 7.5908e+17, 6.2379e+17, 5.0568e+17, 5.5135e+17, 2.5993e+17,
        4.0702e+17, 2.9588e+17, 5.5275e+17, 4.0101e+17, 3.6485e+17, 5.0745e+17,
        2.5344e+17, 3.7238e+17, 7.3342e+17, 7.3570e+17, 5.8038e+17, 4.4927e+17,
        3.4029e+17, 3.6977e+17, 8.9220e+17, 4.7189e+17, 6.4591e+17, 3.0539e+17,
        2.4075e+17, 4.7178e+17, 5.4130e+17, 4.2276e+17, 6.5633e+17, 4.9512e+17,
        9.3768e+17, 3.2400e+17, 3.4410e+17, 2.2730e+17, 6.7640e+17, 3.1585e+17,
        1.1957e+18, 2.4545e+17, 8.1949e+17, 4.4182e+17, 3.3019e+17, 7.1705e+17,
        8.1177e+17, 6.0067e+17, 1.7230e+17, 8.7101e+17, 5.4195e+17, 5.9620e+17,
        1.4283e+17, 3.5186e+17, 6.0739e+17, 5.9345e+17])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.7380e+17, 3.0887e+17, 1.4395e+17, 5.8669e+17, 6.5275e+17, 2.1727e+17,
        5.3245e+17, 2.7937e+17, 5.4678e+17, 2.7143e+17, 3.4713e+17, 3.5093e+17,
        1.8680e+17, 1.5076e+17, 1.2115e+17, 4.1047e+17, 3.9953e+17, 2.2785e+17,
        3.3158e+17, 2.9085e+17, 2.2562e+17, 2.2477e+17, 3.7100e+17, 2.9748e+17,
        2.6991e+17, 2.1563e+17, 3.8026e+17, 4.0605e+17, 9.8289e+16, 5.4096e+17,
        2.5782e+17, 3.2543e+17, 5.0685e+17, 3.2797e+17, 2.0171e+17, 2.5223e+17,
        4.6916e+17, 5.2529e+17, 2.8545e+17, 3.0847e+17, 2.2032e+17, 2.0531e+17,
        2.6260e+17, 2.3552e+17, 2.7820e+17, 2.1294e+17, 4.5065e+17, 4.5992e+17,
        4.7027e+17, 2.7158e+17, 3.0631e+17, 3.8717e+17, 3.4814e+17, 2.8837e+17,
        4.0860e+17, 2.0349e+17, 2.5846e+17, 5.5400e+17, 1.8906e+17, 4.0428e+17,
        4.8103e+17, 2.9158e+17, 2.9124e+17, 2.3416e+17])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.1966e+17, 2.7814e+17, 3.9433e+17, 3.7483e+17, 5.3348e+17, 5.3672e+17,
        8.0982e+17, 3.9825e+17, 1.7549e+17, 1.8454e+17, 3.3712e+17, 2.8403e+17,
        3.5787e+17, 4.2171e+17, 2.0732e+17, 5.9298e+17, 4.3891e+17, 6.1227e+17,
        2.9744e+17, 5.0146e+17, 5.5497e+17, 5.6603e+17, 2.1246e+17, 1.4749e+17,
        2.1211e+17, 5.2709e+17, 1.5667e+17, 2.3258e+17, 5.9723e+17, 7.4929e+17,
        1.4729e+17, 5.9585e+17, 3.8491e+17, 1.7689e+17, 4.1506e+17, 3.8392e+17,
        6.1858e+17, 2.2792e+17, 4.3336e+17, 7.8230e+17, 2.4832e+17, 1.4902e+17,
        4.1115e+17, 7.5669e+17, 1.4922e+17, 5.3886e+17, 3.8533e+17, 3.5743e+17,
        3.6134e+17, 2.9052e+17, 4.2301e+17, 2.8292e+17, 4.7700e+17, 4.8210e+17,
        6.7234e+17, 3.0560e+17, 5.1858e+17, 7.2263e+17, 4.6090e+17, 3.6996e+17,
        3.6591e+17, 1.2966e+17, 4.7031e+17, 9.5274e+17])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5054e+14, 6.3877e+14, 8.0418e+15, 5.9952e+16, 6.3861e+15, 5.6310e+15,
        2.3647e+15, 2.6755e+14, 3.0561e+14, 6.7818e+15, 1.2317e+16, 1.3017e+15,
        2.9849e+15, 2.0572e+15, 1.5949e+15, 4.1614e+14, 2.3801e+17, 1.6988e+15,
        8.7270e+14, 3.2868e+15, 6.2405e+15, 2.4948e+15, 4.7608e+14, 3.6797e+16,
        3.2750e+14, 4.5639e+15, 3.5580e+14, 8.6997e+14, 7.1294e+14, 8.8503e+13,
        1.0836e+15, 4.4393e+16, 6.9934e+15, 9.4996e+14, 1.1602e+15, 1.4999e+15,
        1.9400e+14, 2.4233e+14, 5.7402e+14, 2.4456e+14, 3.0167e+14, 1.4198e+14,
        6.2562e+15, 8.2153e+14, 3.6268e+15, 1.0743e+15, 4.1540e+14, 3.2455e+14,
        2.5493e+15, 9.4020e+14, 3.1037e+14, 2.3834e+15, 2.4163e+14, 1.3615e+15,
        3.9280e+15, 1.5131e+15, 4.4082e+14, 3.8544e+15, 6.5344e+14, 5.7260e+14,
        5.8857e+16, 7.9163e+14, 6.0534e+14, 6.2829e+14, 2.3538e+16, 7.3810e+15,
        1.8035e+15, 5.8355e+14, 7.2622e+14, 5.8855e+15, 6.0540e+14, 1.0627e+15,
        1.8102e+16, 1.4326e+15, 9.4036e+14, 1.6146e+15, 1.1441e+15, 5.3953e+14,
        1.1867e+15, 2.8874e+15, 2.5690e+14, 8.9424e+15, 1.7137e+16, 1.3656e+15,
        9.6344e+16, 8.0347e+14, 3.4890e+14, 1.7171e+16, 4.0853e+14, 7.2000e+14,
        2.8622e+14, 3.4464e+15, 9.3096e+14, 1.9315e+14, 2.4678e+15, 1.0715e+15,
        2.5586e+16, 9.0881e+15, 2.9593e+14, 2.4351e+14, 1.4656e+15, 8.8360e+14,
        2.4173e+15, 6.2898e+16, 3.4639e+15, 2.1824e+15, 1.0747e+15, 1.9417e+15,
        1.0817e+16, 6.2789e+14, 1.4961e+15, 2.9136e+15, 4.7388e+14, 5.5268e+14,
        7.5555e+14, 1.5190e+16, 7.3883e+14, 5.3650e+14, 1.9512e+14, 2.8965e+14,
        2.8220e+14, 7.0515e+14, 5.4018e+14, 9.6908e+15, 2.8230e+14, 1.3487e+15,
        7.4185e+14, 2.6739e+15, 2.0600e+14, 1.8302e+15, 3.6411e+14, 7.8377e+15,
        5.3943e+14, 4.3422e+14, 7.9623e+15, 7.3266e+14, 1.1542e+16, 1.3535e+15,
        1.2154e+15, 1.7493e+15, 3.2678e+16, 2.0452e+16, 2.3167e+16, 2.2102e+17,
        3.2201e+15, 1.7580e+14, 1.2330e+15, 5.4209e+14, 5.5594e+14, 2.0622e+14,
        7.6882e+16, 1.1504e+15, 4.0373e+14, 1.3771e+16, 5.7635e+15, 4.6456e+16,
        1.0040e+15, 9.3954e+14, 7.6342e+13, 1.2564e+16, 6.6396e+16, 5.5276e+15,
        7.6282e+14, 2.7753e+14, 2.9778e+15, 1.8981e+16, 1.4431e+15, 4.9860e+15,
        8.5648e+14, 3.5468e+14, 1.1951e+16, 5.0184e+15, 1.2632e+16, 6.4369e+14,
        1.6219e+16, 1.2063e+16, 6.1116e+14, 9.3306e+14, 2.5672e+14, 6.2262e+14,
        6.1619e+14, 5.3711e+14, 3.1720e+14, 5.2407e+14, 4.3726e+14, 3.3593e+15,
        6.0968e+14, 2.6949e+15, 4.8969e+16, 1.4615e+15, 3.6315e+14, 3.5397e+14,
        3.9096e+14, 5.2847e+14, 1.3440e+15, 6.5337e+15, 5.8413e+14, 5.4167e+15,
        4.0376e+15, 5.2928e+15, 7.7788e+14, 1.0292e+15, 1.1029e+15, 3.0742e+14,
        9.6797e+14, 4.7554e+16, 1.8785e+14, 6.6660e+14, 7.5197e+14, 7.3560e+14,
        6.8026e+15, 3.4954e+15, 2.3507e+14, 5.5947e+14, 7.5023e+15, 1.2814e+15,
        1.5284e+15, 2.6016e+14, 7.5995e+15, 4.5980e+14, 9.8638e+14, 2.3555e+14,
        8.5657e+14, 1.1854e+15, 4.8111e+14, 1.0799e+15, 1.3685e+14, 9.2318e+13,
        1.0094e+15, 1.1527e+15, 6.6223e+13, 1.1492e+14, 4.2328e+15, 6.7866e+14,
        1.0048e+15, 1.3558e+18, 1.1645e+16, 2.4708e+15, 3.4972e+14, 4.1105e+14,
        4.8457e+14, 4.9341e+14, 1.7620e+15, 5.2210e+16, 7.7999e+14, 1.3974e+15,
        1.2707e+14, 1.5723e+15, 9.2926e+15, 1.4783e+15, 4.5331e+14, 6.5568e+14,
        1.2217e+14, 1.6303e+15, 4.6644e+16, 6.9610e+15])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6102e+15, 2.2577e+15, 6.6352e+15, 5.7707e+16, 5.0673e+15, 7.7023e+15,
        1.6569e+15, 5.3307e+14, 1.9102e+15, 5.1091e+15, 1.3881e+16, 8.5064e+14,
        1.2988e+15, 3.4797e+15, 1.0494e+15, 2.5810e+15, 2.3750e+17, 8.0240e+14,
        2.8842e+15, 5.5760e+15, 5.8382e+15, 6.6725e+14, 1.2108e+15, 3.7690e+16,
        4.8236e+14, 4.2325e+15, 4.6662e+14, 8.7415e+14, 2.2245e+15, 1.1739e+15,
        1.7800e+15, 4.4953e+16, 5.9603e+15, 8.5037e+14, 2.7597e+14, 1.0451e+14,
        1.0069e+15, 2.4861e+15, 7.1039e+14, 1.2461e+15, 2.1644e+15, 5.6267e+14,
        6.8164e+15, 2.0281e+14, 3.6437e+15, 1.8462e+15, 4.7089e+14, 1.4454e+15,
        1.1346e+15, 1.2119e+14, 1.1452e+15, 1.6770e+15, 7.7744e+14, 2.1266e+15,
        9.6217e+15, 3.7499e+14, 1.2735e+15, 2.8868e+15, 2.3350e+15, 3.4967e+14,
        5.9076e+16, 7.8974e+14, 8.0666e+14, 2.2163e+15, 2.5363e+16, 6.1218e+15,
        1.5727e+15, 9.7984e+14, 1.9271e+15, 5.2954e+15, 8.0114e+14, 4.6090e+14,
        1.8033e+16, 1.6160e+14, 6.9348e+14, 3.0854e+15, 2.0217e+14, 1.7411e+15,
        2.9375e+14, 1.7743e+15, 2.2628e+15, 9.6325e+15, 1.6340e+16, 1.2598e+15,
        9.6679e+16, 1.6173e+15, 1.4730e+15, 1.7019e+16, 1.3750e+15, 6.2378e+14,
        1.5151e+15, 1.5101e+15, 1.9712e+15, 4.8826e+14, 4.2786e+15, 6.5299e+14,
        2.8294e+16, 1.0518e+16, 1.4295e+15, 8.3467e+14, 2.6866e+15, 3.2845e+15,
        2.1101e+15, 6.4380e+16, 4.6649e+15, 2.8107e+14, 2.2472e+15, 8.3196e+14,
        1.2262e+16, 8.8588e+14, 1.0516e+15, 9.3055e+15, 1.7161e+15, 8.1862e+14,
        1.8223e+15, 1.5483e+16, 3.4361e+14, 5.4608e+14, 1.4155e+15, 3.3216e+14,
        1.3909e+15, 5.8903e+14, 4.4087e+14, 9.9681e+15, 1.2818e+15, 2.6258e+15,
        1.3299e+15, 2.2047e+15, 1.7370e+15, 4.0339e+14, 1.8833e+15, 4.2966e+15,
        7.0252e+14, 9.5646e+14, 5.3935e+15, 7.2593e+14, 1.3117e+16, 4.1315e+14,
        8.5594e+14, 3.2296e+15, 3.1168e+16, 2.1507e+16, 2.4729e+16, 2.2270e+17,
        1.9164e+15, 1.0385e+15, 4.8136e+15, 8.5836e+14, 4.5764e+14, 1.8535e+15,
        7.7092e+16, 1.0614e+15, 7.5352e+14, 1.1508e+16, 4.4599e+15, 4.4369e+16,
        5.8363e+14, 1.0911e+15, 9.6056e+14, 1.3691e+16, 6.5890e+16, 6.8200e+15,
        1.0684e+15, 1.2360e+15, 1.3721e+15, 2.0010e+16, 2.9244e+14, 1.1545e+15,
        2.4809e+15, 6.4778e+14, 1.1329e+16, 1.1913e+16, 9.9663e+15, 1.7143e+15,
        1.8180e+16, 1.1320e+16, 1.4684e+15, 6.9393e+14, 9.9655e+14, 2.5053e+15,
        2.6152e+15, 1.1078e+15, 7.2559e+14, 2.1650e+15, 1.5391e+15, 1.6630e+15,
        1.1622e+15, 8.7667e+14, 4.7311e+16, 6.4034e+14, 1.0151e+15, 1.0128e+15,
        9.9813e+14, 1.2644e+15, 1.2694e+15, 5.9971e+15, 7.9787e+14, 5.6356e+15,
        3.8694e+15, 8.0693e+15, 1.0479e+15, 3.4633e+15, 2.6098e+14, 1.7373e+15,
        3.1472e+15, 4.7325e+16, 8.8746e+14, 4.8749e+14, 1.2852e+15, 8.0133e+14,
        6.4366e+15, 3.1475e+15, 9.0889e+14, 3.0802e+14, 6.2712e+15, 1.0200e+15,
        9.1882e+14, 2.1311e+15, 1.0413e+16, 9.4215e+14, 2.7969e+14, 1.9078e+15,
        3.1176e+15, 4.0339e+14, 1.4290e+15, 2.2352e+15, 1.4079e+15, 5.0395e+14,
        1.9394e+15, 3.7353e+14, 1.1612e+15, 1.1738e+15, 3.0427e+15, 6.3143e+14,
        5.1925e+14, 1.3549e+18, 1.3269e+16, 5.5404e+14, 1.1050e+15, 1.1365e+15,
        1.5108e+15, 1.3712e+15, 3.8880e+14, 4.9827e+16, 1.0310e+15, 4.6195e+14,
        1.8132e+15, 7.8385e+14, 1.0715e+16, 2.5662e+15, 1.8095e+15, 1.7679e+15,
        1.0866e+15, 7.4802e+15, 4.5928e+16, 7.4759e+15])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.2908e+15, 3.4580e+15, 2.8633e+15, 2.9821e+15, 3.7377e+15, 4.3521e+15,
        3.1651e+15, 3.0017e+15, 3.3803e+15, 2.1481e+15, 1.7459e+15, 3.2664e+15,
        1.0043e+15, 3.1190e+15, 4.1025e+15, 2.0786e+15, 1.2692e+15, 3.5810e+15,
        1.8514e+15, 1.5948e+15, 1.7986e+15, 3.8025e+15, 2.3760e+15, 2.5570e+15,
        2.5008e+15, 1.9259e+15, 3.2628e+15, 2.5767e+15, 5.8212e+14, 2.5386e+15,
        4.1770e+15, 1.6059e+15, 5.0122e+15, 1.8027e+15, 3.5960e+15, 4.1090e+15,
        3.0687e+15, 2.0434e+15, 2.3415e+15, 2.5865e+15, 1.9021e+15, 1.7127e+15,
        2.2838e+15, 2.0203e+15, 4.7691e+15, 2.8960e+15, 2.7753e+15, 3.5086e+15,
        1.6917e+15, 2.6866e+15, 3.1512e+15, 2.0081e+15, 2.8110e+15, 4.5629e+15,
        1.0468e+15, 2.2948e+15, 2.5918e+15, 3.0921e+15, 4.1746e+15, 3.4365e+15,
        2.0702e+15, 4.4785e+15, 3.1256e+15, 1.3251e+15])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.8594e+15, 1.6241e+15, 5.4492e+15, 4.6984e+15, 2.8473e+15, 1.6244e+15,
        1.7644e+15, 4.0359e+15, 5.0019e+15, 3.1619e+15, 2.2511e+15, 6.4344e+15,
        2.4312e+15, 2.0830e+15, 2.0678e+15, 3.3103e+15, 5.6576e+15, 2.0642e+15,
        4.6213e+15, 1.5075e+15, 6.8223e+15, 2.8301e+15, 3.9464e+15, 5.7131e+15,
        4.2574e+15, 1.9169e+15, 4.7155e+15, 3.8441e+15, 3.3783e+15, 2.4857e+15,
        1.6973e+15, 6.2909e+15, 3.0049e+15, 2.4530e+15, 5.7010e+15, 3.1290e+15,
        3.8401e+15, 3.5153e+15, 4.9368e+15, 3.3578e+15, 5.5758e+15, 4.3254e+15,
        4.6840e+15, 3.3124e+15, 3.8601e+15, 4.1815e+15, 3.2619e+15, 2.4478e+15,
        2.4737e+15, 2.8440e+15, 6.0054e+15, 2.0081e+15, 4.0641e+15, 3.2992e+15,
        3.4966e+15, 2.2716e+15, 4.7725e+15, 5.0211e+15, 1.9933e+15, 5.2547e+15,
        1.5372e+15, 5.4092e+15, 1.6981e+15, 4.0163e+15])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.7524e+14, 1.9781e+13, 2.6774e+14, 1.6635e+14, 4.1383e+14, 3.7044e+14,
        3.6449e+14, 3.1917e+14, 3.0804e+14, 2.0983e+14, 4.0257e+15, 1.9081e+14,
        5.2053e+13, 5.6429e+14, 4.0446e+14, 6.1013e+13, 1.1060e+14, 2.4199e+13,
        2.1596e+14, 3.1091e+14, 6.1901e+14, 8.5154e+13, 3.8524e+14, 1.9502e+14,
        3.7638e+13, 4.9483e+13, 4.0657e+14, 1.3948e+15, 8.7062e+13, 1.7755e+13,
        4.5932e+14, 1.2070e+14, 1.6445e+15, 1.1837e+14, 1.6587e+15, 3.1500e+14,
        1.8549e+14, 1.8338e+14, 1.3414e+14, 2.5230e+13, 3.6402e+14, 3.8231e+14,
        7.8743e+14, 3.6240e+14, 3.7554e+14, 3.9478e+14, 2.4884e+13, 9.6605e+13,
        2.4423e+14, 4.2996e+14, 2.2612e+14, 7.7609e+14, 1.6433e+14, 4.1509e+14,
        6.4401e+15, 3.4601e+14, 9.7843e+13, 5.1243e+14, 1.9731e+14, 2.2800e+14,
        6.2705e+13, 2.1388e+13, 1.9407e+13, 1.6136e+14, 7.0135e+14, 4.7240e+14,
        3.4210e+14, 7.3951e+13, 5.8770e+13, 4.7248e+14, 1.3292e+14, 1.8877e+14,
        5.7461e+14, 1.7955e+14, 3.5046e+14, 2.3864e+14, 6.8417e+12, 8.5348e+13,
        1.8714e+13, 5.9974e+14, 2.3654e+15, 7.2367e+14, 5.5605e+14, 1.4837e+14,
        9.4371e+13, 1.0710e+14, 2.3878e+14, 7.0346e+14, 2.4632e+14, 6.1881e+14,
        6.5078e+13, 9.7942e+13, 9.3259e+13, 2.4903e+14, 2.0947e+14, 2.4904e+14,
        1.2590e+15, 1.6107e+15, 3.2128e+13, 3.6604e+13, 2.7608e+14, 7.1107e+14,
        9.8354e+14, 4.5757e+14, 3.5486e+14, 2.8400e+14, 1.5069e+14, 2.6711e+14,
        1.4952e+15, 3.4113e+14, 5.6180e+13, 3.6426e+15, 3.7325e+14, 1.0751e+14,
        1.2465e+14, 1.8212e+14, 1.7531e+14, 1.8547e+14, 1.1968e+14, 3.9384e+14,
        2.9903e+14, 3.3996e+13, 1.6615e+15, 2.8804e+14, 3.0775e+14, 2.0253e+13,
        3.8416e+13, 1.7257e+13, 1.1238e+14, 4.8363e+14, 9.9837e+13, 2.4568e+15,
        4.3936e+14, 3.9633e+14, 5.1314e+14, 5.2728e+14, 1.0656e+14, 4.5196e+14,
        1.8333e+14, 2.3046e+14, 7.3835e+13, 1.6478e+14, 7.5103e+14, 1.4501e+14,
        4.4615e+14, 6.8471e+12, 1.9297e+13, 1.3015e+14, 1.4543e+14, 3.9843e+14,
        1.4508e+14, 5.4853e+14, 1.9890e+14, 6.2789e+14, 3.5831e+14, 6.2694e+14,
        2.3745e+13, 4.6589e+14, 1.4641e+13, 2.4792e+14, 6.2525e+14, 5.4939e+14,
        9.3267e+13, 6.1388e+13, 8.3335e+13, 3.5244e+14, 2.9039e+14, 2.4456e+15,
        2.3549e+14, 1.0796e+14, 3.9994e+14, 6.7571e+15, 1.1459e+15, 8.6668e+13,
        2.2870e+14, 3.1821e+14, 1.9854e+14, 2.7092e+14, 4.3920e+14, 8.1907e+14,
        1.1979e+14, 3.4356e+14, 1.5051e+14, 1.9201e+14, 2.6083e+14, 2.1903e+14,
        4.3730e+14, 1.1240e+13, 1.4776e+15, 5.5975e+13, 8.7943e+13, 1.4003e+14,
        1.8811e+14, 3.3087e+14, 9.4796e+13, 2.6839e+14, 1.8639e+14, 9.9412e+14,
        3.1189e+14, 2.2025e+14, 3.1153e+14, 3.9491e+14, 8.1283e+14, 4.0606e+14,
        5.9559e+13, 2.8984e+14, 1.6815e+14, 2.0707e+14, 8.2783e+13, 9.5853e+14,
        4.1957e+14, 2.8810e+14, 1.5565e+13, 4.2619e+13, 2.3253e+14, 5.6103e+14,
        1.4410e+14, 9.9454e+13, 4.1136e+14, 5.4391e+14, 1.8899e+14, 1.9788e+14,
        5.9245e+13, 9.1305e+14, 6.1680e+14, 1.5493e+15, 1.4922e+14, 5.6121e+13,
        1.3117e+14, 2.2356e+14, 1.8549e+14, 1.0152e+14, 1.5448e+15, 5.8298e+13,
        3.1567e+14, 2.9496e+14, 1.5365e+14, 3.9545e+14, 7.6650e+13, 2.6794e+14,
        1.6337e+14, 2.4286e+13, 3.1422e+14, 2.8739e+14, 1.8834e+13, 4.1775e+14,
        3.7290e+14, 6.0664e+13, 1.0716e+14, 1.4174e+14, 6.5551e+14, 1.3265e+14,
        5.0803e+14, 4.4820e+15, 2.1647e+14, 4.5482e+14])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0619e+15, 5.2381e+14, 6.3814e+14, 2.6523e+14, 3.6575e+14, 8.2007e+14,
        7.4940e+14, 1.2622e+15, 3.4838e+14, 2.8898e+14, 6.4054e+14, 9.1236e+14,
        4.5641e+14, 8.4777e+14, 4.1390e+14, 5.7084e+14, 1.4341e+15, 6.2840e+14,
        8.8594e+14, 2.0754e+14, 3.2404e+14, 5.8863e+14, 3.0571e+14, 3.4144e+14,
        5.3962e+14, 8.7267e+14, 6.3465e+14, 7.0342e+14, 8.9826e+14, 1.4393e+15,
        3.9189e+14, 6.7379e+14, 9.0216e+14, 3.0571e+14, 1.0222e+15, 6.6757e+14,
        8.3831e+14, 2.8499e+14, 1.8088e+15, 1.1371e+15, 3.1031e+14, 8.8405e+14,
        5.4349e+14, 3.5964e+14, 4.7913e+14, 7.9962e+14, 7.3204e+14, 1.9599e+14,
        2.7076e+14, 1.3754e+15, 2.7951e+14, 1.9414e+14, 3.2901e+14, 7.1580e+14,
        1.3269e+15, 3.8728e+14, 3.1495e+14, 6.4014e+14, 2.2644e+14, 9.3804e+14,
        2.5425e+14, 8.8493e+14, 1.7007e+15, 7.5227e+14])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.5347e+14, 1.0313e+15, 1.9209e+15, 1.5815e+15, 1.0211e+15, 1.2719e+15,
        1.7794e+15, 1.7750e+15, 9.7752e+14, 1.1715e+15, 1.5234e+15, 1.4456e+15,
        5.5049e+14, 9.3226e+14, 9.7946e+14, 5.3323e+14, 1.4263e+15, 1.1647e+15,
        3.4019e+14, 9.0979e+14, 3.4205e+14, 2.1287e+14, 1.0072e+15, 3.9392e+14,
        9.5759e+14, 1.1363e+15, 1.5615e+15, 1.5115e+15, 3.7787e+14, 1.2432e+15,
        1.1639e+15, 8.3650e+14, 1.9251e+15, 1.5032e+15, 5.1517e+14, 5.7566e+14,
        3.7662e+14, 3.9282e+14, 2.6599e+15, 4.1853e+14, 1.8933e+14, 7.0973e+14,
        6.7463e+14, 1.4089e+15, 1.8529e+15, 1.1924e+15, 1.3019e+15, 1.1576e+15,
        1.2341e+15, 1.9022e+15, 1.4228e+15, 1.8543e+14, 1.1830e+15, 1.8601e+15,
        3.0781e+14, 1.3956e+15, 1.3635e+15, 9.7373e+14, 4.2053e+14, 1.2149e+15,
        9.7561e+14, 4.0308e+14, 1.7922e+15, 3.9442e+14])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.2387e+12, 1.2572e+12, 1.7016e+13, 5.0745e+12, 1.1798e+13, 5.2211e+12,
        3.5092e+12, 4.4324e+12, 3.3560e+12, 1.5929e+12, 5.5538e+15, 3.4317e+12,
        9.2711e+12, 4.4387e+12, 1.3921e+13, 4.2380e+11, 6.3435e+12, 4.0026e+12,
        1.3390e+14, 1.9068e+12, 5.0667e+12, 1.5670e+14, 3.0477e+12, 1.9235e+13,
        1.7618e+12, 1.0473e+12, 7.9994e+12, 5.3336e+12, 1.7211e+12, 1.8460e+12,
        4.9653e+14, 2.2929e+12, 3.8896e+12, 3.3263e+11, 2.1392e+12, 3.1656e+12,
        2.8403e+12, 3.3946e+12, 2.3603e+11, 8.3125e+11, 4.5553e+12, 2.0876e+12,
        4.5908e+12, 1.2023e+13, 5.4682e+12, 3.6001e+12, 1.8158e+12, 5.6088e+14,
        4.8852e+12, 7.5046e+12, 2.8895e+12, 2.9917e+12, 6.0086e+12, 4.6635e+12,
        2.8106e+12, 1.4793e+13, 1.3824e+12, 1.4453e+14, 1.5408e+13, 6.8471e+12,
        1.9599e+12, 5.7972e+12, 4.0790e+11, 6.9089e+11, 3.2767e+12, 9.4438e+12,
        1.8263e+12, 4.7020e+11, 6.7100e+12, 8.0138e+13, 5.3390e+12, 5.1080e+12,
        5.7968e+12, 5.1574e+12, 6.3274e+11, 4.6629e+12, 1.7817e+12, 3.5635e+12,
        2.3642e+12, 2.7024e+12, 3.6248e+13, 3.4702e+12, 6.0143e+12, 1.3270e+12,
        2.7531e+12, 5.6544e+13, 8.5999e+13, 1.5587e+13, 2.0094e+12, 2.0933e+13,
        4.9092e+12, 2.4484e+12, 8.7474e+11, 9.2483e+11, 2.0545e+12, 4.5401e+12,
        3.8334e+12, 3.9178e+12, 1.8385e+12, 1.8228e+12, 2.0819e+13, 6.9206e+12,
        7.2597e+12, 7.3565e+12, 1.5747e+12, 7.3574e+12, 4.5359e+12, 2.6551e+12,
        4.4983e+12, 8.4733e+12, 2.0523e+12, 1.6603e+13, 5.1221e+12, 1.0426e+13,
        2.0872e+12, 1.6801e+13, 5.0498e+12, 7.5861e+12, 8.7514e+11, 7.3182e+12,
        5.1142e+12, 1.7116e+12, 1.3551e+13, 9.8646e+12, 6.4259e+12, 2.3899e+12,
        2.0810e+12, 3.3488e+12, 1.6701e+12, 8.7795e+11, 3.8568e+11, 2.1936e+13,
        4.7704e+12, 2.9831e+12, 3.6139e+12, 5.9026e+12, 2.4189e+13, 3.5454e+12,
        3.8805e+12, 3.6727e+13, 2.8112e+12, 6.2649e+12, 2.5657e+12, 7.6083e+12,
        4.3650e+12, 1.3947e+12, 7.2807e+11, 6.0040e+11, 5.5857e+11, 7.7440e+12,
        6.9148e+12, 1.6687e+12, 2.9301e+11, 3.3527e+12, 2.6602e+12, 7.4429e+12,
        1.0682e+12, 2.8358e+12, 1.8503e+12, 3.3243e+12, 4.3683e+12, 4.4237e+12,
        1.2195e+12, 3.6164e+12, 7.1032e+12, 3.6100e+12, 3.2207e+12, 3.7027e+12,
        6.0837e+12, 1.6178e+12, 2.8323e+12, 1.9021e+12, 8.8625e+12, 5.6881e+12,
        4.5556e+12, 2.4872e+13, 2.8096e+13, 2.6045e+12, 2.2698e+12, 5.8648e+12,
        4.7008e+11, 7.0560e+12, 4.8669e+12, 2.0322e+12, 6.6177e+12, 2.4239e+12,
        4.5023e+12, 1.5613e+12, 5.2200e+12, 7.4715e+11, 1.0397e+12, 1.3268e+12,
        2.1619e+12, 4.5353e+12, 1.6382e+12, 1.2532e+13, 8.4819e+12, 4.9697e+12,
        6.0737e+12, 3.8781e+13, 7.5899e+11, 3.2881e+12, 1.0349e+12, 4.4808e+12,
        4.3294e+13, 3.6374e+12, 1.2848e+12, 2.8343e+12, 5.8307e+11, 9.3761e+12,
        5.9431e+12, 7.1960e+12, 1.3335e+13, 4.6305e+11, 8.9944e+13, 6.6417e+12,
        3.5456e+12, 2.0093e+13, 6.8969e+12, 3.9717e+12, 3.4975e+12, 1.2901e+12,
        6.8978e+11, 4.5749e+11, 2.1831e+13, 6.3222e+12, 4.2755e+12, 7.7545e+11,
        6.0557e+12, 1.4441e+13, 3.5280e+11, 4.6229e+12, 4.1223e+12, 9.4276e+11,
        2.6979e+12, 7.5368e+12, 5.4878e+12, 1.7798e+13, 1.1890e+12, 5.6700e+13,
        4.9559e+12, 1.0585e+12, 5.2002e+12, 6.6915e+12, 3.8819e+13, 3.7919e+12,
        1.6461e+13, 3.9378e+12, 1.2422e+12, 5.5219e+12, 7.2593e+12, 6.0201e+14,
        4.5373e+12, 1.7068e+12, 5.3668e+13, 1.3012e+13])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.2616e+12, 9.5127e+12, 5.2379e+12, 5.2492e+12, 7.0010e+12, 3.5948e+12,
        5.8702e+12, 4.9803e+12, 5.9466e+12, 8.3781e+12, 3.6948e+12, 7.5277e+12,
        6.7792e+12, 7.2073e+12, 6.0455e+12, 4.5303e+12, 4.4188e+12, 1.0010e+13,
        5.2953e+12, 7.1742e+12, 7.0087e+12, 1.1014e+13, 6.1154e+12, 5.4363e+12,
        5.0722e+12, 7.8004e+12, 5.6476e+12, 8.5195e+12, 5.3248e+12, 4.4967e+12,
        5.6138e+12, 3.9250e+12, 6.5574e+12, 4.5383e+12, 4.7034e+12, 7.5189e+12,
        4.3097e+12, 6.4704e+12, 3.2520e+12, 4.3259e+12, 7.1040e+12, 7.9854e+12,
        3.6084e+12, 7.3576e+12, 7.5847e+12, 3.1600e+12, 6.5215e+12, 1.1042e+13,
        4.5046e+12, 8.8036e+12, 5.0887e+12, 4.6016e+12, 6.0854e+12, 6.4351e+12,
        5.7730e+12, 5.7977e+12, 6.6114e+12, 6.3060e+12, 5.6814e+12, 8.2223e+12,
        6.6987e+12, 6.7355e+12, 3.6650e+12, 8.6095e+12, 7.2426e+12, 9.0272e+12,
        7.6329e+12, 4.7757e+12, 5.8993e+12, 9.2009e+12, 5.5753e+12, 5.4084e+12,
        2.7715e+12, 6.1802e+12, 6.6297e+12, 9.6881e+12, 7.0285e+12, 6.0319e+12,
        7.3595e+12, 4.3984e+12, 5.6862e+12, 5.5762e+12, 8.1530e+12, 3.5224e+12,
        8.4645e+12, 4.7694e+12, 3.8374e+12, 8.1903e+12, 6.7080e+12, 6.3076e+12,
        8.9265e+12, 3.5419e+12, 1.0511e+13, 7.0945e+12, 4.7291e+12, 6.4647e+12,
        6.0061e+12, 6.7500e+12, 6.8341e+12, 8.4826e+12, 5.2205e+12, 5.1090e+12,
        8.3445e+12, 7.0822e+12, 5.7818e+12, 5.8527e+12, 5.9577e+12, 5.5448e+12,
        7.5354e+12, 7.4370e+12, 6.0132e+12, 8.8092e+12, 4.2390e+12, 6.6977e+12,
        6.3172e+12, 3.7712e+12, 2.5859e+12, 3.7669e+12, 3.6668e+12, 5.6868e+12,
        7.0898e+12, 3.1303e+12, 6.5593e+12, 6.9746e+12, 4.5882e+12, 6.1676e+12,
        5.5993e+12, 5.4146e+12])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.2818e+12, 1.4414e+13, 9.4494e+12, 1.4714e+13, 5.6435e+12, 9.0505e+12,
        9.5682e+12, 7.8584e+12, 1.2518e+13, 8.1133e+12, 1.3043e+13, 1.0331e+13,
        7.0505e+12, 1.3326e+13, 1.0221e+13, 1.5152e+13, 1.1079e+13, 7.0688e+12,
        8.6764e+12, 7.7735e+12, 1.0537e+13, 8.0377e+12, 6.7184e+12, 9.1788e+12,
        1.1679e+13, 1.0761e+13, 1.1847e+13, 8.8631e+12, 1.3543e+13, 8.0881e+12,
        1.1617e+13, 1.1087e+13, 9.9261e+12, 5.8869e+12, 1.2652e+13, 7.3653e+12,
        8.6665e+12, 9.1770e+12, 9.1725e+12, 1.2615e+13, 5.3056e+12, 1.2392e+13,
        1.0515e+13, 1.2133e+13, 1.0382e+13, 5.3476e+12, 1.6077e+13, 7.9580e+12,
        1.0407e+13, 6.3465e+12, 5.6809e+12, 1.2976e+13, 6.5919e+12, 8.2749e+12,
        9.3836e+12, 1.1836e+13, 7.6599e+12, 1.2222e+13, 7.1674e+12, 9.9024e+12,
        9.8338e+12, 9.0290e+12, 1.1724e+13, 7.1443e+12, 7.0074e+12, 6.2922e+12,
        7.9784e+12, 8.8975e+12, 8.8607e+12, 7.7339e+12, 1.0027e+13, 1.1607e+13,
        8.8462e+12, 9.0611e+12, 4.9445e+12, 8.4286e+12, 9.6355e+12, 1.4564e+13,
        1.0668e+13, 7.4145e+12, 1.0719e+13, 6.3460e+12, 7.2974e+12, 1.0550e+13,
        1.0919e+13, 1.0341e+13, 7.6165e+12, 9.7313e+12, 4.7716e+12, 1.2554e+13,
        8.6370e+12, 1.2333e+13, 1.1791e+13, 6.5879e+12, 7.6494e+12, 6.6971e+12,
        1.5953e+13, 6.9304e+12, 7.5339e+12, 1.3875e+13, 6.8804e+12, 7.6746e+12,
        7.6184e+12, 7.1824e+12, 8.3910e+12, 1.0483e+13, 1.0265e+13, 1.0140e+13,
        8.9387e+12, 1.0294e+13, 5.6561e+12, 1.4342e+13, 6.0854e+12, 1.2688e+13,
        1.3498e+13, 9.3328e+12, 6.1343e+12, 5.7433e+12, 8.8764e+12, 6.8327e+12,
        1.0404e+13, 1.1036e+13, 5.7637e+12, 4.5027e+12, 8.2111e+12, 8.7028e+12,
        6.9171e+12, 1.3123e+13])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.6040e+11, 1.0278e+11, 2.3522e+10, 1.2324e+11, 4.1246e+10, 1.5902e+12,
        3.0556e+10, 1.6022e+12, 4.9916e+10, 6.8723e+11, 1.6893e+11, 1.0083e+11,
        3.9015e+10, 4.6621e+10, 5.2921e+12, 9.0410e+10, 2.0011e+11, 3.4414e+11,
        3.4292e+11, 4.4221e+12, 2.9920e+11, 4.6897e+11, 1.1722e+11, 1.5144e+11,
        3.0117e+11, 4.7859e+10, 4.2019e+10, 5.1406e+11, 1.2553e+11, 1.3034e+11,
        1.0480e+11, 1.5908e+11, 1.8373e+12, 4.7614e+10, 1.7807e+10, 3.8436e+10,
        4.3536e+10, 2.5182e+10, 2.0583e+10, 4.0720e+11, 7.5987e+10, 8.8291e+10,
        1.2031e+11, 5.4866e+11, 9.8885e+10, 7.2005e+10, 8.5518e+10, 1.4461e+11,
        4.8295e+10, 4.8798e+10, 6.2113e+10, 1.0357e+11, 1.5250e+11, 1.1286e+10,
        5.1723e+09, 3.3506e+10, 1.8491e+10, 6.0062e+12, 8.9178e+10, 5.2046e+09,
        2.8637e+10, 1.0984e+12, 1.6498e+10, 7.5272e+10, 3.7149e+10, 2.0903e+12,
        1.0868e+11, 3.5708e+10, 2.2728e+10, 5.2099e+10, 4.1127e+11, 9.8165e+09,
        1.6047e+10, 4.8425e+11, 2.0982e+11, 6.7408e+10, 1.0863e+11, 1.2629e+11,
        2.3849e+11, 6.5496e+10, 4.0367e+10, 3.3432e+10, 2.0682e+10, 2.5237e+10,
        8.6744e+11, 3.7352e+11, 5.1354e+10, 1.4583e+10, 2.0194e+10, 4.6306e+10,
        6.7554e+10, 1.1957e+11, 9.1017e+10, 3.6998e+11, 9.4387e+10, 1.3217e+10,
        1.2802e+10, 4.9086e+11, 3.0392e+10, 1.7645e+11, 9.3182e+10, 2.1429e+12,
        3.1718e+10, 8.2229e+10, 1.6842e+11, 8.4481e+10, 7.7473e+10, 1.0549e+11,
        6.9914e+10, 2.8417e+10, 6.9616e+10, 7.2545e+10, 2.5405e+11, 2.8925e+10,
        9.7947e+12, 2.7941e+11, 1.6434e+12, 8.7531e+10, 6.5429e+10, 1.4583e+10,
        8.9479e+11, 2.1904e+10, 3.4594e+10, 4.2065e+10, 1.0736e+11, 9.1219e+10,
        5.9904e+12, 3.5905e+10, 2.4231e+12, 3.3204e+10, 3.5255e+11, 5.7867e+11,
        1.1336e+11, 3.9804e+11, 2.8148e+10, 1.9618e+11, 9.7198e+10, 4.3176e+10,
        4.2035e+10, 2.6087e+11, 1.5649e+12, 3.0057e+10, 1.5291e+11, 1.1188e+10,
        5.4916e+10, 5.9824e+11, 2.3333e+11, 2.0800e+12, 4.1795e+10, 7.5164e+10,
        3.1882e+10, 1.2759e+10, 1.1509e+11, 2.5691e+11, 3.1692e+11, 8.3316e+10,
        1.4105e+10, 6.4829e+10, 5.9926e+10, 4.8741e+10, 2.8076e+10, 6.1136e+10,
        4.6170e+11, 1.8446e+10, 3.3968e+10, 1.1235e+11, 6.3084e+09, 3.2807e+10,
        5.9969e+11, 4.4100e+10, 9.0762e+11, 4.9890e+11, 9.3687e+09, 9.2554e+10,
        1.3791e+10, 4.8878e+10, 1.5411e+11, 4.1829e+10, 2.2701e+12, 2.9931e+10,
        4.5492e+10, 7.2516e+10, 2.2250e+10, 1.3641e+11, 2.5514e+11, 1.0083e+11,
        2.4679e+10, 1.7311e+11, 3.1495e+10, 1.1227e+11, 6.0243e+09, 2.7543e+10,
        5.1849e+10, 3.3345e+10, 2.6448e+11, 2.0211e+11, 1.0059e+12, 1.5011e+10,
        1.3667e+10, 2.6429e+10, 1.5593e+10, 4.0605e+11, 5.6864e+10, 1.4526e+12,
        1.7724e+12, 5.7377e+10, 7.1313e+10, 1.6131e+11, 7.3637e+10, 1.2937e+11,
        3.4211e+10, 3.7453e+11, 1.6857e+10, 1.7495e+11, 9.8637e+10, 1.6672e+10,
        2.8523e+10, 5.2455e+10, 1.8346e+10, 4.2102e+11, 2.4970e+10, 4.9165e+11,
        3.0525e+11, 5.7941e+10, 7.3091e+10, 7.6535e+10, 2.2052e+10, 7.2232e+11,
        2.3965e+11, 1.3309e+11, 2.5531e+11, 4.9729e+11, 5.4916e+10, 1.1179e+11,
        4.6448e+11, 9.1336e+11, 2.1216e+11, 8.3723e+10, 3.2510e+12, 1.3490e+11,
        2.0039e+10, 7.1466e+11, 4.0734e+10, 2.1056e+10, 9.9242e+10, 6.4000e+10,
        3.1680e+10, 3.3681e+10, 1.2477e+11, 8.7334e+11, 3.5366e+10, 5.0406e+10,
        2.1631e+10, 6.1918e+10, 1.6540e+10, 4.2386e+10, 3.7580e+10, 1.5378e+10,
        6.0975e+09, 1.7544e+11, 2.2178e+10, 2.0636e+11, 1.3764e+11, 6.3630e+10,
        4.5487e+10, 1.1766e+12, 5.9393e+10, 1.3500e+11, 6.2315e+10, 9.4147e+10,
        6.7389e+10, 1.5226e+12, 3.1405e+12, 2.6400e+10, 6.8933e+10, 1.5021e+11,
        1.7826e+10, 5.5267e+10, 1.1690e+12, 3.2041e+10, 6.0616e+10, 2.7919e+10,
        1.4161e+11, 4.8113e+10, 3.6273e+11, 4.5162e+10, 4.0472e+10, 6.7944e+10,
        8.4056e+10, 9.6239e+10, 3.8503e+10, 1.7271e+10, 4.2492e+09, 1.5033e+10,
        7.0156e+10, 8.7636e+10, 1.3330e+11, 3.3862e+11, 1.2387e+10, 2.3141e+10,
        3.1466e+10, 2.7650e+11, 1.1663e+11, 8.6348e+10, 9.6879e+10, 4.4596e+10,
        4.4373e+10, 9.7554e+10, 9.7227e+10, 2.2388e+11, 3.9022e+10, 2.1322e+11,
        5.7794e+10, 2.4013e+10, 6.3046e+10, 4.8072e+11, 4.3166e+10, 5.4698e+11,
        3.6047e+10, 7.0371e+11, 1.1949e+12, 1.0667e+12, 2.4728e+10, 5.2123e+10,
        9.7105e+09, 5.8039e+11, 1.7293e+11, 6.3149e+11, 2.2116e+10, 5.3285e+11,
        2.4118e+10, 1.3941e+11, 7.3062e+11, 6.1724e+10, 1.0546e+11, 1.2355e+11,
        3.3761e+10, 5.9800e+10, 3.6808e+11, 2.8096e+11, 9.4095e+10, 7.7497e+10,
        2.1514e+12, 5.4067e+10, 4.6558e+10, 8.1434e+09, 5.6731e+10, 1.7736e+12,
        1.6970e+11, 2.0281e+10, 3.1442e+10, 4.8529e+09, 1.8261e+10, 9.1879e+10,
        2.3132e+10, 3.1307e+10, 7.3753e+10, 1.6911e+11, 5.3812e+11, 3.0542e+10,
        3.5382e+10, 1.0740e+12, 2.9748e+10, 2.5365e+10, 1.2509e+11, 1.5274e+11,
        6.5321e+10, 9.6316e+09, 6.8735e+10, 5.5837e+10, 1.5784e+10, 4.6218e+09,
        3.7430e+10, 1.7131e+11, 2.2632e+11, 1.2604e+11, 3.6161e+10, 7.1459e+10,
        1.6902e+10, 1.8007e+10, 5.2524e+10, 4.9804e+11, 2.1587e+10, 1.7757e+10,
        5.3283e+10, 4.7282e+10, 2.7383e+10, 6.6298e+12, 1.0233e+11, 3.0332e+12,
        4.2110e+10, 6.7539e+10, 2.9128e+11, 3.2604e+11, 6.1761e+10, 4.6526e+10,
        1.3254e+11, 1.7460e+11, 7.2706e+10, 7.1988e+10, 4.0551e+10, 2.9256e+10,
        6.4579e+10, 4.4984e+10, 7.6089e+10, 1.0276e+11, 5.6125e+10, 8.4068e+11,
        6.7464e+10, 2.0770e+11, 3.5890e+11, 1.0661e+12, 3.4883e+10, 6.0657e+10,
        1.3344e+10, 4.9386e+11, 7.3426e+10, 3.4001e+10, 1.5537e+11, 2.3730e+12,
        9.0433e+10, 1.0114e+11, 3.0247e+10, 1.6287e+11, 1.7094e+12, 8.3607e+10,
        3.2477e+10, 3.0074e+10, 5.8876e+11, 1.8713e+11, 3.6823e+10, 2.0954e+10,
        3.0432e+11, 1.4191e+10, 4.5043e+10, 4.7801e+11, 5.5789e+10, 4.0112e+11,
        4.6326e+10, 2.9853e+11, 1.0500e+11, 1.4247e+12, 1.8926e+11, 3.7771e+10,
        4.7559e+11, 6.0638e+11, 7.9906e+10, 4.4474e+10, 1.3388e+12, 7.2690e+10,
        2.3793e+10, 7.6900e+10, 1.2628e+11, 2.7350e+10, 1.3698e+12, 6.3847e+11,
        8.6141e+10, 1.8837e+10, 1.5484e+11, 3.9240e+10, 2.9952e+10, 1.2399e+11,
        2.1745e+10, 5.1657e+11, 2.8308e+10, 2.4839e+13, 1.2992e+11, 3.0586e+11,
        5.8380e+10, 2.5661e+10, 4.9456e+10, 3.1880e+10, 4.6869e+10, 2.2368e+11,
        1.9486e+11, 6.6977e+11, 4.6387e+10, 1.5670e+13, 2.7328e+12, 8.5376e+10,
        6.2902e+11, 2.1797e+10, 4.3521e+11, 6.2996e+10, 1.0701e+12, 9.5879e+11,
        1.9234e+10, 4.3124e+10, 4.7500e+11, 1.4204e+11, 1.2656e+11, 7.2691e+10,
        2.2462e+11, 4.4215e+10, 4.0431e+11, 2.2261e+10, 6.6978e+10, 7.1064e+10,
        7.7381e+10, 4.6197e+10, 3.0408e+10, 4.7616e+11, 6.9869e+10, 7.7120e+11,
        1.3143e+11, 7.8459e+10, 1.3904e+11, 6.2695e+10, 3.2032e+10, 3.1289e+10,
        5.3326e+09, 9.5717e+11])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.4390e+11, 1.7953e+11, 1.0703e+11, 2.3536e+11, 1.5784e+10, 1.1707e+12,
        4.0395e+10, 1.5531e+12, 3.3848e+10, 6.9185e+11, 1.3671e+11, 1.6655e+11,
        2.4142e+10, 1.3055e+11, 5.2978e+12, 1.6815e+11, 2.4281e+11, 3.2116e+11,
        2.9198e+11, 4.4571e+12, 3.5868e+11, 2.2041e+11, 3.1136e+10, 1.4993e+11,
        3.2450e+11, 1.4664e+10, 3.8287e+10, 5.0761e+11, 8.2467e+10, 1.8342e+11,
        1.8914e+11, 4.4024e+10, 1.7772e+12, 1.7563e+11, 1.1176e+11, 1.4009e+11,
        5.4000e+10, 7.0396e+10, 3.4649e+10, 3.6119e+11, 3.7371e+10, 4.5877e+10,
        1.5650e+10, 5.3593e+11, 3.8663e+10, 2.2642e+10, 2.0391e+11, 1.6004e+11,
        4.6086e+10, 1.7317e+10, 1.2437e+11, 1.8186e+10, 5.5698e+10, 1.4260e+11,
        6.9835e+10, 9.0618e+10, 1.1411e+11, 5.9544e+12, 2.5325e+10, 4.0411e+10,
        7.3355e+10, 1.1094e+12, 5.4535e+10, 1.6689e+11, 3.3368e+10, 2.0527e+12,
        1.7062e+11, 5.7268e+10, 6.0824e+10, 3.2335e+10, 3.5775e+11, 9.3334e+10,
        1.4277e+11, 6.5035e+11, 1.2139e+11, 5.4659e+10, 1.8001e+10, 1.2404e+11,
        2.7026e+11, 4.7854e+10, 6.6524e+10, 3.9732e+10, 1.3055e+11, 7.3645e+10,
        8.6461e+11, 2.5498e+11, 7.4842e+09, 1.0206e+11, 9.2736e+10, 4.9100e+10,
        1.1395e+11, 3.2938e+10, 1.4231e+11, 3.1624e+11, 3.9713e+10, 7.4811e+10,
        7.1135e+10, 4.6023e+11, 8.2811e+10, 8.1358e+10, 8.4175e+09, 2.1370e+12,
        1.1891e+11, 1.0215e+11, 1.4891e+11, 5.8830e+10, 5.2828e+10, 1.3050e+10,
        2.3196e+11, 1.0296e+11, 1.7274e+11, 4.2699e+10, 1.8729e+11, 6.2485e+10,
        9.8743e+12, 2.2358e+11, 1.7019e+12, 1.8125e+10, 1.5162e+11, 5.9266e+10,
        9.3849e+11, 9.8720e+10, 3.2160e+10, 1.0768e+11, 5.8128e+10, 1.5128e+10,
        5.9568e+12, 5.0555e+10, 2.3683e+12, 4.1829e+10, 4.4682e+11, 6.8254e+11,
        2.3035e+10, 3.1202e+11, 6.8682e+10, 2.0758e+11, 1.4039e+10, 5.2409e+10,
        2.4670e+10, 2.4062e+11, 1.5570e+12, 9.4216e+10, 1.2473e+10, 9.1988e+10,
        3.2635e+10, 4.5041e+11, 3.1356e+11, 2.0940e+12, 1.1206e+10, 1.7382e+10,
        6.1121e+10, 8.0185e+10, 2.1061e+11, 2.7920e+11, 2.9254e+11, 5.7505e+10,
        9.3125e+10, 1.2252e+11, 5.8644e+10, 1.0198e+11, 6.6241e+10, 2.4405e+10,
        3.5660e+11, 8.8944e+10, 4.3100e+10, 2.0609e+10, 5.9035e+10, 7.7532e+10,
        6.1241e+11, 4.9495e+10, 1.0980e+12, 5.8877e+11, 1.2321e+11, 1.6255e+11,
        6.1934e+10, 1.8553e+10, 1.2758e+11, 8.7119e+10, 2.3504e+12, 9.7950e+10,
        1.7883e+10, 8.4187e+10, 9.8054e+10, 1.1978e+11, 1.9455e+11, 2.2487e+11,
        5.5065e+10, 8.5097e+10, 9.1954e+10, 6.8748e+10, 8.4286e+10, 7.0498e+10,
        3.1502e+10, 5.7434e+10, 1.4617e+11, 1.1831e+11, 1.0588e+12, 1.0351e+11,
        6.9314e+10, 3.2918e+10, 3.3311e+10, 3.2359e+11, 1.7931e+11, 1.4469e+12,
        1.7519e+12, 4.5231e+10, 1.2330e+11, 4.5696e+10, 5.3860e+10, 4.6404e+10,
        9.4500e+10, 2.7010e+11, 4.5596e+10, 4.3337e+10, 3.9518e+10, 7.8042e+10,
        9.9509e+10, 9.3676e+10, 1.3728e+11, 3.8814e+11, 6.4935e+10, 5.4371e+11,
        1.5161e+11, 6.3910e+10, 4.1579e+10, 7.4509e+10, 8.0347e+10, 8.3236e+11,
        1.0911e+11, 1.0122e+11, 2.8740e+11, 4.7638e+11, 3.8206e+10, 3.3486e+10,
        5.3911e+11, 8.5901e+11, 2.2239e+11, 6.0132e+10, 3.2470e+12, 1.0794e+11,
        7.2179e+10, 6.3564e+11, 1.1200e+11, 9.7684e+10, 1.1473e+11, 1.0089e+10,
        1.2871e+11, 5.4873e+10, 1.8609e+10, 8.7261e+11, 1.1935e+10, 1.1586e+10,
        9.5858e+10, 1.1873e+11, 1.0169e+11, 1.0481e+11, 3.6396e+10, 8.3769e+10,
        5.9749e+10, 3.1755e+11, 8.8760e+10, 1.0698e+12, 2.2653e+11, 2.9706e+10,
        4.0577e+10, 1.1786e+12, 2.7351e+10, 2.4247e+11, 1.0089e+11, 1.4042e+10,
        5.6222e+10, 1.4830e+12, 3.1205e+12, 4.7220e+10, 1.7653e+11, 2.0893e+11,
        4.7175e+10, 1.3766e+11, 1.2807e+12, 5.6044e+10, 2.8317e+10, 5.5180e+10,
        5.9364e+10, 4.8579e+10, 3.0301e+11, 8.7236e+10, 6.5550e+10, 1.7184e+11,
        4.9062e+10, 6.1429e+10, 3.7917e+10, 8.8622e+10, 6.7990e+10, 3.8365e+10,
        2.9793e+10, 1.3028e+11, 3.8699e+10, 4.8742e+11, 9.5485e+10, 1.3748e+11,
        9.3844e+10, 2.1523e+11, 4.8988e+11, 2.6055e+10, 1.6929e+11, 7.1337e+10,
        6.7784e+10, 2.7149e+10, 2.9290e+10, 1.6074e+11, 4.0208e+10, 8.8666e+10,
        1.6248e+11, 6.9506e+10, 3.2938e+10, 4.6162e+11, 4.4846e+10, 5.9011e+11,
        4.8222e+10, 5.9628e+11, 1.1478e+12, 1.0769e+12, 5.7712e+10, 1.1632e+11,
        4.4848e+10, 5.9219e+11, 7.4773e+10, 6.0152e+11, 1.3741e+10, 6.1354e+11,
        8.4526e+10, 4.8598e+10, 6.9918e+11, 4.2659e+10, 1.6674e+11, 1.0862e+11,
        7.1480e+10, 1.2674e+10, 3.1851e+11, 9.5972e+11, 1.5589e+11, 9.2157e+10,
        2.1478e+12, 1.6630e+11, 4.8180e+10, 7.1458e+10, 2.9452e+10, 1.7651e+12,
        6.0963e+10, 9.2860e+10, 3.9884e+10, 8.5583e+10, 9.1894e+10, 1.1432e+11,
        5.3681e+10, 1.4843e+11, 5.0948e+10, 1.3853e+11, 5.5263e+11, 6.2518e+10,
        6.2561e+10, 1.0950e+12, 7.0081e+10, 1.1986e+11, 6.2035e+10, 3.7574e+10,
        4.5826e+10, 9.3383e+10, 2.0930e+10, 6.4493e+10, 9.8001e+10, 5.4997e+10,
        4.2171e+10, 3.7966e+10, 2.4347e+11, 3.3712e+10, 8.5643e+10, 3.2210e+10,
        5.5405e+10, 5.2100e+10, 7.2374e+10, 6.1313e+11, 5.5499e+10, 5.6863e+10,
        4.4716e+10, 6.9344e+10, 2.6239e+10, 6.5652e+12, 2.8812e+10, 2.9866e+12,
        6.7906e+09, 2.4923e+10, 4.4049e+11, 2.0217e+11, 1.1687e+11, 3.9403e+10,
        5.3797e+10, 1.5800e+11, 3.5521e+10, 1.4251e+11, 4.5057e+10, 7.7223e+10,
        1.5946e+10, 8.8853e+10, 9.4320e+09, 1.2068e+11, 3.6758e+10, 8.0962e+11,
        2.6191e+10, 1.2785e+11, 2.9906e+11, 9.9005e+11, 6.5466e+10, 2.3857e+10,
        9.1664e+10, 4.3395e+11, 2.7381e+11, 5.4090e+10, 1.3853e+11, 2.3984e+12,
        3.5374e+10, 1.1963e+11, 8.4803e+10, 1.5856e+11, 1.6482e+12, 1.1480e+11,
        4.6578e+10, 4.9350e+10, 5.2372e+11, 1.8568e+11, 1.3265e+10, 6.6810e+10,
        1.9480e+11, 9.5737e+10, 2.6605e+10, 5.4233e+11, 5.1243e+10, 4.8604e+11,
        1.3505e+11, 3.7613e+11, 2.3661e+10, 1.4994e+12, 9.1803e+10, 3.6593e+10,
        4.8065e+11, 5.7066e+11, 1.4918e+11, 2.7870e+10, 1.2889e+12, 4.3920e+10,
        3.8635e+10, 5.4853e+10, 7.6322e+10, 7.8583e+10, 1.3344e+12, 6.4946e+11,
        2.0606e+10, 6.3256e+10, 1.9938e+11, 2.0674e+10, 4.8659e+10, 5.6362e+10,
        1.2427e+11, 4.9711e+11, 7.6158e+10, 2.4742e+13, 6.6474e+10, 2.5335e+11,
        6.8273e+10, 5.5797e+10, 3.9792e+10, 4.6325e+10, 3.4762e+10, 2.1587e+11,
        1.9611e+11, 6.5731e+11, 1.1583e+11, 1.5704e+13, 2.7982e+12, 1.0710e+11,
        7.3386e+11, 1.1781e+11, 5.1042e+11, 8.8500e+10, 9.9157e+11, 1.0074e+12,
        4.1565e+10, 6.7726e+10, 4.4585e+11, 6.6451e+10, 2.6532e+10, 7.5583e+10,
        1.6904e+11, 9.0727e+10, 4.2371e+11, 5.2309e+10, 3.1360e+10, 7.7434e+10,
        1.6714e+10, 1.5600e+10, 7.1967e+10, 5.8832e+11, 5.7794e+10, 7.0240e+11,
        7.6416e+10, 1.1227e+11, 1.2942e+11, 6.1601e+10, 5.1828e+10, 8.1868e+10,
        6.1907e+10, 8.6400e+11])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.7185e+11, 1.2128e+11, 1.1850e+11, 1.9758e+11, 2.6077e+11, 2.0445e+11,
        2.4605e+11, 9.2918e+10, 2.1120e+11, 2.4815e+11, 1.9519e+11, 2.1022e+11,
        2.0329e+11, 1.2105e+11, 1.6008e+11, 2.3377e+11, 2.3251e+11, 2.7228e+11,
        1.0967e+11, 2.4403e+11, 1.3479e+11, 1.7756e+11, 1.9959e+11, 2.4034e+11,
        2.3812e+11, 1.9559e+11, 2.7018e+11, 2.1806e+11, 1.4487e+11, 1.7829e+11,
        1.4913e+11, 1.8907e+11, 2.7518e+11, 2.0278e+11, 2.9718e+11, 1.6337e+11,
        2.6935e+11, 1.5767e+11, 2.7498e+11, 1.2065e+11, 2.6257e+11, 2.0318e+11,
        7.8857e+10, 1.5764e+11, 1.5864e+11, 2.5761e+11, 2.3326e+11, 1.4729e+11,
        1.2370e+11, 1.8595e+11, 1.6741e+11, 1.4136e+11, 1.9910e+11, 1.9451e+11,
        1.2915e+11, 2.0429e+11, 2.2786e+11, 3.3425e+11, 2.1573e+11, 2.6506e+11,
        1.8990e+11, 1.9683e+11, 1.4583e+11, 2.3178e+11, 1.9680e+11, 1.0766e+11,
        1.7752e+11, 3.0423e+11, 2.2967e+11, 1.8406e+11, 1.8779e+11, 1.5638e+11,
        8.5050e+10, 1.2890e+11, 1.3405e+11, 1.6049e+11, 1.5906e+11, 1.8541e+11,
        1.9415e+11, 1.5800e+11, 1.2227e+11, 2.6981e+11, 2.2765e+11, 2.4564e+11,
        1.7995e+11, 1.3788e+11, 2.3074e+11, 1.6728e+11, 1.2639e+11, 1.7635e+11,
        1.7494e+11, 2.2193e+11, 1.9185e+11, 2.2035e+11, 1.9984e+11, 9.7979e+10,
        2.0105e+11, 1.6940e+11, 3.5873e+11, 1.1369e+11, 2.5544e+11, 3.2860e+11,
        1.7790e+11, 1.8066e+11, 1.8958e+11, 2.4294e+11, 2.2273e+11, 9.3210e+10,
        1.6498e+11, 2.1185e+11, 2.1102e+11, 1.0713e+11, 1.9831e+11, 1.9113e+11,
        1.2634e+11, 1.5715e+11, 2.3499e+11, 1.6028e+11, 1.9415e+11, 3.3439e+11,
        1.5577e+11, 2.6158e+11, 1.5484e+11, 1.2934e+11, 1.6363e+11, 1.9376e+11,
        1.3296e+11, 1.4137e+11])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.2931e+11, 3.5923e+11, 4.0916e+11, 2.1062e+11, 3.4880e+11, 1.6943e+11,
        4.7663e+11, 3.8313e+11, 2.4179e+11, 6.2592e+11, 1.9217e+11, 2.2603e+11,
        3.8512e+11, 2.5039e+11, 3.7309e+11, 1.8749e+11, 4.1340e+11, 4.0835e+11,
        4.2120e+11, 4.4029e+11, 3.0315e+11, 1.4493e+11, 2.4121e+11, 1.8657e+11,
        2.9325e+11, 1.8470e+11, 3.6047e+11, 1.7360e+11, 2.8979e+11, 3.8675e+11,
        4.6598e+11, 5.4147e+11, 4.4536e+11, 2.8659e+11, 4.3507e+11, 2.9249e+11,
        3.9556e+11, 4.0058e+11, 3.1491e+11, 1.4632e+11, 3.9453e+11, 3.0829e+11,
        2.5984e+11, 2.2112e+11, 6.5303e+11, 3.0817e+11, 2.1780e+11, 3.2551e+11,
        5.0429e+11, 3.0129e+11, 4.0614e+11, 2.8353e+11, 4.6649e+11, 2.4832e+11,
        4.0642e+11, 4.6943e+11, 2.8324e+11, 3.4154e+11, 3.9757e+11, 6.0337e+11,
        1.7959e+11, 3.5242e+11, 4.1619e+11, 1.9082e+11, 3.4963e+11, 4.2175e+11,
        2.0418e+11, 2.6301e+11, 2.4872e+11, 2.3548e+11, 4.3664e+11, 3.4398e+11,
        2.6762e+11, 2.2434e+11, 2.9024e+11, 4.3479e+11, 2.5504e+11, 2.3211e+11,
        3.9248e+11, 1.7680e+11, 5.6183e+11, 1.7781e+11, 3.6982e+11, 5.9494e+11,
        3.5157e+11, 2.0152e+11, 3.6894e+11, 3.4893e+11, 1.7709e+11, 3.8178e+11,
        4.6128e+11, 2.7966e+11, 4.6152e+11, 2.5185e+11, 4.1054e+11, 2.6712e+11,
        2.2103e+11, 3.4662e+11, 4.4625e+11, 3.3018e+11, 2.6587e+11, 4.8545e+11,
        3.1037e+11, 3.2462e+11, 1.6963e+11, 4.2849e+11, 2.7176e+11, 3.1318e+11,
        3.6383e+11, 3.2054e+11, 3.1538e+11, 4.8189e+11, 1.6250e+11, 3.4574e+11,
        2.0503e+11, 3.3933e+11, 3.0157e+11, 3.1698e+11, 2.7303e+11, 3.7493e+11,
        4.3280e+11, 6.3580e+11, 3.9730e+11, 4.0148e+11, 3.8111e+11, 4.8473e+11,
        4.0391e+11, 3.4373e+11])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.5060e+09, 1.8541e+10, 3.9379e+09, 3.3415e+11, 2.7209e+09, 4.3993e+11,
        1.1433e+10, 1.0256e+10, 8.5275e+09, 9.7514e+09, 1.2218e+10, 7.7291e+09,
        1.5174e+10, 7.0638e+10, 8.1097e+09, 4.0317e+09, 1.9111e+10, 6.2103e+09,
        4.5275e+09, 1.4074e+10, 1.0042e+10, 2.1678e+11, 8.3161e+09, 2.8654e+10,
        1.3971e+10, 8.3935e+09, 7.0520e+09, 1.3717e+10, 1.7611e+10, 7.2209e+09,
        1.6087e+10, 4.3644e+09, 4.1595e+09, 3.4690e+10, 4.3898e+09, 2.7057e+09,
        8.8410e+09, 3.8227e+09, 1.8829e+09, 1.3242e+11, 4.0762e+10, 4.0615e+09,
        6.7678e+09, 3.6910e+09, 6.9599e+09, 9.6372e+09, 7.6197e+10, 1.4063e+10,
        3.5560e+09, 1.0081e+10, 5.8294e+10, 8.6637e+09, 4.6355e+10, 2.2629e+09,
        1.8114e+09, 4.3390e+09, 6.8377e+09, 5.3262e+10, 1.0867e+10, 1.6974e+09,
        7.9300e+09, 5.7964e+09, 4.6331e+09, 9.9731e+08, 4.0487e+09, 5.8286e+09,
        7.4531e+09, 1.9483e+09, 1.3816e+10, 1.1650e+10, 5.3023e+09, 5.9191e+08,
        5.8678e+09, 6.6972e+09, 1.0583e+11, 6.1361e+09, 7.6784e+09, 7.9934e+09,
        8.1182e+09, 1.9705e+09, 2.2562e+10, 6.2786e+09, 6.7188e+09, 8.7275e+09,
        4.9744e+09, 1.0584e+10, 5.6898e+09, 1.2517e+09, 1.7534e+09, 3.4804e+09,
        1.1875e+10, 2.3509e+09, 1.9523e+11, 7.4218e+09, 1.3989e+10, 1.2944e+11,
        2.2593e+09, 3.7258e+10, 1.0212e+10, 1.1689e+10, 9.6133e+09, 8.2380e+09,
        4.3204e+09, 7.8277e+09, 6.6752e+09, 1.2211e+10, 4.7735e+09, 1.3839e+09,
        2.4483e+09, 8.9351e+09, 1.3782e+10, 1.0386e+10, 1.7485e+10, 2.5055e+09,
        5.8643e+09, 5.0103e+09, 8.9683e+09, 1.1296e+10, 5.9407e+09, 2.9516e+10,
        8.6777e+09, 1.5518e+10, 4.7253e+09, 5.7609e+08, 4.3768e+09, 8.8261e+08,
        1.2105e+10, 5.0260e+10, 1.1124e+10, 1.2465e+09, 8.3878e+09, 1.2739e+10,
        6.2121e+09, 9.7142e+09, 4.1853e+09, 9.8709e+09, 4.6110e+09, 1.0543e+09,
        1.4045e+09, 1.0405e+10, 1.4602e+10, 7.4916e+09, 4.7535e+09, 8.2437e+09,
        9.6062e+09, 1.0874e+11, 7.4231e+09, 5.7992e+09, 9.9543e+09, 1.2955e+10,
        5.8812e+09, 1.7537e+10, 1.7731e+09, 1.8779e+10, 1.0723e+10, 1.4192e+10,
        2.5548e+09, 1.5006e+11, 4.1133e+09, 1.3002e+10, 6.9106e+09, 3.3845e+09,
        3.3615e+10, 1.3067e+10, 1.2803e+09, 7.2908e+09, 2.1772e+09, 9.9339e+09,
        4.1187e+10, 8.0650e+09, 2.2997e+11, 7.7102e+09, 3.1854e+09, 9.9676e+09,
        2.3675e+09, 1.2063e+10, 2.6143e+11, 2.6744e+09, 1.1563e+10, 7.1924e+09,
        1.0977e+10, 9.8642e+09, 1.3400e+10, 8.6258e+10, 6.3938e+09, 9.0261e+10,
        1.0197e+10, 1.4847e+11, 2.3852e+10, 2.6750e+10, 5.7614e+09, 7.4200e+09,
        5.7645e+09, 2.3124e+09, 6.7486e+09, 9.9442e+09, 1.4457e+10, 1.1410e+10,
        2.1295e+09, 6.5180e+09, 9.4027e+09, 7.6701e+09, 8.5200e+10, 1.3242e+10,
        1.2721e+10, 8.0283e+09, 1.1202e+10, 5.3078e+10, 1.5031e+10, 4.2364e+09,
        3.9331e+09, 6.8464e+09, 4.6736e+09, 1.4468e+10, 1.2893e+10, 5.5930e+09,
        2.5188e+09, 6.5846e+09, 2.3755e+09, 1.1557e+10, 2.2459e+09, 1.2786e+10,
        8.5351e+09, 8.9319e+09, 1.2860e+10, 9.0349e+09, 1.0668e+09, 1.2341e+10,
        5.5024e+09, 2.1091e+10, 8.7933e+09, 1.0401e+10, 2.0622e+09, 2.2654e+10,
        7.1035e+09, 1.4615e+10, 8.6914e+09, 2.1044e+10, 3.9312e+09, 7.8360e+10,
        8.9860e+09, 8.9120e+09, 6.1215e+09, 7.4242e+09, 1.8572e+11, 7.9848e+09,
        2.6996e+09, 1.1769e+09, 2.2967e+10, 1.0311e+10, 1.7823e+10, 8.4191e+09,
        1.5014e+10, 1.4085e+10, 1.5579e+09, 1.2466e+10, 8.9188e+09, 9.8461e+09,
        4.9701e+09, 9.1538e+09, 1.2391e+10, 8.4425e+11, 1.2968e+10, 2.0703e+10,
        6.2928e+09, 1.0031e+10, 2.2847e+10, 7.1828e+09, 9.8631e+10, 1.4199e+09,
        3.1571e+09, 6.8023e+09, 8.1617e+09, 1.7086e+09, 3.0472e+10, 1.5056e+11,
        1.1282e+10, 1.0055e+10, 1.1187e+10, 8.9658e+08, 1.0516e+10, 2.6884e+09,
        4.1498e+10, 6.0881e+09, 9.3595e+08, 1.1106e+09, 1.0491e+10, 1.1634e+10,
        2.9589e+09, 2.6785e+09, 8.9699e+08, 1.2308e+10, 9.6553e+09, 1.6281e+10,
        7.5929e+09, 9.4986e+09, 3.5227e+09, 6.9428e+10, 4.5410e+09, 7.8242e+09,
        7.0807e+09, 1.2837e+11, 5.1755e+11, 9.9514e+09, 1.5189e+10, 1.4225e+09,
        5.7810e+10, 4.1997e+09, 9.0565e+09, 1.3085e+10, 6.1658e+09, 3.8609e+10,
        6.8324e+09, 1.2164e+10, 6.4949e+09, 9.8550e+09, 6.7240e+08, 1.0942e+10,
        5.1711e+09, 4.7084e+09, 2.1224e+10, 6.4448e+09, 6.0773e+09, 6.4114e+08,
        1.2431e+10, 6.9950e+10, 1.1353e+10, 1.2364e+10, 3.0481e+09, 1.3572e+10,
        9.1379e+09, 1.2131e+10, 8.1493e+09, 2.2215e+09, 9.5045e+09, 7.5997e+09,
        1.1072e+09, 3.4403e+09, 1.2346e+10, 6.1044e+11, 4.6034e+09, 4.1652e+09,
        8.5212e+09, 8.6686e+09, 1.7896e+09, 1.8570e+10, 1.3859e+10, 1.4142e+10,
        5.7004e+09, 7.9510e+10, 1.6636e+10, 5.3932e+09, 3.8898e+09, 1.6670e+10,
        2.3738e+09, 4.8657e+09, 9.4151e+09, 1.6104e+10, 2.7755e+10, 8.0508e+09,
        3.9874e+09, 1.1798e+10, 3.6286e+09, 6.1322e+09, 5.1006e+09, 4.7486e+09,
        1.1399e+09, 8.0782e+09, 1.1752e+10, 6.1413e+09, 4.2145e+09, 5.9568e+09,
        1.3841e+10, 8.2706e+09, 9.7115e+09, 3.6491e+09, 6.8741e+09, 6.1574e+09,
        8.1677e+09, 6.7462e+09, 1.9245e+09, 1.4980e+10, 4.9358e+08, 1.7942e+09,
        1.0781e+10, 9.7336e+09, 9.1725e+09, 1.9485e+10, 4.0175e+09, 7.4436e+09,
        1.1673e+10, 1.5818e+10, 1.5310e+10, 1.4774e+10, 1.9601e+10, 3.5144e+08,
        9.2707e+09, 8.1070e+09, 9.6407e+09, 1.1108e+10, 9.1618e+09, 3.0763e+09,
        2.4166e+09, 1.0697e+10, 3.5520e+09, 5.7891e+10, 8.8126e+09, 1.6352e+10,
        1.1032e+10, 8.8545e+09, 1.1819e+10, 8.1798e+09, 9.0458e+09, 5.6043e+09,
        3.5882e+09, 5.4853e+10, 2.1864e+11, 1.1905e+10, 9.5071e+09, 8.5285e+09,
        5.7833e+09, 9.7545e+09, 5.0820e+09, 2.8645e+09, 1.9322e+10, 5.3206e+09,
        5.6608e+09, 8.0408e+09, 8.6338e+09, 5.8000e+09, 5.6124e+09, 1.0045e+10,
        9.5656e+09, 1.5979e+09, 5.9492e+09, 1.5056e+10, 7.4378e+09, 1.0758e+10,
        1.0922e+10, 8.5602e+09, 6.0924e+09, 8.4217e+09, 7.2772e+09, 1.6085e+09,
        1.4876e+10, 9.2018e+09, 4.2183e+09, 1.4258e+10, 5.7116e+09, 2.1832e+09,
        1.7751e+09, 4.7112e+09, 1.9034e+10, 1.1078e+10, 9.5018e+09, 3.7943e+09,
        1.2758e+10, 1.4029e+09, 1.2672e+10, 1.6848e+10, 9.0625e+08, 8.0415e+09,
        1.1350e+10, 9.3773e+09, 5.0520e+09, 1.5682e+11, 1.1755e+10, 6.0381e+09,
        8.0069e+09, 5.8382e+09, 1.3569e+10, 4.5035e+09, 5.0488e+10, 1.0148e+10,
        7.7373e+09, 3.9984e+09, 3.2145e+09, 7.9033e+09, 4.9456e+10, 1.1355e+10,
        2.9350e+09, 3.4789e+09, 6.3198e+09, 1.2706e+10, 8.3730e+09, 1.6443e+10,
        3.0153e+09, 9.0180e+09, 4.6717e+10, 5.7450e+09, 1.6749e+09, 5.6934e+09,
        1.1163e+10, 8.3681e+09, 4.5917e+09, 2.1696e+09, 1.2698e+09, 1.3616e+10,
        2.3913e+09, 5.1380e+09, 4.0911e+09, 1.1354e+10, 9.1252e+09, 1.1546e+10,
        8.7923e+09, 9.3940e+09, 8.0839e+09, 7.9931e+10, 7.0288e+09, 1.1546e+10,
        9.9865e+09, 8.9248e+09])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5192e+10, 2.5168e+10, 1.9128e+10, 2.3169e+10, 2.8561e+10, 2.3557e+10,
        1.5943e+10, 2.9476e+10, 1.5332e+10, 1.8320e+10, 2.1077e+10, 2.4554e+10,
        2.1386e+10, 3.1377e+10, 2.3248e+10, 2.9207e+10, 1.9690e+10, 1.6571e+10,
        2.1648e+10, 1.4217e+10, 2.9293e+10, 1.5853e+10, 1.0428e+10, 2.2897e+10,
        2.2325e+10, 2.0295e+10, 2.5355e+10, 1.9694e+10, 2.7415e+10, 2.5310e+10,
        3.0347e+10, 3.1030e+10, 3.5311e+10, 4.7159e+10, 1.7115e+10, 1.1416e+10,
        2.5785e+10, 3.7232e+10, 2.8952e+10, 1.6004e+10, 1.6699e+10, 1.8797e+10,
        3.0732e+10, 1.9546e+10, 2.0920e+10, 2.8315e+10, 1.1338e+10, 3.8433e+10,
        1.9484e+10, 3.7231e+10, 2.5141e+10, 1.8108e+10, 1.0685e+10, 2.5519e+10,
        2.0232e+10, 2.5301e+10, 3.6196e+10, 2.7145e+10, 2.6336e+10, 1.5199e+10,
        3.2351e+10, 2.0337e+10, 1.3933e+10, 2.1275e+10, 2.1751e+10, 2.2424e+10,
        2.3229e+10, 1.6243e+10, 2.5990e+10, 2.2263e+10, 1.5172e+10, 2.1636e+10,
        1.8618e+10, 1.7936e+10, 3.1453e+10, 2.0364e+10, 1.6342e+10, 1.8059e+10,
        3.3145e+10, 3.3978e+10, 2.7775e+10, 2.0000e+10, 2.0267e+10, 1.8548e+10,
        1.9371e+10, 2.3235e+10, 3.4278e+10, 2.5348e+10, 2.4322e+10, 3.6140e+10,
        2.1379e+10, 1.8325e+10, 2.9704e+10, 2.7142e+10, 3.2572e+10, 2.7591e+10,
        2.2050e+10, 3.5140e+10, 3.0848e+10, 1.0878e+10, 9.9849e+09, 2.3507e+10,
        1.6267e+10, 1.9002e+10, 2.0766e+10, 2.0658e+10, 3.0787e+10, 1.8003e+10,
        2.7213e+10, 1.9514e+10, 3.4222e+10, 1.9108e+10, 3.2954e+10, 2.2099e+10,
        2.4383e+10, 1.9290e+10, 2.2085e+10, 1.9674e+10, 2.2824e+10, 2.8792e+10,
        1.9946e+10, 1.5364e+10, 1.6298e+10, 2.1598e+10, 3.1276e+10, 2.9685e+10,
        1.8020e+10, 1.9217e+10])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.9210e+10, 3.2672e+10, 3.4483e+10, 2.1081e+10, 5.8100e+10, 3.3929e+10,
        2.2695e+10, 5.1729e+10, 2.0782e+10, 3.2206e+10, 5.5082e+10, 6.1725e+10,
        4.5896e+10, 4.2103e+10, 2.8461e+10, 3.8702e+10, 7.5447e+10, 4.4987e+10,
        7.2937e+10, 3.6611e+10, 5.4527e+10, 6.8579e+10, 3.8733e+10, 3.1809e+10,
        3.0303e+10, 5.2008e+10, 3.5559e+10, 5.6798e+10, 1.8342e+10, 4.6149e+10,
        4.4225e+10, 3.0815e+10, 4.2491e+10, 5.3991e+10, 3.4081e+10, 2.4684e+10,
        3.1372e+10, 5.1487e+10, 2.3770e+10, 3.5021e+10, 6.4647e+10, 4.6308e+10,
        2.5881e+10, 1.9512e+10, 4.1695e+10, 5.2907e+10, 3.1929e+10, 5.3920e+10,
        3.7881e+10, 3.1903e+10, 3.6808e+10, 4.1940e+10, 4.1762e+10, 2.9959e+10,
        3.1300e+10, 4.9714e+10, 4.3740e+10, 2.4445e+10, 6.1158e+10, 3.4837e+10,
        4.2892e+10, 1.8340e+10, 3.8632e+10, 5.5401e+10, 3.8425e+10, 3.9703e+10,
        7.0347e+10, 2.4963e+10, 6.4377e+10, 3.4777e+10, 4.4775e+10, 4.2540e+10,
        4.6477e+10, 4.9348e+10, 4.3813e+10, 3.2862e+10, 4.5967e+10, 2.5006e+10,
        4.6603e+10, 3.5914e+10, 7.9200e+10, 3.7155e+10, 4.1305e+10, 2.0318e+10,
        1.6578e+10, 4.1043e+10, 4.3028e+10, 4.6306e+10, 3.1370e+10, 3.7596e+10,
        3.3696e+10, 4.2360e+10, 3.1134e+10, 3.3913e+10, 4.3164e+10, 3.1863e+10,
        4.5163e+10, 4.4281e+10, 2.4991e+10, 6.6994e+10, 5.5923e+10, 1.8473e+10,
        3.6414e+10, 2.4151e+10, 3.0729e+10, 3.9501e+10, 3.3315e+10, 6.0651e+10,
        4.2233e+10, 3.0157e+10, 3.2251e+10, 3.4103e+10, 4.3306e+10, 5.3363e+10,
        4.6802e+10, 3.4849e+10, 4.2822e+10, 2.9601e+10, 2.8418e+10, 4.1922e+10,
        3.7243e+10, 3.5214e+10, 5.4800e+10, 2.3233e+10, 3.0631e+10, 2.7591e+10,
        3.2062e+10, 6.3967e+10])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.7327e+08, 4.1234e+08, 7.2003e+07, 5.9524e+08, 1.0169e+08, 9.6361e+08,
        4.1916e+08, 3.9986e+08, 4.7542e+08, 1.1728e+09, 7.6725e+08, 3.5297e+08,
        4.8059e+08, 2.8746e+09, 3.9254e+08, 3.8541e+09, 6.4378e+08, 5.4071e+08,
        1.0075e+09, 9.5489e+08, 1.7780e+09, 5.0080e+08, 2.1958e+09, 8.0818e+08,
        3.6339e+08, 4.3620e+08, 7.5228e+08, 1.2938e+09, 7.1036e+08, 1.2197e+08,
        7.2531e+09, 4.2102e+07, 2.0580e+08, 8.0868e+10, 1.3596e+08, 2.5222e+10,
        4.7485e+08, 8.8408e+07, 2.4971e+08, 1.3472e+09, 5.2874e+08, 2.4056e+09,
        1.8080e+07, 3.5993e+08, 1.7652e+09, 2.8541e+08, 6.3674e+08, 5.2245e+08,
        4.3440e+08, 1.5930e+08, 1.0201e+09, 2.9293e+09, 5.8568e+08, 1.5051e+08,
        4.9714e+07, 1.2765e+08, 9.2544e+08, 4.6269e+10, 1.8588e+08, 3.2979e+08,
        1.8558e+08, 1.1273e+09, 1.0408e+09, 2.5176e+08, 3.3989e+09, 1.7509e+09,
        4.5725e+08, 1.8649e+08, 6.7375e+09, 6.8630e+08, 3.8561e+08, 1.9880e+08,
        9.2778e+08, 8.2169e+08, 4.2664e+08, 6.9965e+08, 1.3284e+09, 4.7657e+09,
        4.7243e+08, 2.1102e+08, 9.4693e+08, 4.2396e+09, 5.4599e+08, 1.0566e+09,
        5.5253e+08, 4.9239e+08, 1.0017e+09, 4.4206e+08, 1.2051e+10, 1.3502e+08,
        5.0716e+08, 1.1031e+08, 4.3660e+08, 9.6634e+08, 7.0125e+08, 1.2907e+11,
        2.9083e+08, 1.4814e+09, 9.2301e+08, 1.0024e+09, 4.9284e+08, 3.3253e+08,
        2.6891e+09, 2.3422e+08, 2.4055e+08, 4.8028e+08, 4.5011e+08, 1.6948e+08,
        9.3658e+07, 6.7065e+07, 4.6734e+08, 1.9956e+08, 1.4154e+10, 1.5240e+08,
        1.8894e+09, 4.9471e+08, 3.3776e+08, 4.3605e+08, 1.5298e+09, 2.2500e+10,
        5.1121e+08, 6.7865e+08, 2.0411e+08, 1.1249e+08, 2.5353e+08, 5.7219e+08,
        1.1174e+10, 5.5281e+08, 3.4203e+08, 4.2276e+07, 9.0581e+08, 1.1170e+09,
        7.2688e+08, 2.2214e+09, 3.1205e+08, 1.0212e+09, 1.6283e+08, 1.7604e+08,
        2.8229e+07, 8.5299e+09, 1.4081e+09, 6.1490e+08, 7.3432e+08, 1.0792e+09,
        3.8628e+08, 8.3684e+08, 5.6941e+08, 2.8592e+08, 1.0941e+09, 2.8882e+08,
        1.0285e+09, 8.2386e+08, 8.7814e+07, 6.0982e+08, 3.4693e+08, 4.3427e+08,
        8.4372e+08, 6.7779e+08, 3.8843e+08, 9.7547e+08, 5.3958e+08, 3.1584e+08,
        7.8659e+08, 3.4105e+09, 1.7824e+08, 2.5106e+08, 9.4937e+07, 1.2276e+09,
        2.0129e+09, 3.5235e+08, 5.3186e+08, 4.3041e+08, 4.1866e+07, 2.5384e+08,
        6.6809e+09, 4.9466e+09, 5.5979e+08, 2.5068e+08, 1.0157e+09, 4.6960e+08,
        6.3852e+07, 4.4538e+08, 3.6496e+09, 6.0971e+08, 7.2667e+08, 1.7475e+09,
        2.2290e+08, 4.1939e+08, 4.2693e+09, 5.2958e+08, 8.9729e+08, 9.3585e+08,
        5.6927e+08, 1.7546e+08, 4.3299e+08, 3.9826e+08, 7.1911e+08, 1.7142e+07,
        4.0808e+07, 9.9341e+08, 6.0924e+08, 1.2418e+08, 6.0084e+08, 6.0264e+09,
        2.1712e+09, 1.0596e+09, 1.1711e+09, 1.4697e+09, 4.4740e+08, 1.6285e+08,
        1.4403e+08, 2.1048e+08, 1.5944e+08, 1.8081e+08, 6.5498e+09, 1.1737e+08,
        4.5211e+08, 7.1018e+08, 1.0440e+08, 4.7304e+08, 1.7149e+09, 6.7953e+08,
        9.2186e+07, 1.0005e+09, 5.7108e+08, 1.8086e+09, 6.0210e+08, 2.4662e+08,
        7.6409e+08, 4.2438e+08, 5.1380e+09, 8.4322e+08, 1.1543e+08, 1.1133e+09,
        2.8780e+08, 8.8853e+08, 9.9803e+08, 6.5996e+08, 2.3488e+08, 4.9654e+08,
        1.1523e+09, 6.8126e+08, 7.2098e+08, 2.5314e+08, 8.3812e+08, 1.0397e+09,
        1.0343e+08, 6.7773e+07, 7.0626e+08, 4.0899e+08, 6.2638e+08, 6.2725e+08,
        3.7664e+09, 8.7272e+09, 9.2495e+07, 8.1206e+08, 7.7169e+08, 7.1109e+08,
        2.1575e+08, 2.9393e+08, 6.4387e+08, 6.7645e+08, 4.6197e+08, 7.3737e+08,
        2.2682e+08, 3.8178e+08, 8.8098e+08, 4.6613e+08, 8.1293e+08, 5.7652e+08,
        1.7947e+08, 5.0079e+08, 3.5070e+08, 2.5237e+07, 6.4528e+08, 1.0009e+09,
        5.1713e+08, 1.2066e+10, 7.5362e+09, 1.0556e+08, 3.6513e+09, 1.5342e+09,
        7.0011e+08, 4.7499e+08, 3.0948e+08, 8.2459e+07, 1.5629e+09, 2.6390e+08,
        4.9377e+07, 3.1289e+08, 8.1862e+07, 8.5073e+08, 5.4808e+08, 6.5150e+08,
        4.6802e+08, 5.7090e+08, 1.1988e+08, 3.8402e+08, 3.6465e+08, 3.3773e+07,
        5.6029e+09, 7.6936e+08, 5.5836e+08, 6.7779e+08, 7.3221e+07, 7.7447e+07,
        8.4666e+08, 4.4945e+08, 9.9035e+08, 7.3196e+08, 2.0451e+09, 3.3191e+10,
        9.0854e+08, 3.5515e+08, 4.7349e+08, 5.3879e+08, 6.9777e+07, 2.6596e+08,
        4.0005e+08, 4.2048e+08, 6.6568e+08, 2.8470e+08, 5.6301e+08, 2.4685e+08,
        4.7064e+08, 6.3996e+08, 5.0622e+08, 5.0710e+08, 2.7576e+07, 1.0081e+09,
        3.9432e+08, 4.1976e+08, 3.4201e+08, 2.8367e+08, 1.7760e+08, 1.7786e+08,
        2.2838e+08, 1.2087e+09, 1.4828e+09, 1.2188e+10, 9.5279e+08, 8.2492e+07,
        9.4532e+08, 8.3861e+08, 1.1066e+10, 3.6800e+09, 4.8938e+08, 5.4896e+08,
        2.1953e+08, 2.4062e+08, 7.6931e+08, 1.1862e+09, 1.9287e+08, 6.2226e+08,
        3.8587e+08, 4.6928e+08, 2.7939e+09, 7.7834e+08, 7.4956e+08, 2.1831e+09,
        1.6028e+08, 6.3255e+08, 7.8358e+07, 1.7798e+08, 7.1439e+08, 3.1938e+08,
        1.7847e+10, 5.0809e+08, 1.7324e+09, 9.3821e+09, 1.3598e+08, 1.7028e+08,
        3.6160e+07, 5.4559e+08, 8.9340e+08, 3.9625e+07, 6.0818e+07, 3.8286e+08,
        1.6471e+09, 7.9174e+08, 2.6419e+08, 1.0392e+09, 8.2247e+07, 6.8719e+07,
        1.0484e+10, 3.9999e+08, 4.6426e+08, 8.8779e+08, 3.1509e+08, 3.9797e+08,
        5.5380e+08, 9.1835e+08, 4.2404e+08, 7.1413e+09, 6.9084e+08, 5.1855e+08,
        7.6374e+08, 3.2716e+08, 7.5411e+08, 5.0321e+09, 1.7297e+09, 4.0096e+09,
        3.8624e+07, 5.9659e+08, 1.0308e+08, 4.7592e+08, 4.7448e+08, 5.1757e+08,
        5.4482e+09, 1.4627e+09, 8.0383e+08, 2.8255e+08, 7.8205e+08, 7.9735e+08,
        1.8353e+08, 1.0142e+09, 5.0609e+08, 5.3275e+08, 9.7371e+08, 2.9479e+08,
        4.9336e+08, 2.4230e+08, 1.9891e+08, 5.5707e+08, 9.1418e+08, 4.3583e+08,
        7.7996e+07, 1.0367e+09, 4.2764e+08, 1.8756e+08, 5.1627e+07, 9.8440e+08,
        5.6267e+09, 8.9871e+07, 2.8253e+08, 8.2309e+08, 1.0719e+09, 3.9504e+08,
        4.8959e+08, 1.4930e+08, 8.6051e+08, 7.5884e+08, 1.4007e+09, 1.9442e+08,
        1.0167e+10, 6.5474e+08, 4.1176e+07, 3.0249e+08, 2.0532e+08, 6.3247e+08,
        1.2849e+08, 2.4322e+08, 7.1340e+08, 1.1843e+09, 3.9534e+08, 2.1631e+09,
        8.9237e+08, 3.2862e+08, 7.7512e+08, 1.0969e+10, 5.1961e+07, 5.3407e+07,
        1.5978e+08, 1.3100e+08, 2.2529e+07, 8.6733e+08, 8.5138e+09, 4.5694e+08,
        5.2598e+08, 5.6397e+08, 2.2991e+08, 7.6776e+07, 5.5986e+08, 3.2240e+09,
        9.1811e+08, 2.0219e+08, 1.5955e+08, 5.5542e+08, 7.5379e+08, 1.0103e+09,
        4.2216e+08, 1.5785e+08, 6.3794e+08, 6.0999e+08, 5.9264e+08, 3.1593e+09,
        4.9036e+07, 2.1580e+08, 1.0078e+09, 6.0688e+08, 2.2428e+09, 7.2537e+08,
        1.2267e+09, 4.4097e+08, 5.5577e+08, 1.8750e+08, 2.1807e+08, 9.9086e+08,
        1.0845e+08, 3.3546e+08, 4.0862e+08, 4.4430e+08, 3.8909e+09, 1.0175e+09,
        3.3574e+08, 1.1781e+09, 3.5331e+08, 9.3177e+08, 1.8369e+08, 2.5840e+08,
        2.3454e+08, 3.9135e+08])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.4883e+09, 1.6732e+09, 1.8140e+09, 2.4562e+09, 2.5067e+09, 3.0735e+09,
        3.8804e+09, 1.9450e+09, 3.2222e+09, 1.4687e+09, 3.2800e+09, 2.1300e+09,
        1.0285e+09, 1.0833e+09, 1.7959e+09, 1.6874e+09, 1.9560e+09, 1.5313e+09,
        1.9730e+09, 2.0570e+09, 2.5268e+09, 1.6413e+09, 2.2653e+09, 2.3203e+09,
        1.9596e+09, 1.7357e+09, 2.0869e+09, 2.6160e+09, 3.0173e+09, 1.6319e+09,
        1.0728e+09, 2.4231e+09, 2.1212e+09, 1.9658e+09, 1.4487e+09, 2.5067e+09,
        1.9490e+09, 1.1124e+09, 1.8578e+09, 3.2589e+09, 3.1784e+09, 2.4536e+09,
        1.6830e+09, 2.7019e+09, 1.7594e+09, 1.8250e+09, 1.5586e+09, 2.3188e+09,
        1.1516e+09, 4.4639e+09, 3.2186e+09, 1.7907e+09, 1.0934e+09, 2.4832e+09,
        1.6767e+09, 1.9910e+09, 2.3201e+09, 2.0911e+09, 1.5204e+09, 2.0842e+09,
        2.0328e+09, 1.1678e+09, 2.8218e+09, 2.1967e+09, 2.5682e+09, 2.0173e+09,
        2.0381e+09, 1.1929e+09, 1.6637e+09, 1.1682e+09, 2.2808e+09, 1.4933e+09,
        2.4171e+09, 9.0468e+08, 1.5517e+09, 1.9433e+09, 2.0991e+09, 1.3815e+09,
        1.9594e+09, 1.4424e+09, 1.7081e+09, 1.3416e+09, 1.5899e+09, 9.6036e+08,
        1.9678e+09, 2.4111e+09, 9.3957e+08, 1.4476e+09, 3.2211e+09, 1.7440e+09,
        1.5785e+09, 2.6980e+09, 3.0221e+09, 3.3750e+09, 1.7908e+09, 1.7993e+09,
        3.2859e+09, 1.7665e+09, 3.0716e+09, 2.6573e+09, 2.1023e+09, 1.6151e+09,
        2.1982e+09, 1.1262e+09, 1.6050e+09, 2.7903e+09, 1.1693e+09, 2.3451e+09,
        1.1054e+09, 1.7978e+09, 1.9085e+09, 2.3103e+09, 2.2901e+09, 2.3548e+09,
        1.2615e+09, 1.7005e+09, 2.3772e+09, 3.2647e+09, 2.5963e+09, 2.8269e+09,
        1.2805e+09, 2.9423e+09, 2.6134e+09, 1.8142e+09, 3.6199e+09, 3.8005e+09,
        1.5230e+09, 1.3731e+09])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.4305e+09, 5.6065e+09, 2.4913e+09, 5.8560e+09, 3.1103e+09, 3.4252e+09,
        2.0412e+09, 4.2900e+09, 2.4154e+09, 3.1173e+09, 2.9160e+09, 3.9558e+09,
        7.3493e+09, 3.4534e+09, 4.2823e+09, 3.6737e+09, 3.3964e+09, 5.1578e+09,
        3.0729e+09, 3.7578e+09, 2.1059e+09, 2.5660e+09, 3.3171e+09, 3.3007e+09,
        2.6788e+09, 3.0935e+09, 2.8921e+09, 2.0198e+09, 2.5834e+09, 4.7985e+09,
        3.9343e+09, 6.1624e+09, 2.6059e+09, 3.5198e+09, 3.1403e+09, 4.0916e+09,
        3.2653e+09, 2.9830e+09, 3.2112e+09, 1.8600e+09, 3.1736e+09, 4.8503e+09,
        2.0246e+09, 4.6135e+09, 3.2887e+09, 3.9651e+09, 5.3633e+09, 3.1028e+09,
        5.9315e+09, 3.0825e+09, 5.2908e+09, 2.3395e+09, 3.2008e+09, 5.2008e+09,
        2.5295e+09, 3.3011e+09, 4.0382e+09, 2.1231e+09, 2.9681e+09, 2.6275e+09,
        6.3766e+09, 3.5804e+09, 4.0475e+09, 2.3005e+09, 2.3674e+09, 2.5071e+09,
        4.7971e+09, 4.0815e+09, 4.5011e+09, 2.6283e+09, 3.0350e+09, 3.0717e+09,
        4.2237e+09, 5.8861e+09, 3.9073e+09, 2.4576e+09, 1.7719e+09, 2.3239e+09,
        2.9129e+09, 2.2925e+09, 4.3479e+09, 3.8100e+09, 2.8647e+09, 1.6139e+09,
        6.8516e+09, 3.1762e+09, 4.7626e+09, 2.9319e+09, 1.8378e+09, 2.8809e+09,
        2.8103e+09, 3.4164e+09, 8.0032e+09, 3.8953e+09, 2.4454e+09, 1.8666e+09,
        2.6054e+09, 3.5861e+09, 2.5640e+09, 3.1620e+09, 5.1706e+09, 2.5423e+09,
        3.5001e+09, 3.3501e+09, 4.4822e+09, 3.4831e+09, 4.6144e+09, 5.9004e+09,
        2.4017e+09, 2.3286e+09, 3.6657e+09, 4.0330e+09, 2.3404e+09, 2.9914e+09,
        2.8719e+09, 1.8925e+09, 2.8401e+09, 3.5627e+09, 2.7667e+09, 2.7050e+09,
        4.8057e+09, 2.0627e+09, 3.1868e+09, 1.0746e+09, 4.9202e+09, 2.1865e+09,
        4.0709e+09, 2.6464e+09])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.6818e+07, 2.3206e+07, 5.3311e+06, 2.0281e+07, 1.5817e+07, 1.4562e+07,
        5.2229e+07, 5.7342e+07, 6.2984e+07, 7.7226e+07, 3.3773e+07, 1.5536e+07,
        7.1377e+07, 8.0294e+07, 4.1621e+07, 2.5500e+08, 8.0162e+07, 2.6049e+07,
        5.4078e+07, 7.2945e+07, 4.3274e+08, 1.7685e+07, 2.3238e+07, 2.9511e+07,
        5.3211e+07, 8.2913e+07, 5.8636e+07, 4.6531e+07, 2.9402e+07, 7.4368e+08,
        3.7664e+07, 9.4109e+06, 1.4389e+07, 7.9160e+07, 1.0347e+08, 1.4694e+08,
        7.0009e+07, 1.2877e+08, 9.0530e+06, 3.4229e+07, 1.4483e+07, 1.5620e+08,
        1.0610e+07, 6.0501e+06, 4.6917e+08, 2.3589e+07, 7.4146e+07, 6.0021e+07,
        1.3934e+07, 3.2048e+07, 2.2377e+08, 9.0736e+07, 2.4936e+06, 7.2440e+06,
        1.1906e+07, 1.0690e+07, 8.1465e+07, 1.3179e+08, 1.4612e+07, 4.0141e+07,
        2.1344e+07, 3.9442e+07, 5.9207e+07, 2.3428e+07, 7.5395e+07, 4.0646e+07,
        7.3905e+07, 9.9704e+06, 5.7168e+07, 1.3439e+07, 1.1540e+07, 2.4728e+07,
        3.6348e+08, 6.0182e+07, 4.5760e+07, 3.0207e+06, 3.5626e+08, 5.0377e+07,
        4.5490e+07, 3.9278e+06, 1.8924e+08, 6.4928e+07, 9.1456e+08, 4.9983e+07,
        7.5871e+06, 2.6785e+07, 4.2100e+07, 2.8660e+07, 3.1856e+07, 8.9246e+06,
        6.3518e+07, 5.1155e+08, 1.2633e+07, 3.1554e+08, 5.6300e+07, 3.9571e+07,
        1.6073e+07, 5.7484e+07, 5.6135e+07, 6.0214e+07, 1.1374e+07, 3.1787e+07,
        2.2859e+07, 6.3107e+07, 4.7426e+06, 3.7717e+07, 7.4227e+06, 9.3211e+06,
        4.8242e+06, 2.1230e+07, 5.3522e+06, 1.7204e+07, 4.8634e+08, 1.5394e+07,
        5.0656e+07, 1.4334e+06, 1.6569e+09, 3.5097e+08, 4.5202e+07, 2.7506e+07,
        2.3072e+07, 5.9975e+07, 3.6177e+06, 7.8115e+06, 1.0901e+07, 7.0949e+06,
        3.1929e+07, 3.4573e+07, 4.6833e+07, 1.2391e+07, 1.4641e+08, 3.8229e+07,
        1.4350e+07, 4.5052e+07, 7.9878e+06, 1.4854e+07, 6.2801e+06, 1.7302e+07,
        8.1054e+06, 4.8988e+07, 6.6027e+07, 2.1679e+07, 6.6850e+06, 5.4592e+07,
        2.7595e+07, 5.9266e+07, 4.5836e+07, 2.1987e+07, 7.5256e+07, 9.4429e+08,
        3.8354e+07, 5.4972e+08, 1.0288e+09, 7.9049e+07, 2.6818e+07, 3.9425e+07,
        8.5078e+06, 3.6214e+07, 1.4129e+07, 2.2623e+07, 9.8239e+06, 5.3281e+06,
        4.1226e+07, 3.7921e+09, 4.7383e+06, 3.7769e+06, 1.2911e+07, 5.2882e+07,
        3.7925e+07, 8.8479e+07, 1.8976e+07, 3.5658e+06, 1.2768e+07, 5.5499e+07,
        6.3094e+07, 4.4241e+07, 3.9079e+07, 6.2533e+06, 3.9351e+07, 1.5379e+07,
        2.4674e+07, 2.0273e+07, 5.0622e+07, 2.0986e+07, 1.1435e+07, 3.0374e+07,
        2.0082e+07, 7.8377e+07, 8.0515e+07, 6.4712e+07, 2.7544e+07, 2.0722e+07,
        3.0428e+06, 5.5851e+06, 1.9033e+07, 1.5986e+07, 2.1320e+07, 1.6621e+07,
        8.5614e+06, 1.3387e+08, 9.6453e+07, 1.7443e+07, 1.8575e+07, 4.0524e+07,
        1.5481e+09, 5.6238e+07, 1.4888e+08, 4.2662e+07, 1.5863e+07, 5.5055e+06,
        3.0850e+06, 1.1270e+07, 1.4301e+07, 2.2296e+07, 4.2455e+07, 1.6488e+07,
        1.5023e+07, 3.6824e+07, 1.7300e+07, 6.3273e+06, 1.2193e+07, 4.1241e+07,
        3.9389e+08, 5.7021e+07, 1.5018e+07, 1.7058e+09, 3.0456e+08, 1.8870e+07,
        2.0151e+07, 1.9982e+07, 4.7739e+07, 3.9212e+07, 4.5116e+06, 4.0945e+07,
        7.2704e+06, 1.3251e+08, 2.1436e+08, 1.7794e+08, 3.1630e+06, 8.4049e+06,
        6.1625e+08, 2.3565e+07, 8.2332e+07, 1.2093e+07, 4.0310e+08, 6.3420e+07,
        4.2639e+07, 6.5190e+07, 1.2553e+09, 1.8677e+07, 1.6562e+07, 3.6964e+07,
        5.1048e+07, 5.9051e+07, 1.6805e+07, 6.1968e+07, 2.4978e+07, 1.9347e+07,
        1.6007e+07, 1.3659e+07, 2.4333e+08, 4.7105e+07, 6.4771e+07, 5.1026e+07,
        1.9008e+07, 1.2939e+07, 4.8956e+07, 1.0138e+08, 1.1594e+08, 1.5932e+08,
        5.8880e+06, 3.2138e+07, 5.4275e+07, 3.7892e+06, 4.8002e+07, 3.8609e+07,
        1.9770e+07, 4.7865e+07, 5.1372e+07, 2.3179e+07, 2.0410e+09, 4.1694e+08,
        6.9074e+07, 2.7373e+07, 2.0230e+07, 5.5380e+06, 6.9008e+07, 2.9560e+09,
        1.6583e+06, 6.5441e+06, 9.4070e+06, 8.2733e+06, 4.0987e+07, 4.9732e+07,
        2.2143e+07, 2.0644e+08, 7.7484e+06, 4.9194e+07, 1.0762e+06, 2.1809e+07,
        1.3136e+07, 6.3972e+07, 1.5741e+08, 2.8881e+07, 1.8058e+07, 1.6090e+07,
        4.4017e+07, 2.8218e+07, 7.5045e+07, 5.1866e+07, 1.6702e+09, 4.5189e+07,
        4.8598e+07, 1.2201e+08, 2.2278e+07, 5.7297e+07, 1.8491e+07, 4.6494e+07,
        6.6706e+07, 1.4760e+06, 6.3347e+07, 2.4264e+07, 2.2249e+08, 1.2714e+07,
        3.2619e+07, 6.9034e+07, 2.0908e+07, 9.6752e+07, 2.5031e+06, 2.0704e+07,
        5.6004e+07, 3.5710e+07, 8.2820e+06, 1.0199e+07, 1.3775e+08, 2.4549e+08,
        6.1159e+06, 1.3625e+07, 1.5648e+08, 1.2231e+10, 9.7601e+07, 1.1546e+07,
        1.9578e+08, 2.7877e+07, 5.0232e+07, 2.0277e+07, 3.6268e+07, 1.2165e+07,
        2.5469e+07, 3.3922e+07, 5.5344e+07, 1.1248e+08, 5.0932e+07, 5.1312e+07,
        7.3898e+06, 5.7277e+07, 3.3558e+07, 1.8915e+07, 1.2694e+07, 1.0206e+08,
        8.4741e+06, 3.7561e+07, 3.2321e+07, 2.0246e+07, 1.4206e+08, 3.7945e+06,
        1.9125e+07, 3.6756e+06, 4.1159e+07, 4.5102e+07, 2.8454e+06, 1.2194e+08,
        2.8134e+06, 3.7236e+07, 4.8770e+07, 1.1694e+07, 1.3199e+07, 1.4778e+07,
        1.0252e+08, 5.6460e+07, 9.8501e+05, 5.0098e+07, 1.8917e+07, 4.5917e+06,
        2.8348e+08, 1.7340e+08, 1.8516e+07, 5.5318e+07, 7.5328e+06, 7.5518e+06,
        5.2055e+07, 4.8975e+06, 6.5636e+07, 5.4776e+07, 4.4657e+07, 2.3404e+07,
        4.2211e+07, 1.0103e+07, 2.5119e+07, 2.1671e+07, 4.5452e+07, 3.4540e+07,
        1.9183e+07, 3.8895e+07, 8.4800e+06, 2.7628e+08, 2.4768e+07, 1.6392e+07,
        7.3003e+07, 1.1520e+07, 1.6778e+07, 1.1899e+07, 3.5513e+07, 3.6154e+07,
        2.3137e+07, 5.8799e+06, 2.6208e+07, 2.0714e+07, 6.2926e+07, 2.1028e+07,
        1.8197e+07, 1.7701e+07, 2.2268e+06, 5.7833e+07, 1.4969e+07, 2.0267e+07,
        9.1237e+05, 5.3526e+07, 1.7542e+08, 3.7032e+06, 1.0425e+07, 4.0775e+07,
        5.2044e+07, 4.5851e+06, 9.2466e+06, 4.9082e+07, 2.4787e+07, 1.2483e+07,
        7.2999e+06, 5.3305e+07, 2.5560e+07, 4.6784e+08, 1.2052e+08, 5.5723e+06,
        4.6809e+07, 1.2086e+08, 2.8798e+07, 1.4100e+07, 1.4266e+07, 8.5934e+08,
        3.3761e+07, 3.6344e+06, 1.3956e+07, 5.2903e+07, 5.6595e+06, 4.7052e+07,
        4.5084e+07, 5.7569e+06, 3.4693e+07, 8.3481e+07, 1.6813e+07, 1.9620e+07,
        7.9262e+06, 2.2390e+07, 2.0674e+08, 1.6964e+08, 4.0551e+07, 3.1823e+06,
        5.3901e+06, 4.9213e+07, 1.4577e+07, 3.5543e+06, 1.0541e+07, 5.6270e+07,
        6.0661e+07, 2.9908e+07, 1.0493e+07, 1.1284e+07, 2.4936e+08, 9.6676e+08,
        7.3697e+06, 1.1769e+08, 1.0604e+08, 6.5663e+06, 2.8951e+08, 5.3921e+07,
        7.3664e+06, 3.7486e+06, 6.5560e+08, 4.2309e+07, 3.3912e+07, 4.7437e+07,
        1.0514e+08, 5.6108e+07, 5.6499e+07, 9.6577e+06, 2.2941e+07, 4.8492e+07,
        4.1758e+08, 1.6295e+07, 6.4606e+07, 5.3980e+07, 6.8440e+07, 5.4726e+08,
        2.1147e+07, 3.9625e+07, 9.3384e+06, 3.9210e+07, 7.2392e+06, 1.7464e+07,
        2.6674e+07, 4.4562e+07])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.4614e+07, 8.0679e+07, 5.9101e+07, 3.7158e+07, 3.5998e+07, 8.0358e+07,
        3.7532e+07, 6.6047e+07, 5.7755e+07, 7.0657e+07, 4.8972e+07, 7.7981e+07,
        8.5124e+07, 6.8621e+07, 4.3425e+07, 5.4155e+07, 7.7693e+07, 6.3394e+07,
        6.8068e+07, 7.0103e+07, 7.0677e+07, 7.7370e+07, 7.1427e+07, 5.6624e+07,
        4.6649e+07, 3.6846e+07, 1.0487e+08, 6.5159e+07, 7.2071e+07, 6.0817e+07,
        6.5146e+07, 5.7210e+07, 9.4995e+07, 6.6039e+07, 4.2304e+07, 6.3112e+07,
        6.9021e+07, 8.1534e+07, 8.0374e+07, 6.4569e+07, 8.2467e+07, 7.7479e+07,
        6.0893e+07, 9.4828e+07, 6.6442e+07, 5.7351e+07, 8.0859e+07, 8.5685e+07,
        9.0264e+07, 7.6243e+07, 5.2971e+07, 6.8846e+07, 5.7582e+07, 7.1366e+07,
        5.8792e+07, 4.0493e+07, 8.4139e+07, 6.4392e+07, 9.7085e+07, 6.9950e+07,
        5.3710e+07, 5.6206e+07, 5.1352e+07, 6.5301e+07, 5.2554e+07, 6.7579e+07,
        8.6355e+07, 6.6364e+07, 5.6364e+07, 3.9668e+07, 7.8344e+07, 5.5623e+07,
        5.6479e+07, 7.7499e+07, 4.4131e+07, 5.4169e+07, 6.3888e+07, 9.4555e+07,
        6.4809e+07, 5.9166e+07, 5.6005e+07, 7.1795e+07, 6.4910e+07, 5.1347e+07,
        8.1365e+07, 5.7324e+07, 5.0108e+07, 6.2944e+07, 5.4657e+07, 7.4236e+07,
        5.5448e+07, 6.4752e+07, 5.5648e+07, 6.4076e+07, 7.9004e+07, 6.4040e+07,
        1.0683e+08, 7.2227e+07, 6.9632e+07, 6.8421e+07, 4.9764e+07, 5.8709e+07,
        5.5927e+07, 1.0307e+08, 7.8000e+07, 4.0795e+07, 7.5247e+07, 5.2741e+07,
        6.3887e+07, 6.1846e+07, 5.6068e+07, 4.2644e+07, 6.4404e+07, 4.7386e+07,
        5.8955e+07, 6.5715e+07, 5.4328e+07, 5.6173e+07, 6.0984e+07, 8.1451e+07,
        4.2799e+07, 5.9615e+07, 7.6390e+07, 7.1863e+07, 5.1804e+07, 5.2745e+07,
        4.3334e+07, 7.4957e+07, 7.0952e+07, 4.7505e+07, 7.1755e+07, 7.8727e+07,
        7.5003e+07, 6.8304e+07, 5.8583e+07, 7.8933e+07, 6.3780e+07, 6.0481e+07,
        6.5830e+07, 6.9643e+07, 6.7881e+07, 7.6580e+07, 8.9152e+07, 5.0276e+07,
        6.6491e+07, 4.6272e+07, 6.8070e+07, 7.6026e+07, 6.3805e+07, 7.6506e+07,
        7.9385e+07, 6.5900e+07, 7.6779e+07, 6.3770e+07, 6.7300e+07, 7.3379e+07,
        6.2089e+07, 5.4843e+07, 5.8935e+07, 6.9698e+07, 4.0703e+07, 7.1583e+07,
        8.0276e+07, 6.7822e+07, 6.6308e+07, 5.2886e+07, 5.8815e+07, 6.8886e+07,
        5.7373e+07, 7.5689e+07, 5.7220e+07, 5.4611e+07, 6.7184e+07, 7.0736e+07,
        7.3935e+07, 5.3087e+07, 5.4127e+07, 5.7331e+07, 5.5220e+07, 9.3246e+07,
        6.2034e+07, 5.2567e+07, 4.6769e+07, 6.9346e+07, 5.8614e+07, 6.5878e+07,
        3.6923e+07, 8.8555e+07, 6.4905e+07, 9.0117e+07, 4.7069e+07, 7.3842e+07,
        8.0073e+07, 6.5624e+07, 8.1333e+07, 5.5776e+07, 7.6036e+07, 4.4719e+07,
        7.0230e+07, 6.1066e+07, 4.1366e+07, 5.9991e+07, 5.6717e+07, 6.1437e+07,
        6.6322e+07, 9.5769e+07, 4.9317e+07, 5.5153e+07, 5.7715e+07, 5.1128e+07,
        7.9542e+07, 6.2189e+07, 7.0530e+07, 8.0883e+07, 5.1453e+07, 5.5037e+07,
        9.9892e+07, 6.5018e+07, 4.2095e+07, 5.4226e+07, 7.8771e+07, 6.8948e+07,
        6.8290e+07, 5.1558e+07, 4.3489e+07, 6.8271e+07, 8.4032e+07, 6.4183e+07,
        1.1238e+08, 7.5156e+07, 7.8075e+07, 8.7948e+07, 5.8753e+07, 6.2424e+07,
        4.4996e+07, 5.0940e+07, 6.9176e+07, 5.0467e+07, 7.7996e+07, 5.7514e+07,
        6.1977e+07, 4.6857e+07, 7.0336e+07, 7.8489e+07, 9.9232e+07, 6.6942e+07,
        6.0512e+07, 5.5581e+07, 5.9908e+07, 5.9008e+07, 4.8015e+07, 4.8142e+07,
        4.9372e+07, 7.0097e+07, 8.9153e+07, 8.4169e+07])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1415e+08, 1.6716e+08, 1.4582e+08, 1.4670e+08, 1.4042e+08, 9.2906e+07,
        1.3421e+08, 1.1606e+08, 1.4617e+08, 1.4883e+08, 1.1361e+08, 1.3678e+08,
        1.1288e+08, 1.3148e+08, 1.5047e+08, 1.1594e+08, 1.6637e+08, 1.4258e+08,
        1.0138e+08, 1.2254e+08, 1.3646e+08, 1.8436e+08, 1.2619e+08, 1.0913e+08,
        1.2399e+08, 1.0207e+08, 1.7724e+08, 1.3867e+08, 1.6085e+08, 1.5760e+08,
        1.6265e+08, 1.8545e+08, 1.4899e+08, 1.6081e+08, 1.3191e+08, 1.0127e+08,
        1.5267e+08, 1.4866e+08, 1.1856e+08, 1.2232e+08, 1.7760e+08, 1.2031e+08,
        1.8979e+08, 1.7320e+08, 1.7089e+08, 9.6084e+07, 1.0974e+08, 9.9679e+07,
        1.6338e+08, 1.8518e+08, 1.9975e+08, 1.3027e+08, 1.2859e+08, 1.5775e+08,
        1.0334e+08, 1.1981e+08, 1.6641e+08, 1.5340e+08, 1.0055e+08, 1.3413e+08,
        1.4430e+08, 1.2417e+08, 1.6253e+08, 1.3143e+08, 1.0691e+08, 1.7682e+08,
        1.6391e+08, 1.3344e+08, 1.5466e+08, 1.2024e+08, 1.2328e+08, 1.5045e+08,
        1.0566e+08, 1.2413e+08, 9.5783e+07, 1.1504e+08, 1.6356e+08, 1.3807e+08,
        1.1918e+08, 1.2262e+08, 1.1223e+08, 2.1688e+08, 1.5285e+08, 1.3613e+08,
        1.7650e+08, 1.0535e+08, 1.1485e+08, 1.6221e+08, 9.1998e+07, 1.0315e+08,
        1.2213e+08, 2.0888e+08, 9.1905e+07, 1.4691e+08, 1.4092e+08, 1.1866e+08,
        1.4005e+08, 1.9444e+08, 1.1850e+08, 1.3563e+08, 1.2885e+08, 1.5008e+08,
        1.3048e+08, 1.3239e+08, 1.2024e+08, 1.1871e+08, 1.8967e+08, 1.4135e+08,
        1.2752e+08, 1.8383e+08, 1.1238e+08, 1.1056e+08, 1.3062e+08, 1.5179e+08,
        1.5507e+08, 1.5132e+08, 1.4717e+08, 1.4450e+08, 1.0848e+08, 1.4916e+08,
        1.5280e+08, 1.2470e+08, 1.2899e+08, 1.1477e+08, 1.4584e+08, 1.1366e+08,
        1.8374e+08, 1.5290e+08, 1.2262e+08, 1.1836e+08, 1.4582e+08, 1.1193e+08,
        1.3977e+08, 1.8156e+08, 1.1802e+08, 1.8739e+08, 1.3636e+08, 1.8900e+08,
        1.3204e+08, 9.7785e+07, 1.2691e+08, 1.4331e+08, 1.1931e+08, 1.3495e+08,
        7.9585e+07, 1.2715e+08, 1.2767e+08, 1.6667e+08, 1.4496e+08, 1.0787e+08,
        1.3171e+08, 1.6891e+08, 8.9817e+07, 1.4925e+08, 1.5786e+08, 9.2310e+07,
        1.5444e+08, 1.3175e+08, 1.4295e+08, 1.2258e+08, 1.4294e+08, 1.1965e+08,
        6.9601e+07, 1.4156e+08, 1.3010e+08, 1.3592e+08, 1.6185e+08, 1.9263e+08,
        1.3200e+08, 1.5260e+08, 1.2192e+08, 1.1769e+08, 8.1814e+07, 1.6800e+08,
        9.9933e+07, 1.3920e+08, 1.6662e+08, 1.3469e+08, 1.2878e+08, 1.5759e+08,
        1.9922e+08, 1.2515e+08, 1.5999e+08, 1.2915e+08, 1.5200e+08, 1.3159e+08,
        1.5734e+08, 1.0136e+08, 2.0292e+08, 1.3569e+08, 1.3671e+08, 1.6496e+08,
        1.1517e+08, 9.8158e+07, 1.3389e+08, 1.5358e+08, 1.7284e+08, 2.3957e+08,
        1.7242e+08, 1.2181e+08, 1.8857e+08, 1.6826e+08, 1.8718e+08, 1.0208e+08,
        1.6873e+08, 1.6564e+08, 1.4390e+08, 1.5118e+08, 1.3695e+08, 1.1968e+08,
        1.1437e+08, 1.5648e+08, 7.9931e+07, 1.2707e+08, 1.6322e+08, 9.9954e+07,
        1.0698e+08, 1.3650e+08, 1.3252e+08, 1.8714e+08, 2.0407e+08, 1.5531e+08,
        1.3720e+08, 1.4055e+08, 1.2094e+08, 1.3843e+08, 1.4620e+08, 1.4722e+08,
        1.1388e+08, 1.2348e+08, 1.4298e+08, 1.0044e+08, 1.2306e+08, 1.2629e+08,
        1.6260e+08, 1.0516e+08, 1.3220e+08, 1.4394e+08, 1.3160e+08, 1.1559e+08,
        1.4072e+08, 9.5003e+07, 1.5704e+08, 1.0971e+08, 1.2768e+08, 1.7244e+08,
        1.7903e+08, 1.1681e+08, 1.8361e+08, 1.0590e+08, 1.4060e+08, 1.4986e+08,
        1.2496e+08, 1.3648e+08, 1.5221e+08, 1.8507e+08])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([43760192.0000,  1605605.5000,  2862600.0000,  ...,
         5079469.5000, 15948838.0000, 21333502.0000])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([42458316.0000,   842584.3125,  4536744.5000,  ...,
         3367084.0000, 16577313.0000, 19040588.0000])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4344933.5000, 6569436.5000, 3383447.7500, 5756681.0000, 4743741.5000,
        3527801.2500, 4225275.0000, 5136989.5000, 3615992.0000, 4094511.0000,
        2140948.2500, 3142007.5000, 4496391.5000, 4864841.5000, 4624737.5000,
        6443178.5000, 4922484.5000, 3683298.5000, 2731636.2500, 3494198.2500,
        3388248.0000, 2899086.7500, 3471289.5000, 3586835.7500, 3061152.2500,
        3743269.0000, 3046225.0000, 5377879.0000, 3137363.7500, 3047973.5000,
        4556560.0000, 2973587.7500, 5071639.0000, 3170555.0000, 6108531.0000,
        5132514.0000, 3267126.7500, 5769452.0000, 6192313.0000, 2885580.5000,
        3383973.0000, 4730891.5000, 3862770.0000, 4146875.0000, 3887779.7500,
        3536432.0000, 3278351.5000, 3472402.0000, 2694437.0000, 5676376.5000,
        4125300.0000, 4199821.0000, 4628979.5000, 3396976.0000, 4603022.0000,
        2583734.0000, 4999393.5000, 3199726.5000, 4917667.0000, 3716469.7500,
        3863489.5000, 3990227.7500, 4178195.2500, 5650306.5000, 4078826.0000,
        4028256.0000, 6201148.0000, 5381295.5000, 4513616.5000, 4235850.0000,
        4551687.5000, 3694507.5000, 3619842.5000, 3994848.7500, 4611277.0000,
        4689594.5000, 2432722.5000, 3417631.7500, 4364809.0000, 3438635.0000,
        3332571.0000, 2221213.5000, 4948190.0000, 5639261.0000, 3019811.7500,
        4370330.5000, 3629415.7500, 4231493.0000, 5671011.5000, 2164193.2500,
        4658433.5000, 3927784.0000, 3323250.0000, 2975645.5000, 3186493.5000,
        5301957.5000, 2947473.2500, 3022265.7500, 6413656.5000, 4449851.5000,
        4290402.0000, 4834320.5000, 3330628.0000, 5454459.0000, 3335915.7500,
        4822595.0000, 3544141.2500, 5158047.5000, 3610603.2500, 3783519.7500,
        5736892.5000, 4098899.7500, 4030558.0000, 3136477.7500, 3702228.2500,
        3261876.5000, 4497767.5000, 4040976.7500, 5791538.5000, 3697950.7500,
        5361410.5000, 4441122.0000, 3674459.5000, 3670735.5000, 4206215.0000,
        3686497.5000, 4904487.5000, 5291378.0000, 5532588.0000, 5631489.5000,
        4042498.2500, 3752913.5000, 3571780.2500, 3901546.5000, 4208003.0000,
        5562578.5000, 3335879.7500, 3619190.0000, 3121762.2500, 5804276.0000,
        3832189.2500, 2550019.5000, 4652868.5000, 3628646.0000, 4538151.5000,
        3039849.7500, 4776060.5000, 4588492.5000, 4121025.5000, 3890961.0000,
        3507844.2500, 5700950.0000, 5504687.5000, 4701265.0000, 3429674.0000,
        4052808.5000, 3858805.7500, 3870404.0000, 4843418.5000, 3320026.0000,
        4074824.0000, 2435474.2500, 3546096.0000, 3029831.7500, 6149206.0000,
        4230946.0000, 5145151.0000, 3180729.0000, 2306337.0000, 5362376.5000,
        3068186.5000, 4281495.0000, 4784658.0000, 5685393.5000, 3186493.0000,
        2476273.2500, 5255788.5000, 5098542.0000, 3267242.0000, 3676796.7500,
        5040254.5000, 4260679.0000, 5027479.5000, 6095629.5000, 3470177.7500,
        5185963.0000, 3424718.7500, 4054720.7500, 3152393.7500, 2774520.7500,
        3361392.7500, 4416311.0000, 3429807.5000, 3635863.2500, 6363315.5000,
        5896711.0000, 3181571.2500, 5796212.0000, 3396533.7500, 3754664.0000,
        5434511.5000, 3995129.2500, 4118432.2500, 4199481.0000, 3999748.2500,
        4241148.0000, 5131373.0000, 5177687.0000, 5712954.5000, 4753248.0000,
        3860987.5000, 5934548.0000, 6593853.0000, 5026175.5000, 5267010.5000,
        5075592.0000, 4633225.5000, 3434775.0000, 3174876.0000, 3349352.0000,
        4104149.5000, 5009204.0000, 4706847.5000, 2672512.5000, 6674060.0000,
        3209931.5000, 4090615.7500, 3485876.5000, 3404192.7500, 2501427.5000,
        3410518.5000, 3428086.7500, 3550160.2500, 3655040.2500, 3682748.0000,
        5521744.5000, 4166453.5000, 5432800.5000, 3560078.5000, 3341741.5000,
        4155286.5000, 5579167.5000, 4253674.0000, 5797619.0000, 4257096.0000,
        5639077.0000, 4126275.5000, 4529384.0000, 4398704.5000, 3929440.0000,
        7777694.5000, 3042098.0000, 3531402.7500, 2777668.5000, 3801978.7500,
        5592063.5000])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 8159223.5000, 11960602.0000, 11506868.0000, 18788140.0000,
        16195715.0000, 11447966.0000, 19344536.0000, 15302845.0000,
        15882461.0000, 10470958.0000, 10453069.0000, 14870267.0000,
        16514190.0000,  7881916.5000, 12684948.0000, 15229268.0000,
        16542862.0000, 13576841.0000, 10681297.0000, 12366913.0000,
        14649476.0000, 10605036.0000, 10876123.0000, 10227030.0000,
        12414080.0000, 18416118.0000, 14605586.0000, 21005254.0000,
         9338347.0000,  8045750.5000, 12335846.0000,  9437317.0000,
        12905480.0000, 19665208.0000, 13112521.0000, 15698167.0000,
         8958403.0000, 12185229.0000, 22212628.0000, 10711560.0000,
        10632839.0000, 14533511.0000, 11243620.0000, 14505228.0000,
        11179577.0000, 11376110.0000,  9799418.0000, 11791348.0000,
        10932835.0000, 11886597.0000, 14530838.0000,  9145355.0000,
        11149609.0000, 13945451.0000, 13290147.0000, 12751349.0000,
        11327636.0000,  9758695.0000, 16888520.0000, 10981096.0000,
         7563716.0000, 16518926.0000,  8691427.0000, 17220186.0000,
        19261208.0000, 18468600.0000,  8166283.0000,  9614413.0000,
        10886407.0000, 17812772.0000, 12849336.0000,  9057029.0000,
        10138006.0000, 13781234.0000, 15252423.0000, 10651992.0000,
        10442204.0000, 11912717.0000, 12925657.0000, 12243861.0000,
        14755851.0000,  9634212.0000, 13320469.0000, 16908678.0000,
        10684933.0000, 13609900.0000, 22005960.0000, 21446040.0000,
         9849374.0000,  8594260.0000, 11061105.0000, 11032744.0000,
        13664668.0000, 15739590.0000, 17744732.0000, 11155038.0000,
        16403534.0000, 15425890.0000, 15638001.0000, 10507180.0000,
        12980483.0000, 10404905.0000, 14590065.0000, 12957541.0000,
        19419012.0000, 12094917.0000, 15981590.0000, 13747711.0000,
        17643480.0000, 16362798.0000, 11630684.0000, 10954378.0000,
         8421576.0000, 10619575.0000, 10989336.0000, 14223167.0000,
        18415832.0000, 10745457.0000, 13988585.0000, 13223810.0000,
        12850820.0000, 13705842.0000, 11807038.0000, 17411642.0000,
         7774530.0000, 13301934.0000, 11558084.0000, 10534784.0000,
        11017955.0000,  8537675.0000, 16751667.0000,  8309285.0000,
        13527489.0000, 12996240.0000,  5876434.0000, 13905140.0000,
         9773629.0000,  9696126.0000, 10626528.0000, 11986166.0000,
         6210136.0000, 15251936.0000, 15047129.0000, 10966547.0000,
        14877776.0000, 11919810.0000, 11215699.0000, 18462258.0000,
         6494904.0000, 10067247.0000, 20525818.0000, 14920217.0000,
        11039455.0000, 17560512.0000, 11704175.0000, 15555616.0000,
         8340553.0000,  9816422.0000, 17513478.0000, 10605602.0000,
        14636565.0000, 11919889.0000, 10370387.0000,  9072021.0000,
        11979390.0000, 10401223.0000,  9617434.0000, 14070774.0000,
        14906189.0000, 15512312.0000, 11739707.0000, 14723366.0000,
        14484204.0000, 12971279.0000, 12236649.0000, 13018289.0000,
        21655716.0000, 16226483.0000, 13453244.0000, 10382142.0000,
        11020763.0000,  8644896.0000, 21196540.0000, 18158722.0000,
        17195842.0000, 16350502.0000,  8951371.0000, 11879215.0000,
        18920880.0000, 15077408.0000, 11215790.0000, 13043582.0000,
        14096751.0000, 13202229.0000, 15610825.0000, 13186431.0000,
        12862796.0000, 10014931.0000,  8835488.0000,  9916947.0000,
        10556841.0000, 12756498.0000, 10514778.0000, 15369671.0000,
        18416450.0000, 11866129.0000, 11994320.0000, 13081122.0000,
        11433166.0000,  9940129.0000, 10424829.0000, 18643300.0000,
        13405790.0000,  7110419.0000,  9162870.0000,  9607084.0000,
        14328136.0000,  7356336.0000,  8444101.0000, 13750063.0000,
        17422582.0000, 11411847.0000, 12210144.0000, 11333909.0000,
        10809226.0000, 10298590.0000, 10323033.0000, 16061929.0000,
        12535096.0000, 11447506.0000, 15518613.0000, 15470818.0000,
        16683940.0000, 16836634.0000, 16781852.0000, 11446014.0000,
        14010527.0000, 13466671.0000, 15065171.0000, 10655422.0000,
         8135243.5000, 13245278.0000, 11683676.0000, 13855985.0000,
        12710859.0000, 11535904.0000, 15633985.0000, 11726347.0000,
        18288118.0000, 11446920.0000, 11412065.0000, 10912450.0000,
         5165190.0000, 12405478.0000, 15036084.0000, 12294627.0000])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1008242.8750,  294856.0938,  705798.0625,  ...,  291953.3438,
         285461.8438,  368914.4375])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1056053.5000, 1022278.3125, 1082629.5000, 1257334.5000,  956152.5625,
         775118.5625,  802832.5625,  884855.7500,  935335.3125,  985338.4375,
         950118.7500,  799382.3750,  931508.9375,  876199.2500, 1110001.1250,
         838399.0000,  995275.1250,  877997.3125,  839318.6875,  883337.6875,
         796098.7500,  671448.8125,  729904.7500,  820740.0000, 1234298.3750,
        1097281.1250, 1105586.2500, 1085277.5000,  975546.1250,  802408.5000,
        1100889.6250, 1060033.3750,  994550.3125,  969301.6250,  651470.3750,
        1246183.5000,  943690.7500,  549255.1250, 1235346.0000,  980257.8125,
         976252.6875, 1035104.3750,  884555.3125,  981538.0000, 1124114.1250,
        1152251.3750,  647613.7500,  667016.3125, 1039002.6250,  956215.3750,
         499270.4688,  835920.1250, 1556681.7500,  955431.1250,  756022.0625,
         621261.0625,  999430.2500,  730914.7500,  627473.0625,  708936.2500,
         953672.6250,  619061.2500,  713267.3750,  862541.5625,  893395.4375,
        1027110.5000,  978667.0625,  752673.8750, 1008968.4375,  972735.6250,
        1115629.3750,  817186.9375, 1074982.0000, 1013743.3125,  850500.5625,
         816960.3125, 1034893.2500,  866783.5000,  822567.2500,  954686.7500,
         886785.5000,  688243.1250, 1320923.2500, 1052145.8750,  667655.7500,
        1055606.8750,  774455.3750, 1004306.5000, 1048307.8750, 1275864.8750,
         750695.8125,  877210.7500,  716203.6875,  815558.1250, 1074598.5000,
        1179281.2500, 1111737.5000,  623621.0625,  989493.5000,  681664.0000,
         764299.8125, 1316375.0000, 1147757.1250,  843951.0625,  535078.5000,
        1214687.8750, 1240200.3750,  897513.0000,  663954.0625,  561681.2500,
         897496.2500,  885443.8125,  889109.3750,  558456.7500, 1045300.8125,
         927708.3125,  900355.2500, 1064991.3750,  905529.0000, 1233578.7500,
         952833.6250,  778086.1875, 1151085.1250, 1107551.1250, 1127041.2500,
         911987.8750,  777122.6875,  655026.9375,  762382.3125, 1204719.1250,
         680095.2500,  874592.5625,  960452.1875, 1150267.7500,  872557.9375,
        1073817.7500,  595778.5000,  833526.5625, 1056146.5000,  710679.5000,
         824856.4375,  972637.1250, 1289860.1250,  700734.6250,  600314.3750,
         602638.3125,  621031.7500,  559995.4375,  810743.0625, 1169195.6250,
        1247203.0000,  746958.5000,  679717.5000,  923934.3125, 1053417.0000,
        1014192.0000,  774222.0000,  950958.6250,  611239.6250,  977515.0625,
         791893.8125,  847951.8125,  676861.2500,  836164.0625,  949103.5000,
         812892.5625,  981214.6250,  982712.0625,  917354.0000,  773180.9375,
        1372223.5000,  686212.8750,  780697.3125, 1017021.7500,  712650.4375,
         900787.8125,  981267.4375,  948498.5625,  965989.5625,  905514.5625,
         921975.1250,  843153.0000,  844347.4375,  969689.3125, 1314433.5000,
         775684.8750,  731791.6875,  626570.3125,  639489.2500, 1565652.5000,
         833261.0625,  830543.7500, 1408115.6250,  939042.3750,  893777.3750,
         856740.7500, 1348278.2500,  736396.6875,  909690.0625,  890185.1250,
         776654.6875, 1066752.3750, 1031598.8125,  519520.6562,  976051.2500,
        1194171.7500, 1336479.1250,  820685.0625, 1109855.8750,  840808.1250,
        1093246.8750,  989545.8750, 1157848.2500, 1211809.2500,  657888.5000,
         961394.5000, 1024187.4375,  768241.4375,  631847.1250, 1040878.5625,
         990211.0625, 1048596.0000, 1036852.0000,  827928.7500,  807602.9375,
        1178916.3750,  739764.6250,  604878.8125,  992263.5000,  739347.8125,
         965393.1875, 1188564.5000, 1037573.5000,  924867.1875, 1099241.8750,
         801696.9375, 1123186.0000, 1029138.3750,  941729.6875,  939705.3125,
         842539.0625,  887883.1875, 1146202.0000,  868353.5000,  819608.1250,
         689591.5000, 1023375.0000,  692281.6250, 1158807.6250,  937912.0000,
         782104.1875, 1455109.2500, 1014793.5625,  763229.6875,  970930.1875,
         936357.6250])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2867319.7500, 2736750.7500, 4113316.0000, 2894762.0000, 3313976.5000,
        2375058.7500, 2565662.2500, 2044962.1250, 3875998.7500, 2917825.5000,
        3200749.5000, 2599620.5000, 2310389.0000, 3097302.0000, 2781669.0000,
        4086983.0000, 3010788.2500, 2597624.0000, 3130533.5000, 3697592.2500,
        2928150.2500, 3363093.2500, 5651011.5000, 3493236.7500, 4633059.0000,
        3801973.7500, 4346131.0000, 2834478.2500, 3025936.5000, 3356280.5000,
        2609437.2500, 2247414.0000, 3185895.5000, 3053334.7500, 2160889.0000,
        3091208.5000, 2755017.5000, 4173028.0000, 2843120.2500, 2659913.0000,
        2810857.2500, 3120838.7500, 2595859.7500, 3525124.5000, 2096924.2500,
        4466589.5000, 2850106.2500, 2270288.0000, 2393557.0000, 2331806.2500,
        3674943.5000, 2456992.7500, 4608173.5000, 2922492.5000, 2273331.5000,
        3596521.5000, 3912267.5000, 3684218.2500, 2819413.5000, 4426449.5000,
        3298785.2500, 3114086.2500, 2913485.5000, 2525449.2500, 2244403.5000,
        3566726.2500, 2914431.7500, 2058964.5000, 4418817.0000, 4848844.0000,
        2421212.5000, 3098117.0000, 2827543.0000, 3600554.5000, 3099435.5000,
        2150252.5000, 2920024.2500, 2801283.7500, 3799877.7500, 3501024.5000,
        4461920.5000, 3612206.5000, 3486563.2500, 4021826.5000, 3156986.5000,
        3247159.2500, 4073957.2500, 4593104.0000, 2761539.5000, 3199420.2500,
        2954086.5000, 3579562.0000, 3278929.7500, 3682522.0000, 3597686.5000,
        3768500.2500, 3445731.2500, 3329581.0000, 3059030.0000, 3961301.7500,
        3564652.5000, 2031812.7500, 3891337.0000, 2650123.5000, 2906243.0000,
        3261200.2500, 2729712.7500, 2848943.2500, 3316953.7500, 4784888.0000,
        3490933.7500, 3517747.5000, 3506901.5000, 4776926.0000, 4557744.5000,
        3498856.5000, 3451074.7500, 2273042.7500, 3066936.2500, 3400558.5000,
        2016732.8750, 3677115.0000, 3135474.0000, 2120269.0000, 3246471.7500,
        3289471.0000, 2652780.5000, 2794370.2500, 4311888.5000, 1746052.8750,
        4094278.5000, 2758695.7500, 2549629.5000, 3443044.7500, 5076608.5000,
        2460767.5000, 1552985.1250, 2597573.2500, 3027105.2500, 3732327.0000,
        3811262.5000, 3293501.2500, 4269175.5000, 3558448.0000, 1841588.7500,
        2597099.7500, 3685660.7500, 2827191.7500, 2936333.7500, 4102132.2500,
        4527013.0000, 3958801.5000, 3844418.2500, 2924811.2500, 4506347.0000,
        2498543.2500, 3540955.7500, 4999627.5000, 2302774.2500, 3239220.2500,
        3870503.2500, 2807315.0000, 2410179.5000, 3572997.0000, 2227143.7500,
        2855919.0000, 2261018.2500, 3113031.7500, 3614746.7500, 1919353.5000,
        3351940.2500, 3259610.7500, 2978127.7500, 3580357.2500, 2572242.2500,
        3966736.5000, 3289439.7500, 2734776.0000, 3973300.5000, 3697058.0000,
        3904170.7500, 3359369.7500, 3384767.7500, 3049816.7500, 3004707.2500,
        2867215.7500, 2890079.7500, 2866800.2500, 2247264.2500, 3289525.7500,
        2860473.5000, 2898510.5000, 4257401.0000, 2745117.5000, 3814668.2500,
        3838628.5000, 2417549.2500, 3707984.0000, 4885762.0000, 2905249.0000,
        1756237.5000, 3613094.7500, 3318564.7500, 2765193.0000, 2436139.7500,
        2292678.0000, 1856698.7500, 2946845.7500, 2269814.7500, 2612289.5000,
        3180361.7500, 3426546.5000, 3333834.2500, 2444983.2500, 2501145.0000,
        3869249.2500, 3585845.7500, 3046178.7500, 2492901.7500, 2303841.2500,
        2675902.5000, 3427635.7500, 2625730.2500, 2101805.0000, 4318769.0000,
        2349885.2500, 1431905.3750, 2096936.3750, 2463121.7500, 5125234.0000,
        2492738.2500, 2453181.5000, 2590962.0000, 3093477.5000, 3163369.7500,
        2939326.0000, 3095309.2500, 3561377.0000, 3316765.5000, 4237142.0000,
        3695143.0000, 2937910.5000, 2546541.5000, 4027062.5000, 4301769.5000,
        2493834.5000, 3155243.5000, 3001837.2500, 2918548.7500, 2241106.2500,
        2846905.5000, 2271241.7500, 2876908.0000, 2458187.7500, 2847453.0000,
        4288996.5000])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([934904.1875,  30318.2109, 597176.1875,  ...,  71088.9141,
         14081.7441,  83846.5156])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([124960.8672,  88472.9766, 185124.0781, 192218.2188, 239078.8750,
        164594.5312, 204255.1562, 206266.1250, 126847.8984, 316060.1562,
        147247.8281, 213164.4062, 136089.5000, 209658.2812, 242074.6875,
        236002.7188, 169111.4531, 164239.2656, 192778.6406, 137687.1406,
        193181.2500, 207906.2969, 111320.2500, 205707.1250, 128449.0703,
        151738.3750, 157012.1094, 186034.6875, 152076.2969,  87511.7656,
        117245.6328, 148352.2656, 296517.4688, 121363.2891, 315576.6250,
        179155.5625, 199359.6562, 126702.1094, 158969.2031,  88787.3672,
        122236.7031, 101568.4062, 186828.9531, 293315.9375,  95117.1094,
        136362.8281, 259784.0312, 173360.3594, 146130.6719, 146046.3750,
        141419.7344, 188090.4844, 157804.4844, 159353.7188, 119032.5547,
        186008.7500, 263601.1562, 252747.5781, 259341.9062, 130670.7422,
        118759.8750, 174164.7031, 116763.2656, 311254.3125, 103775.0156,
        140461.7969, 259649.5000, 250175.5625, 154932.9375, 273114.2500,
        169831.3906, 148634.9375, 179799.6875, 162692.1562, 140289.9219,
        178392.3438, 153873.0000, 222895.7031, 222262.8594, 223991.7500,
        185264.2656, 142953.4531, 154786.2500, 191062.9062, 210065.4688,
        134136.0312, 124814.0234, 167796.2188,  80382.1406, 116167.7969,
        194061.0469, 239477.2188, 186979.0000, 140368.0938, 120581.6797,
        162409.5781, 133786.3906, 182475.0938, 118353.1797, 148030.6875,
        131484.3594, 226540.9062, 200948.3125, 161722.5156, 276219.4062,
        154516.4219,  96238.3359, 129912.6719, 156172.5469, 213181.2031,
        151657.6719, 161417.9062, 163152.0781, 181580.3906, 164261.5625,
        165904.5312, 174113.4531, 196026.4375, 219945.2031, 210049.2031,
        273999.2500, 184010.0938, 296250.2188, 151956.9375, 164392.2812,
        176964.3438, 204492.5938, 230299.3281, 292765.0312, 167116.1875,
        133565.0156, 157628.5156, 256953.6094, 136124.7969, 152964.7656,
         90492.5781, 118712.3125, 160179.8906, 160647.6406, 115999.3125,
        138227.5312, 285266.3750, 290295.2812, 167401.0625, 292092.3125,
        235321.0469, 116113.5703, 216804.4688, 178277.9844, 132057.3750,
        158425.5156, 135057.8906, 141163.6719, 187947.0156, 197802.2500,
        193248.0000, 220097.3125, 102835.2891, 185568.6406, 189409.9375,
        141386.0156, 223885.2500, 103514.0078, 113001.1562, 166888.5000,
        210879.5156,  86297.1328, 128684.0000, 168441.6875, 187972.8281,
        210805.0156, 158503.4219, 150389.1719, 111668.0000, 155878.5312,
        168513.5625,  94844.5234, 145546.8438, 254472.7188, 122372.0703,
        284516.4688, 188213.6719, 151791.6875,  98383.2109, 141731.5625,
        129170.7188, 153404.4688, 233608.3438, 194618.6562, 192930.0000,
        171150.5000, 242769.6719, 168908.2188,  99421.4531, 166871.7031,
        180994.3281, 277653.8438, 158875.5469, 208024.9375, 247563.3281,
        152332.7812, 114444.4922, 234508.0469, 126743.5000, 145664.9531,
        147114.0781, 187027.4062, 251587.2656, 127252.8203, 192031.3906,
        186473.9375, 190312.7656, 222880.2812, 148767.9219, 201998.3438,
        154746.2500, 277392.5000, 149653.0312, 159680.9062, 172256.7656,
        133124.3594, 159049.5781, 151263.9688, 156454.5781, 249271.4219,
        292736.0625, 194575.0000, 241777.2031, 248725.8125, 155076.4688,
        144913.7188, 144602.7031, 205933.9062, 133426.0781, 211918.8594,
        134750.2969, 123170.9141, 188677.3125,  66051.3828, 220781.9375,
        184706.7656, 127255.2344,  92777.3828, 146982.1719, 218856.7031,
        147397.3438, 192094.4062, 176928.1250, 122898.9062, 329255.9688,
        142113.4375, 294108.9688, 206867.6406, 107950.6953, 268085.4375,
        207228.6719])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 658344.3750,  811423.6875,  984130.4375,  977158.1250,  746422.4375,
         997644.2500,  732669.4375,  521120.8438,  428464.3125, 1368295.6250,
         706014.5000,  622563.1250,  480750.1562,  941100.1875, 1220213.5000,
         762389.3750, 1073322.1250,  538335.0000,  659186.3125,  496857.3750,
         581340.3750, 1088073.2500,  528894.5000,  751936.5000,  444447.7500,
         914807.0000,  584443.3750, 1250417.5000,  496951.6250,  666086.7500,
         785597.4375,  620064.8125, 1034216.2500,  687213.6250,  729248.8125,
         583226.1250,  745777.6875, 1032683.5625,  932230.8750,  858929.3125,
         815584.7500,  723193.7500, 1151826.0000,  847527.0625,  556508.9375,
         401393.4688,  662738.4375,  655720.1875,  520031.3750,  472731.0000,
         959364.5625,  770002.8125,  731130.8750,  659213.0000,  751559.0000,
         536305.5000,  901796.6250,  570133.4375,  416085.3750,  999460.0000,
         487836.7500,  573642.8125,  350146.9375,  432997.3125,  739221.0000,
         598499.6875,  870668.9375,  684699.0625,  822154.1875,  828008.5625,
         523840.8125,  553240.1875,  563640.8125,  652459.8125,  617954.5000,
         723640.8125,  652146.6875,  707712.1250,  514709.8125,  530314.5625,
         448247.6250,  542493.1875,  798663.7500,  486101.6562,  547109.3125,
         448581.0312,  679639.9375,  724875.2500,  546919.9375,  577308.5000,
         790433.3125,  707383.4375,  445592.6875,  627918.0000,  757957.1250,
         971509.8125,  628596.6875,  861555.8125,  740330.4375,  481212.4062,
         725322.0625,  456531.1562,  534355.3125, 1214341.1250, 1117650.7500,
         673013.7500,  667351.9375,  408586.6250,  658568.7500,  645106.5625,
         795377.6250,  386733.1250,  811583.8125,  589299.3125,  810241.8125,
         670372.6250,  667682.0625,  733860.8125,  449127.0625, 1019891.0625,
         490632.9062,  545264.8750, 1372655.8750,  630813.3125,  584132.9375,
         443213.0312,  652382.6875,  529430.6250,  586529.1250,  617441.9375,
         817709.9375,  595240.5625,  667803.5625,  493899.3750,  989890.9375,
         841146.9375,  718786.7500,  740530.8125,  620456.1250,  474616.0312,
         688027.1875,  829309.3750,  672680.1250,  656404.5000,  498240.0938,
         610252.1875,  506291.6250,  631583.1250,  682437.6250,  511820.5938,
         612392.2500, 1244828.7500,  733785.4375, 1075747.3750,  989596.5625,
        1042106.5625,  733448.9375,  988970.5625,  962604.7500,  729053.5000,
         839009.3750,  399436.8438,  599225.1875,  601934.3750,  987094.0625,
         743493.8750,  645034.6875,  811286.6250,  762564.9375,  723723.2500,
        1013499.2500, 1299235.0000,  746395.0000,  377982.3125,  605598.5625,
         800950.6250,  585868.7500,  831591.0000,  550879.8750,  462262.3438,
         476200.9062,  849992.5000, 1301533.1250,  494835.9688,  414416.8125,
         550440.3125,  661698.5000,  737975.0625,  781944.2500,  562462.7500,
         659738.4375,  500042.0000,  640160.0000,  530215.4375,  835489.1875,
         404522.5938, 1137687.3750, 1063697.5000,  648492.1875,  687871.6875,
         732637.1875,  584380.6250,  481003.1562,  971950.6875,  735671.0625,
         498673.9688, 1277406.6250,  680773.2500,  786212.3125,  540625.3125,
         729698.1875,  868796.8750,  456984.9375,  458498.4062,  863626.7500,
         516909.6250,  695247.3750,  665117.2500,  664072.8125,  648737.9375,
         419920.5625,  798670.0625,  813595.1250,  547050.1250,  296907.2188,
         780599.0625,  579363.7500,  434192.6562,  617931.8125,  583910.7500,
         611920.8750,  415283.8438,  575649.3750,  971441.0000,  459748.1562,
         741845.1875,  963242.8125,  888304.6875,  767213.6875,  611594.6875,
         751077.2500, 1017682.8125,  575558.8125,  675194.0625,  651556.3125,
         942843.5000, 1345945.1250,  693343.8750,  594796.5625,  662445.3750,
         427770.5625,  438599.9062, 1170432.3750,  519922.8125,  824867.1250,
         652356.3750])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([28783.5527,  9393.9102, 41400.4023,  ..., 29050.7266,   440.4366,
        27774.6016])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([143352.6875,  89424.5781, 121599.0469, 138578.2969,  82925.3281,
         94360.6328, 103011.8750,  90602.8906, 157743.3594,  56696.4102,
        112749.4297, 119203.5078,  86673.8516, 113456.5469,  75525.7109,
        106353.6797,  76970.7344, 103276.4453, 143714.9062,  97549.5625,
        128200.5859,  95540.7734, 122894.8438,  60233.9844, 104970.8047,
         73025.4766, 144556.1875,  77186.1719, 102560.0469, 114032.1250,
        124086.9922, 101918.7891,  97251.2109,  97567.3203, 101238.6406,
        136947.5469,  99259.5625,  87845.4141, 164073.7031,  80837.4844,
         82523.2734, 123285.1953,  91178.5938,  95430.6484, 105748.5625,
        116577.3047, 138558.0469, 137635.6250, 109826.2344,  93229.0938,
         82194.2188, 134171.7969,  86447.8281, 118397.0078,  71658.5625,
         54291.1914,  94855.8203,  96023.1172, 114649.8984,  99521.6328,
         61275.2930,  97603.4844,  63815.5234, 116671.0391, 111620.0391,
        103604.5703,  98696.3203, 133386.9375, 116409.4375, 169734.5625,
        136818.1719, 146180.4219, 106998.9219, 105434.5156,  70466.0703,
        105835.5781,  87245.4375, 111853.0156,  88791.3672, 145515.3906,
         76953.9766,  60725.1250,  69111.7812, 107952.1328, 124064.2656,
         94419.7500, 134147.6562,  91448.1562,  91729.6953, 102101.7344,
         72645.0547,  93435.8906,  87751.7422,  90056.7266,  64658.0352,
         64758.0938, 116903.4453,  83287.1172,  58636.6875,  91597.7344,
         72724.3984,  92340.4297,  97927.4609, 163084.6094, 109526.4688,
         76367.6641,  94172.0469,  87616.8516, 110447.4141,  94399.9922,
        217252.7812,  96429.6250, 105811.5703,  53873.9023, 123486.2500,
        132785.6406,  78111.5625, 105620.4766, 151753.1250, 142508.9531,
        107215.7500, 123822.6562, 118490.1406, 147546.3281, 146538.7812,
        104181.7812, 133901.6562, 113419.2734, 179839.0156, 106856.5156,
         98954.9531, 125818.0000, 118543.7812,  47794.1406,  77704.4219,
        124119.8984,  53223.6484, 135125.2188, 120426.2656, 118621.8047,
         76667.7109, 168868.2344,  76444.1641,  87378.9219, 159796.2031,
        107307.6094, 168342.8594, 139477.3438, 129883.4375, 146986.8281,
         45650.5273, 112229.1797,  75432.4141,  88430.5078, 226426.9688,
         85271.3672, 128275.3828, 123438.8672,  67199.0469,  68748.7188,
        114749.4531, 117659.6250, 104849.6875,  69147.8906,  86815.2656,
         60854.4492,  98779.9922,  83401.7188,  92198.2578,  57855.2891,
        134572.1094,  62000.3906, 133823.0938, 131057.5078, 150918.0156,
         69248.0312,  93399.7891, 114857.4766,  64318.6914, 145884.2812,
        128043.9922, 127143.2266,  80717.1797,  88560.5000,  90879.0391,
         91920.1328,  89809.1641, 106877.0625,  82209.2578,  63267.7578,
        140886.5781, 112665.9688, 138177.7500, 133160.3750, 156652.1094,
         76913.2109,  88115.0547,  83610.8516, 109576.7344,  66718.1016,
         92457.2422,  56679.2891,  64963.9102, 247751.7031,  78370.6484,
         76821.7969, 133608.0938, 117977.4609, 153733.6094, 140970.8125,
        141508.7500, 107239.5703,  88525.4375, 130954.0312, 141318.5469,
         70793.6797,  60592.4492,  93394.0312, 118785.6172, 194060.9219,
        112019.4062,  98346.2578, 157117.3125,  81210.6328, 125905.8594,
         74465.2812, 121191.6953,  87025.1484, 126902.9219,  66340.5312,
         84821.2578,  81300.1562,  78258.4844, 103155.0625,  92828.1641,
        141132.7031,  53946.4609,  99532.5312, 108109.0859,  56117.4023,
         68523.3594,  82309.8047, 175077.0781,  83031.9922, 117305.8438,
        124255.4141, 153690.5625,  95433.0156, 119079.3906, 125313.4062,
         61850.4023,  84826.0781, 103442.1484,  84667.8594, 154796.8125,
         92794.9531])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([367531.4688, 560154.4375, 651586.1875, 379112.8125, 328301.6562,
        571215.8125, 434518.5312, 561799.8125, 531066.8125, 362586.3125,
        503990.2812, 358660.0312, 426960.1250, 447354.7188, 497954.3750,
        589388.6250, 465093.8750, 632304.0625, 446751.9688, 426674.2188,
        471189.8125, 624273.6250, 617356.0000, 468708.0625, 335305.0000,
        567793.3750, 389583.0625, 419748.9375, 373339.3438, 398495.8750,
        464693.9688, 502562.2188, 581195.1250, 690097.6875, 529296.0000,
        487295.7812, 531612.6875, 403445.2188, 408966.5625, 867383.2500,
        479055.9062, 751318.1875, 386314.8750, 665210.1250, 489717.0938,
        352145.5625, 420737.8438, 418339.1562, 418171.1562, 501732.5625,
        361120.3438, 526973.5000, 403497.6250, 449340.9062, 545907.6250,
        570494.0625, 348491.2812, 436492.1250, 370543.3750, 483576.2812,
        439584.8750, 538308.3125, 452899.1875, 653770.0625, 614596.3125,
        622924.6250, 399875.1250, 420085.0000, 230416.5312, 440074.9375,
        590437.5000, 353295.1562, 477893.9375, 536054.4375, 345742.0938,
        792881.7500, 649523.8750, 434947.8750, 384547.2188, 335915.1875,
        391297.7188, 522584.2812, 579136.0625, 361214.2500, 587235.6250,
        557765.1875, 585183.0625, 699068.2500, 512815.4062, 354652.3438,
        336948.9688, 526405.4375, 654668.8750, 417513.3750, 218555.7188,
        374272.9375, 910897.0000, 335031.5312, 331237.9062, 500607.5312,
        517737.0625, 310856.2188, 435670.7500, 437978.6250, 298161.9375,
        509423.2500, 470415.6250, 182172.5469, 327984.4062, 519510.2188,
        338501.0625, 604055.0625, 313152.5938, 585644.8125, 521457.5000,
        390349.2812, 401694.5938, 372761.5625, 279835.7500, 528532.0625,
        386169.9062, 471838.5938, 599062.5000, 442539.4062, 561851.3750,
        295885.1875, 429233.8750, 368396.0312, 679698.9375, 439113.4688,
        464331.7812, 353968.1562, 635506.4375, 647573.9375, 514699.2500,
        570720.8125, 794178.5625, 535553.6250, 315424.9688, 516968.0000,
        857210.3750, 548680.7500, 387678.6875, 322530.2188, 559098.7500,
        330213.5000, 598121.2500, 275949.0625, 602114.1250, 377260.0625,
        371088.1875, 474524.8438, 414648.2812, 488642.5000, 357963.7500,
        349473.8750, 500585.1562, 349759.5000, 361130.9375, 400291.3750,
        515232.1562, 302878.3438, 217476.2969, 651507.1875, 725679.1875,
        410880.5000, 256436.2969, 465667.5625, 349581.6562, 192904.6719,
        650289.1875, 273564.0312, 373341.3125, 301302.0938, 412515.6250,
        470460.4375, 440204.5625, 428415.8125, 321660.7188, 489254.7812,
        327510.2188, 571856.0000, 476273.8125, 426508.3125, 515564.1250,
        536113.2500, 640387.8750, 508625.9062, 506361.1250, 349946.5312,
        476691.7812, 395418.1562, 428785.2188, 469738.9375, 619323.9375,
        634385.0000, 642626.8125, 254439.9062, 482643.3125, 500410.2500,
        336815.6875, 311050.0625, 565175.3125, 574441.5625, 411440.2500,
        662952.5625, 812087.4375, 429629.1562, 312781.1875, 358295.9375,
        756109.5000, 505774.1875, 256516.9844, 380479.8438, 328417.5000,
        423298.5938, 235859.6719, 376926.2500, 302167.2812, 266465.6250,
        753406.1250, 383834.7188, 376464.1875, 537054.5000, 627389.4375,
        577384.0625, 352164.3438, 305296.7500, 469420.8750, 343024.3125,
        380629.0625, 454818.8438, 507385.1875, 426152.0625, 393135.1875,
        543740.1250, 388875.3125, 467155.0938, 505822.8750, 317558.4688,
        317310.3438, 381125.6875, 677083.3125, 752820.3125, 198418.8906,
        295705.3438, 685159.6250, 599158.1875, 650808.7500, 717180.0625,
        437536.0000, 452292.4375, 279536.1562, 962176.3750, 548205.8750,
        633707.3750])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 8490.5273, 12034.9521, 10681.7598,  ..., 25423.9062,  5218.6548,
         2248.8904])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 49306.3359,  41981.0508,  68336.2188,  66805.2734,  55221.5742,
         55391.6094,  63862.4219,  94152.9609,  63761.2500,  73259.0469,
         78035.0547,  69445.2500, 101449.3906,  70898.6484,  55046.8750,
         60738.9688,  50217.6367,  86732.4922,  62205.7773,  66203.3906,
         62359.7812, 101217.0859,  73821.1250,  59103.4023,  54330.2578,
         51134.6133,  66713.0234,  55817.0547,  74659.5469,  93870.1250,
         60933.4648, 113629.9062,  49393.4570,  44684.6992,  80451.4219,
         65445.0938,  70449.4453,  39050.8789,  44275.1758,  90814.3438,
         76326.1250,  58173.7930,  93811.1797,  96914.2422,  47621.6602,
         65943.8203,  68840.8516,  57371.4844,  62836.7383,  82003.2109,
         58261.3672,  95120.4688,  55210.7148,  83824.4375,  77820.9844,
         48819.0742,  38252.8633, 107714.9688,  50615.0469,  53006.4570,
         50334.1094,  88666.9297,  52951.5078,  56721.7461,  35822.8711,
         54560.7383,  42806.0820,  46600.8750,  38009.2656,  68301.2266,
        111439.6719,  46387.0469,  39366.6641,  59148.3711,  42932.1328,
         69947.4453,  65448.1719,  63575.8516,  84005.9141,  50690.9180,
         40528.7656,  57894.2578,  60364.9336,  84269.5781,  35604.3789,
         86109.5391,  60713.8477,  47435.3516,  46572.7852,  68959.9062,
         79864.0469,  68519.6328,  39135.4453,  51590.3398,  55438.2461,
         60520.1172,  53934.5312,  35385.2773,  85833.7266,  60707.0898,
         95277.7969,  37803.5117,  41515.7695,  52992.0508,  93957.5703,
         69039.0625,  40743.5000, 101992.1172, 113617.7500,  85373.4609,
         53943.0781,  62591.3789,  65057.4531,  58352.0703,  45162.2695,
         63899.1055,  92192.2500,  72962.6953,  99079.7500,  72690.1328,
         66762.4297,  74160.7969,  91864.8984,  81320.1016,  38232.9648,
         32032.3047,  45322.6641,  77807.2734,  37395.9023,  57103.9297,
         70170.0078,  60711.4883,  94545.8359,  94580.9375,  37082.3594,
         70343.1719,  71787.8594,  59147.5547,  37174.4648,  38187.1016,
         60418.6562,  25393.6484,  66451.8828,  77507.8359,  56118.4766,
         59451.2969, 134969.6562,  69566.8438,  43550.2891,  35641.6758,
         36739.1094,  39091.2773,  59252.4297,  86769.7500,  43910.8945,
         67229.3281,  79131.1172,  92903.1875,  87815.3750,  79091.2812,
         60764.7969,  60450.6328,  50665.2969,  51077.7383,  85903.9219,
         79941.1172,  61432.8516,  31610.0879,  75912.7188,  67782.9844,
         33152.8438,  57298.8867,  25693.3359,  71560.8672, 115494.0547,
         60645.3203,  55198.5469,  46061.1523,  63306.6016,  48818.9023,
         50299.8672,  62050.2695,  43910.0938,  45706.5117,  60521.0547,
         76838.1719,  78724.5859,  39364.7812,  57408.3672,  41736.3438,
         41616.4336,  65234.9297,  50409.2461,  88272.6797,  87038.1562,
         94599.2266,  73977.1719,  48162.7227,  83144.7188,  53550.8633,
         65146.6328,  75565.8125,  64043.5977,  73383.9375,  78685.0703,
         48889.6797,  41185.4492,  31535.6016,  50063.8828,  46488.0547,
         57593.3945,  58452.6133,  52791.8047,  50820.3633,  84113.5234,
         80082.9531,  54289.6680,  89945.4766,  63926.7695,  49012.4570,
         73958.4531,  39626.6523,  66931.8594,  60928.6523,  46330.7500,
         27487.1113,  67689.4375,  49404.6523,  90943.2422,  73776.6484,
         55619.5625,  74242.2266,  63946.5742,  72387.3828,  30487.9453,
         60674.6914,  40405.9375,  84287.7500,  81158.9844,  42623.2344,
         57674.3516,  46091.0625,  82465.7969,  61131.3828,  67686.5469,
         43742.9453,  70114.1719,  32072.9102,  39865.6758,  64474.6719,
         69610.8203,  77892.2656,  48157.4648,  61598.7969,  80658.9609,
         94791.5312])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([223154.2031, 225682.1719, 355724.5000, 567169.1875, 403276.7500,
        247636.4844, 358597.9062, 186359.8125, 890046.4375, 246340.6562,
        493786.4375, 401177.4062, 341584.2812, 488950.2500, 503442.4062,
        467210.7500, 345991.6562, 243053.9062, 392776.2500, 200119.2656,
        461034.1250, 355515.5938, 208928.3750, 287236.6562, 127145.7812,
        470651.4688, 318098.0000, 499404.4375, 339424.0938, 364953.4375,
        426607.7812, 263732.3750, 568139.3750, 384238.6875, 352590.3750,
        334662.9688, 165252.7656, 193995.1719, 332527.6250, 352169.5000,
        516698.6250, 376586.6562, 384928.1250, 271046.6250, 174217.5312,
        388980.7812, 297845.1250, 336778.9062, 400511.3125, 386832.5000,
        356562.5000, 386107.0625, 479492.1875, 282508.1875, 500052.0625,
        391247.0625, 260687.7344, 380738.3750, 233168.7188, 279089.1250,
        259845.9688, 563353.6875, 226970.6406, 220099.8281, 219639.2812,
        335676.4062, 342846.5938, 330048.1250, 231382.0469, 290038.9375,
        478613.4688, 320680.4688, 278543.4375, 405321.3750, 209708.3125,
        462782.1875, 211692.0312, 306165.0938, 427035.5312, 413251.0000,
        278854.3750, 333429.4062, 316797.7500, 360927.4062, 178826.1094,
        207690.4219, 211585.8906, 413908.6875, 233489.0625, 409782.5625,
        301132.5312, 257911.1719, 503715.0625, 174214.0781, 350861.0312,
        619102.5625, 249564.5312, 181721.1250, 487997.3125, 248557.2188,
        566960.3750, 339198.5625, 337898.6562, 330436.0625, 273586.8438,
        438816.2812, 280158.8438, 375113.8750, 245910.4062, 432499.8125,
        269795.8750, 201104.1719, 198304.9062, 263774.7500, 178601.2812,
        284097.0625, 407558.2500, 400650.7188, 188463.3906, 238978.2969,
        411259.8125, 282523.3438, 413312.6562, 317798.1875, 243508.6719,
        330310.5625, 322960.8438, 365117.4688, 213989.9688, 479844.0625,
        368358.1562, 329542.4688, 393035.4375, 439418.4062, 403417.4688,
        319621.5938, 240324.7812, 540131.8125, 272768.7812, 293179.8125,
        376409.1562, 418397.0938, 308959.1875, 510650.6250, 274681.0938,
        307690.3438, 413356.8125, 267933.1875, 496583.3750, 281374.3125,
        239009.4375, 325305.9375, 284211.3125, 332461.9062, 258084.6250,
        280571.5000, 374436.0000, 391249.1875, 229656.4844, 162268.4531,
        434381.1875, 328724.3438, 435103.7188, 278233.9375, 253346.6719,
        358434.6562, 481124.4375, 442977.8438, 543221.0625, 323405.3125,
        309312.3125, 400503.1875, 222598.9219, 428647.2500, 334909.3438,
        465726.7500, 299396.6250, 447304.6875, 302362.1562, 553046.6250,
        495474.9375, 556541.6875, 392993.5312, 221720.8281, 373858.7812,
        379815.2812, 316778.0000, 140983.6562, 312789.8438, 428981.7812,
        390387.5625, 305167.7188, 346414.0625, 316941.3125, 282704.1562,
        334276.2812, 614531.4375, 573030.7500, 381349.7500, 378582.9375,
        402160.5625, 305963.1562, 389931.1875, 307409.7812, 572836.0000,
        366081.0000, 657995.8750, 474783.3438, 377911.5625, 248872.2500,
        238363.4844, 271367.1250, 399712.2188, 154996.0938, 232439.6719,
        256204.7188, 288855.3438, 233983.1406, 297180.6562, 222113.1562,
        198070.9062, 247288.5625, 297822.0938, 285405.6562, 394073.9375,
        249963.9375, 431129.2500, 163953.7969, 346780.8125, 385906.4375,
        290043.7500, 367273.4375, 201202.4844, 267986.6875, 461230.8125,
        258088.2031, 330578.1875, 493038.8438, 289442.0938, 315011.0625,
        408485.5938, 220917.6875, 375896.6875, 426453.6250, 603226.6250,
        349081.7500, 473698.0938, 284051.0312, 369441.7500, 169218.5000,
        370863.6562, 419655.2812, 368447.1562, 273592.3125, 309949.1562,
        285539.8438])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4320.0815,  739.6246, 3759.6055,  ..., 3851.8003,  360.4684,
        5464.9351])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1564.3352, 1559.6708, 1541.1404, 1442.3275, 1106.5758, 1693.6501,
        1512.9973, 1296.0500, 1281.8137, 1106.9441, 1429.6230, 1212.5343,
        1709.9266, 1837.9396, 1147.6350, 1737.3080, 1125.2252, 1851.5450,
        1244.3810, 1875.1654, 1891.0111, 1610.2805, 1376.8668, 1560.1052,
        1337.4709, 1676.6760, 1407.7845, 1725.0701, 1701.0675, 1248.7780,
        1458.4371, 1311.8861, 1502.0706, 1223.5234, 1901.1169, 1595.6168,
        1284.0647, 1145.9520, 1746.8135, 1361.5233, 1331.4183, 1759.9015,
        1139.0721, 1245.4349, 1412.8088, 1917.9415, 1330.4421, 1077.3098,
        1246.2000, 1467.2861, 1120.7175, 1335.7609, 1415.1123, 1398.1234,
        1482.8831, 1354.2612, 1379.4288, 1827.5707, 1297.1277, 1632.4512,
        1392.8501, 1041.2555, 1981.2570, 1580.8337, 1370.6525, 1575.4178,
        1954.4028, 1247.0831, 1273.6813, 1500.1863, 1518.7466, 1637.5900,
        1243.7173, 1484.5232, 1174.3983, 1587.1346, 1371.3412, 1256.7313,
        1755.4290, 1122.9220, 1052.8503, 1320.6176, 1402.3126, 1331.0702,
        1817.1465, 1770.8857, 1141.7031,  937.1766, 2164.9502, 1497.6666,
        1052.5352, 1583.3724, 1496.4717, 1413.4861, 1484.4709, 1510.6176,
        1494.6135, 1169.5278, 1821.4307, 2079.5156, 1369.3416, 1641.7410,
        1304.8607, 1452.7428, 1551.2019, 1575.3120, 1217.6215, 1130.6130,
        1242.2941, 1230.9402, 1457.3195, 1623.4800, 1892.1793, 1317.6533,
        1153.1527, 1331.2424, 1371.2816, 1267.2129, 1610.6340, 1250.2545,
        1355.5219, 1257.2391, 1350.1488, 1701.0194, 1523.9370, 1449.9362,
        1209.3743, 1868.3003, 1959.5967, 1571.6394, 1272.5428, 1392.7450,
        1593.3770, 1653.9568, 1196.4550, 1075.5186, 1272.9103, 2015.2219,
        1357.0870, 1595.2791, 1424.9625,  979.6042, 1463.2052, 1087.3275,
        1387.5482, 1423.1578, 1319.3545, 1171.0620, 1508.4677, 1571.8080,
        1353.8516, 1357.8818,  839.8962,  999.2913, 1561.9678, 1116.9078,
        1565.6217, 1592.4141, 1272.1293, 1512.5920, 1334.5886, 1948.3481,
        1636.9736, 1507.9819, 1382.0547, 1695.0587, 1196.5431, 1641.4265,
        1718.4346, 1450.1265, 1362.3311, 1231.7168, 1174.4309, 1486.0914,
        1423.0133, 1691.6823, 1578.1484, 1071.5402, 1513.6398, 1617.5552,
        1594.3185, 1079.5913, 1204.3488, 1441.0872, 1276.6559, 1667.3458,
        1409.4647, 1447.7988, 1565.2667,  952.4382, 1841.0697, 1630.0330,
        1702.8794, 1960.7343, 1780.5464,  942.8417, 1143.4456, 1891.3463,
        1080.7059, 1431.3872, 1740.5194, 1338.7054, 1571.4280, 1111.9064,
        1709.8890, 1601.7729, 1556.8740, 1359.2076, 1513.1343, 1502.8684,
        1283.2147, 1491.8585, 1413.4308, 1726.5134, 1800.0524, 1563.5198,
         998.7415, 1292.3564, 1221.7997, 1312.6597, 1516.0868, 1320.4359,
        1414.7506, 1119.9526, 1594.1984, 1712.9836, 1275.9181, 1141.6427,
        1548.6962, 1417.9653, 1049.1241, 1458.2113, 1075.2939, 1549.0570,
        1561.2277, 1250.2590, 1969.8253, 1523.9403, 1319.4633, 1411.7997,
        1211.3861, 1232.6553, 1493.8125, 1738.6136, 1730.8990, 1456.2345,
        1634.2344, 1488.7834, 1641.1780, 1145.9788, 1585.5807, 1692.5009,
        1522.9100, 1155.8511, 1205.1674, 1589.7583, 1503.8831, 2496.5051,
        1347.9729, 1539.0359, 1559.5793, 1868.9945, 1266.6437, 1333.1060,
        1302.4021, 1745.7382, 1639.5348, 1426.4301, 1373.6741, 1000.7200,
        1642.6298, 1465.6488, 1879.0668, 1515.6055, 1614.5153, 1433.8792,
        1520.0376, 1487.6882, 1796.7307, 1329.2322,  999.5403, 1952.6868,
        1685.8363, 2164.0425, 1234.0121, 1175.0466, 1519.2385, 1162.5370,
        1155.8892, 1738.3435, 1520.2090, 1474.3939, 1759.0487, 1382.8336,
        1823.4789, 1774.6377, 1565.3234, 1564.3728, 1551.2527, 1198.5665,
        1288.7896, 1230.1028, 1358.1392, 1154.4685, 1575.7948, 1454.5529,
        1696.4811, 1680.8939, 1546.9082, 1679.5371, 1604.2051, 1305.1215,
        1950.6594, 1673.6202, 1602.0486, 1684.6846, 1396.1179, 1072.7268,
        1324.6122, 1524.7614, 1125.2224, 1575.3928, 1383.0162, 1632.4282,
        2011.3577, 1580.6044, 2169.1836, 1475.5911, 1174.0242, 1786.4188,
        1124.3391, 1236.3684, 1210.5251, 1947.5199, 1218.3551, 1890.0538,
        1671.0121, 1012.2147, 1641.9767, 1584.8354, 1114.0394, 2002.3915,
        1413.6542, 1769.2726, 1415.7698, 1668.9312, 1219.0243, 1391.4312,
        1660.1621, 1254.1726, 1747.1677, 1134.5509, 1669.9943, 1579.4181,
        1239.1921, 1972.6523, 1314.6152, 1716.5098, 1395.0530, 1345.8992,
        1392.4083, 1427.7645, 1484.8801, 1559.4941, 1385.5042, 1071.0537,
        1374.8199,  977.9795, 1361.9537, 1686.5060, 1059.6089,  984.9319,
        1324.9401, 1586.8203, 1945.2181,  952.1406, 1278.0027, 1290.9871,
        1449.7070, 1303.9718, 1709.8053, 1663.0120, 1151.2977, 1660.1061,
        1399.2756, 1144.1312, 1202.6461, 1835.4094, 1090.0192, 1845.6782,
        1365.3036, 1388.4784, 1764.0875, 1957.2231, 1472.9176, 1542.6229,
        1211.6422, 1081.3500, 1154.5249, 1392.0483, 1821.5272, 1441.2695,
        1203.0240, 1348.8207, 1522.6948, 1172.7972, 1614.7242, 1942.4348,
        1510.9330, 1379.9069, 1502.4066, 1216.8610, 1464.7225, 1342.6873,
        1481.5853, 1313.0421, 1370.8485, 1249.4788, 1794.7634, 1806.6564,
        1438.3979, 1575.3438, 1721.2578, 1640.8643, 1401.2772, 1328.6276,
        1623.2498, 1288.7411, 1707.2689, 1267.9165, 1059.7445, 1511.1447,
        1822.4968,  998.9425, 1168.4399, 1229.4182, 1715.5840, 1870.6133,
        1067.0924, 1712.5499, 1266.8217, 1258.0043, 1512.4884, 1435.5275,
        1830.8147, 1716.7672, 1242.4237, 1189.4750, 1479.2412, 1380.6412,
        2013.5829, 1387.5485,  865.1384, 1583.9391, 1560.5887, 1513.4082,
        1044.9226, 1641.1719, 1634.4824, 1498.1342, 1873.9899, 1470.2131,
        1854.0179, 1506.9877, 1514.6146, 1918.5026, 1435.2039, 1444.9801,
        1319.0345, 1119.4672, 1426.3330, 1216.0914, 1664.3318, 1046.2843,
        1113.6320, 1329.3527, 1473.5900, 1441.3293, 1676.3899, 1441.9788,
        1261.7190, 1220.9828, 1778.4331, 1765.4147, 1197.3864, 1595.0568,
        1504.4360, 1526.8726, 1065.5983, 1527.0818, 1293.1957, 1345.1686,
        1023.7810, 1360.6350, 1094.2822, 1311.1334, 1788.4675, 1701.1439,
        1168.9497, 1522.4532, 1465.2706, 1548.0441, 1554.5759, 1661.5364,
        1386.4321, 1457.6947, 1638.2347, 1515.8163, 1447.7117, 1533.4363,
        1196.5771, 1862.4369])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([14013.8584, 18950.5879, 16476.4473, 15053.5205, 14929.6670, 14365.0176,
        13275.8838, 15142.4775, 13531.7881, 17297.7637, 22916.8848, 19599.4043,
        15920.5176, 13359.1357, 15886.0098, 16328.4004, 14131.3047, 20195.6875,
        16981.1855, 17361.6523, 24406.7812, 18341.4922, 21031.1113, 15955.2471,
        15014.3887, 14512.6436, 12212.6182, 10829.2100, 18636.6816, 14870.7812,
        13640.1016, 15757.0791, 14651.9775, 13713.2715, 23034.7305, 18412.4609,
        26115.6348, 15612.3174, 17225.4941, 17552.1172, 14375.5146, 15932.5361,
        25424.9043, 20653.5488, 14192.5361, 16960.9863, 16788.8555, 21144.3672,
        16520.2617, 16972.8887, 15935.8379, 14797.1250, 15171.1182, 16250.9707,
        20313.4180, 12654.6582, 16444.4609, 21386.5234, 17800.5312, 17232.3535,
        16560.2520, 16284.5635, 17772.4414, 17933.5977, 21547.9746, 13311.0801,
        13669.6523, 22247.6152, 20095.0996, 14471.2510, 17860.9648, 17366.4551,
        14262.3604, 16844.4805, 18791.7363, 17488.7188, 15019.7900, 18034.5781,
        17229.4668, 16845.6934, 20460.7891, 20283.0020, 21017.7070, 16289.1299,
        17324.8984, 16835.7637, 14917.3916, 24146.9258, 19926.4922, 14980.6289,
        13260.4883, 18191.1250, 24559.1855, 19089.4004, 20295.6973, 17471.9746,
        18674.6855, 17495.2949, 23792.1348, 16688.3301, 17235.7793, 18601.1426,
        22689.3164, 16785.4590, 15526.0879, 19789.0723, 20105.6738, 21175.7422,
        14275.3389, 13744.6797, 22634.8613, 15678.8115, 13043.4346, 19680.1914,
        18199.8242, 15166.2803, 17802.7207, 15590.9238, 15243.7383, 17652.7129,
        11404.8223, 17845.9980, 21648.0918, 18090.8809, 13812.8379, 13091.8408,
        14363.7627, 20769.6094, 14161.4355, 20428.1250, 17631.1543, 11786.6631,
        19389.9746, 13816.6543, 22286.0820, 12808.7861, 17717.4258, 18823.4102,
        17178.0840, 23511.8555, 14996.7041, 18019.4766, 20853.3711, 17982.1465,
        14149.2686, 17655.5234, 21159.5625, 17130.9258, 21793.6777, 16958.8516,
        17568.5176, 14696.3760, 20017.0781, 15313.4404, 21149.2812, 16223.3633,
        14548.6758, 15010.4404, 16960.4160, 20333.0898, 15407.4961, 17471.7129,
        12960.5664, 16644.4727, 19228.5137, 16161.8018, 17416.4238, 14697.2852,
        19738.6758, 20444.2793, 15743.8857, 14684.6152, 19045.0898, 16527.5273,
        17091.7734, 18560.9395, 15341.3828, 15150.5947, 22178.5645, 17137.0762,
        15277.9521, 17954.7930, 15818.8076, 20571.6934, 17280.1406, 15010.6475,
        16201.7900, 19529.9004, 17457.0254, 24631.6973, 18433.0840, 17405.5918,
        14767.6875, 19710.8730, 18021.7578, 17512.8711, 22712.3652, 14565.5049,
        17478.9121, 13650.9854, 14122.2363, 17100.5840, 11775.9102, 15414.2188,
        16926.9551, 13267.4121, 10241.1494, 15137.9434, 24461.2109, 20217.1309,
        18045.4375, 16007.1455, 15656.4756, 15524.3525, 20051.3516, 13665.3281,
        11686.4629, 15798.0332, 19885.5781, 18928.9727, 12043.5000, 11081.9512,
        12815.6328, 12633.8779, 21970.6953, 16319.2354, 17541.7930, 16609.2305,
        14540.2832, 15019.5352, 13332.4053, 16282.2529, 19665.8340, 16906.2578,
        17592.1133, 19200.8555, 14112.7686, 20521.9844, 15410.4941, 16136.2744,
        22533.5996, 17354.5332, 14756.4619, 15530.1670, 10951.7275, 12275.6865,
        14756.2285, 18176.8223, 13526.9570, 15407.0938, 13990.8330, 15261.2402,
        15552.8848, 13905.8457, 17219.3086, 21981.6895, 16703.4805, 22698.4531,
        16542.6523, 25754.4004, 20987.2305, 17874.9688, 13740.5156, 17180.3340,
        24372.5859, 13617.8350, 20117.8887, 15058.5537, 21533.4102, 15846.3389,
        16540.5527, 14960.6094, 20512.8438, 12896.8291, 14283.8711, 19701.8750,
        12416.0498, 14952.3047, 16138.3760, 12422.8066, 16912.1113, 17404.1250,
        19221.6855, 12316.8984, 14445.2744, 17649.6367, 14108.9414, 14611.2695,
        15498.4258, 22708.7109, 19554.7520, 18959.4141, 17599.4824, 13698.4980,
        16628.9629, 13860.2969, 12249.0283, 14905.7861, 16035.0322, 11522.0732,
        18480.7324, 19897.0020, 13968.0723, 14514.8857, 21066.8730, 13248.1660,
        14477.2607, 16964.4141, 15112.1934, 18110.1797, 22043.9180, 15460.4229,
        20721.8965, 13084.3379, 18335.2520, 17692.7969, 12487.3457, 16111.0166,
        25542.9844, 15341.3135, 17125.5508, 15922.7432, 19119.4121, 19774.5527,
        14249.6221, 19314.3984, 18262.6562, 17046.6289, 18908.3574, 15149.4131,
        17797.9805, 13743.7803, 15715.1445, 13938.3574, 15196.3613, 19061.9844,
        13981.6055, 15697.6689, 18592.6699, 17507.4258, 16329.4805, 16578.3145,
        17286.1895, 19403.6035, 14111.1670, 15484.5615, 16041.4043, 17247.4902,
        23181.5176, 16961.5684, 17898.6016, 19233.8555, 13092.8506, 17588.9941,
        16592.8047, 13235.9688, 15537.3037, 14417.0645, 15851.2402, 24434.8750,
        16382.8799, 23721.6562, 18391.8477, 16183.1836, 16345.3281, 16500.0117,
        17482.0430, 15576.3408, 16516.5430, 17030.2363, 16373.4521, 17579.2344,
        15804.7246, 14963.6768, 14312.6182, 15853.5928, 16823.9492, 17246.4316,
        18638.7793, 15851.3770, 22725.1387, 12550.9756, 21882.7090, 16823.1016,
        17259.5273, 15306.2842, 18958.9844, 17157.5137, 21870.7656, 18497.7969,
        13837.5986, 17860.5391, 19140.5488, 19903.9551, 17584.9355, 19338.2734,
        19563.2500, 16567.8086, 10687.6016, 18650.2070, 21003.2969, 17999.1699,
        16484.5000, 11866.3115, 18094.2461, 20743.3457, 13362.5986, 13957.0801,
        17730.2383, 25690.7578, 13847.9893, 15480.3623, 15893.1836, 15510.7246,
        15110.2539, 20239.5469, 14563.6670, 16259.8740, 21103.7383, 15177.0352,
        18531.4902, 16686.4707, 20393.0605, 15167.6504, 12795.4170, 19774.3027,
        12669.2100, 18372.3457, 14782.9795, 13468.0225, 15514.5840, 15325.2588,
        15334.6719, 19323.8926, 18510.4648, 15579.7949, 13490.1201, 13632.5322,
        16072.2139, 19118.6465, 22151.7246, 14940.9316, 16682.1602, 13819.5781,
        18276.7930, 19237.2461, 19731.4023, 22449.6250, 13878.4072, 18903.4277,
        18952.9297, 21218.3242, 15185.6885, 15476.2031, 12294.9463, 22179.9922,
        17374.3125, 16400.9414, 16519.7695, 23429.4766, 18610.8008, 16445.3965,
        15080.8311, 16252.1104, 12884.2012, 18109.0820, 14905.4424, 17547.4863,
        21701.6562, 14481.9648, 17075.3477, 18413.1113, 21362.3223, 17414.8633,
        20323.8652, 17710.0547, 15020.3516, 15660.6631, 21335.0234, 13910.6094,
        14064.2852, 22842.5898, 19450.7285, 17380.3184, 10655.6123, 19349.8438,
        17039.9707, 16779.5801, 22266.6445, 14095.5078, 12319.8906, 12570.7402,
        15031.3340, 21953.2734, 13059.0840, 19490.5645, 16885.3125, 15700.1914,
        15619.9082, 15952.9990, 23210.8516, 14754.3887, 17658.9863, 19155.8809,
        20337.6738, 16338.9521, 15669.4863, 13869.1270, 12937.2793, 20482.6816,
        17414.1445, 13783.7246])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([17.7065, 71.1194, 93.5455,  ..., 41.5640,  1.4791, 59.6769])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 80.2102, 187.9359, 488.9817,  ..., 142.7514, 106.8985, 183.3658])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 725.0208,  512.5368,  546.8334,  609.3519,  686.8332,  547.1913,
         694.3075,  897.4496,  636.2817,  701.2676,  539.1137,  589.5580,
         846.0403,  546.7064,  735.2566,  686.4422,  705.2509,  494.7728,
         441.7454,  456.2416,  495.5529,  838.5092,  657.5087,  466.2188,
         538.6364,  708.5312,  506.6108,  646.4686,  450.2277,  660.3006,
         665.3102,  493.1514,  559.5341,  537.6163,  479.5219,  677.7544,
         661.4471,  590.4113,  806.4150,  553.6832,  649.1224,  400.7883,
         587.5968,  611.7940,  522.6160,  548.9668,  479.7351,  615.2446,
         633.2087,  457.6659,  523.1210,  668.8289,  543.6594,  461.7062,
         418.6004,  482.7686,  451.6269,  564.3391,  713.0280,  450.6923,
         656.8864,  551.6299,  798.0164,  531.3923,  829.6577,  401.4323,
         635.9435,  706.6647,  477.4111,  716.2493,  605.1030,  508.0162,
         703.9246,  866.8152,  658.1637,  518.3065,  593.5428,  609.7406,
         589.4813,  758.1639,  613.3283,  479.6905,  566.7521,  580.6537,
         442.0045,  660.9429,  546.3202,  434.9042,  633.9486,  546.4732,
         690.4296,  631.1379,  809.0116,  473.8309,  716.8271,  498.5225,
         566.0816,  519.6462,  494.5909,  517.8997,  553.4406,  763.4311,
         565.5286,  423.7626,  553.2650,  537.2343,  529.9996,  558.3422,
         559.4462,  451.1221,  917.9465,  639.0188,  537.0304,  592.0185,
         387.8650,  612.4969,  524.8771,  783.2631,  631.4561,  692.5895,
         422.9159,  565.7307,  499.2393,  793.2964,  769.2101,  415.3230,
         793.6600,  677.9891,  633.6896, 1007.9651,  481.0393,  314.5382,
         512.6058,  611.3232,  611.0300,  708.1711,  620.0060,  600.3052,
         629.7321,  684.4947,  602.5052,  487.0291,  533.8341,  451.6090,
         488.6251,  560.5341,  569.7480,  674.4122,  613.3956,  493.7903,
         631.0730,  544.2849,  602.9278,  669.2590,  602.2839,  589.2103,
         542.6732,  691.0671,  608.4241,  551.0906,  520.7200,  483.4122,
         760.7542,  671.4831,  773.5474,  637.4815,  494.2072,  477.8673,
         607.7322,  548.8145,  647.6414,  473.6018,  771.0219,  556.9450,
         566.8578,  558.4059,  584.0756,  604.1721,  732.1992,  425.7771,
         446.2696,  772.3568,  465.8275,  662.6846,  628.7932,  647.3217,
         547.0475,  534.3349,  677.8600,  727.9966,  505.6886,  722.5225,
         714.0460,  499.2876,  696.7473,  527.7996,  515.6093,  683.4022,
         581.9554,  573.9666,  617.3433,  401.1624,  625.2045,  643.7843,
         428.8125,  404.1911,  717.8959,  615.2910,  701.8096,  421.8622,
         558.8524,  473.8157,  547.7778,  636.4941,  341.8980,  607.2777,
         594.0967,  482.7354,  509.6910,  390.6674,  622.1546,  647.6306,
        1067.6167,  622.0349,  475.7525,  685.2078,  499.9067,  385.9630,
         434.1899,  459.6413,  409.4134,  489.6378,  549.6704,  589.2752,
         544.5634,  569.0644,  737.5892,  701.2195,  639.4927,  529.9333,
         716.1356,  566.0175,  654.6334,  586.3445,  552.9459,  761.2667,
         595.5931,  652.6789,  588.2670,  568.7820,  598.0052,  888.7477,
         341.9900,  591.6473,  762.3323,  505.3201,  847.0504,  494.1859,
         585.8583,  784.0761,  538.4266,  636.3077,  780.5654,  493.0457,
         408.4689,  629.7104,  486.8144,  632.5928,  490.5204,  550.4694,
         671.2670,  597.9732,  777.2631,  532.7033,  758.3762,  602.5040,
         503.4476,  618.5405,  684.0588,  522.2811,  444.9008,  554.0106,
         647.7861,  809.3911,  489.7874,  523.4155,  626.1213,  726.9000,
         628.3177,  390.4287,  592.7460,  421.0307,  503.8489,  453.4793,
         446.2729,  519.5129,  464.5390,  390.1428,  860.5898,  622.1614,
         567.0482,  580.4386,  528.2022,  770.8735,  411.8868,  570.6053,
         654.3604,  519.3378,  603.8483,  641.9081,  383.5238,  608.1922,
         567.6560,  516.7888,  767.3608,  508.3742,  636.2299,  677.1859,
         578.4877,  701.2331,  610.4170,  549.1462,  582.0562,  557.8182,
         503.6161,  920.8510,  549.3082,  576.0775,  597.5801,  470.0439,
         551.0726,  521.2063,  544.9313,  562.1700,  727.4542,  510.8658,
         532.7361,  464.6353,  629.9048,  543.0557,  749.8119,  915.8076,
         739.3361,  506.2975,  464.2870,  494.9476,  460.0901,  759.5481,
         612.0718,  563.8416,  729.6321,  580.0829,  483.0459,  570.8664,
         427.2546,  499.3967,  633.4227,  879.8748,  540.3114,  633.7670,
         617.4966,  805.5018,  518.9600,  328.9196,  647.5859,  691.1877,
         652.6965,  506.9386,  622.6135,  625.7189,  581.9332,  543.5562,
         497.7951,  681.5146,  696.2853,  522.4011,  534.0325,  721.3737,
         581.6420,  540.1929,  567.7764,  517.3599,  320.4787,  464.8681,
         532.7279,  627.8760,  589.6824,  743.0645,  819.3166,  550.0970,
         564.8415,  497.8427,  559.8549,  881.4730,  725.3206,  535.2291,
         687.7439,  853.6914,  505.6282,  703.5308,  497.9552,  611.1218,
         480.6922,  617.1942,  691.4570,  491.2095,  408.6069,  803.1221,
         674.8971,  585.4872,  607.1787,  615.6159,  606.2612,  478.2947,
         479.1441,  519.3622,  377.2673,  568.6140,  600.0408,  979.3196,
         518.0572,  414.5155,  595.6844,  594.5029,  733.8650,  490.0286,
         762.5773,  654.4999,  790.4111,  410.8015,  472.9372,  614.2560,
         616.1571,  565.8272,  567.0370,  495.8617,  617.7515,  428.6544,
         562.8909,  690.5806,  710.0219,  758.6097,  425.2236,  542.5799,
         899.0670,  796.3143,  627.5774,  516.1981,  482.5433,  608.6715,
         556.0516,  423.5140,  599.6569,  515.6364,  972.3326,  719.3616,
         621.5957,  467.1690,  552.4922,  457.9198,  514.2492,  615.9064,
         444.0019,  631.9019,  490.3552,  740.4024,  487.2854,  485.4996,
         732.7200,  628.7455,  434.7138,  454.2152,  582.8734,  650.0563,
         738.7708,  767.1752,  421.4630,  267.1965,  588.7971,  526.9196,
         524.5267,  616.1782,  980.1461,  481.1102,  534.3758,  750.5308,
         535.4708,  584.2938,  582.8958,  630.2964,  696.3696,  484.3745,
         689.8357,  611.5245,  661.5165,  519.6218,  540.4089,  745.1151,
         639.6416,  457.0607,  541.1756,  501.8452,  666.9655,  458.5113,
         504.8437,  517.0309,  689.0097,  385.0252,  622.6727,  659.7145,
         608.1292,  621.7058])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6138.8423, 6180.1479, 7477.7871, 5628.9585, 5159.8516, 5617.1304,
        5638.9517, 6502.3564, 4995.0249, 5104.3340, 6293.9185, 6366.5449,
        4568.5601, 6499.2529, 5378.7842, 4902.4263, 5239.0176, 5089.0371,
        5568.9312, 5889.9902, 5851.9062, 5454.5356, 5836.3818, 6927.0415,
        5964.3325, 4616.2998, 5462.2764, 3950.5398, 5049.1943, 7098.8774,
        3824.2969, 5096.8071, 5055.4438, 6879.8423, 5716.0889, 6401.4570,
        5091.3208, 4615.0220, 4389.0215, 4943.5640, 5514.2490, 8566.6445,
        4245.1807, 6236.7334, 5951.9048, 4255.7681, 4252.0142, 5517.1201,
        6464.4854, 5244.7134, 5248.2046, 6196.6479, 5385.1006, 6357.0508,
        5373.5620, 5362.7979, 6435.6152, 5365.3496, 5381.7490, 4174.6064,
        6249.1782, 4589.2114, 5530.8862, 5628.1030, 5323.0269, 5079.6445,
        4226.4263, 4307.0557, 6771.6460, 6248.6416, 4311.8823, 3396.1877,
        4869.1401, 4079.1685, 6683.8467, 8868.0264, 6393.5498, 5479.8604,
        6038.4146, 5100.9683, 5367.0503, 6161.7993, 4323.3882, 3769.2434,
        5349.5718, 4029.6924, 6957.3232, 8570.9844, 5990.2192, 5949.8794,
        5903.9092, 5192.7583, 6313.9824, 5143.7891, 6253.8535, 5831.4507,
        4280.4541, 3336.1609, 5126.1572, 5394.9585, 6559.3379, 5783.2090,
        6110.2119, 4126.8008, 4728.2310, 7459.7505, 5026.2808, 6677.9282,
        6661.7695, 6193.6309, 6657.0781, 7633.6475, 5126.8394, 5197.5327,
        4879.3633, 5890.9375, 4670.9341, 4196.1040, 5350.0850, 4709.9746,
        6435.0938, 5323.0435, 5466.8159, 5512.6772, 7084.8452, 8171.9976,
        5213.5728, 4277.4961, 6496.2578, 7157.7788, 6229.9746, 5587.3384,
        6054.1387, 7249.4629, 6414.3291, 4762.0464, 4437.3691, 4085.4961,
        5283.7153, 5605.4717, 5218.2778, 4316.9883, 4723.9673, 4199.9521,
        5456.6201, 5293.4731, 3873.0906, 5086.2544, 6585.2642, 5688.2285,
        5503.1123, 5255.1973, 4838.3340, 4201.2466, 5291.7070, 9085.3350,
        5477.4380, 5313.2119, 5575.3140, 4551.7881, 4456.7007, 6947.7124,
        6351.6011, 4765.4658, 7118.9570, 6498.5020, 6298.1968, 5537.3799,
        5498.1445, 5054.9014, 5558.3091, 4052.6631, 5804.4385, 4999.6289,
        5054.0112, 4174.0493, 3774.3010, 4907.6328, 6960.4395, 5569.1113,
        5554.5781, 4563.6963, 5036.6538, 4993.3618, 4533.0190, 7666.8647,
        4842.1885, 7681.3066, 4359.5142, 5121.9189, 5438.1411, 5641.4160,
        6210.4932, 4867.9121, 8201.9189, 6130.1836, 9535.0039, 4885.2314,
        6885.7461, 4880.9990, 6474.4790, 4845.3433, 5979.7681, 4482.1489,
        5044.6494, 5546.8511, 6401.1333, 6175.8545, 5203.0215, 5270.4263,
        5181.5728, 7352.4165, 5309.1626, 5320.7168, 5773.3940, 6078.8130,
        6249.2197, 5684.2266, 5468.5527, 8011.9707, 5512.3306, 5897.1406,
        5377.0938, 6226.9043, 5002.8281, 4967.8262, 5658.9292, 6674.2686,
        4914.3701, 6194.5747, 6622.5015, 5920.6577, 5531.4590, 6909.4316,
        4137.9546, 5059.6387, 4597.4663, 5963.2661, 4825.8135, 8453.1221,
        4984.8281, 4558.1826, 5713.6479, 5827.5234, 6958.6748, 5562.8872,
        5655.0156, 4769.7905, 4728.7773, 7473.3110, 6136.3477, 4436.0342,
        4412.5059, 4902.0078, 4196.6504, 7077.3203, 5116.7188, 6270.3960,
        5113.1582, 4049.2178, 6446.2700, 5156.1675, 4194.3237, 5047.0508,
        4616.9683, 5397.6870, 4057.1387, 7455.9072, 4888.7085, 4308.2529,
        6156.2393, 4356.3271, 4189.5103, 4960.7690, 5794.9604, 6559.6504,
        7398.4028, 7841.2148, 7156.9141, 5628.1396, 5469.9502, 6330.4995,
        4546.4009, 5124.9541, 7435.2666, 6234.7153, 5891.2461, 5608.7773,
        4702.5566, 3254.4163, 5746.3711, 5767.7017, 5896.9370, 5497.7671,
        4203.1084, 6158.8364, 6498.7275, 5831.3926, 5781.9072, 4655.6294,
        6309.8354, 5105.7129, 4805.0020, 4032.7593, 5875.8394, 4847.2012,
        6525.6890, 5857.4429, 4608.5400, 4476.0010, 4627.6445, 5649.8120,
        5292.0801, 7636.7676, 4620.0767, 5062.1821, 4582.2012, 5291.8789,
        5356.6221, 6493.3848, 6324.4072, 5845.4795, 4687.3848, 6969.1567,
        5604.6343, 3539.6545, 5954.7139, 5472.9580, 4897.1553, 5036.9478,
        4728.2954, 5744.3716, 5631.2793, 5379.0620, 6617.3545, 5473.7773,
        3633.1353, 6671.8027, 5011.8862, 4458.2573, 4849.6836, 5434.2207,
        4800.7666, 6899.9727, 4495.0024, 4928.2812, 5171.5483, 4415.6685,
        4570.4697, 6443.5000, 4315.3589, 8102.1250, 5838.0962, 5793.9771,
        6296.0098, 7490.5854, 6352.0776, 4922.1006, 4596.3071, 4874.0508,
        4407.2349, 4729.0801, 4762.8257, 7393.8276, 6562.1455, 4665.4692,
        7360.7188, 5269.8486, 8356.2861, 5394.3955, 5409.8306, 5621.3892,
        6848.3052, 4443.5327, 5913.7656, 4766.0654, 4896.5015, 4688.2178,
        4571.7764, 4871.7549, 4606.3696, 5462.3892, 5750.1558, 5631.7202,
        7259.6812, 4335.7554, 7557.8545, 4796.3545, 5187.5874, 4422.6421,
        5597.0288, 6394.2632, 4638.2510, 6696.1743, 6256.3232, 4884.1069,
        5997.9644, 5258.8330, 5819.8252, 6400.8105, 4872.4209, 5918.5078,
        5381.7354, 7206.6079, 7202.6333, 7396.4038, 6099.8174, 5369.3613,
        4479.9077, 7024.8740, 5201.7134, 8602.2354, 4851.0474, 4053.9917,
        4080.3801, 5797.1899, 4331.5703, 6365.6885, 4559.8970, 5397.6216,
        4509.6260, 5971.0103, 5350.6816, 4531.8584, 6109.8896, 3983.6929,
        4892.7490, 6465.6592, 5483.8511, 7120.8672, 7712.9185, 3889.1841,
        7617.4873, 6022.1987, 7903.3667, 4766.4492, 6337.7637, 4500.4526,
        4948.4834, 4745.6523, 5637.5400, 4833.7100, 6283.2686, 5665.5552,
        5921.1509, 4450.2769, 4325.8428, 7985.6182, 5343.3066, 5964.7837,
        4953.0703, 7746.7983, 4797.4272, 4073.8264, 6533.4790, 4786.0483,
        4904.7666, 6362.9849, 5463.9663, 7918.3672, 5076.5488, 6550.0400,
        5565.0210, 5582.2280, 4120.1992, 8186.6265, 4926.7676, 5721.2007,
        6041.7568, 4081.8030, 6396.8740, 6704.2715, 5694.9346, 5768.1758,
        5572.8096, 3811.5105, 6372.5059, 4948.2402, 5565.2339, 6477.0327,
        4964.5684, 4256.3594, 5330.6797, 6208.6240, 5086.8447, 5122.0381,
        5815.4126, 7474.8828, 5528.0103, 3577.0647, 7720.2354, 5512.1567,
        5274.3081, 5617.5435, 5368.0615, 5032.4390, 4545.1816, 7678.1030,
        3853.6270, 4487.8740, 5402.8906, 6532.4414, 6844.4756, 5441.9590,
        4283.1914, 7715.2646, 5053.6802, 4259.5137, 7378.1206, 5778.1904,
        5717.8232, 5055.9492])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([303.0096, 854.1733,  88.5086,  ...,  24.7348,  49.5360,  75.7281])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([557.1049, 375.9537, 442.6879, 463.8874, 479.0766, 444.2345, 257.6333,
        425.6827, 513.1993, 395.3495, 439.3041, 514.5259, 409.3359, 391.0354,
        447.1735, 407.9825, 438.6623, 377.5027, 348.3342, 356.6704, 390.8120,
        327.8729, 488.5967, 362.9756, 299.1349, 341.7238, 370.3123, 233.6685,
        411.4610, 299.4386, 504.9509, 441.1025, 409.8217, 370.7880, 349.7728,
        360.8284, 481.7479, 589.5242, 460.2612, 488.6972, 390.6271, 268.8086,
        321.8279, 472.6610, 237.3542, 240.2612, 529.8237, 306.1332, 452.2435,
        404.0005, 313.7620, 347.7446, 341.0895, 476.9650, 442.7297, 794.6809,
        595.7423, 395.4825, 522.8210, 346.8127, 345.0607, 281.6294, 310.4824,
        480.8990, 614.3409, 415.0455, 458.9433, 540.8955, 425.0918, 353.5574,
        571.3778, 306.6757, 347.2410, 461.9352, 462.0645, 343.6008, 432.1187,
        471.0687, 451.3065, 317.6401, 485.1573, 474.1699, 295.1249, 348.2860,
        358.2915, 468.5769, 537.2573, 312.6792, 484.6848, 429.9828, 625.8580,
        358.1245, 636.4814, 392.5977, 319.7039, 714.4587, 431.0147, 367.6608,
        335.0394, 468.3244, 483.6283, 391.3431, 449.5709, 386.1355, 310.3524,
        314.4536, 245.4482, 496.9053, 397.9197, 563.2336, 430.9748, 357.5871,
        524.2637, 401.0258, 329.6269, 447.0111, 393.6903, 603.3818, 480.7943,
        368.2336, 398.3618, 344.5089, 304.3781, 512.4453, 452.3849, 394.1199,
        473.6245, 349.5945, 471.5274, 398.9081, 606.2131, 277.6793, 567.4377,
        473.7932, 284.8823, 357.3403, 425.5879, 340.5664, 421.9460, 456.5438,
        397.8050, 330.6052, 463.9997, 436.5700, 336.7571, 468.3269, 307.0762,
        399.2626, 363.2435, 479.6235, 659.0165, 472.2955, 399.1513, 512.1534,
        302.1736, 423.9328, 404.8259, 392.8168, 655.6096, 600.8044, 424.4455,
        673.0756, 507.0393, 456.8269, 265.6448, 390.2895, 403.0751, 327.6950,
        365.2920, 419.4568, 417.4537, 400.0587, 475.2065, 449.1578, 367.4771,
        319.1154, 307.4514, 634.4153, 572.7197, 504.1671, 293.6259, 466.6743,
        414.8788, 369.0520, 380.0060, 709.6561, 411.6064, 443.2235, 618.3771,
        410.4925, 442.6927, 410.5348, 446.5199, 380.8589, 483.8787, 352.1572,
        288.5874, 335.7119, 605.8679, 325.9063, 393.3185, 922.7944, 521.8190,
        585.3776, 318.3868, 319.7111, 416.9431, 576.2302, 601.1115, 371.2734,
        517.8282, 410.1826, 404.9424, 369.1172, 549.0538, 404.8139, 331.4509,
        539.8273, 396.5595, 396.6600, 415.7936, 502.4596, 403.9723, 443.7162,
        451.9315, 525.0098, 549.2995, 323.3951, 269.7263, 374.6074, 368.2221,
        339.4144, 269.1339, 552.6834, 644.6868, 236.4317, 417.7964, 318.5181,
        430.6242, 496.0161, 662.0310, 437.5439, 496.8746, 469.4548, 541.8269,
        401.7689, 397.4384, 384.1766, 477.9734, 418.9807, 400.2982, 469.6892,
        281.3969, 273.4519, 469.8541, 480.1309, 360.6489, 504.2419, 286.0768,
        527.0897, 431.2928, 530.6988, 444.0058, 396.5170, 521.3260, 419.4390,
        331.0762, 389.4046, 210.5966, 404.8068, 593.4984, 326.2258, 291.2545,
        384.1298, 489.4395, 388.3044, 360.2116, 354.4911, 412.9244, 477.8307,
        405.1861, 391.1548, 295.5510, 370.7442, 556.7224, 410.4190, 461.3388,
        349.1338, 549.4235, 352.3429, 390.5453, 375.3690, 335.4994, 424.2047,
        501.0995, 514.7598, 566.5123, 381.2785, 468.6190, 464.1078, 507.8401,
        373.0137, 513.4575, 347.4258, 508.3836, 333.8374, 378.1199, 424.1165,
        494.9432, 387.7294, 248.8926, 388.0561, 442.2849, 337.5688, 400.2434,
        480.4182, 299.2366, 431.0483, 376.7412, 454.5045, 440.4207, 338.7719,
        639.2276, 641.8870, 316.1347, 264.7806, 314.9871, 433.5316, 453.1983,
        462.5862, 392.8272, 426.9489, 434.5544, 478.6361, 529.0309, 306.2223,
        696.3392, 422.6641, 505.0224, 495.7091, 510.8813, 384.4794, 381.1030,
        378.3220, 525.7317, 481.2873, 315.3270, 563.9951, 257.4350, 258.4850,
        288.8270, 293.5883, 564.5278, 620.7292, 487.0143, 430.8462, 481.6963,
        522.6145, 440.5052, 412.6846, 358.6063, 665.5130, 477.0407, 373.3181,
        376.4706, 571.8577, 274.1889, 508.8611, 360.0251, 471.1992, 575.4628,
        483.3756, 468.4850, 567.7272, 455.7410, 459.6324, 316.2635, 648.6956,
        409.7137, 374.5785, 593.6154, 440.5287, 463.9684, 453.2699, 506.8343,
        369.0564, 365.8795, 404.2765, 641.8812, 504.5149, 446.0502, 480.2100,
        357.4575, 470.9959, 433.9260, 284.2307, 266.5375, 664.5989, 285.0235,
        387.1201, 290.0988, 433.3627, 397.3694, 372.1118, 247.3240, 392.3931,
        535.1342, 376.2911, 541.8207, 468.5727, 285.3929, 677.2303, 519.7990,
        569.7701, 438.0022, 424.6134, 337.2326, 395.0325, 458.4238, 459.4613,
        400.8339, 307.6996, 441.2840, 409.5925, 467.2737, 402.1796, 375.1619,
        575.2647, 351.7787, 466.4698, 477.4066, 544.6212, 485.5634, 298.2123,
        582.2411, 507.1006, 293.7934, 388.9348, 315.4344, 430.7161, 475.8848,
        377.8980, 351.5364, 434.9243, 367.1732, 431.1599, 488.6893, 314.5964,
        480.2322, 374.2933, 419.7337, 513.9684, 465.4247, 413.3086, 319.3216,
        488.3199, 463.8455, 417.8232, 556.5967, 489.1596, 319.2060, 418.2568,
        418.3386, 359.1202, 599.9508, 355.3253, 574.4722, 421.8689, 446.5276,
        477.3907, 406.4013, 305.4182, 205.9245, 263.2499, 348.2844, 479.1817,
        493.9909, 422.8898, 360.7709, 485.0733, 604.7234, 294.1568, 429.9285,
        655.0853, 499.5026, 363.2568, 336.3094, 434.9681, 483.2133, 332.7842,
        531.1960, 661.2645, 319.8696, 567.0681, 852.8608, 458.9836, 449.2009,
        517.8926, 325.3504, 321.5891, 356.4852, 344.0934, 376.6051, 519.5825,
        366.1970, 644.3263, 565.4715, 627.4258, 405.3476, 354.4958, 407.6794,
        338.0831])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4375.9717, 4714.1865, 2807.2268, 3086.0161, 3334.4434, 6599.1421,
        3460.7808, 3432.0042, 3607.0107, 8923.6514, 3586.7148, 4188.8384,
        2718.7268, 4392.0771, 5424.3086, 3599.8538, 5269.4194, 2908.7944,
        6568.5312, 4283.6260, 5098.7705, 3705.5220, 3284.4497, 6343.1299,
        3847.1936, 4866.9487, 2840.0525, 4103.9507, 5446.9741, 5752.1924,
        4915.2900, 5144.5034, 3945.6082, 3690.8511, 2274.8818, 3716.7505,
        3046.2129, 6011.9365, 4552.6074, 2844.2385, 3910.2959, 4710.3022,
        4170.6421, 2702.3831, 4272.6777, 3867.8691, 5998.4248, 3476.7542,
        3925.3425, 3106.0955, 4870.6699, 4810.9526, 3076.9739, 5167.6035,
        3200.1567, 3612.2446, 5254.2109, 4285.9370, 4822.0156, 4220.0439,
        3407.8420, 4343.9883, 3758.7371, 4597.4316, 4947.1948, 7222.3535,
        3864.6733, 3238.5291, 4836.8042, 4357.1382, 5178.5854, 4164.7466,
        4580.4902, 4728.1021, 2947.0601, 5110.0566, 3807.3447, 3525.9006,
        3827.3955, 2698.1221, 3591.4150, 4136.0908, 4997.8682, 3519.5649,
        4383.0854, 4846.1958, 4484.6538, 2273.1287, 3806.8611, 4084.6238,
        4412.9209, 4759.4136, 4643.5137, 4492.6777, 5471.4380, 4151.9561,
        3295.6914, 5118.2656, 3858.2310, 5156.4482, 3062.7808, 6201.3301,
        7982.4277, 4476.9170, 5426.9067, 4710.1743, 3863.6230, 5218.4224,
        4285.2905, 3264.1030, 5244.8364, 3394.3481, 5563.7798, 2845.2666,
        3172.1633, 4366.3027, 3911.2712, 3643.0076, 3516.1138, 3273.5967,
        3677.5605, 3278.2595, 4369.6196, 3481.6143, 4976.8286, 4417.1968,
        4560.2520, 2737.4897, 3558.3716, 5158.8408, 5596.4849, 3999.0037,
        5402.0229, 6544.3413, 2088.8770, 3083.1575, 3958.1013, 4103.0400,
        3645.1592, 3295.8601, 5365.7349, 5067.9204, 3466.6384, 3897.2805,
        3839.9263, 5753.8672, 3292.3286, 5042.8198, 2891.6738, 4394.1797,
        2789.6914, 3900.0632, 5466.3438, 3661.1677, 5751.3687, 3434.5012,
        5648.0122, 4566.3569, 3617.2417, 3483.3184, 4793.7935, 3852.3069,
        3014.6343, 3844.3193, 4174.4111, 6407.0996, 3793.4150, 3511.6895,
        4883.1006, 5775.5552, 4157.6821, 3241.0244, 5473.6846, 7173.7607,
        5604.7085, 6502.2520, 6263.7837, 3084.7625, 2997.2466, 4297.5674,
        4627.1147, 4876.5557, 2841.8833, 4180.4634, 3814.3062, 4590.3843,
        3892.3091, 4424.7949, 2945.3789, 4314.5659, 5378.7207, 3576.6624,
        5491.3174, 3537.9424, 4255.8877, 4863.0581, 4836.7910, 2759.9319,
        4288.2051, 3985.6230, 4963.1313, 5251.6450, 3395.9341, 4776.4048,
        4223.5483, 2922.0610, 5396.2002, 3891.4724, 4574.0620, 3803.7063,
        4718.2163, 2628.7166, 4626.4243, 2900.4944, 4379.3057, 4496.0464,
        3044.3828, 4688.5317, 4815.5649, 2993.2683, 3778.2114, 3202.7476,
        2597.5745, 3782.3875, 5779.5601, 2636.3977, 4559.2764, 2495.7163,
        6149.4277, 6226.3989, 4476.0220, 3385.6541, 3873.0017, 6427.6011,
        6719.1440, 3442.3225, 3968.2991, 2696.6423, 2571.6357, 3452.7317,
        4002.5703, 3819.1426, 4148.9106, 4708.3130, 2939.3130, 3969.4531,
        4623.8340, 3564.5618, 5164.0850, 6362.2939, 2410.3584, 3980.7310,
        2989.8259, 3707.6443, 5180.8711, 4630.2544, 4689.9565, 3823.4319,
        4933.5381, 3970.4807, 2523.1497, 5973.9307, 4446.4741, 4292.2798,
        6910.6724, 5550.4019, 4864.0527, 2900.5789, 5079.1338, 4371.7710,
        4943.4453, 3730.4692, 4993.1035, 4053.9055, 3663.7275, 3573.0188,
        4945.8940, 5605.4590, 5137.6924, 4513.1572, 4614.8955, 5171.0171,
        4096.2388, 7057.4736, 5760.0581, 3169.5530, 4232.7397, 3439.6594,
        5043.8032, 2934.9507, 4152.2241, 2926.7874, 5013.7842, 5000.9014,
        4896.9316, 3615.5247, 4196.7310, 3062.4375, 3437.2258, 4480.2690,
        6530.1987, 4025.3430, 5655.1201, 3845.7441, 7205.9575, 3513.8308,
        7917.7905, 2979.2449, 4145.4917, 4755.2915, 4463.5522, 4120.1416,
        5710.6973, 6231.6294, 3416.9753, 3374.9495, 6942.2490, 3607.2002,
        4512.0513, 4191.6968, 3638.1672, 3986.3145, 5538.3843, 7596.4746,
        5440.6514, 5172.9629, 3886.7456, 5822.5020, 4969.9248, 3626.3479,
        5379.3574, 3266.0261, 2721.2051, 4716.4121, 4860.3931, 3669.2000,
        4520.9263, 4012.1257, 5563.6616, 3376.1733, 4332.3809, 4004.2827,
        4373.9209, 4349.9551, 5932.3228, 3481.5779, 5839.2705, 4300.2109,
        4261.1064, 4289.5806, 3927.6853, 4877.6460, 6644.4526, 4962.1162,
        3160.9946, 3659.7329, 3210.2480, 4842.3721, 3292.8599, 3793.2527,
        3709.1890, 4306.8047, 6309.0498, 3568.0405, 2578.6765, 3187.8992,
        5836.2720, 4453.4170, 6023.6411, 2359.6602, 7200.5981, 3892.6311,
        3940.7834, 3535.7605, 4638.8745, 4243.2383, 4240.2549, 3701.8960,
        3583.7156, 4875.9585, 4273.7529, 2384.9695, 2972.2048, 2663.2893,
        2939.2988, 4500.4399, 6020.3765, 5222.7124, 5057.3325, 4052.7839,
        4709.0444, 5261.8271, 3766.0081, 4540.4048, 4803.2046, 3941.6636,
        4102.4189, 3113.5669, 6159.8672, 3130.8181, 3745.3044, 3768.9131,
        5194.2778, 3801.1719, 4433.9609, 3971.8623, 5323.8584, 3757.0906,
        2693.5815, 5272.1650, 4112.6367, 4129.8516, 3847.1167, 3138.0552,
        4870.9648, 4145.5713, 3386.0076, 5126.0073, 4386.4077, 4820.6401,
        4296.5923, 2761.9756, 5354.0469, 4213.2490, 3707.2195, 4008.9871,
        3439.3662, 3628.3459, 4056.3889, 6277.1279, 3414.8420, 3687.5493,
        3850.0957, 4166.9360, 5368.7520, 5871.7412, 3172.5747, 5357.7852,
        4952.2021, 5831.2939, 3865.8367, 7050.9854, 3026.4377, 5903.7998,
        3795.3333, 3308.9521, 6015.0459, 4054.7876, 4130.4521, 3810.2942,
        3917.7314, 4464.1831, 4679.2041, 5256.8496, 5817.5176, 3196.9490,
        4438.5034, 5288.7407, 4193.8306, 3623.6431, 4223.6338, 4818.1768,
        3893.7144, 4343.0991, 4558.9458, 2784.2322, 6175.1558, 2464.5100,
        3851.8413, 2638.1838, 4989.5312, 4862.4741, 5380.6113, 3499.3538,
        3039.1975, 5768.4678, 3443.0671, 4128.3833, 7058.5830, 4989.3853,
        2714.1064, 5751.6265, 2828.8608, 4475.7461, 4592.1421, 4544.9053,
        4102.5791, 6013.9897, 3568.5784, 4314.3413, 3262.3560, 3446.0422,
        4211.0923, 4863.4785, 2981.6074, 3940.3887, 5102.7861, 3767.8188,
        3487.1350, 3109.2900, 3181.4919, 6286.7964, 3597.3982, 4220.6475,
        4340.9980, 5233.3184, 7098.6558, 3283.0022, 3336.8040, 4099.8555,
        3889.1453, 3678.4915])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 61.2634, 357.1800,   6.8611,  ...,  25.6439,   0.0000, 383.0622])
[DEBUG] Global concept maps computed with 53 layers.
[INFO] Using device: cuda
Layer: conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0')}}
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0')}}
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0')}}
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0')}}
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}}
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0')}}
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}}
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')}}
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0')}}
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}}
[DEBUG] Global pruning mask: OrderedDict([('conv1', {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1.], device='cuda:0')}}), ('encoder.4.0.conv1', {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 0.], device='cuda:0')}}), ('encoder.4.0.conv2', {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')}}), ('encoder.4.0.conv3', {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1.], device='cuda:0')}}), ('encoder.4.0.downsample.0', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.4.1.conv1', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0')}}), ('encoder.4.1.conv2', {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0')}}), ('encoder.4.1.conv3', {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.4.2.conv1', {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}}), ('encoder.4.2.conv2', {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0')}}), ('encoder.4.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.5.0.conv1', {'Conv2d': {'weight': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1.], device='cuda:0')}}), ('encoder.5.0.conv2', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1.], device='cuda:0')}}), ('encoder.5.0.conv3', {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}}), ('encoder.5.0.downsample.0', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1.], device='cuda:0')}}), ('encoder.5.1.conv1', {'Conv2d': {'weight': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,
        0., 0.], device='cuda:0')}}), ('encoder.5.1.conv2', {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1.], device='cuda:0')}}), ('encoder.5.1.conv3', {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')}}), ('encoder.5.2.conv1', {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0.], device='cuda:0')}}), ('encoder.5.2.conv2', {'Conv2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1.], device='cuda:0')}}), ('encoder.5.2.conv3', {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 1.], device='cuda:0')}}), ('encoder.5.3.conv1', {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0.], device='cuda:0')}}), ('encoder.5.3.conv2', {'Conv2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,
        1., 0.], device='cuda:0')}}), ('encoder.5.3.conv3', {'Conv2d': {'weight': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.0.conv1', {'Conv2d': {'weight': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.0.conv2', {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.0.conv3', {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.0.downsample.0', {'Conv2d': {'weight': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1.,  ..., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.1.conv1', {'Conv2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.1.conv2', {'Conv2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.1.conv3', {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.2.conv1', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.2.conv2', {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.2.conv3', {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}}), ('encoder.6.3.conv1', {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.3.conv2', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.3.conv3', {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}}), ('encoder.6.4.conv1', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.4.conv2', {'Conv2d': {'weight': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.4.conv3', {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 1., 0.], device='cuda:0')}}), ('encoder.6.5.conv1', {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        0., 1., 1., 1.], device='cuda:0')}}), ('encoder.6.5.conv2', {'Conv2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 1.], device='cuda:0')}}), ('encoder.6.5.conv3', {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0')}}), ('encoder.7.0.conv1', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')}}), ('encoder.7.0.conv2', {'Conv2d': {'weight': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0'), 'bias': tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,
        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 1., 1., 0.], device='cuda:0')}}), ('encoder.7.0.conv3', {'Conv2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}}), ('encoder.7.0.downsample.0', {'Conv2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}}), ('encoder.7.1.conv1', {'Conv2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1.], device='cuda:0')}}), ('encoder.7.1.conv2', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1.], device='cuda:0')}}), ('encoder.7.1.conv3', {'Conv2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 1.,  ..., 0., 0., 1.], device='cuda:0')}}), ('encoder.7.2.conv1', {'Conv2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,
        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')}}), ('encoder.7.2.conv2', {'Conv2d': {'weight': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 1.], device='cuda:0')}}), ('encoder.7.2.conv3', {'Conv2d': {'weight': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([1., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}})])
Full profiling results saved to /home/paul/projects/CV4RS-main/pruning_callgraph.txt
=== Round 2/4 ===
Applying pruning mask for Round 2...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
Training and communication for Round 2...
Epoch 1/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8616.0, Mean: 1069.2637939453125, Std: 1088.90576171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 79896.1171875, Mean: 9914.49609375, Std: 10097.5283203125
[DEBUG] Loss for batch 0: 0.30464928648564776
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13416.0, Mean: 1003.4780883789062, Std: 1074.1920166015625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 124406.9921875, Mean: 9304.458984375, Std: 9961.0869140625
[DEBUG] Loss for batch 1: 0.2322406034235892
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7652.0, Mean: 1091.160888671875, Std: 1102.3341064453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70956.8515625, Mean: 10117.55078125, Std: 10222.0517578125
[DEBUG] Loss for batch 2: 0.2711701088820266
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1012.5390014648438, Std: 1079.4674072265625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9388.4814453125, Std: 10010.005859375
[DEBUG] Loss for batch 3: 0.19101848670988816
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15888.0, Mean: 1036.5062255859375, Std: 1075.257568359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 147330.078125, Mean: 9610.7314453125, Std: 9970.9677734375
[DEBUG] Loss for batch 4: 0.25142150799064206
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8880.0, Mean: 1091.123291015625, Std: 1097.8033447265625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 82344.21875, Mean: 10117.203125, Std: 10180.037109375
[DEBUG] Loss for batch 5: 0.216096360685119
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9968.0, Mean: 1061.79638671875, Std: 1103.0633544921875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 92433.3515625, Mean: 9845.2490234375, Std: 10228.814453125
[DEBUG] Loss for batch 6: 0.1827487149993186
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15440.0, Mean: 1001.7598876953125, Std: 1076.7178955078125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 143175.734375, Mean: 9288.525390625, Std: 9984.509765625
[DEBUG] Loss for batch 7: 0.19386747334677112
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7764.0, Mean: 996.8436889648438, Std: 1064.6241455078125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 71995.4375, Mean: 9242.9365234375, Std: 9872.36328125
[DEBUG] Loss for batch 8: 0.14963711000138555
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7368.0, Mean: 1041.3382568359375, Std: 1059.732421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68323.2890625, Mean: 9655.5400390625, Std: 9827.001953125
[DEBUG] Loss for batch 9: 0.16932609838605164
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10536.0, Mean: 1127.5174560546875, Std: 1090.2052001953125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 97700.46875, Mean: 10454.689453125, Std: 10109.578125
[DEBUG] Loss for batch 10: 0.18230787767188722
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14640.0, Mean: 1069.7108154296875, Std: 1082.2470703125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 135757.265625, Mean: 9918.642578125, Std: 10035.7822265625
[DEBUG] Loss for batch 11: 0.17181341457991509
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 1088.5213623046875, Std: 1093.8553466796875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 10093.0732421875, Std: 10143.427734375
[DEBUG] Loss for batch 12: 0.1707304593417005
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7368.0, Mean: 1091.1622314453125, Std: 1061.01904296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68323.2890625, Mean: 10117.5615234375, Std: 9838.93359375
[DEBUG] Loss for batch 13: 0.16426292651866897
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10880.0, Mean: 1159.7877197265625, Std: 1094.8125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 100890.4140625, Mean: 10753.93359375, Std: 10152.3037109375
[DEBUG] Loss for batch 14: 0.15821453032684996
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10408.0, Mean: 1055.7017822265625, Std: 1077.84912109375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96513.5078125, Mean: 9788.734375, Std: 9995.0
[DEBUG] Loss for batch 15: 0.16089141766085382
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10232.0, Mean: 1119.8233642578125, Std: 1109.4361572265625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 94881.4453125, Mean: 10383.3408203125, Std: 10287.9091796875
[DEBUG] Loss for batch 16: 0.18525384312659274
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8180.0, Mean: 1018.8137817382812, Std: 1071.9832763671875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 75853.046875, Mean: 9446.66796875, Std: 9940.60546875
[DEBUG] Loss for batch 17: 0.14819759220965037
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13112.0, Mean: 1080.77001953125, Std: 1080.7640380859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 121587.96875, Mean: 10021.193359375, Std: 10022.0302734375
[DEBUG] Loss for batch 18: 0.1493745334240918
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15976.0, Mean: 1069.81884765625, Std: 1060.431396484375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148146.125, Mean: 9919.64453125, Std: 9833.484375
[DEBUG] Loss for batch 19: 0.1658428576577424
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1043.347412109375, Std: 1079.6361083984375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 9674.1708984375, Std: 10011.5703125
[DEBUG] Loss for batch 20: 0.15952988219419537
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9368.0, Mean: 1059.1416015625, Std: 1071.3709716796875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 86869.4921875, Mean: 9820.62890625, Std: 9934.9267578125
[DEBUG] Loss for batch 21: 0.14835564316414238
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7812.0, Mean: 1026.267333984375, Std: 1091.8173828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72440.546875, Mean: 9515.78515625, Std: 10124.529296875
[DEBUG] Loss for batch 22: 0.17209910965072148
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8032.0, Mean: 1041.2735595703125, Std: 1067.77392578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74480.625, Mean: 9654.939453125, Std: 9901.5712890625
[DEBUG] Loss for batch 23: 0.16864317001151624
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1011.0671997070312, Std: 1067.7744140625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9374.833984375, Std: 9901.576171875
[DEBUG] Loss for batch 24: 0.16770741785254167
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8520.0, Mean: 1058.9635009765625, Std: 1109.2357177734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 79005.90625, Mean: 9818.9814453125, Std: 10286.05078125
[DEBUG] Loss for batch 25: 0.16232792028430545
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10568.0, Mean: 1058.681396484375, Std: 1087.81640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 97997.203125, Mean: 9816.36328125, Std: 10087.4267578125
[DEBUG] Loss for batch 26: 0.16811112000031794
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16032.0, Mean: 1079.5595703125, Std: 1096.735595703125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148665.40625, Mean: 10009.96875, Std: 10170.1357421875
[DEBUG] Loss for batch 27: 0.17272031858329295
[DEBUG] Batch 28: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15000.0, Mean: 1094.712890625, Std: 1098.5181884765625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 139095.578125, Mean: 10150.48828125, Std: 10186.6650390625
[DEBUG] Loss for batch 28: 0.15069765365536855
[DEBUG] Batch 29: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17136.0, Mean: 1061.37939453125, Std: 1088.14453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 158902.90625, Mean: 9841.3837890625, Std: 10090.4697265625
[DEBUG] Loss for batch 29: 0.1483543156983327
[DEBUG] Batch 30: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14242.0, Mean: 1064.14404296875, Std: 1079.928955078125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 132066.5625, Mean: 9867.0205078125, Std: 10014.2861328125
[DEBUG] Loss for batch 30: 0.16000960474565373
[DEBUG] Batch 31: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16032.0, Mean: 1010.3033447265625, Std: 1079.5740966796875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148665.40625, Mean: 9367.7509765625, Std: 10010.99609375
[DEBUG] Loss for batch 31: 0.13802992517747625
[DEBUG] Batch 32: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7476.0, Mean: 978.6024169921875, Std: 1061.266845703125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69324.78125, Mean: 9073.783203125, Std: 9841.23046875
[DEBUG] Loss for batch 32: 0.12905138527391363
[DEBUG] Batch 33: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1054.7537841796875, Std: 1098.5072021484375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9779.943359375, Std: 10186.564453125
[DEBUG] Loss for batch 33: 0.1546326559801612
[DEBUG] Batch 34: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16608.0, Mean: 1110.6259765625, Std: 1066.04736328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 154006.71875, Mean: 10298.0537109375, Std: 9885.560546875
[DEBUG] Loss for batch 34: 0.15532199561273444
[DEBUG] Batch 35: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10448.0, Mean: 1064.046142578125, Std: 1084.617431640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96884.4375, Mean: 9866.111328125, Std: 10057.7626953125
[DEBUG] Loss for batch 35: 0.16062545840041306
[DEBUG] Batch 36: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1054.5380859375, Std: 1071.078857421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 9777.943359375, Std: 9932.2177734375
[DEBUG] Loss for batch 36: 0.15909981695782782
[DEBUG] Batch 37: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 1049.4984130859375, Std: 1085.052734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 9731.2109375, Std: 10061.7998046875
[DEBUG] Loss for batch 37: 0.14839021807776281
[DEBUG] Batch 38: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1057.9521484375, Std: 1090.7982177734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9809.6015625, Std: 10115.078125
[DEBUG] Loss for batch 38: 0.14519136432833
[DEBUG] Batch 39: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7240.0, Mean: 1040.267333984375, Std: 1090.068359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67136.3359375, Mean: 9645.6083984375, Std: 10108.3095703125
[DEBUG] Loss for batch 39: 0.13613464041767895
[DEBUG] Batch 40: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7480.0, Mean: 1016.56201171875, Std: 1095.8497314453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69361.875, Mean: 9425.787109375, Std: 10161.9208984375
[DEBUG] Loss for batch 40: 0.14412195342679088
[DEBUG] Batch 41: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1033.297607421875, Std: 1090.025390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9580.978515625, Std: 10107.9111328125
[DEBUG] Loss for batch 41: 0.13634741834457184
[DEBUG] Batch 42: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12245.0, Mean: 995.3208618164062, Std: 1070.689697265625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 113548.1875, Mean: 9228.81640625, Std: 9928.6103515625
[DEBUG] Loss for batch 42: 0.12970875541662163
[DEBUG] Batch 43: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7788.0, Mean: 1082.1800537109375, Std: 1079.6827392578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72217.9921875, Mean: 10034.2685546875, Std: 10012.0029296875
[DEBUG] Loss for batch 43: 0.1467419747804115
[DEBUG] Batch 44: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8068.0, Mean: 1045.953857421875, Std: 1090.445556640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74814.4609375, Mean: 9698.3427734375, Std: 10111.8076171875
[DEBUG] Loss for batch 44: 0.13614654850686603
[DEBUG] Batch 45: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7504.0, Mean: 1027.284912109375, Std: 1077.54248046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69584.4296875, Mean: 9525.2216796875, Std: 9992.1572265625
[DEBUG] Loss for batch 45: 0.13799367680520677
[DEBUG] Batch 46: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1024.138427734375, Std: 1078.625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9496.044921875, Std: 10002.1943359375
[DEBUG] Loss for batch 46: 0.13736088157924387
[DEBUG] Batch 47: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8048.0, Mean: 1041.3797607421875, Std: 1094.4403076171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74629.0, Mean: 9655.923828125, Std: 10148.8515625
[DEBUG] Loss for batch 47: 0.1329007791949221
[DEBUG] Batch 48: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7832.0, Mean: 1069.6087646484375, Std: 1097.26416015625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72626.0078125, Mean: 9917.6953125, Std: 10175.037109375
[DEBUG] Loss for batch 48: 0.1468202669278005
[DEBUG] Batch 49: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10312.0, Mean: 1121.713623046875, Std: 1100.014892578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 95623.296875, Mean: 10400.8681640625, Std: 10200.5458984375
[DEBUG] Loss for batch 49: 0.1458911911157431
[DEBUG] Batch 50: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17376.0, Mean: 1032.3837890625, Std: 1075.8406982421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161128.453125, Mean: 9572.50390625, Std: 9976.3759765625
[DEBUG] Loss for batch 50: 0.16235253730364363
[DEBUG] Batch 51: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10928.0, Mean: 1052.0538330078125, Std: 1069.537353515625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 101335.5234375, Mean: 9754.90625, Std: 9917.923828125
[DEBUG] Loss for batch 51: 0.14781464717094941
[DEBUG] Batch 52: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7768.0, Mean: 1015.0260009765625, Std: 1069.707275390625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 72032.53125, Mean: 9411.5439453125, Std: 9919.5
[DEBUG] Loss for batch 52: 0.1419514355668325
[DEBUG] Batch 53: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7964.0, Mean: 1070.964599609375, Std: 1088.7135009765625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 73850.0546875, Mean: 9930.267578125, Std: 10095.74609375
[DEBUG] Loss for batch 53: 0.14171976423562632
[DEBUG] Batch 54: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8280.0, Mean: 1050.7696533203125, Std: 1052.0323486328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 76780.359375, Mean: 9742.9970703125, Std: 9755.5986328125
[DEBUG] Loss for batch 54: 0.15788554773393426
[DEBUG] Batch 55: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8032.0, Mean: 1021.2383422851562, Std: 1091.3873291015625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74480.625, Mean: 9469.1513671875, Std: 10120.5400390625
[DEBUG] Loss for batch 55: 0.14129783810371438
[DEBUG] Batch 56: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7396.0, Mean: 1031.876953125, Std: 1069.58349609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68582.9375, Mean: 9567.8037109375, Std: 9918.3515625
[DEBUG] Loss for batch 56: 0.15153073144646992
[DEBUG] Batch 57: Data shape: torch.Size([213, 10, 120, 120]), Labels shape: torch.Size([213, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12040.0, Mean: 1036.313720703125, Std: 1069.9886474609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 111647.203125, Mean: 9608.9453125, Std: 9922.1083984375
[DEBUG] Loss for batch 57: 0.14023565880693706
[INFO] Training epoch completed successfully.
Epoch 2/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13416.0, Mean: 1084.658447265625, Std: 1083.7796630859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 124406.9921875, Mean: 10057.2529296875, Std: 10049.994140625
[DEBUG] Loss for batch 0: 0.14752158910650487
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7580.0, Mean: 1052.0701904296875, Std: 1085.32177734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70289.1875, Mean: 9755.0576171875, Std: 10064.294921875
[DEBUG] Loss for batch 1: 0.13150167091025888
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10256.0, Mean: 1030.403564453125, Std: 1061.2589111328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 95104.0, Mean: 9554.142578125, Std: 9841.1572265625
[DEBUG] Loss for batch 2: 0.12234494227405583
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16608.0, Mean: 1030.7877197265625, Std: 1066.4029541015625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 154006.71875, Mean: 9557.703125, Std: 9888.8583984375
[DEBUG] Loss for batch 3: 0.13661975226642953
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7396.0, Mean: 1074.15087890625, Std: 1058.091064453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68582.9375, Mean: 9959.8154296875, Std: 9811.78125
[DEBUG] Loss for batch 4: 0.135229020096571
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11552.0, Mean: 993.9566650390625, Std: 1081.1558837890625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 107121.9375, Mean: 9216.1650390625, Std: 10025.6630859375
[DEBUG] Loss for batch 5: 0.13424070095071197
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7684.0, Mean: 1064.307373046875, Std: 1076.2481689453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 71253.5859375, Mean: 9868.53515625, Std: 9980.1533203125
[DEBUG] Loss for batch 6: 0.1330500162149998
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14640.0, Mean: 1076.48486328125, Std: 1088.5223388671875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 135757.265625, Mean: 9981.458984375, Std: 10093.9736328125
[DEBUG] Loss for batch 7: 0.1360907721388242
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9728.0, Mean: 1044.8006591796875, Std: 1102.8748779296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 90207.8046875, Mean: 9687.6474609375, Std: 10227.0654296875
[DEBUG] Loss for batch 8: 0.14226493405460047
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7964.0, Mean: 1031.123779296875, Std: 1106.23095703125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 73850.0546875, Mean: 9560.8212890625, Std: 10258.1865234375
[DEBUG] Loss for batch 9: 0.13960848477881882
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7228.0, Mean: 1065.799072265625, Std: 1073.339599609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 67025.0546875, Mean: 9882.3662109375, Std: 9953.1826171875
[DEBUG] Loss for batch 10: 0.12701424617021254
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14456.0, Mean: 1059.5933837890625, Std: 1088.7860107421875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 134051.015625, Mean: 9824.8212890625, Std: 10096.4189453125
[DEBUG] Loss for batch 11: 0.12532506520792713
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1085.653076171875, Std: 1076.0291748046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 10066.474609375, Std: 9978.1240234375
[DEBUG] Loss for batch 12: 0.13480411508554593
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9496.0, Mean: 1056.9375, Std: 1081.784912109375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 88056.4453125, Mean: 9800.1923828125, Std: 10031.49609375
[DEBUG] Loss for batch 13: 0.13153160251033344
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7548.0, Mean: 1076.6199951171875, Std: 1080.96142578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69992.4453125, Mean: 9982.7109375, Std: 10023.8603515625
[DEBUG] Loss for batch 14: 0.1204999790785455
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1041.9150390625, Std: 1090.119140625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9660.888671875, Std: 10108.78125
[DEBUG] Loss for batch 15: 0.13941112231364053
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8015.0, Mean: 1102.747802734375, Std: 1086.330810546875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 74322.984375, Mean: 10224.9970703125, Std: 10073.6513671875
[DEBUG] Loss for batch 16: 0.12980980662528135
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10408.0, Mean: 1060.014892578125, Std: 1101.46240234375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 96513.5078125, Mean: 9828.732421875, Std: 10213.9677734375
[DEBUG] Loss for batch 17: 0.12764474009747617
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8580.0, Mean: 1033.89404296875, Std: 1071.3355712890625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 79562.2890625, Mean: 9586.5078125, Std: 9934.5986328125
[DEBUG] Loss for batch 18: 0.11665646773093616
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9088.0, Mean: 1048.823974609375, Std: 1075.9464111328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 84273.0234375, Mean: 9724.9560546875, Std: 9977.35546875
[DEBUG] Loss for batch 19: 0.12361063093084453
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1113.35546875, Std: 1099.2327880859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 10323.3623046875, Std: 10193.29296875
[DEBUG] Loss for batch 20: 0.12751880348444258
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14242.0, Mean: 1073.6689453125, Std: 1080.890869140625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 132066.5625, Mean: 9955.345703125, Std: 10023.2060546875
[DEBUG] Loss for batch 21: 0.13788946728972665
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15440.0, Mean: 1067.2169189453125, Std: 1105.873046875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 143175.734375, Mean: 9895.515625, Std: 10254.8681640625
[DEBUG] Loss for batch 22: 0.12658698794112247
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 970.16357421875, Std: 1059.4058837890625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 8995.529296875, Std: 9823.9736328125
[DEBUG] Loss for batch 23: 0.13143663989551033
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1072.0604248046875, Std: 1112.3919677734375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161870.296875, Mean: 9940.4306640625, Std: 10315.3193359375
[DEBUG] Loss for batch 24: 0.15197618733856916
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8440.0, Mean: 1054.032958984375, Std: 1085.4296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 78264.0546875, Mean: 9773.259765625, Std: 10065.294921875
[DEBUG] Loss for batch 25: 0.13412611215759498
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7536.0, Mean: 1058.755615234375, Std: 1084.3858642578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69881.171875, Mean: 9817.0537109375, Std: 10055.6162109375
[DEBUG] Loss for batch 26: 0.12727461075663946
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1075.973876953125, Std: 1091.9583740234375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9976.7177734375, Std: 10125.8369140625
[DEBUG] Loss for batch 27: 0.12739711754621139
[DEBUG] Batch 28: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7764.0, Mean: 1006.591552734375, Std: 1073.4451904296875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 71995.4375, Mean: 9333.3310546875, Std: 9954.162109375
[DEBUG] Loss for batch 28: 0.11601721549166757
[DEBUG] Batch 29: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15976.0, Mean: 1059.1142578125, Std: 1078.7071533203125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148146.125, Mean: 9820.3779296875, Std: 10002.9560546875
[DEBUG] Loss for batch 29: 0.13958920004774683
[DEBUG] Batch 30: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7612.0, Mean: 1035.292236328125, Std: 1069.0731201171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70585.9296875, Mean: 9599.474609375, Std: 9913.619140625
[DEBUG] Loss for batch 30: 0.12283587223159449
[DEBUG] Batch 31: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7580.0, Mean: 1025.9989013671875, Std: 1079.1495361328125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70289.1875, Mean: 9513.296875, Std: 10007.05859375
[DEBUG] Loss for batch 31: 0.11548759083182149
[DEBUG] Batch 32: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13112.0, Mean: 1076.7716064453125, Std: 1085.706298828125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 121587.96875, Mean: 9984.1162109375, Std: 10067.8603515625
[DEBUG] Loss for batch 32: 0.11729980219426786
[DEBUG] Batch 33: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10928.0, Mean: 1010.7432250976562, Std: 1050.7484130859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 101335.5234375, Mean: 9371.8291015625, Std: 9743.6923828125
[DEBUG] Loss for batch 33: 0.1347490038685672
[DEBUG] Batch 34: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16032.0, Mean: 1054.076416015625, Std: 1095.8883056640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148665.40625, Mean: 9773.6630859375, Std: 10162.2783203125
[DEBUG] Loss for batch 34: 0.11886890437908298
[DEBUG] Batch 35: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10568.0, Mean: 1074.79150390625, Std: 1095.6673583984375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 97997.203125, Mean: 9965.7548828125, Std: 10160.2294921875
[DEBUG] Loss for batch 35: 0.12170520826334633
[DEBUG] Batch 36: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9968.0, Mean: 1059.2193603515625, Std: 1073.132568359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 92433.3515625, Mean: 9821.3525390625, Std: 9951.2626953125
[DEBUG] Loss for batch 36: 0.11456734373138369
[DEBUG] Batch 37: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7484.0, Mean: 1014.6900024414062, Std: 1075.8609619140625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 69398.96875, Mean: 9408.4287109375, Std: 9976.5634765625
[DEBUG] Loss for batch 37: 0.12204263109057063
[DEBUG] Batch 38: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8216.0, Mean: 1077.350830078125, Std: 1074.794921875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 76186.8828125, Mean: 9989.4892578125, Std: 9966.677734375
[DEBUG] Loss for batch 38: 0.13685476266904179
[DEBUG] Batch 39: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7924.0, Mean: 1071.4012451171875, Std: 1083.871826171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 73479.1328125, Mean: 9934.3154296875, Std: 10050.8486328125
[DEBUG] Loss for batch 39: 0.12581727329256304
[DEBUG] Batch 40: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7342.0, Mean: 1052.401611328125, Std: 1076.1505126953125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68082.1875, Mean: 9758.1298828125, Std: 9979.2490234375
[DEBUG] Loss for batch 40: 0.11853815321387147
[DEBUG] Batch 41: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 9432.0, Mean: 1030.7823486328125, Std: 1081.0350341796875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 87462.96875, Mean: 9557.6533203125, Std: 10024.54296875
[DEBUG] Loss for batch 41: 0.1275820466708988
[DEBUG] Batch 42: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12816.0, Mean: 1062.43603515625, Std: 1090.3145751953125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 118843.1328125, Mean: 9851.1826171875, Std: 10110.5927734375
[DEBUG] Loss for batch 42: 0.12456660248807193
[DEBUG] Batch 43: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12040.0, Mean: 1051.5518798828125, Std: 1104.59814453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 111647.203125, Mean: 9750.2529296875, Std: 10243.0458984375
[DEBUG] Loss for batch 43: 0.11367803001156598
[DEBUG] Batch 44: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17504.0, Mean: 1062.082275390625, Std: 1068.1038818359375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162315.40625, Mean: 9847.900390625, Std: 9904.630859375
[DEBUG] Loss for batch 44: 0.12619358755674637
[DEBUG] Batch 45: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7632.0, Mean: 1107.745849609375, Std: 1116.4814453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 70771.390625, Mean: 10271.34375, Std: 10353.2412109375
[DEBUG] Loss for batch 45: 0.12573945944049358
[DEBUG] Batch 46: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 12245.0, Mean: 993.7787475585938, Std: 1065.390380859375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 113548.1875, Mean: 9214.5166015625, Std: 9879.46875
[DEBUG] Loss for batch 46: 0.11340583808456804
[DEBUG] Batch 47: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 7392.0, Mean: 1079.1468505859375, Std: 1087.5528564453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 68545.84375, Mean: 10006.1435546875, Std: 10084.9833984375
[DEBUG] Loss for batch 47: 0.12412097486733528
[DEBUG] Batch 48: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17136.0, Mean: 1033.0626220703125, Std: 1088.5394287109375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 158902.90625, Mean: 9578.80078125, Std: 10094.1318359375
[DEBUG] Loss for batch 48: 0.12553450440268246
[DEBUG] Batch 49: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1084.146484375, Std: 1049.1689453125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 10052.5048828125, Std: 9729.044921875
[DEBUG] Loss for batch 49: 0.14524826288309253
[DEBUG] Batch 50: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17536.0, Mean: 1019.9722290039062, Std: 1067.701171875
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162612.15625, Mean: 9457.4111328125, Std: 9900.896484375
[DEBUG] Loss for batch 50: 0.11953834955741133
[DEBUG] Batch 51: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15000.0, Mean: 1049.081787109375, Std: 1074.4306640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 139095.578125, Mean: 9727.3466796875, Std: 9963.30078125
[DEBUG] Loss for batch 51: 0.12742560658790772
[DEBUG] Batch 52: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17376.0, Mean: 1033.6500244140625, Std: 1080.806884765625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 161128.453125, Mean: 9584.24609375, Std: 10022.427734375
[DEBUG] Loss for batch 52: 0.1301938872154352
[DEBUG] Batch 53: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16032.0, Mean: 1057.825927734375, Std: 1067.327392578125
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 148665.40625, Mean: 9808.4306640625, Std: 9897.4306640625
[DEBUG] Loss for batch 53: 0.12717058878506723
[DEBUG] Batch 54: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10880.0, Mean: 1018.7340087890625, Std: 1089.943603515625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 100890.4140625, Mean: 9445.9267578125, Std: 10107.15234375
[DEBUG] Loss for batch 54: 0.11645347887120897
[DEBUG] Batch 55: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8208.0, Mean: 1078.2275390625, Std: 1111.9739990234375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 76112.6953125, Mean: 9997.6181640625, Std: 10311.443359375
[DEBUG] Loss for batch 55: 0.12601804088791801
[DEBUG] Batch 56: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17488.0, Mean: 1034.5596923828125, Std: 1095.64306640625
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 162167.046875, Mean: 9592.6806640625, Std: 10160.0048828125
[DEBUG] Loss for batch 56: 0.1337700284149026
[DEBUG] Batch 57: Data shape: torch.Size([213, 10, 120, 120]), Labels shape: torch.Size([213, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15045.0, Mean: 1064.097900390625, Std: 1078.1683349609375
[DEBUG] Mean used for standardization: 0.09610633552074432, Std used for standardization: 0.10783882439136505
[DEBUG] Data after standardization: Min: 8.3818941116333, Max: 139512.859375, Mean: 9866.591796875, Std: 9997.9599609375
[DEBUG] Loss for batch 57: 0.12229243330598755
[INFO] Training epoch completed successfully.
Epoch 1/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18448.0, Mean: 1438.50439453125, Std: 1673.634765625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157727.078125, Mean: 12298.22265625, Std: 14309.3486328125
[DEBUG] Loss for batch 0: 0.4751989832978947
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15187.0, Mean: 1459.350830078125, Std: 1675.0283203125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 129845.9765625, Mean: 12476.4560546875, Std: 14321.2626953125
[DEBUG] Loss for batch 1: 0.2876935948721008
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8915.0, Mean: 1360.6983642578125, Std: 1615.5716552734375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 76221.234375, Mean: 11632.9912109375, Std: 13812.9169921875
[DEBUG] Loss for batch 2: 0.23641839079263183
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8124.0, Mean: 1434.4044189453125, Std: 1645.8924560546875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 69458.296875, Mean: 12263.1669921875, Std: 14072.15625
[DEBUG] Loss for batch 3: 0.18492595148316596
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18368.0, Mean: 1494.850830078125, Std: 1665.2642822265625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157043.09375, Mean: 12779.9765625, Std: 14237.7822265625
[DEBUG] Loss for batch 4: 0.18626340463500476
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20624.0, Mean: 1408.14453125, Std: 1641.1776123046875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176331.578125, Mean: 12038.6484375, Std: 14031.8447265625
[DEBUG] Loss for batch 5: 0.17649090888814825
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19088.0, Mean: 1452.061279296875, Std: 1660.302001953125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 163198.984375, Mean: 12414.130859375, Std: 14195.35546875
[DEBUG] Loss for batch 6: 0.1708953018562928
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15324.0, Mean: 1394.9691162109375, Std: 1630.1591796875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 131017.3125, Mean: 11926.0009765625, Std: 13937.6376953125
[DEBUG] Loss for batch 7: 0.17475528888349226
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20672.0, Mean: 1480.8883056640625, Std: 1656.639892578125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176741.96875, Mean: 12660.5986328125, Std: 14164.0439453125
[DEBUG] Loss for batch 8: 0.17228252506451408
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18416.0, Mean: 1404.956298828125, Std: 1641.1519775390625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157453.484375, Mean: 12011.390625, Std: 14031.625
[DEBUG] Loss for batch 9: 0.16771894791883568
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13928.0, Mean: 1405.7508544921875, Std: 1635.6600341796875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 119081.703125, Mean: 12018.18359375, Std: 13984.669921875
[DEBUG] Loss for batch 10: 0.14265280903646116
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19056.0, Mean: 1427.4500732421875, Std: 1649.9591064453125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162925.390625, Mean: 12203.708984375, Std: 14106.92578125
[DEBUG] Loss for batch 11: 0.16208514419887493
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18720.0, Mean: 1379.6114501953125, Std: 1623.697265625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 160052.640625, Mean: 11794.6962890625, Std: 13882.3896484375
[DEBUG] Loss for batch 12: 0.13761759225682405
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 1403.041748046875, Std: 1637.882080078125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176057.984375, Mean: 11995.0205078125, Std: 14003.66796875
[DEBUG] Loss for batch 13: 0.15600577411850258
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1466.1610107421875, Std: 1651.864013671875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 12534.681640625, Std: 14123.2119140625
[DEBUG] Loss for batch 14: 0.15949702476948835
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1418.5599365234375, Std: 1647.383056640625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 12127.6982421875, Std: 14084.900390625
[DEBUG] Loss for batch 15: 0.17192350205516999
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18512.0, Mean: 1368.6181640625, Std: 1608.6826171875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 158274.265625, Mean: 11700.7060546875, Std: 13754.0166015625
[DEBUG] Loss for batch 16: 0.14170312894075876
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15116.0, Mean: 1529.33642578125, Std: 1684.6168212890625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 129238.9375, Mean: 13074.8232421875, Std: 14403.2431640625
[DEBUG] Loss for batch 17: 0.15427471371890167
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20544.0, Mean: 1509.7037353515625, Std: 1654.0478515625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 175647.59375, Mean: 12906.9677734375, Std: 14141.8837890625
[DEBUG] Loss for batch 18: 0.14915993385903728
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1388.7606201171875, Std: 1640.0396728515625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 11872.919921875, Std: 14022.115234375
[DEBUG] Loss for batch 19: 0.1338486496581227
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18288.0, Mean: 1373.736572265625, Std: 1608.8031005859375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156359.09375, Mean: 11744.466796875, Std: 13755.046875
[DEBUG] Loss for batch 20: 0.17670396540768873
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1484.866455078125, Std: 1653.6898193359375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 12694.6123046875, Std: 14138.8212890625
[DEBUG] Loss for batch 21: 0.1432121395410835
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18384.0, Mean: 1545.20263671875, Std: 1671.4578857421875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157179.890625, Mean: 13210.4775390625, Std: 14290.736328125
[DEBUG] Loss for batch 22: 0.18417551559782003
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20640.0, Mean: 1423.843505859375, Std: 1639.118408203125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176468.375, Mean: 12172.873046875, Std: 14014.23828125
[DEBUG] Loss for batch 23: 0.1402839783622331
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 1356.2218017578125, Std: 1619.1229248046875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176057.984375, Mean: 11594.7177734375, Std: 13843.2802734375
[DEBUG] Loss for batch 24: 0.14075905464612737
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16448.0, Mean: 1341.9512939453125, Std: 1616.9794921875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 140627.34375, Mean: 11472.70703125, Std: 13824.953125
[DEBUG] Loss for batch 25: 0.1261385651792826
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11814.0, Mean: 1378.86181640625, Std: 1601.8297119140625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 101007.2890625, Mean: 11788.2861328125, Std: 13695.4248046875
[DEBUG] Loss for batch 26: 0.13869680611870433
[DEBUG] Batch 27: Data shape: torch.Size([238, 10, 120, 120]), Labels shape: torch.Size([238, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20608.0, Mean: 1490.2786865234375, Std: 1637.6121826171875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176194.78125, Mean: 12740.8857421875, Std: 14001.3603515625
[DEBUG] Loss for batch 27: 0.14327019093401971
[INFO] Training epoch completed successfully.
Epoch 2/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20624.0, Mean: 1453.8299560546875, Std: 1628.1279296875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176331.578125, Mean: 12429.2529296875, Std: 13920.271484375
[DEBUG] Loss for batch 0: 0.1370278045979583
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17280.0, Mean: 1450.0726318359375, Std: 1660.8045654296875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 147740.828125, Mean: 12397.1298828125, Std: 14199.65234375
[DEBUG] Loss for batch 1: 0.14068665316983617
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15140.0, Mean: 1385.5574951171875, Std: 1646.10302734375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 129444.1328125, Mean: 11845.533203125, Std: 14073.9560546875
[DEBUG] Loss for batch 2: 0.12725570373721357
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19088.0, Mean: 1471.3399658203125, Std: 1657.253662109375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 163198.984375, Mean: 12578.9619140625, Std: 14169.2919921875
[DEBUG] Loss for batch 3: 0.1417826901263094
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18368.0, Mean: 1365.9910888671875, Std: 1625.6126708984375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157043.09375, Mean: 11678.2421875, Std: 13898.765625
[DEBUG] Loss for batch 4: 0.11637413027159636
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1574.676513671875, Std: 1673.456298828125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 13462.4765625, Std: 14307.822265625
[DEBUG] Loss for batch 5: 0.15177403851955903
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20640.0, Mean: 1553.5697021484375, Std: 1689.059814453125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176468.375, Mean: 13282.013671875, Std: 14441.23046875
[DEBUG] Loss for batch 6: 0.12400532844146507
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20608.0, Mean: 1387.027099609375, Std: 1632.19921875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176194.78125, Mean: 11858.095703125, Std: 13955.0810546875
[DEBUG] Loss for batch 7: 0.11894098046328404
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19056.0, Mean: 1377.1717529296875, Std: 1615.5716552734375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162925.390625, Mean: 11773.8369140625, Std: 13812.9169921875
[DEBUG] Loss for batch 8: 0.15845096129930095
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1402.0611572265625, Std: 1629.00732421875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 11986.63671875, Std: 13927.7900390625
[DEBUG] Loss for batch 9: 0.11433182577835552
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15411.0, Mean: 1360.7943115234375, Std: 1629.8787841796875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 131761.15625, Mean: 11633.8115234375, Std: 13935.2412109375
[DEBUG] Loss for batch 10: 0.1295650015918
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20544.0, Mean: 1345.90673828125, Std: 1606.6324462890625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 175647.59375, Mean: 11506.5244140625, Std: 13736.48828125
[DEBUG] Loss for batch 11: 0.12140571868952318
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20592.0, Mean: 1480.2120361328125, Std: 1669.125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176057.984375, Mean: 12654.8173828125, Std: 14270.791015625
[DEBUG] Loss for batch 12: 0.14256823679738198
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17888.0, Mean: 1377.01318359375, Std: 1619.058349609375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 152939.15625, Mean: 11772.482421875, Std: 13842.7275390625
[DEBUG] Loss for batch 13: 0.13558976755576221
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13248.0, Mean: 1417.77099609375, Std: 1636.3646240234375
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 113267.796875, Mean: 12120.9541015625, Std: 13990.693359375
[DEBUG] Loss for batch 14: 0.1371339543761791
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20688.0, Mean: 1461.9090576171875, Std: 1670.6539306640625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176878.765625, Mean: 12498.328125, Std: 14283.8623046875
[DEBUG] Loss for batch 15: 0.14342569181306516
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18720.0, Mean: 1437.5057373046875, Std: 1623.9334716796875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 160052.640625, Mean: 12289.6845703125, Std: 13884.41015625
[DEBUG] Loss for batch 16: 0.14579358655132782
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 13928.0, Mean: 1498.5223388671875, Std: 1646.3458251953125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 119081.703125, Mean: 12811.3681640625, Std: 14076.03125
[DEBUG] Loss for batch 17: 0.14430738699865717
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17888.0, Mean: 1424.1439208984375, Std: 1650.338134765625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 152939.15625, Mean: 12175.44140625, Std: 14110.1650390625
[DEBUG] Loss for batch 18: 0.1293248913834654
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17824.0, Mean: 1486.780517578125, Std: 1648.6553955078125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 152391.953125, Mean: 12710.9775390625, Std: 14095.7783203125
[DEBUG] Loss for batch 19: 0.1492576117971748
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16448.0, Mean: 1388.881591796875, Std: 1647.3873291015625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 140627.34375, Mean: 11873.955078125, Std: 14084.9375
[DEBUG] Loss for batch 20: 0.1359505338625046
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18416.0, Mean: 1343.8521728515625, Std: 1621.7369384765625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157453.484375, Mean: 11488.958984375, Std: 13865.6298828125
[DEBUG] Loss for batch 21: 0.11568934720008368
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 20672.0, Mean: 1514.19091796875, Std: 1665.4041748046875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 176741.96875, Mean: 12945.330078125, Std: 14238.978515625
[DEBUG] Loss for batch 22: 0.1579844933547373
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 19056.0, Mean: 1395.41796875, Std: 1632.37060546875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 162925.390625, Mean: 11929.83984375, Std: 13956.5458984375
[DEBUG] Loss for batch 23: 0.1211432371198115
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14664.0, Mean: 1406.005615234375, Std: 1637.9560546875
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 125374.3984375, Mean: 12020.361328125, Std: 14004.30078125
[DEBUG] Loss for batch 24: 0.14122371173189513
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18384.0, Mean: 1427.3001708984375, Std: 1651.6319580078125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 157179.890625, Mean: 12202.4287109375, Std: 14121.2275390625
[DEBUG] Loss for batch 25: 0.1180401933504407
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18352.0, Mean: 1407.968505859375, Std: 1628.5582275390625
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156906.28125, Mean: 12037.14453125, Std: 13923.9501953125
[DEBUG] Loss for batch 26: 0.13821054021479395
[DEBUG] Batch 27: Data shape: torch.Size([238, 10, 120, 120]), Labels shape: torch.Size([238, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18288.0, Mean: 1420.3984375, Std: 1640.5167236328125
[DEBUG] Mean used for standardization: 0.0928414911031723, Std used for standardization: 0.11696092784404755
[DEBUG] Data after standardization: Min: 7.7560811042785645, Max: 156359.09375, Mean: 12143.4189453125, Std: 14026.1943359375
[DEBUG] Loss for batch 27: 0.13088265069495952
[INFO] Training epoch completed successfully.
Epoch 1/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18752.0, Mean: 1813.69580078125, Std: 1153.0133056640625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240524.640625, Mean: 23262.287109375, Std: 14789.34375
[DEBUG] Loss for batch 0: 0.48015611524399565
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15721.0, Mean: 1789.5206298828125, Std: 1144.2412109375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 201646.953125, Mean: 22952.203125, Std: 14676.8251953125
[DEBUG] Loss for batch 1: 0.2471728057011446
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18800.0, Mean: 1781.975830078125, Std: 1126.0643310546875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241140.328125, Mean: 22855.4296875, Std: 14443.6767578125
[DEBUG] Loss for batch 2: 0.23390915300627385
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16752.0, Mean: 1787.8841552734375, Std: 1138.1572265625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 214871.265625, Mean: 22931.2109375, Std: 14598.7890625
[DEBUG] Loss for batch 3: 0.2443890311357511
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16800.0, Mean: 1805.8409423828125, Std: 1173.78564453125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215486.9375, Mean: 23161.53515625, Std: 15055.783203125
[DEBUG] Loss for batch 4: 0.25406522373144447
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15480.0, Mean: 1785.1796875, Std: 1157.19775390625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 198555.71875, Mean: 22896.521484375, Std: 14843.015625
[DEBUG] Loss for batch 5: 0.2246794094298319
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18784.0, Mean: 1818.1766357421875, Std: 1171.20654296875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240935.09375, Mean: 23319.763671875, Std: 15022.701171875
[DEBUG] Loss for batch 6: 0.22503038754512011
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1798.7618408203125, Std: 1155.3302001953125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 23070.73828125, Std: 14819.0615234375
[DEBUG] Loss for batch 7: 0.20351735761614767
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1795.2230224609375, Std: 1173.0377197265625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 23025.34375, Std: 15046.1904296875
[DEBUG] Loss for batch 8: 0.1990637169903738
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14694.0, Mean: 1765.373046875, Std: 1136.90185546875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 188473.9375, Mean: 22642.46875, Std: 14582.6865234375
[DEBUG] Loss for batch 9: 0.18773443630224548
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16104.0, Mean: 1779.7685546875, Std: 1145.9852294921875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 206559.5625, Mean: 22827.1171875, Std: 14699.1962890625
[DEBUG] Loss for batch 10: 0.20132845561293766
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18768.0, Mean: 1804.666015625, Std: 1170.7906494140625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240729.875, Mean: 23146.466796875, Std: 15017.3681640625
[DEBUG] Loss for batch 11: 0.17351888754522438
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15139.0, Mean: 1785.25048828125, Std: 1139.0804443359375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 194181.8125, Mean: 22897.4296875, Std: 14610.6298828125
[DEBUG] Loss for batch 12: 0.17849825178596299
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15528.0, Mean: 1792.644775390625, Std: 1150.043701171875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 199171.390625, Mean: 22992.271484375, Std: 14751.251953125
[DEBUG] Loss for batch 13: 0.1969167516319135
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18784.0, Mean: 1793.3026123046875, Std: 1134.306396484375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240935.09375, Mean: 23000.712890625, Std: 14549.3955078125
[DEBUG] Loss for batch 14: 0.1840465757823416
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15132.0, Mean: 1817.9586181640625, Std: 1174.9154052734375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 194092.03125, Mean: 23316.96875, Std: 15070.2744140625
[DEBUG] Loss for batch 15: 0.18460633287607284
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17424.0, Mean: 1793.8480224609375, Std: 1139.5689697265625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223490.796875, Mean: 23007.70703125, Std: 14616.896484375
[DEBUG] Loss for batch 16: 0.16818908955798387
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1808.2244873046875, Std: 1173.8743896484375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 23192.109375, Std: 15056.9228515625
[DEBUG] Loss for batch 17: 0.18133138805971108
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17456.0, Mean: 1803.8466796875, Std: 1148.5101318359375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223901.25, Mean: 23135.95703125, Std: 14731.58203125
[DEBUG] Loss for batch 18: 0.19018830282088028
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15928.0, Mean: 1779.0982666015625, Std: 1141.277099609375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 204302.078125, Mean: 22818.517578125, Std: 14638.8056640625
[DEBUG] Loss for batch 19: 0.19785138122899176
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17616.0, Mean: 1812.13818359375, Std: 1149.34716796875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225953.515625, Mean: 23242.30859375, Std: 14742.318359375
[DEBUG] Loss for batch 20: 0.1746675396838616
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10120.0, Mean: 1793.4085693359375, Std: 1134.187255859375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 129804.6640625, Mean: 23002.072265625, Std: 14547.8681640625
[DEBUG] Loss for batch 21: 0.18505882718200248
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1788.8170166015625, Std: 1147.542236328125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 22943.177734375, Std: 14719.1669921875
[DEBUG] Loss for batch 22: 0.1603528763595721
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17632.0, Mean: 1796.2059326171875, Std: 1114.5650634765625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 226158.75, Mean: 23037.951171875, Std: 14296.1796875
[DEBUG] Loss for batch 23: 0.15879100183522085
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17408.0, Mean: 1803.45654296875, Std: 1156.6256103515625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223285.578125, Mean: 23130.955078125, Std: 14835.6767578125
[DEBUG] Loss for batch 24: 0.18416234991298247
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15968.0, Mean: 1798.915283203125, Std: 1123.2913818359375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 204815.140625, Mean: 23072.703125, Std: 14408.109375
[DEBUG] Loss for batch 25: 0.17014004770362887
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15824.0, Mean: 1789.6268310546875, Std: 1147.604248046875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 202968.09375, Mean: 22953.564453125, Std: 14719.9619140625
[DEBUG] Loss for batch 26: 0.17323479406833764
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1805.6484375, Std: 1147.2894287109375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 23159.0703125, Std: 14715.923828125
[DEBUG] Loss for batch 27: 0.1827824123615516
[DEBUG] Batch 28: Data shape: torch.Size([12, 10, 120, 120]), Labels shape: torch.Size([12, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 6548.0, Mean: 1757.7911376953125, Std: 1248.435791015625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 83987.734375, Mean: 22545.216796875, Std: 16013.296875
[DEBUG] Loss for batch 28: 0.2993304330407945
[INFO] Training epoch completed successfully.
Epoch 2/2
----------
[DEBUG] Batch 0: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18752.0, Mean: 1801.843994140625, Std: 1155.8841552734375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240524.640625, Mean: 23110.26953125, Std: 14826.166015625
[DEBUG] Loss for batch 0: 0.16863526870984089
[DEBUG] Batch 1: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1801.5185546875, Std: 1108.1494140625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 23106.09765625, Std: 14213.8876953125
[DEBUG] Loss for batch 1: 0.16745164774032573
[DEBUG] Batch 2: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 14712.0, Mean: 1800.022216796875, Std: 1125.4072265625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 188704.8125, Mean: 23086.904296875, Std: 14435.2490234375
[DEBUG] Loss for batch 2: 0.17609813846685482
[DEBUG] Batch 3: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1809.2197265625, Std: 1160.4178466796875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 23204.87890625, Std: 14884.3193359375
[DEBUG] Loss for batch 3: 0.18301350414417195
[DEBUG] Batch 4: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16832.0, Mean: 1798.78662109375, Std: 1177.9119873046875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215897.40625, Mean: 23071.056640625, Std: 15108.7109375
[DEBUG] Loss for batch 4: 0.18693312726970673
[DEBUG] Batch 5: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16784.0, Mean: 1801.1590576171875, Std: 1169.7398681640625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 215281.71875, Mean: 23101.484375, Std: 15003.890625
[DEBUG] Loss for batch 5: 0.15862162548834502
[DEBUG] Batch 6: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15136.0, Mean: 1818.0662841796875, Std: 1149.7034912109375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 194143.328125, Mean: 23318.349609375, Std: 14746.888671875
[DEBUG] Loss for batch 6: 0.18740026951891403
[DEBUG] Batch 7: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 16752.0, Mean: 1773.7406005859375, Std: 1137.416259765625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 214871.265625, Mean: 22749.794921875, Std: 14589.2841796875
[DEBUG] Loss for batch 7: 0.18350479317293017
[DEBUG] Batch 8: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15139.0, Mean: 1778.558837890625, Std: 1122.254150390625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 194181.8125, Mean: 22811.595703125, Std: 14394.8046875
[DEBUG] Loss for batch 8: 0.15367497309904954
[DEBUG] Batch 9: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17616.0, Mean: 1794.962646484375, Std: 1159.2735595703125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225953.515625, Mean: 23022.00390625, Std: 14869.640625
[DEBUG] Loss for batch 9: 0.1691154689638727
[DEBUG] Batch 10: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15968.0, Mean: 1799.5933837890625, Std: 1155.8896484375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 204815.140625, Mean: 23081.40625, Std: 14826.2373046875
[DEBUG] Loss for batch 10: 0.17569195479415822
[DEBUG] Batch 11: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18832.0, Mean: 1779.6868896484375, Std: 1144.8060302734375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241550.78125, Mean: 22826.068359375, Std: 14684.0712890625
[DEBUG] Loss for batch 11: 0.17516285812488322
[DEBUG] Batch 12: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18784.0, Mean: 1811.6605224609375, Std: 1154.9755859375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240935.09375, Mean: 23236.18359375, Std: 14814.5126953125
[DEBUG] Loss for batch 12: 0.16597523016619525
[DEBUG] Batch 13: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17408.0, Mean: 1798.2662353515625, Std: 1144.8228759765625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 223285.578125, Mean: 23064.37890625, Std: 14684.287109375
[DEBUG] Loss for batch 13: 0.1722243953208243
[DEBUG] Batch 14: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18800.0, Mean: 1790.8912353515625, Std: 1158.2896728515625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 241140.328125, Mean: 22969.78515625, Std: 14857.0205078125
[DEBUG] Loss for batch 14: 0.16482869298362215
[DEBUG] Batch 15: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18656.0, Mean: 1809.86767578125, Std: 1152.0511474609375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 239293.28125, Mean: 23213.185546875, Std: 14777.001953125
[DEBUG] Loss for batch 15: 0.15931715603975286
[DEBUG] Batch 16: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 8528.0, Mean: 1763.7147216796875, Std: 1134.429443359375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 109384.578125, Mean: 22621.19921875, Std: 14550.9736328125
[DEBUG] Loss for batch 16: 0.15918549100310844
[DEBUG] Batch 17: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17584.0, Mean: 1784.2657470703125, Std: 1136.4744873046875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 225543.0625, Mean: 22884.802734375, Std: 14577.205078125
[DEBUG] Loss for batch 17: 0.17206377568686146
[DEBUG] Batch 18: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15912.0, Mean: 1805.7716064453125, Std: 1137.095947265625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 204096.84375, Mean: 23160.650390625, Std: 14585.17578125
[DEBUG] Loss for batch 18: 0.15288890697597735
[DEBUG] Batch 19: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 10752.0, Mean: 1797.25, Std: 1149.768798828125
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 137911.125, Mean: 23051.34375, Std: 14747.7265625
[DEBUG] Loss for batch 19: 0.16923853970251018
[DEBUG] Batch 20: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18784.0, Mean: 1810.816162109375, Std: 1177.1527099609375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240935.09375, Mean: 23225.35546875, Std: 15098.970703125
[DEBUG] Loss for batch 20: 0.14855911188634327
[DEBUG] Batch 21: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 18768.0, Mean: 1789.727783203125, Std: 1155.200927734375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 240729.875, Mean: 22954.861328125, Std: 14817.40234375
[DEBUG] Loss for batch 21: 0.15433744738534405
[DEBUG] Batch 22: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15400.0, Mean: 1796.0198974609375, Std: 1161.307373046875
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 197529.578125, Mean: 23035.56640625, Std: 14895.728515625
[DEBUG] Loss for batch 22: 0.1830182798321366
[DEBUG] Batch 23: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15872.0, Mean: 1806.2685546875, Std: 1154.2515869140625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 203583.78125, Mean: 23167.025390625, Std: 14805.2255859375
[DEBUG] Loss for batch 23: 0.16097541179078179
[DEBUG] Batch 24: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 11889.0, Mean: 1775.8131103515625, Std: 1134.33837890625
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 152495.078125, Mean: 22776.376953125, Std: 14549.8056640625
[DEBUG] Loss for batch 24: 0.16082715609675166
[DEBUG] Batch 25: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 17632.0, Mean: 1785.8580322265625, Std: 1148.22412109375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 226158.75, Mean: 22905.22265625, Std: 14727.9140625
[DEBUG] Loss for batch 25: 0.17946857033877864
[DEBUG] Batch 26: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15416.0, Mean: 1797.0875244140625, Std: 1163.8333740234375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 197734.8125, Mean: 23049.2578125, Std: 14928.1279296875
[DEBUG] Loss for batch 26: 0.1512494919542741
[DEBUG] Batch 27: Data shape: torch.Size([256, 10, 120, 120]), Labels shape: torch.Size([256, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 15129.0, Mean: 1803.4970703125, Std: 1143.3197021484375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 11.404500961303711, Max: 194053.546875, Mean: 23131.474609375, Std: 14665.0068359375
[DEBUG] Loss for batch 27: 0.16833578109647573
[DEBUG] Batch 28: Data shape: torch.Size([12, 10, 120, 120]), Labels shape: torch.Size([12, 19])
[DEBUG] Data before standardization: Min: 33.0, Max: 7704.0, Mean: 1854.2628173828125, Std: 1151.5101318359375
[DEBUG] Mean used for standardization: 0.11087723821401596, Std used for standardization: 0.07796243578195572
[DEBUG] Data after standardization: Min: 421.8585205078125, Max: 98815.390625, Mean: 23782.6328125, Std: 14770.0625
[DEBUG] Loss for batch 28: 0.15251256752462564
[INFO] Training epoch completed successfully.
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
[INFO] Starting validation for Round 2...
[DEBUG] Batch 0: Data shape: torch.Size([128, 10, 120, 120]), Labels shape: torch.Size([128, 19])
[DEBUG] Data before standardization: Min: 1.0, Max: 4038.0, Mean: 208.4311065673828, Std: 119.26319122314453
[DEBUG] Mean used for standardization: 0.0981091856956482, Std used for standardization: 0.1000843197107315
[DEBUG] Data after standardization: Min: 9.011309623718262, Max: 40344.99609375, Mean: 2081.574951171875, Std: 1191.626953125
[ERROR] NaN or Inf detected in logits.
Logits: tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')
