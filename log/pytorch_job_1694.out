Lines that potentially need to be canonized 309
Using device: cuda:0
Initialized conv1.weight with Xavier uniform.
Initialized encoder.1.bias with zeros.
Initialized encoder.4.0.conv1.weight with Xavier uniform.
Initialized encoder.4.0.bn1.bias with zeros.
Initialized encoder.4.0.conv2.weight with Xavier uniform.
Initialized encoder.4.0.bn2.bias with zeros.
Initialized encoder.4.0.conv3.weight with Xavier uniform.
Initialized encoder.4.0.bn3.bias with zeros.
Initialized encoder.4.0.downsample.0.weight with Xavier uniform.
Initialized encoder.4.0.downsample.1.bias with zeros.
Initialized encoder.4.1.conv1.weight with Xavier uniform.
Initialized encoder.4.1.bn1.bias with zeros.
Initialized encoder.4.1.conv2.weight with Xavier uniform.
Initialized encoder.4.1.bn2.bias with zeros.
Initialized encoder.4.1.conv3.weight with Xavier uniform.
Initialized encoder.4.1.bn3.bias with zeros.
Initialized encoder.4.2.conv1.weight with Xavier uniform.
Initialized encoder.4.2.bn1.bias with zeros.
Initialized encoder.4.2.conv2.weight with Xavier uniform.
Initialized encoder.4.2.bn2.bias with zeros.
Initialized encoder.4.2.conv3.weight with Xavier uniform.
Initialized encoder.4.2.bn3.bias with zeros.
Initialized encoder.5.0.conv1.weight with Xavier uniform.
Initialized encoder.5.0.bn1.bias with zeros.
Initialized encoder.5.0.conv2.weight with Xavier uniform.
Initialized encoder.5.0.bn2.bias with zeros.
Initialized encoder.5.0.conv3.weight with Xavier uniform.
Initialized encoder.5.0.bn3.bias with zeros.
Initialized encoder.5.0.downsample.0.weight with Xavier uniform.
Initialized encoder.5.0.downsample.1.bias with zeros.
Initialized encoder.5.1.conv1.weight with Xavier uniform.
Initialized encoder.5.1.bn1.bias with zeros.
Initialized encoder.5.1.conv2.weight with Xavier uniform.
Initialized encoder.5.1.bn2.bias with zeros.
Initialized encoder.5.1.conv3.weight with Xavier uniform.
Initialized encoder.5.1.bn3.bias with zeros.
Initialized encoder.5.2.conv1.weight with Xavier uniform.
Initialized encoder.5.2.bn1.bias with zeros.
Initialized encoder.5.2.conv2.weight with Xavier uniform.
Initialized encoder.5.2.bn2.bias with zeros.
Initialized encoder.5.2.conv3.weight with Xavier uniform.
Initialized encoder.5.2.bn3.bias with zeros.
Initialized encoder.5.3.conv1.weight with Xavier uniform.
Initialized encoder.5.3.bn1.bias with zeros.
Initialized encoder.5.3.conv2.weight with Xavier uniform.
Initialized encoder.5.3.bn2.bias with zeros.
Initialized encoder.5.3.conv3.weight with Xavier uniform.
Initialized encoder.5.3.bn3.bias with zeros.
Initialized encoder.6.0.conv1.weight with Xavier uniform.
Initialized encoder.6.0.bn1.bias with zeros.
Initialized encoder.6.0.conv2.weight with Xavier uniform.
Initialized encoder.6.0.bn2.bias with zeros.
Initialized encoder.6.0.conv3.weight with Xavier uniform.
Initialized encoder.6.0.bn3.bias with zeros.
Initialized encoder.6.0.downsample.0.weight with Xavier uniform.
Initialized encoder.6.0.downsample.1.bias with zeros.
Initialized encoder.6.1.conv1.weight with Xavier uniform.
Initialized encoder.6.1.bn1.bias with zeros.
Initialized encoder.6.1.conv2.weight with Xavier uniform.
Initialized encoder.6.1.bn2.bias with zeros.
Initialized encoder.6.1.conv3.weight with Xavier uniform.
Initialized encoder.6.1.bn3.bias with zeros.
Initialized encoder.6.2.conv1.weight with Xavier uniform.
Initialized encoder.6.2.bn1.bias with zeros.
Initialized encoder.6.2.conv2.weight with Xavier uniform.
Initialized encoder.6.2.bn2.bias with zeros.
Initialized encoder.6.2.conv3.weight with Xavier uniform.
Initialized encoder.6.2.bn3.bias with zeros.
Initialized encoder.6.3.conv1.weight with Xavier uniform.
Initialized encoder.6.3.bn1.bias with zeros.
Initialized encoder.6.3.conv2.weight with Xavier uniform.
Initialized encoder.6.3.bn2.bias with zeros.
Initialized encoder.6.3.conv3.weight with Xavier uniform.
Initialized encoder.6.3.bn3.bias with zeros.
Initialized encoder.6.4.conv1.weight with Xavier uniform.
Initialized encoder.6.4.bn1.bias with zeros.
Initialized encoder.6.4.conv2.weight with Xavier uniform.
Initialized encoder.6.4.bn2.bias with zeros.
Initialized encoder.6.4.conv3.weight with Xavier uniform.
Initialized encoder.6.4.bn3.bias with zeros.
Initialized encoder.6.5.conv1.weight with Xavier uniform.
Initialized encoder.6.5.bn1.bias with zeros.
Initialized encoder.6.5.conv2.weight with Xavier uniform.
Initialized encoder.6.5.bn2.bias with zeros.
Initialized encoder.6.5.conv3.weight with Xavier uniform.
Initialized encoder.6.5.bn3.bias with zeros.
Initialized encoder.7.0.conv1.weight with Xavier uniform.
Initialized encoder.7.0.bn1.bias with zeros.
Initialized encoder.7.0.conv2.weight with Xavier uniform.
Initialized encoder.7.0.bn2.bias with zeros.
Initialized encoder.7.0.conv3.weight with Xavier uniform.
Initialized encoder.7.0.bn3.bias with zeros.
Initialized encoder.7.0.downsample.0.weight with Xavier uniform.
Initialized encoder.7.0.downsample.1.bias with zeros.
Initialized encoder.7.1.conv1.weight with Xavier uniform.
Initialized encoder.7.1.bn1.bias with zeros.
Initialized encoder.7.1.conv2.weight with Xavier uniform.
Initialized encoder.7.1.bn2.bias with zeros.
Initialized encoder.7.1.conv3.weight with Xavier uniform.
Initialized encoder.7.1.bn3.bias with zeros.
Initialized encoder.7.2.conv1.weight with Xavier uniform.
Initialized encoder.7.2.bn1.bias with zeros.
Initialized encoder.7.2.conv2.weight with Xavier uniform.
Initialized encoder.7.2.bn2.bias with zeros.
Initialized encoder.7.2.conv3.weight with Xavier uniform.
Initialized encoder.7.2.bn3.bias with zeros.
Initialized FC.weight with Xavier uniform.
Initialized FC.bias with zeros.
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initialized conv1.weight with Xavier uniform.
Initialized encoder.1.bias with zeros.
Initialized encoder.4.0.conv1.weight with Xavier uniform.
Initialized encoder.4.0.bn1.bias with zeros.
Initialized encoder.4.0.conv2.weight with Xavier uniform.
Initialized encoder.4.0.bn2.bias with zeros.
Initialized encoder.4.0.conv3.weight with Xavier uniform.
Initialized encoder.4.0.bn3.bias with zeros.
Initialized encoder.4.0.downsample.0.weight with Xavier uniform.
Initialized encoder.4.0.downsample.1.bias with zeros.
Initialized encoder.4.1.conv1.weight with Xavier uniform.
Initialized encoder.4.1.bn1.bias with zeros.
Initialized encoder.4.1.conv2.weight with Xavier uniform.
Initialized encoder.4.1.bn2.bias with zeros.
Initialized encoder.4.1.conv3.weight with Xavier uniform.
Initialized encoder.4.1.bn3.bias with zeros.
Initialized encoder.4.2.conv1.weight with Xavier uniform.
Initialized encoder.4.2.bn1.bias with zeros.
Initialized encoder.4.2.conv2.weight with Xavier uniform.
Initialized encoder.4.2.bn2.bias with zeros.
Initialized encoder.4.2.conv3.weight with Xavier uniform.
Initialized encoder.4.2.bn3.bias with zeros.
Initialized encoder.5.0.conv1.weight with Xavier uniform.
Initialized encoder.5.0.bn1.bias with zeros.
Initialized encoder.5.0.conv2.weight with Xavier uniform.
Initialized encoder.5.0.bn2.bias with zeros.
Initialized encoder.5.0.conv3.weight with Xavier uniform.
Initialized encoder.5.0.bn3.bias with zeros.
Initialized encoder.5.0.downsample.0.weight with Xavier uniform.
Initialized encoder.5.0.downsample.1.bias with zeros.
Initialized encoder.5.1.conv1.weight with Xavier uniform.
Initialized encoder.5.1.bn1.bias with zeros.
Initialized encoder.5.1.conv2.weight with Xavier uniform.
Initialized encoder.5.1.bn2.bias with zeros.
Initialized encoder.5.1.conv3.weight with Xavier uniform.
Initialized encoder.5.1.bn3.bias with zeros.
Initialized encoder.5.2.conv1.weight with Xavier uniform.
Initialized encoder.5.2.bn1.bias with zeros.
Initialized encoder.5.2.conv2.weight with Xavier uniform.
Initialized encoder.5.2.bn2.bias with zeros.
Initialized encoder.5.2.conv3.weight with Xavier uniform.
Initialized encoder.5.2.bn3.bias with zeros.
Initialized encoder.5.3.conv1.weight with Xavier uniform.
Initialized encoder.5.3.bn1.bias with zeros.
Initialized encoder.5.3.conv2.weight with Xavier uniform.
Initialized encoder.5.3.bn2.bias with zeros.
Initialized encoder.5.3.conv3.weight with Xavier uniform.
Initialized encoder.5.3.bn3.bias with zeros.
Initialized encoder.6.0.conv1.weight with Xavier uniform.
Initialized encoder.6.0.bn1.bias with zeros.
Initialized encoder.6.0.conv2.weight with Xavier uniform.
Initialized encoder.6.0.bn2.bias with zeros.
Initialized encoder.6.0.conv3.weight with Xavier uniform.
Initialized encoder.6.0.bn3.bias with zeros.
Initialized encoder.6.0.downsample.0.weight with Xavier uniform.
Initialized encoder.6.0.downsample.1.bias with zeros.
Initialized encoder.6.1.conv1.weight with Xavier uniform.
Initialized encoder.6.1.bn1.bias with zeros.
Initialized encoder.6.1.conv2.weight with Xavier uniform.
Initialized encoder.6.1.bn2.bias with zeros.
Initialized encoder.6.1.conv3.weight with Xavier uniform.
Initialized encoder.6.1.bn3.bias with zeros.
Initialized encoder.6.2.conv1.weight with Xavier uniform.
Initialized encoder.6.2.bn1.bias with zeros.
Initialized encoder.6.2.conv2.weight with Xavier uniform.
Initialized encoder.6.2.bn2.bias with zeros.
Initialized encoder.6.2.conv3.weight with Xavier uniform.
Initialized encoder.6.2.bn3.bias with zeros.
Initialized encoder.6.3.conv1.weight with Xavier uniform.
Initialized encoder.6.3.bn1.bias with zeros.
Initialized encoder.6.3.conv2.weight with Xavier uniform.
Initialized encoder.6.3.bn2.bias with zeros.
Initialized encoder.6.3.conv3.weight with Xavier uniform.
Initialized encoder.6.3.bn3.bias with zeros.
Initialized encoder.6.4.conv1.weight with Xavier uniform.
Initialized encoder.6.4.bn1.bias with zeros.
Initialized encoder.6.4.conv2.weight with Xavier uniform.
Initialized encoder.6.4.bn2.bias with zeros.
Initialized encoder.6.4.conv3.weight with Xavier uniform.
Initialized encoder.6.4.bn3.bias with zeros.
Initialized encoder.6.5.conv1.weight with Xavier uniform.
Initialized encoder.6.5.bn1.bias with zeros.
Initialized encoder.6.5.conv2.weight with Xavier uniform.
Initialized encoder.6.5.bn2.bias with zeros.
Initialized encoder.6.5.conv3.weight with Xavier uniform.
Initialized encoder.6.5.bn3.bias with zeros.
Initialized encoder.7.0.conv1.weight with Xavier uniform.
Initialized encoder.7.0.bn1.bias with zeros.
Initialized encoder.7.0.conv2.weight with Xavier uniform.
Initialized encoder.7.0.bn2.bias with zeros.
Initialized encoder.7.0.conv3.weight with Xavier uniform.
Initialized encoder.7.0.bn3.bias with zeros.
Initialized encoder.7.0.downsample.0.weight with Xavier uniform.
Initialized encoder.7.0.downsample.1.bias with zeros.
Initialized encoder.7.1.conv1.weight with Xavier uniform.
Initialized encoder.7.1.bn1.bias with zeros.
Initialized encoder.7.1.conv2.weight with Xavier uniform.
Initialized encoder.7.1.bn2.bias with zeros.
Initialized encoder.7.1.conv3.weight with Xavier uniform.
Initialized encoder.7.1.bn3.bias with zeros.
Initialized encoder.7.2.conv1.weight with Xavier uniform.
Initialized encoder.7.2.bn1.bias with zeros.
Initialized encoder.7.2.conv2.weight with Xavier uniform.
Initialized encoder.7.2.bn2.bias with zeros.
Initialized encoder.7.2.conv3.weight with Xavier uniform.
Initialized encoder.7.2.bn3.bias with zeros.
Initialized FC.weight with Xavier uniform.
Initialized FC.bias with zeros.
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    7150 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    4263 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initialized conv1.weight with Xavier uniform.
Initialized encoder.1.bias with zeros.
Initialized encoder.4.0.conv1.weight with Xavier uniform.
Initialized encoder.4.0.bn1.bias with zeros.
Initialized encoder.4.0.conv2.weight with Xavier uniform.
Initialized encoder.4.0.bn2.bias with zeros.
Initialized encoder.4.0.conv3.weight with Xavier uniform.
Initialized encoder.4.0.bn3.bias with zeros.
Initialized encoder.4.0.downsample.0.weight with Xavier uniform.
Initialized encoder.4.0.downsample.1.bias with zeros.
Initialized encoder.4.1.conv1.weight with Xavier uniform.
Initialized encoder.4.1.bn1.bias with zeros.
Initialized encoder.4.1.conv2.weight with Xavier uniform.
Initialized encoder.4.1.bn2.bias with zeros.
Initialized encoder.4.1.conv3.weight with Xavier uniform.
Initialized encoder.4.1.bn3.bias with zeros.
Initialized encoder.4.2.conv1.weight with Xavier uniform.
Initialized encoder.4.2.bn1.bias with zeros.
Initialized encoder.4.2.conv2.weight with Xavier uniform.
Initialized encoder.4.2.bn2.bias with zeros.
Initialized encoder.4.2.conv3.weight with Xavier uniform.
Initialized encoder.4.2.bn3.bias with zeros.
Initialized encoder.5.0.conv1.weight with Xavier uniform.
Initialized encoder.5.0.bn1.bias with zeros.
Initialized encoder.5.0.conv2.weight with Xavier uniform.
Initialized encoder.5.0.bn2.bias with zeros.
Initialized encoder.5.0.conv3.weight with Xavier uniform.
Initialized encoder.5.0.bn3.bias with zeros.
Initialized encoder.5.0.downsample.0.weight with Xavier uniform.
Initialized encoder.5.0.downsample.1.bias with zeros.
Initialized encoder.5.1.conv1.weight with Xavier uniform.
Initialized encoder.5.1.bn1.bias with zeros.
Initialized encoder.5.1.conv2.weight with Xavier uniform.
Initialized encoder.5.1.bn2.bias with zeros.
Initialized encoder.5.1.conv3.weight with Xavier uniform.
Initialized encoder.5.1.bn3.bias with zeros.
Initialized encoder.5.2.conv1.weight with Xavier uniform.
Initialized encoder.5.2.bn1.bias with zeros.
Initialized encoder.5.2.conv2.weight with Xavier uniform.
Initialized encoder.5.2.bn2.bias with zeros.
Initialized encoder.5.2.conv3.weight with Xavier uniform.
Initialized encoder.5.2.bn3.bias with zeros.
Initialized encoder.5.3.conv1.weight with Xavier uniform.
Initialized encoder.5.3.bn1.bias with zeros.
Initialized encoder.5.3.conv2.weight with Xavier uniform.
Initialized encoder.5.3.bn2.bias with zeros.
Initialized encoder.5.3.conv3.weight with Xavier uniform.
Initialized encoder.5.3.bn3.bias with zeros.
Initialized encoder.6.0.conv1.weight with Xavier uniform.
Initialized encoder.6.0.bn1.bias with zeros.
Initialized encoder.6.0.conv2.weight with Xavier uniform.
Initialized encoder.6.0.bn2.bias with zeros.
Initialized encoder.6.0.conv3.weight with Xavier uniform.
Initialized encoder.6.0.bn3.bias with zeros.
Initialized encoder.6.0.downsample.0.weight with Xavier uniform.
Initialized encoder.6.0.downsample.1.bias with zeros.
Initialized encoder.6.1.conv1.weight with Xavier uniform.
Initialized encoder.6.1.bn1.bias with zeros.
Initialized encoder.6.1.conv2.weight with Xavier uniform.
Initialized encoder.6.1.bn2.bias with zeros.
Initialized encoder.6.1.conv3.weight with Xavier uniform.
Initialized encoder.6.1.bn3.bias with zeros.
Initialized encoder.6.2.conv1.weight with Xavier uniform.
Initialized encoder.6.2.bn1.bias with zeros.
Initialized encoder.6.2.conv2.weight with Xavier uniform.
Initialized encoder.6.2.bn2.bias with zeros.
Initialized encoder.6.2.conv3.weight with Xavier uniform.
Initialized encoder.6.2.bn3.bias with zeros.
Initialized encoder.6.3.conv1.weight with Xavier uniform.
Initialized encoder.6.3.bn1.bias with zeros.
Initialized encoder.6.3.conv2.weight with Xavier uniform.
Initialized encoder.6.3.bn2.bias with zeros.
Initialized encoder.6.3.conv3.weight with Xavier uniform.
Initialized encoder.6.3.bn3.bias with zeros.
Initialized encoder.6.4.conv1.weight with Xavier uniform.
Initialized encoder.6.4.bn1.bias with zeros.
Initialized encoder.6.4.conv2.weight with Xavier uniform.
Initialized encoder.6.4.bn2.bias with zeros.
Initialized encoder.6.4.conv3.weight with Xavier uniform.
Initialized encoder.6.4.bn3.bias with zeros.
Initialized encoder.6.5.conv1.weight with Xavier uniform.
Initialized encoder.6.5.bn1.bias with zeros.
Initialized encoder.6.5.conv2.weight with Xavier uniform.
Initialized encoder.6.5.bn2.bias with zeros.
Initialized encoder.6.5.conv3.weight with Xavier uniform.
Initialized encoder.6.5.bn3.bias with zeros.
Initialized encoder.7.0.conv1.weight with Xavier uniform.
Initialized encoder.7.0.bn1.bias with zeros.
Initialized encoder.7.0.conv2.weight with Xavier uniform.
Initialized encoder.7.0.bn2.bias with zeros.
Initialized encoder.7.0.conv3.weight with Xavier uniform.
Initialized encoder.7.0.bn3.bias with zeros.
Initialized encoder.7.0.downsample.0.weight with Xavier uniform.
Initialized encoder.7.0.downsample.1.bias with zeros.
Initialized encoder.7.1.conv1.weight with Xavier uniform.
Initialized encoder.7.1.bn1.bias with zeros.
Initialized encoder.7.1.conv2.weight with Xavier uniform.
Initialized encoder.7.1.bn2.bias with zeros.
Initialized encoder.7.1.conv3.weight with Xavier uniform.
Initialized encoder.7.1.bn3.bias with zeros.
Initialized encoder.7.2.conv1.weight with Xavier uniform.
Initialized encoder.7.2.bn1.bias with zeros.
Initialized encoder.7.2.conv2.weight with Xavier uniform.
Initialized encoder.7.2.bn2.bias with zeros.
Initialized encoder.7.2.conv3.weight with Xavier uniform.
Initialized encoder.7.2.bn3.bias with zeros.
Initialized FC.weight with Xavier uniform.
Initialized FC.bias with zeros.
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    7180 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    3248 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initializing LRP Pruning...
LRP initialized successfully.
=== Round 1/3 ===
Training and communication for Round 1...
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Starting validation after Round 1...
Bug fix for empty classification report.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.1502
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.1345

F端hre LRP-Pruning in Runde 1 durch...
F端hre LRP-Pruning in Runde 1 durch...
Berechne LRP-Pruning-Maske f端r Land: Finland
Erstelle DataLoader f端r Land: Finland
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([5])
Unexpected input shape for LRP: torch.Size([5]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 5])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 5
Input shape before relevance computation: torch.Size([5, 10, 1, 5])
Model output shape: torch.Size([5, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([5, 19]) to 5 for one-hot encoding.
Targets shape: torch.Size([5])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([5, 19])
Grad output shape: torch.Size([5, 19])
Relevance computation successful. Relevance shape: torch.Size([5, 10, 1, 5])
Relevanzkarten berechnet: ['conv1', 'encoder.4.0.conv1', 'encoder.4.0.conv2', 'encoder.4.0.conv3', 'encoder.4.0.downsample.0', 'encoder.4.1.conv1', 'encoder.4.1.conv2', 'encoder.4.1.conv3', 'encoder.4.2.conv1', 'encoder.4.2.conv2', 'encoder.4.2.conv3', 'encoder.5.0.conv1', 'encoder.5.0.conv2', 'encoder.5.0.conv3', 'encoder.5.0.downsample.0', 'encoder.5.1.conv1', 'encoder.5.1.conv2', 'encoder.5.1.conv3', 'encoder.5.2.conv1', 'encoder.5.2.conv2', 'encoder.5.2.conv3', 'encoder.5.3.conv1', 'encoder.5.3.conv2', 'encoder.5.3.conv3', 'encoder.6.0.conv1', 'encoder.6.0.conv2', 'encoder.6.0.conv3', 'encoder.6.0.downsample.0', 'encoder.6.1.conv1', 'encoder.6.1.conv2', 'encoder.6.1.conv3', 'encoder.6.2.conv1', 'encoder.6.2.conv2', 'encoder.6.2.conv3', 'encoder.6.3.conv1', 'encoder.6.3.conv2', 'encoder.6.3.conv3', 'encoder.6.4.conv1', 'encoder.6.4.conv2', 'encoder.6.4.conv3', 'encoder.6.5.conv1', 'encoder.6.5.conv2', 'encoder.6.5.conv3', 'encoder.7.0.conv1', 'encoder.7.0.conv2', 'encoder.7.0.conv3', 'encoder.7.0.downsample.0', 'encoder.7.1.conv1', 'encoder.7.1.conv2', 'encoder.7.1.conv3', 'encoder.7.2.conv1', 'encoder.7.2.conv2', 'encoder.7.2.conv3']
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.6730e+31, 7.8170e+31, 5.7895e+31, 6.9606e+31, 1.1035e+32, 1.2733e+32,
        8.7101e+31, 1.0066e+32, 1.0239e+32, 1.0723e+32, 5.4556e+31, 1.2842e+32,
        6.8186e+31, 7.4394e+31, 1.7539e+32, 4.1352e+31, 1.2586e+32, 4.9257e+31,
        5.1011e+31, 1.0975e+32, 3.5889e+31, 8.7373e+31, 6.2949e+31, 5.8583e+31,
        9.7922e+31, 5.6798e+31, 1.0040e+32, 8.4782e+31, 7.3021e+31, 8.3380e+31,
        5.1298e+31, 1.2069e+32, 6.3236e+31, 7.3901e+31, 9.3889e+31, 1.3815e+32,
        7.3349e+31, 1.4949e+32, 1.0049e+32, 1.3848e+32, 1.4875e+32, 6.5424e+31,
        5.8275e+31, 5.3738e+31, 6.1101e+31, 6.2715e+31, 7.5853e+31, 8.5179e+31,
        7.3475e+31, 9.5972e+31, 7.1046e+31, 1.3348e+32, 1.8825e+32, 8.1764e+31,
        5.5137e+31, 8.2113e+31, 1.6142e+32, 4.9947e+31, 6.0951e+31, 7.4240e+31,
        3.7655e+31, 8.0619e+31, 3.7195e+31, 7.2040e+31])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.9891e+31, 3.6307e+31, 4.1234e+31, 4.2382e+31, 1.8721e+31, 4.0730e+31,
        4.4953e+31, 4.2507e+31, 2.8337e+31, 2.8139e+31, 2.6328e+31, 5.7071e+31,
        6.9353e+31, 2.4479e+31, 3.3791e+31, 4.1921e+31, 3.0585e+31, 2.3812e+31,
        2.6242e+31, 7.1573e+31, 2.2314e+31, 1.9015e+31, 2.0252e+31, 4.9245e+31,
        6.0370e+31, 3.9780e+31, 4.2746e+31, 5.5910e+31, 4.8387e+31, 3.1746e+31,
        3.0613e+31, 5.0816e+31, 5.2354e+31, 4.6914e+31, 6.3460e+31, 2.1447e+31,
        8.4243e+31, 2.2079e+31, 8.9588e+31, 2.1566e+31, 4.4417e+31, 4.5512e+31,
        4.1611e+31, 2.3432e+31, 3.5767e+31, 5.4134e+31, 5.6335e+31, 3.2811e+31,
        5.1234e+31, 2.4466e+31, 4.7396e+31, 6.2142e+31, 4.7643e+31, 2.0518e+31,
        5.9901e+31, 6.7335e+30, 2.0815e+31, 3.3294e+31, 3.8343e+31, 2.4035e+31,
        8.6024e+31, 7.8925e+31, 2.4566e+31, 3.3934e+31])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.4133e+31, 3.4893e+31, 2.2617e+31, 4.0074e+31, 4.8595e+31, 3.4065e+31,
        2.0381e+31, 1.9189e+31, 3.6077e+31, 1.8551e+31, 5.6272e+31, 5.6095e+31,
        4.4646e+31, 1.9894e+31, 1.7566e+31, 4.3444e+31, 3.4901e+31, 1.8654e+31,
        1.7699e+31, 2.6060e+31, 4.3548e+31, 2.8370e+31, 6.3712e+31, 2.8706e+31,
        1.6506e+31, 2.3320e+31, 3.7079e+31, 3.7660e+31, 3.1942e+31, 2.0232e+31,
        1.5782e+31, 2.6650e+31, 4.0864e+31, 2.3797e+31, 4.3334e+31, 1.7863e+31,
        1.6925e+31, 2.1089e+31, 1.6824e+31, 3.0453e+31, 3.7107e+31, 2.1139e+31,
        1.8078e+31, 3.5745e+31, 1.4315e+31, 5.6880e+31, 5.2890e+31, 2.7835e+31,
        4.3573e+31, 3.7662e+31, 1.6470e+31, 2.1098e+31, 4.1249e+31, 2.7499e+31,
        5.3407e+31, 2.3579e+31, 2.2085e+31, 1.8478e+31, 4.2216e+31, 2.1889e+31,
        2.4288e+31, 1.8995e+31, 2.9583e+31, 3.0679e+31])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0126e+29, 4.1093e+30, 1.2852e+30, 1.5201e+30, 7.8759e+29, 3.2145e+31,
        1.4700e+29, 1.4212e+29, 5.1815e+28, 2.8912e+29, 6.2690e+28, 1.8576e+30,
        1.3163e+29, 8.4292e+29, 8.4287e+29, 2.5330e+29, 1.3486e+29, 2.4264e+29,
        8.7951e+29, 5.1713e+28, 6.1063e+28, 5.6766e+29, 2.0628e+29, 8.0050e+28,
        2.1815e+29, 1.1083e+30, 4.6981e+29, 1.9381e+30, 4.7268e+29, 2.0903e+29,
        2.9799e+29, 8.4553e+28, 8.0395e+29, 4.7007e+30, 1.9080e+29, 1.6254e+29,
        2.3591e+29, 3.7561e+29, 3.5551e+29, 3.7930e+29, 8.2090e+28, 5.3769e+30,
        7.4152e+28, 1.3541e+30, 1.0839e+29, 6.8606e+29, 5.5631e+29, 1.1308e+30,
        1.8317e+29, 1.1407e+30, 1.7162e+29, 4.3039e+29, 2.8547e+30, 3.6928e+28,
        3.8922e+30, 8.7667e+30, 7.3793e+29, 1.9551e+29, 1.1566e+29, 3.8587e+28,
        3.8642e+29, 5.3873e+28, 6.1832e+28, 5.7129e+29, 2.4121e+30, 5.1832e+30,
        8.7209e+28, 1.0015e+30, 6.9924e+30, 4.2733e+29, 7.9479e+29, 1.6880e+29,
        6.4132e+30, 2.9318e+29, 3.2348e+29, 6.7031e+28, 5.0918e+28, 5.7786e+29,
        6.0399e+29, 6.3524e+29, 7.3837e+29, 6.7568e+29, 9.2898e+28, 2.6614e+29,
        7.6161e+28, 1.5800e+29, 4.3513e+29, 9.2802e+29, 9.0378e+29, 4.0055e+28,
        6.3420e+28, 1.7486e+29, 7.2831e+29, 1.7620e+29, 8.9167e+28, 2.1608e+28,
        1.3167e+29, 4.0690e+29, 3.6588e+28, 8.9256e+28, 1.0289e+29, 4.2453e+29,
        1.6111e+29, 1.6485e+29, 1.1804e+30, 4.7507e+29, 1.1221e+30, 1.3177e+29,
        3.0985e+28, 1.9801e+29, 1.1969e+29, 4.3982e+28, 2.7772e+29, 1.7201e+29,
        2.4296e+29, 3.7809e+29, 9.0137e+28, 1.7213e+30, 1.3316e+29, 6.6590e+29,
        6.1287e+29, 3.0793e+29, 1.8556e+31, 6.7493e+29, 1.5620e+29, 3.2279e+29,
        5.3572e+29, 4.4077e+29, 1.2503e+29, 4.7145e+29, 4.1850e+29, 5.9843e+29,
        9.4980e+29, 1.0572e+30, 2.0013e+30, 2.3016e+29, 1.5111e+29, 1.7325e+29,
        7.8752e+28, 6.1727e+29, 1.4271e+30, 4.1855e+29, 5.6132e+30, 3.7402e+29,
        4.9109e+30, 2.6007e+28, 2.1019e+30, 3.6817e+29, 9.7251e+28, 7.6198e+29,
        1.6235e+29, 3.8883e+29, 5.8539e+29, 3.7705e+29, 1.4206e+29, 7.0555e+28,
        1.6728e+29, 3.2750e+29, 9.6411e+29, 8.7282e+28, 1.7211e+30, 1.0861e+29,
        6.1090e+29, 1.0132e+29, 1.6821e+30, 4.1897e+29, 1.7752e+29, 1.8658e+28,
        4.7500e+29, 9.3377e+29, 2.4990e+29, 3.2072e+31, 6.6686e+29, 4.0119e+29,
        2.3657e+30, 2.6310e+28, 2.7033e+29, 1.4817e+29, 4.7723e+29, 4.1352e+29,
        9.3506e+29, 2.9211e+29, 2.2123e+29, 3.2891e+29, 6.8289e+28, 6.5179e+30,
        1.8331e+29, 2.1037e+29, 6.9725e+29, 1.4842e+30, 5.8638e+29, 2.3170e+29,
        9.9443e+28, 5.4859e+29, 5.8716e+29, 1.8389e+29, 2.8164e+29, 1.8698e+29,
        1.9075e+29, 5.2896e+28, 1.9664e+29, 3.6343e+29, 1.7980e+29, 5.6761e+29,
        4.2730e+30, 3.5072e+29, 1.6358e+30, 1.2946e+29, 2.3525e+30, 3.9008e+30,
        1.9985e+29, 4.9861e+28, 6.4147e+29, 2.4147e+29, 1.1884e+29, 1.9765e+29,
        3.9959e+30, 8.7693e+29, 1.2360e+29, 9.4665e+28, 1.2585e+29, 2.1854e+28,
        2.6335e+29, 2.3204e+29, 7.9435e+28, 2.3353e+29, 5.5138e+29, 2.0976e+29,
        8.7797e+29, 1.3708e+29, 5.6279e+29, 1.6117e+30, 2.5937e+30, 3.1557e+29,
        5.2797e+29, 6.0502e+28, 3.0071e+30, 1.3812e+29, 2.1721e+29, 4.0976e+28,
        9.2617e+28, 1.0921e+29, 2.9950e+29, 9.4624e+29, 2.7006e+29, 2.7060e+29,
        3.0608e+29, 4.0551e+29, 2.3804e+29, 2.9893e+28, 9.3402e+29, 8.2464e+29,
        2.7879e+28, 2.1390e+30, 2.5118e+29, 1.3399e+29])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3768e+29, 3.8773e+30, 1.2508e+30, 1.6507e+30, 6.3409e+29, 3.2126e+31,
        1.9643e+29, 5.8191e+28, 1.7031e+29, 2.6354e+29, 1.3015e+29, 2.0507e+30,
        1.5679e+29, 9.4369e+29, 6.5535e+29, 5.8745e+29, 1.8024e+29, 3.3178e+29,
        8.9885e+29, 3.1239e+29, 2.9903e+29, 5.3252e+29, 9.7199e+27, 5.4730e+28,
        1.7321e+29, 1.0246e+30, 5.7441e+29, 1.7973e+30, 2.3541e+29, 1.7520e+29,
        2.2281e+29, 2.0374e+29, 7.7240e+29, 4.6232e+30, 4.2266e+28, 1.7917e+29,
        9.0320e+28, 2.0764e+29, 4.4971e+29, 5.7146e+29, 2.1396e+29, 5.2860e+30,
        1.3445e+29, 9.5286e+29, 1.2021e+29, 7.1241e+29, 3.5633e+29, 1.1621e+30,
        3.9781e+29, 1.2127e+30, 3.8066e+29, 3.2660e+29, 2.8038e+30, 3.3150e+29,
        4.0142e+30, 8.5807e+30, 6.1025e+29, 5.3674e+28, 4.7337e+28, 1.7530e+29,
        4.0446e+29, 3.1246e+29, 1.6613e+29, 3.6083e+29, 2.4336e+30, 5.0447e+30,
        1.0203e+29, 7.6504e+29, 6.9492e+30, 6.7725e+29, 6.9178e+29, 1.5493e+28,
        6.7857e+30, 1.9393e+29, 1.9715e+29, 2.8073e+29, 8.6131e+28, 5.5192e+29,
        4.6815e+29, 6.8502e+29, 6.3912e+29, 6.6040e+29, 5.7801e+28, 1.9730e+29,
        7.8382e+28, 4.5626e+28, 3.9560e+29, 9.0394e+29, 9.4457e+29, 4.6383e+28,
        1.0214e+29, 1.2365e+29, 4.1326e+29, 9.0330e+28, 1.4884e+29, 1.2859e+29,
        1.2413e+29, 3.2132e+29, 6.0795e+28, 3.8108e+28, 1.6764e+29, 5.2573e+29,
        1.4959e+29, 3.4233e+29, 1.3508e+30, 5.9361e+29, 1.3186e+30, 7.5760e+28,
        2.0196e+29, 8.3019e+28, 6.7067e+28, 1.1319e+29, 2.2984e+29, 2.1144e+28,
        3.5846e+29, 2.9513e+29, 9.0678e+28, 1.6457e+30, 2.4542e+29, 5.8610e+29,
        5.7852e+29, 2.3556e+29, 1.8575e+31, 7.3462e+29, 4.7740e+29, 4.0752e+29,
        2.2496e+29, 4.2634e+29, 8.0742e+28, 7.1629e+29, 3.0933e+29, 3.4037e+29,
        6.4338e+29, 8.7748e+29, 1.6089e+30, 9.5244e+28, 6.0284e+28, 2.9073e+29,
        1.1813e+29, 7.8338e+29, 1.3018e+30, 2.0951e+29, 5.7244e+30, 2.0997e+29,
        4.8289e+30, 2.2382e+29, 1.9238e+30, 4.9169e+29, 1.3586e+29, 7.4235e+29,
        3.5854e+29, 4.7888e+29, 5.6754e+29, 4.2385e+29, 8.9224e+28, 2.2099e+29,
        2.5413e+29, 3.5382e+29, 1.2112e+30, 9.6718e+28, 1.6273e+30, 1.4287e+29,
        3.9588e+29, 7.8627e+28, 1.6591e+30, 4.4476e+29, 1.9637e+29, 1.4613e+29,
        3.3364e+29, 1.0685e+30, 5.0925e+28, 3.1982e+31, 6.5928e+29, 9.6076e+28,
        2.3506e+30, 1.3566e+29, 3.1709e+29, 5.5208e+28, 2.7237e+29, 2.8408e+29,
        7.4285e+29, 3.8033e+29, 8.5897e+28, 1.2571e+29, 2.8969e+28, 6.4285e+30,
        1.4262e+29, 6.5010e+28, 9.9669e+29, 1.0956e+30, 4.4343e+29, 4.7776e+29,
        1.2629e+29, 5.4619e+29, 3.4839e+29, 1.1800e+29, 1.2171e+29, 3.3255e+28,
        1.0389e+29, 3.9418e+29, 3.3309e+28, 1.4241e+29, 1.2978e+29, 6.8387e+29,
        4.2383e+30, 2.5390e+29, 1.4867e+30, 9.4125e+28, 2.2825e+30, 3.9915e+30,
        8.7245e+28, 7.7607e+28, 5.1958e+29, 3.3431e+29, 2.3454e+29, 1.9117e+29,
        4.0953e+30, 7.0159e+29, 3.3180e+29, 2.7327e+29, 6.6868e+28, 1.0373e+29,
        7.8217e+28, 2.6779e+29, 7.5817e+28, 3.5230e+29, 5.4757e+29, 2.8115e+29,
        8.3245e+29, 3.2089e+29, 8.1661e+29, 1.3818e+30, 2.5194e+30, 2.2511e+29,
        3.4998e+29, 8.1029e+28, 3.1314e+30, 2.1424e+29, 3.4820e+29, 9.1828e+28,
        2.8132e+29, 6.7480e+28, 4.7007e+29, 8.0585e+29, 9.3409e+28, 1.7749e+29,
        3.4647e+29, 2.3997e+29, 3.5673e+29, 1.4182e+29, 8.5803e+29, 8.3025e+29,
        1.4976e+29, 2.1612e+30, 3.0180e+29, 6.4285e+28])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5402e+29, 4.7006e+29, 2.8736e+29, 1.8313e+29, 7.2432e+29, 2.2061e+29,
        4.9745e+29, 2.7766e+29, 2.8617e+29, 5.4960e+29, 4.7765e+29, 6.9849e+29,
        2.1194e+29, 1.7856e+29, 4.2438e+29, 1.8894e+29, 3.1269e+29, 2.5105e+29,
        3.0552e+29, 2.4437e+29, 4.1771e+29, 3.5293e+29, 4.0808e+29, 2.7670e+29,
        2.7847e+29, 4.0573e+29, 1.5535e+29, 4.7168e+29, 2.8509e+29, 6.0830e+29,
        3.4440e+29, 4.1623e+29, 2.3040e+29, 5.5401e+29, 2.8040e+29, 3.2065e+29,
        3.8813e+29, 3.3412e+29, 5.2691e+29, 5.7715e+29, 1.8341e+29, 2.5408e+29,
        3.5795e+29, 2.1881e+29, 2.6503e+29, 1.5367e+29, 1.6283e+29, 4.3074e+29,
        2.4803e+29, 8.2753e+28, 3.5584e+29, 2.3182e+29, 3.1857e+29, 3.7709e+29,
        4.4465e+29, 5.2835e+29, 2.2630e+29, 2.0439e+29, 3.0407e+29, 2.9664e+29,
        7.5762e+29, 2.5811e+29, 1.2160e+29, 2.1816e+29])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5830e+29, 2.2988e+29, 1.2126e+29, 1.9582e+29, 2.4124e+29, 1.8679e+29,
        3.4271e+29, 3.0943e+29, 1.1793e+29, 9.5724e+28, 3.0486e+29, 1.3644e+29,
        2.0083e+29, 1.2014e+29, 2.2625e+29, 4.4476e+29, 1.1978e+29, 1.5404e+29,
        3.3979e+29, 2.6305e+29, 3.9084e+29, 3.1250e+29, 2.5509e+29, 1.7463e+29,
        3.6149e+29, 3.7508e+29, 2.1905e+29, 3.0036e+29, 2.0985e+29, 2.8081e+29,
        3.1587e+29, 2.5362e+29, 2.9719e+29, 2.9706e+29, 1.7033e+29, 2.8385e+29,
        3.1655e+29, 5.5525e+29, 2.2639e+29, 1.5361e+29, 3.6086e+29, 2.2132e+29,
        1.0929e+29, 2.9429e+29, 1.8079e+29, 1.4219e+29, 3.6536e+29, 1.2218e+29,
        1.9291e+29, 4.4291e+29, 4.3380e+29, 1.8284e+29, 1.2585e+29, 5.1954e+29,
        1.9769e+29, 4.3362e+29, 1.3830e+29, 2.7536e+29, 2.4199e+29, 2.6934e+29,
        3.4444e+29, 1.5928e+29, 2.3196e+29, 2.5092e+29])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.7637e+26, 4.9067e+27, 6.3126e+27, 3.9695e+27, 2.4428e+29, 6.8938e+27,
        7.3038e+27, 1.2136e+27, 3.0865e+27, 2.3467e+27, 2.1751e+27, 3.2879e+27,
        1.5112e+27, 8.4126e+27, 1.3351e+27, 1.9500e+29, 2.8761e+27, 2.3621e+27,
        1.4472e+27, 3.7530e+27, 1.8848e+28, 6.5492e+27, 3.4574e+27, 3.8212e+27,
        1.1557e+27, 7.6126e+27, 2.5603e+27, 3.8558e+27, 3.4654e+27, 8.5916e+27,
        2.6484e+27, 3.6981e+27, 4.7198e+27, 5.8461e+27, 3.5784e+27, 4.8885e+27,
        2.8638e+29, 8.8319e+27, 4.4974e+27, 1.7952e+27, 3.5347e+27, 3.4063e+27,
        5.0484e+28, 4.9142e+27, 2.2085e+27, 2.6300e+27, 3.4919e+27, 1.7785e+28,
        1.2267e+27, 1.5247e+28, 1.6980e+27, 2.2996e+27, 1.4558e+27, 1.4445e+28,
        2.3904e+27, 2.8728e+27, 2.4999e+27, 2.6331e+27, 2.3892e+27, 9.2890e+28,
        9.9871e+27, 1.7870e+27, 4.6116e+27, 1.2351e+27, 4.7075e+27, 4.4710e+27,
        5.0222e+27, 2.0887e+27, 2.1740e+27, 3.5713e+27, 3.3542e+27, 2.2724e+28,
        4.1972e+27, 3.1321e+27, 5.7386e+27, 1.3319e+27, 4.6151e+26, 1.1005e+27,
        3.2591e+27, 1.5693e+27, 3.4189e+27, 2.7975e+27, 2.8582e+27, 7.0640e+27,
        6.2369e+27, 3.0936e+28, 5.0274e+27, 2.9366e+27, 2.8616e+27, 2.0873e+28,
        2.8954e+27, 4.7552e+27, 4.0096e+27, 1.7985e+27, 1.5454e+27, 2.3062e+27,
        2.8557e+27, 2.4804e+27, 1.2225e+27, 2.2984e+28, 1.3901e+28, 7.1625e+27,
        3.2712e+27, 3.2188e+27, 3.5562e+27, 3.4774e+27, 2.7674e+27, 1.0715e+28,
        6.9933e+27, 5.6223e+27, 2.3227e+27, 6.0882e+26, 3.9780e+27, 9.8168e+27,
        4.6403e+27, 2.6845e+27, 2.6964e+27, 2.5823e+27, 3.3855e+27, 5.1712e+27,
        4.0568e+27, 5.8156e+27, 2.6039e+27, 3.4049e+27, 1.8528e+27, 2.8941e+27,
        5.7309e+27, 2.8292e+27, 2.9253e+27, 9.0321e+27, 3.1187e+27, 3.6386e+27,
        1.1000e+28, 5.0759e+27, 5.4469e+27, 2.6314e+27, 8.1697e+28, 3.7668e+27,
        1.0603e+28, 1.6285e+27, 2.8497e+27, 2.8731e+27, 3.0862e+27, 1.8766e+27,
        2.9587e+27, 2.1364e+27, 5.9074e+27, 2.0718e+27, 2.1367e+28, 4.7636e+27,
        4.0558e+27, 1.4278e+28, 4.2943e+27, 2.9027e+27, 4.1488e+27, 3.1213e+27,
        7.7987e+26, 3.3286e+27, 3.2551e+27, 8.8063e+27, 2.8732e+27, 6.1647e+27,
        3.0317e+27, 1.8981e+27, 3.0809e+27, 5.1026e+27, 2.6248e+27, 4.0268e+27,
        1.6177e+27, 4.2497e+27, 4.0219e+27, 2.7596e+27, 4.6085e+27, 3.0615e+27,
        2.7253e+27, 2.7202e+27, 2.4659e+27, 2.2265e+27, 4.9737e+27, 4.2694e+27,
        3.5574e+27, 4.8116e+27, 4.7647e+27, 2.9180e+27, 1.3307e+27, 1.9574e+27,
        2.5127e+27, 1.5774e+27, 8.7033e+27, 2.2745e+27, 1.8356e+28, 2.4109e+27,
        1.3584e+27, 6.1626e+27, 1.6875e+27, 1.5383e+28, 3.2379e+27, 1.9705e+28,
        3.3821e+27, 7.2100e+26, 8.3282e+27, 1.1231e+27, 2.7257e+27, 2.7954e+27,
        3.8495e+27, 3.0040e+27, 4.9000e+27, 9.7620e+26, 1.9677e+27, 2.4935e+27,
        5.7274e+27, 5.2043e+27, 5.9219e+27, 4.1896e+27, 3.8812e+27, 5.7168e+27,
        3.5948e+27, 1.4351e+28, 1.0995e+27, 1.6818e+27, 3.1131e+27, 2.3107e+27,
        2.1307e+27, 7.9488e+27, 3.1214e+27, 3.4114e+27, 3.4467e+27, 7.4693e+27,
        5.0680e+27, 6.0537e+27, 2.2503e+27, 4.1504e+27, 5.3025e+27, 1.0518e+28,
        2.6241e+27, 3.0589e+27, 4.0326e+27, 2.5741e+27, 5.2973e+27, 1.4472e+27,
        3.8513e+27, 2.5239e+27, 2.5503e+27, 2.3779e+27, 1.7364e+27, 4.5522e+27,
        5.1972e+27, 5.8226e+27, 5.3160e+27, 2.4478e+28, 1.5076e+28, 2.6494e+27,
        2.3494e+27, 5.8783e+27, 4.8202e+27, 7.2578e+27])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.9429e+27, 4.2232e+27, 3.3307e+27, 3.5122e+27, 2.7788e+27, 8.4201e+27,
        5.7356e+27, 5.6275e+27, 8.1555e+27, 4.8676e+27, 2.9163e+27, 4.4392e+27,
        8.2031e+27, 6.4837e+27, 4.0715e+27, 5.2621e+27, 7.9513e+27, 4.4489e+27,
        6.6550e+27, 3.8092e+27, 5.4209e+27, 6.9476e+27, 9.5962e+27, 9.9254e+27,
        8.0301e+27, 5.2239e+27, 2.3434e+27, 8.1750e+27, 3.6898e+27, 7.1051e+27,
        9.3439e+27, 8.6005e+27, 6.3482e+27, 4.1508e+27, 4.6072e+27, 5.9448e+27,
        8.7469e+27, 5.4784e+27, 8.1314e+27, 6.8398e+27, 7.0587e+27, 4.5961e+27,
        5.0760e+27, 8.2780e+27, 3.4123e+27, 5.5867e+27, 3.7153e+27, 6.3732e+27,
        4.8485e+27, 1.0009e+28, 4.7576e+27, 5.8915e+27, 3.7678e+27, 2.6817e+27,
        4.7892e+27, 4.6405e+27, 6.4809e+27, 1.0079e+28, 4.1682e+27, 9.2937e+27,
        2.3893e+27, 6.4243e+27, 9.0057e+27, 5.5456e+27])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.1574e+27, 5.8044e+27, 3.9983e+27, 5.1338e+27, 2.6469e+27, 7.4280e+27,
        5.3013e+27, 4.7218e+27, 3.9540e+27, 6.5708e+27, 7.7095e+27, 4.6121e+27,
        4.9280e+27, 3.0948e+27, 7.9942e+27, 3.0164e+27, 6.4045e+27, 5.4337e+27,
        4.4658e+27, 6.5059e+27, 2.0696e+27, 2.8848e+27, 3.7498e+27, 5.2377e+27,
        5.4297e+27, 5.1734e+27, 4.0516e+27, 6.1175e+27, 4.4445e+27, 6.7913e+27,
        7.2875e+27, 4.9004e+27, 7.1676e+27, 3.7865e+27, 4.7085e+27, 6.0094e+27,
        8.2803e+27, 4.6333e+27, 5.4517e+27, 6.0246e+27, 3.2628e+27, 5.8277e+27,
        7.1767e+27, 4.4084e+27, 4.5336e+27, 7.1652e+27, 2.4721e+27, 8.2739e+27,
        5.6025e+27, 2.4100e+27, 3.5149e+27, 5.4860e+27, 5.6545e+27, 7.1033e+27,
        4.2832e+27, 6.7544e+27, 8.6971e+27, 5.2552e+27, 3.3017e+27, 4.8172e+27,
        6.0445e+27, 7.5129e+27, 6.7405e+27, 2.2561e+27])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0986e+25, 3.7634e+25, 2.7887e+25, 1.7993e+25, 5.2021e+25, 4.5893e+25,
        1.7762e+25, 4.3808e+25, 3.9382e+24, 1.6056e+25, 3.1809e+25, 2.3447e+25,
        7.0160e+25, 3.1259e+25, 1.5309e+26, 5.4394e+25, 3.4968e+25, 5.5074e+25,
        1.3293e+25, 2.1353e+25, 1.6924e+26, 2.2426e+25, 1.4370e+25, 1.2862e+26,
        2.2166e+25, 1.8316e+25, 5.2549e+24, 2.0722e+25, 1.4780e+25, 2.7211e+25,
        3.9074e+25, 1.7276e+25, 3.8398e+25, 2.4577e+25, 5.4561e+26, 1.2117e+25,
        2.1631e+25, 1.7052e+25, 2.1692e+25, 6.3431e+24, 2.0089e+25, 1.5757e+25,
        3.9787e+25, 1.0602e+25, 8.0460e+24, 1.7859e+25, 1.7415e+25, 1.8977e+25,
        3.3046e+25, 2.8480e+25, 2.2901e+25, 3.9822e+25, 5.2760e+25, 1.7024e+25,
        2.4593e+25, 2.6458e+25, 1.9489e+25, 1.1993e+25, 4.2304e+26, 7.5027e+25,
        2.7607e+25, 5.0667e+24, 5.2218e+25, 1.1089e+25, 1.5218e+25, 2.4511e+25,
        2.3153e+25, 3.5051e+25, 1.4979e+25, 1.6457e+25, 3.8331e+25, 1.5688e+25,
        1.9562e+25, 2.6597e+25, 9.3148e+24, 1.9904e+27, 1.3038e+26, 2.6587e+25,
        2.8023e+25, 6.7756e+24, 1.6127e+25, 1.0579e+25, 2.3807e+25, 2.4527e+27,
        1.4615e+25, 5.3116e+25, 3.0371e+25, 2.7918e+25, 2.1785e+25, 2.0990e+25,
        2.2923e+25, 2.0786e+25, 3.1475e+25, 2.2250e+25, 2.6623e+26, 1.2635e+25,
        2.7091e+25, 2.1286e+25, 1.5496e+25, 4.8371e+24, 9.2681e+26, 2.0193e+25,
        1.0872e+25, 3.8799e+25, 2.5737e+25, 1.4723e+25, 1.2479e+25, 6.0877e+25,
        4.5770e+25, 1.6950e+25, 1.8732e+25, 1.2134e+26, 1.8259e+25, 7.5401e+25,
        9.9022e+24, 7.9986e+24, 1.2616e+25, 3.2856e+24, 3.2376e+25, 1.5513e+25,
        1.8292e+25, 3.0681e+25, 1.3123e+25, 1.2622e+26, 2.6586e+25, 6.6216e+25,
        2.5502e+25, 2.7409e+26, 3.1617e+25, 2.0464e+25, 3.4468e+25, 1.0296e+25,
        3.5779e+25, 2.9488e+25, 2.3197e+25, 2.4055e+25, 1.9307e+25, 3.5366e+25,
        2.1182e+25, 5.5435e+25, 2.7143e+25, 4.0572e+25, 3.0478e+25, 4.3051e+25,
        4.1757e+27, 2.2496e+25, 3.2579e+25, 9.2150e+25, 1.9533e+25, 3.9221e+26,
        1.6748e+25, 3.4862e+25, 1.7604e+25, 1.5731e+25, 1.0158e+26, 3.0837e+25,
        3.0216e+25, 1.4467e+25, 2.8547e+25, 4.1915e+25, 8.5581e+25, 1.4450e+25,
        1.2395e+25, 6.9871e+25, 1.8140e+26, 2.3955e+26, 1.9423e+26, 2.7039e+25,
        2.2526e+25, 3.2676e+25, 2.2522e+25, 2.5442e+25, 9.4464e+25, 1.3991e+25,
        5.5408e+26, 3.2648e+25, 9.2782e+25, 5.7350e+25, 1.3718e+25, 8.3959e+24,
        2.9589e+25, 6.0738e+25, 2.2543e+27, 1.8644e+25, 2.0930e+25, 1.8816e+25,
        8.8901e+24, 6.5076e+24, 4.8139e+26, 4.9954e+25, 3.3654e+25, 7.2690e+24,
        6.6950e+24, 4.0794e+25, 9.7726e+25, 3.0169e+25, 1.9182e+25, 3.9036e+25,
        1.6990e+26, 1.3568e+25, 6.1891e+25, 4.7716e+26, 2.7773e+25, 1.6559e+25,
        2.1406e+25, 5.5075e+24, 3.8403e+25, 3.6739e+26, 5.9076e+25, 1.9660e+25,
        1.5261e+25, 1.2572e+25, 2.1862e+25, 2.9711e+25, 1.1949e+26, 5.8253e+25,
        1.9019e+25, 8.8818e+25, 2.8516e+25, 1.1188e+25, 4.3415e+25, 1.4091e+26,
        1.9008e+25, 2.3878e+25, 2.8643e+25, 1.2528e+25, 1.0985e+26, 2.6172e+25,
        7.3583e+24, 1.4381e+25, 4.8024e+25, 3.6715e+25, 1.7085e+25, 2.1933e+25,
        2.3819e+25, 1.8016e+26, 3.0151e+25, 1.1231e+25, 2.9919e+25, 3.3744e+25,
        1.2657e+25, 1.7910e+25, 2.1341e+25, 2.1017e+25, 3.1266e+25, 2.3187e+25,
        1.4393e+25, 2.4229e+25, 1.1296e+25, 4.2485e+26, 3.8544e+25, 3.4640e+25,
        2.1413e+25, 3.4885e+25, 1.6748e+25, 2.3004e+25])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.5588e+25, 3.9527e+25, 1.4049e+25, 2.7833e+25, 2.3976e+25, 2.3618e+25,
        4.9097e+25, 2.1731e+25, 3.8978e+25, 2.4478e+25, 3.0146e+25, 4.3695e+25,
        4.6369e+25, 1.8897e+25, 2.6970e+25, 1.6691e+25, 3.8694e+25, 4.9313e+25,
        3.4726e+25, 4.8937e+25, 1.5984e+25, 3.3568e+25, 1.2939e+25, 2.5520e+25,
        2.2770e+25, 1.5753e+25, 4.7165e+25, 2.0483e+25, 4.2156e+25, 2.4666e+25,
        3.5115e+25, 2.3677e+25, 1.6777e+25, 1.7829e+25, 3.0044e+25, 2.0458e+25,
        2.4602e+25, 2.1716e+25, 1.5007e+25, 6.3012e+25, 2.9859e+25, 3.1115e+25,
        2.8212e+25, 3.4721e+25, 2.0976e+25, 3.4483e+25, 2.7924e+25, 2.9029e+25,
        3.3648e+25, 3.2049e+25, 4.1963e+25, 3.2757e+25, 1.4227e+25, 3.8608e+25,
        3.3492e+25, 2.4380e+25, 2.3677e+25, 2.1556e+25, 3.3438e+25, 2.1363e+25,
        3.5044e+25, 4.8829e+25, 5.1446e+25, 2.9139e+25, 2.0215e+25, 3.3405e+25,
        2.8082e+25, 4.0038e+25, 4.7809e+25, 2.3600e+25, 3.1099e+25, 2.7210e+25,
        4.1604e+25, 1.6656e+25, 3.9247e+25, 4.3848e+25, 2.6168e+25, 3.3552e+25,
        2.7043e+25, 1.8974e+25, 3.4571e+25, 3.7563e+25, 4.1107e+25, 2.3693e+25,
        6.6788e+25, 4.6900e+25, 2.9262e+25, 2.0851e+25, 2.4638e+25, 3.0170e+25,
        1.9975e+25, 2.4188e+25, 1.4607e+25, 2.3250e+25, 3.6692e+25, 3.5374e+25,
        2.9199e+25, 2.2479e+25, 3.8372e+25, 3.3543e+25, 3.4126e+25, 2.4127e+25,
        2.0163e+25, 2.4416e+25, 1.9852e+25, 5.6321e+25, 3.4053e+25, 2.3486e+25,
        3.2330e+25, 3.2547e+25, 3.1630e+25, 3.2169e+25, 2.4105e+25, 3.0007e+25,
        3.7187e+25, 2.6974e+25, 2.2525e+25, 3.7529e+25, 3.9197e+25, 2.5973e+25,
        3.7527e+25, 3.3293e+25, 2.9731e+25, 4.0745e+25, 3.3257e+25, 3.6348e+25,
        2.4244e+25, 4.9885e+25])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.4632e+25, 2.2883e+25, 2.0781e+25, 1.8753e+25, 2.3850e+25, 2.1918e+25,
        2.1599e+25, 2.8427e+25, 2.2923e+25, 1.5430e+25, 3.3483e+25, 2.3962e+25,
        1.9509e+25, 1.8057e+25, 3.3968e+25, 2.3113e+25, 2.4377e+25, 1.6567e+25,
        1.4008e+25, 2.8348e+25, 2.3571e+25, 2.3026e+25, 1.0885e+25, 3.2149e+25,
        2.4621e+25, 2.2607e+25, 4.5174e+25, 1.9758e+25, 3.5458e+25, 1.6091e+25,
        2.4463e+25, 3.1708e+25, 2.0300e+25, 2.6922e+25, 2.0838e+25, 3.0033e+25,
        3.1772e+25, 3.3970e+25, 2.5605e+25, 2.4837e+25, 1.0654e+25, 2.2773e+25,
        3.8450e+25, 2.4043e+25, 2.7152e+25, 2.4063e+25, 2.8331e+25, 2.5021e+25,
        1.7387e+25, 2.7329e+25, 1.3148e+25, 3.9027e+25, 2.5076e+25, 2.7989e+25,
        1.0853e+25, 2.4273e+25, 2.4401e+25, 3.6121e+25, 3.9288e+25, 4.5008e+25,
        1.9980e+25, 2.5529e+25, 1.8000e+25, 1.5277e+25, 2.1708e+25, 3.3202e+25,
        4.6049e+25, 1.7702e+25, 2.9952e+25, 2.1814e+25, 2.2292e+25, 2.1379e+25,
        3.6644e+25, 1.3590e+25, 1.8181e+25, 3.0558e+25, 2.6565e+25, 1.4329e+25,
        1.2436e+25, 1.9549e+25, 2.9904e+25, 2.6773e+25, 2.6464e+25, 2.0916e+25,
        3.5231e+25, 2.9421e+25, 3.4641e+25, 2.4587e+25, 2.9873e+25, 3.4463e+25,
        2.5739e+25, 3.6857e+25, 1.9178e+25, 3.4755e+25, 2.6332e+25, 1.7634e+25,
        3.3058e+25, 1.9884e+25, 1.9834e+25, 2.0174e+25, 2.3106e+25, 1.4988e+25,
        2.8006e+25, 3.6960e+25, 1.1966e+25, 3.4081e+25, 2.1043e+25, 3.6330e+25,
        2.7150e+25, 2.7659e+25, 3.2786e+25, 2.9115e+25, 2.9390e+25, 1.8595e+25,
        1.7814e+25, 3.6357e+25, 2.8081e+25, 3.4859e+25, 2.6151e+25, 2.7311e+25,
        2.4365e+25, 3.5319e+25, 2.1889e+25, 1.9376e+25, 2.5767e+25, 2.3781e+25,
        2.2638e+25, 1.8428e+25])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0758e+23, 1.2735e+24, 7.0005e+22, 1.9617e+23, 4.4424e+23, 1.8186e+23,
        6.2489e+23, 5.6484e+24, 1.9466e+23, 5.5190e+23, 2.5707e+23, 6.0032e+23,
        2.0248e+23, 6.3708e+22, 4.0997e+22, 6.0207e+22, 1.5127e+23, 2.8136e+23,
        9.6373e+23, 1.2465e+24, 2.2698e+23, 2.1850e+23, 5.7566e+23, 1.8946e+23,
        1.0843e+23, 1.0841e+23, 4.4668e+23, 7.0750e+22, 6.9717e+22, 5.6017e+23,
        5.1940e+22, 9.7044e+22, 7.8961e+22, 3.0879e+22, 1.1339e+23, 8.4567e+22,
        2.2948e+23, 1.5463e+23, 3.3898e+23, 4.7890e+22, 4.0031e+23, 7.0452e+22,
        2.9317e+23, 3.1623e+22, 2.6852e+22, 6.7065e+23, 1.5759e+23, 6.1350e+22,
        9.4487e+22, 7.4349e+22, 3.5794e+23, 6.1450e+22, 9.8566e+22, 6.1207e+22,
        8.4327e+22, 2.5383e+23, 1.3373e+23, 6.2261e+23, 4.5670e+22, 6.4400e+22,
        1.8070e+23, 5.0287e+23, 2.6269e+23, 3.2123e+23, 9.4728e+23, 2.1316e+23,
        4.8063e+23, 4.4642e+23, 3.5685e+23, 8.6224e+22, 1.7272e+23, 1.8744e+23,
        1.3278e+23, 3.7261e+23, 2.4893e+23, 8.6331e+22, 1.3258e+23, 2.7533e+24,
        2.1218e+23, 1.7034e+23, 7.7543e+22, 5.9098e+22, 2.8805e+23, 6.1335e+22,
        2.0427e+23, 7.6050e+22, 2.7009e+23, 5.2990e+23, 2.6310e+25, 8.0323e+22,
        5.6551e+22, 8.1620e+22, 1.4811e+23, 1.2771e+23, 3.6071e+23, 6.9505e+22,
        5.4970e+23, 2.2071e+23, 2.4085e+22, 8.6450e+22, 1.1688e+23, 2.1938e+23,
        7.0307e+22, 8.0683e+22, 3.0820e+23, 9.6133e+22, 5.8184e+23, 9.3814e+22,
        1.7563e+23, 3.4124e+23, 2.0008e+25, 1.0495e+23, 2.8021e+23, 1.0604e+23,
        7.8592e+22, 5.2039e+23, 5.6787e+23, 3.0394e+22, 1.3610e+23, 1.4180e+24,
        4.8179e+22, 9.8431e+22, 2.9495e+22, 7.8613e+22, 1.5364e+23, 6.7814e+23,
        1.9421e+24, 2.1764e+23, 8.0230e+22, 1.2858e+24, 1.2678e+23, 2.0796e+23,
        4.3179e+22, 1.0889e+23, 9.6548e+22, 1.7184e+23, 1.2718e+23, 7.7035e+22,
        2.8229e+23, 1.7896e+24, 7.7914e+22, 9.3653e+23, 1.2250e+23, 1.6952e+23,
        2.1617e+23, 2.1217e+22, 2.0923e+24, 1.3913e+23, 8.3318e+22, 7.5449e+22,
        6.4944e+22, 7.0190e+22, 2.3253e+23, 6.9548e+22, 1.0695e+23, 5.6139e+22,
        3.9551e+23, 6.6298e+23, 6.7802e+22, 3.3466e+22, 1.6411e+23, 3.2727e+23,
        9.8994e+22, 5.3956e+23, 6.4408e+22, 3.4042e+23, 1.8950e+23, 4.1213e+23,
        3.2743e+22, 2.1845e+23, 9.1121e+22, 4.2988e+22, 7.1270e+23, 1.3549e+23,
        7.5049e+22, 4.8841e+23, 1.3753e+23, 1.0358e+23, 9.6306e+22, 5.5527e+22,
        7.5826e+22, 1.8679e+23, 2.8045e+23, 6.3549e+22, 3.2205e+23, 8.7431e+22,
        5.8140e+22, 1.4804e+23, 7.6555e+23, 5.4336e+22, 9.0013e+22, 8.4701e+23,
        1.1216e+23, 7.2113e+23, 1.0028e+23, 2.1421e+23, 2.6052e+23, 1.5393e+23,
        8.9101e+22, 2.1518e+23, 1.5537e+23, 6.7934e+22, 3.5522e+23, 8.1521e+23,
        9.4578e+22, 8.0526e+22, 5.7839e+23, 2.0487e+23, 7.4340e+22, 6.1989e+22,
        1.8730e+23, 5.7172e+22, 1.0884e+23, 1.7766e+23, 7.7048e+22, 2.0581e+24,
        1.7817e+23, 7.9241e+22, 8.1590e+22, 4.7490e+23, 1.7545e+23, 1.4307e+23,
        1.0228e+23, 2.1900e+24, 3.3196e+23, 9.6889e+22, 4.3135e+22, 1.5812e+23,
        1.0108e+24, 1.2996e+23, 9.9608e+22, 7.2230e+22, 1.2219e+23, 6.4197e+23,
        1.4466e+23, 7.4007e+22, 3.4448e+22, 7.4749e+22, 9.7857e+22, 2.1986e+23,
        1.1872e+24, 1.0420e+23, 6.9494e+23, 7.3391e+22, 3.4089e+23, 1.3683e+23,
        9.5030e+22, 1.9494e+23, 3.4944e+23, 2.3501e+23, 4.5288e+22, 5.0788e+23,
        5.5618e+23, 3.6070e+23, 1.5601e+23, 2.5012e+24, 4.8510e+22, 1.3431e+25,
        8.8932e+22, 7.8118e+23, 3.0006e+23, 2.3242e+23, 4.6425e+22, 2.2985e+23,
        1.4772e+23, 2.4696e+23, 6.1984e+22, 8.0409e+22, 5.7398e+22, 9.3640e+22,
        5.1991e+23, 3.0710e+23, 1.0419e+23, 1.7450e+22, 3.0304e+23, 8.4227e+22,
        6.2895e+22, 3.4679e+23, 9.5776e+22, 1.1022e+23, 1.2340e+23, 9.4560e+22,
        3.8315e+23, 8.7929e+22, 5.4104e+22, 9.3349e+22, 2.5173e+23, 3.7686e+22,
        2.1695e+24, 4.3966e+22, 2.9371e+24, 3.0888e+23, 7.2984e+22, 2.1343e+22,
        6.1747e+22, 3.0982e+23, 3.8335e+22, 2.3868e+22, 6.6368e+22, 5.2517e+23,
        1.0379e+23, 1.7877e+24, 1.6983e+23, 1.6235e+23, 6.9683e+22, 1.0200e+23,
        5.7824e+22, 1.1163e+24, 1.9509e+23, 9.5570e+22, 1.9274e+23, 4.2414e+23,
        8.9309e+22, 1.3948e+23, 6.6164e+22, 6.5470e+22, 2.9713e+23, 4.4568e+24,
        1.0785e+23, 4.7588e+23, 6.7831e+22, 8.6631e+22, 5.6862e+23, 4.2028e+22,
        1.9261e+23, 5.8401e+22, 2.8531e+23, 1.3338e+23, 3.5111e+22, 4.4745e+22,
        1.5646e+23, 1.1287e+23, 1.1809e+23, 4.3527e+23, 9.8767e+22, 1.6738e+23,
        6.8886e+22, 1.3061e+23, 1.7212e+23, 7.7049e+22, 1.9507e+23, 1.3190e+23,
        6.4028e+22, 5.1999e+23, 1.8990e+23, 2.1607e+23, 1.3467e+23, 2.4635e+23,
        6.3696e+22, 3.3845e+23, 1.7287e+23, 1.4445e+23, 1.3988e+23, 7.4077e+22,
        6.9107e+22, 1.1182e+23, 1.4659e+23, 4.4688e+22, 2.1599e+23, 1.4360e+23,
        1.2335e+23, 6.4562e+22, 1.2941e+23, 1.9415e+23, 1.5489e+23, 2.1284e+23,
        5.5190e+22, 1.0976e+23, 1.2729e+24, 5.4988e+22, 1.1977e+23, 7.7354e+22,
        5.5291e+22, 1.7049e+23, 2.5330e+23, 5.7051e+22, 1.8175e+24, 1.3997e+23,
        7.0079e+22, 3.3113e+23, 3.0973e+22, 6.3326e+22, 3.7493e+22, 5.5036e+22,
        5.5056e+22, 5.5034e+22, 9.1541e+22, 2.6485e+22, 2.0206e+23, 1.5161e+23,
        7.6055e+22, 6.8802e+23, 2.4610e+24, 7.4607e+22, 8.0102e+22, 1.0198e+24,
        7.3231e+22, 1.6363e+23, 2.8953e+23, 3.3428e+23, 4.9038e+23, 1.1004e+23,
        1.9785e+23, 8.7071e+22, 5.3289e+22, 9.9793e+22, 7.2821e+22, 4.5008e+22,
        2.1100e+23, 1.1266e+23, 1.4138e+23, 3.3500e+23, 3.2131e+22, 5.3055e+23,
        1.3992e+23, 2.6613e+22, 6.6933e+22, 1.5864e+23, 3.7135e+24, 6.2952e+22,
        1.0106e+24, 1.7556e+23, 9.2930e+22, 1.1201e+23, 1.6734e+23, 3.7814e+22,
        1.0016e+23, 1.5202e+23, 1.4535e+23, 6.0867e+22, 2.6256e+23, 3.9711e+22,
        3.5301e+23, 6.1146e+23, 7.5174e+22, 9.0399e+22, 1.9073e+23, 1.1337e+23,
        8.5163e+22, 1.4020e+23, 1.1572e+24, 7.0576e+22, 1.4388e+23, 3.5897e+23,
        1.2885e+23, 2.4829e+24, 8.9114e+22, 4.5902e+23, 6.5271e+22, 2.1700e+23,
        3.2853e+22, 2.6737e+23, 1.5565e+23, 4.1497e+22, 3.9548e+22, 4.6555e+22,
        1.0247e+23, 4.8049e+22, 6.1359e+22, 1.0486e+23, 7.3996e+22, 1.8961e+22,
        8.4200e+22, 2.5140e+23, 1.8602e+23, 4.6325e+22, 1.0199e+23, 2.5630e+22,
        1.3152e+23, 1.0330e+23, 2.2797e+23, 5.2313e+22, 4.0656e+23, 5.1594e+22,
        3.3347e+23, 7.8220e+22, 1.1255e+23, 9.3449e+22, 1.3910e+23, 2.5130e+25,
        4.6353e+22, 6.0312e+22, 9.6810e+23, 9.7379e+22, 1.4148e+23, 3.5152e+22,
        1.1983e+23, 9.5273e+22, 2.7661e+22, 3.6995e+23, 1.8870e+24, 2.7978e+23,
        7.9831e+22, 2.0697e+23, 1.6781e+23, 7.7955e+22, 9.6346e+22, 3.8021e+22,
        1.6478e+23, 1.7413e+23, 1.3691e+23, 1.9925e+23, 8.5972e+22, 7.9719e+23,
        3.2462e+23, 6.0186e+22, 7.4312e+22, 7.0880e+22, 4.4768e+23, 9.7351e+22,
        3.0200e+22, 1.7726e+23])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.9351e+21, 1.2731e+24, 6.5930e+22, 1.5206e+23, 3.3745e+23, 1.7199e+23,
        5.2890e+23, 5.6159e+24, 8.5391e+21, 5.2241e+23, 2.1360e+23, 4.9259e+23,
        2.4421e+23, 6.0564e+22, 7.2588e+22, 1.6685e+23, 9.4360e+22, 2.2304e+23,
        9.6935e+23, 1.2546e+24, 1.5751e+23, 2.3545e+23, 5.4050e+23, 1.5240e+23,
        3.6439e+22, 3.3505e+22, 2.9600e+23, 2.6521e+22, 5.9124e+22, 5.8988e+23,
        1.4132e+23, 5.3461e+22, 8.3881e+21, 7.5043e+22, 3.4531e+22, 3.0344e+22,
        1.9726e+23, 4.9586e+22, 3.7917e+23, 2.9631e+22, 3.2045e+23, 3.0672e+22,
        2.3671e+23, 5.8819e+22, 4.0425e+22, 6.6736e+23, 1.1896e+23, 2.3111e+22,
        1.2774e+22, 5.1911e+22, 4.1543e+23, 1.4908e+22, 1.2347e+23, 9.9624e+22,
        1.1502e+23, 2.5758e+23, 6.0142e+22, 4.5037e+23, 7.6976e+22, 4.6635e+22,
        6.0809e+22, 4.3757e+23, 1.9072e+23, 3.0185e+23, 9.0428e+23, 1.5302e+23,
        4.7551e+23, 4.4332e+23, 3.7367e+23, 4.7511e+22, 6.7111e+22, 7.4970e+22,
        1.2228e+23, 4.5368e+23, 1.9373e+23, 9.9621e+22, 6.9842e+22, 2.7206e+24,
        2.0783e+23, 6.8432e+22, 7.5162e+22, 2.7488e+22, 3.7684e+23, 2.0050e+22,
        1.7408e+23, 1.2115e+22, 1.3786e+23, 6.3143e+23, 2.6369e+25, 4.4840e+22,
        4.2863e+22, 6.6724e+22, 1.8910e+23, 3.8891e+22, 3.1076e+23, 1.3980e+22,
        6.0022e+23, 1.0816e+23, 5.2830e+22, 1.0355e+23, 1.2089e+23, 1.9628e+23,
        1.0819e+22, 2.1667e+22, 3.0017e+23, 5.3983e+22, 5.0641e+23, 2.1765e+22,
        8.9551e+22, 3.3259e+23, 2.0046e+25, 1.2854e+22, 2.6167e+23, 5.3894e+22,
        4.9451e+22, 5.1425e+23, 5.9369e+23, 2.0161e+22, 8.3716e+22, 1.4170e+24,
        2.0614e+22, 4.4355e+22, 1.0187e+23, 1.1272e+23, 4.5363e+22, 6.5255e+23,
        1.9174e+24, 3.3889e+23, 1.6807e+23, 1.2350e+24, 2.1370e+22, 1.8364e+23,
        3.4767e+22, 2.8552e+22, 1.5350e+22, 5.2529e+22, 4.2998e+22, 4.0957e+22,
        2.2481e+23, 1.7725e+24, 4.7331e+22, 9.3218e+23, 4.8021e+22, 1.5708e+22,
        2.2316e+23, 5.7888e+22, 2.0278e+24, 2.0522e+22, 4.0247e+22, 1.9659e+22,
        4.2349e+22, 2.0977e+22, 2.7165e+23, 2.2215e+22, 4.9087e+22, 1.1660e+23,
        4.1365e+23, 7.8829e+23, 1.2245e+22, 5.2402e+22, 1.7793e+23, 2.7520e+23,
        2.1270e+22, 5.5736e+23, 5.6581e+22, 2.8399e+23, 2.0126e+23, 3.7058e+23,
        4.4252e+22, 2.0181e+23, 2.1312e+22, 2.5722e+22, 7.2064e+23, 9.8356e+22,
        7.6134e+22, 4.5628e+23, 8.7632e+21, 4.7327e+22, 1.5401e+22, 5.5005e+22,
        8.7258e+22, 1.1248e+23, 2.8154e+23, 3.9728e+22, 2.8799e+23, 8.0620e+21,
        4.0137e+22, 3.0167e+23, 7.1284e+23, 5.1699e+22, 3.7505e+22, 7.6192e+23,
        3.4127e+22, 6.5778e+23, 3.1505e+22, 1.9013e+23, 2.1262e+23, 6.4150e+22,
        3.4240e+22, 2.3773e+23, 2.3379e+23, 2.8977e+22, 3.4200e+23, 7.7744e+23,
        2.8002e+22, 2.8863e+22, 5.5408e+23, 1.7826e+23, 2.6781e+22, 1.1335e+22,
        1.7130e+23, 2.7043e+22, 2.0349e+22, 1.5177e+23, 7.4229e+22, 2.0623e+24,
        1.0595e+23, 7.9718e+22, 3.3604e+22, 5.4218e+23, 1.1062e+23, 1.3360e+23,
        5.6211e+21, 2.1500e+24, 2.4403e+23, 1.8961e+22, 5.6649e+22, 2.2646e+22,
        9.9786e+23, 2.1348e+23, 9.0300e+22, 1.5115e+23, 3.8106e+22, 6.1209e+23,
        1.2382e+23, 1.3836e+22, 4.9610e+22, 1.8241e+22, 3.5150e+22, 7.4551e+22,
        1.1354e+24, 3.7352e+22, 7.4934e+23, 1.4804e+23, 4.0700e+23, 1.5195e+23,
        3.7820e+22, 3.6232e+22, 2.5172e+23, 1.6489e+23, 8.3601e+22, 4.8001e+23,
        6.0883e+23, 2.6264e+23, 3.0309e+23, 2.5092e+24, 1.7557e+23, 1.3477e+25,
        3.7541e+22, 6.9965e+23, 2.5861e+23, 2.1771e+23, 3.1313e+22, 3.0361e+23,
        1.5908e+23, 2.7304e+23, 4.8476e+22, 4.6762e+22, 7.5813e+22, 3.7564e+22,
        4.7677e+23, 3.1880e+23, 9.7482e+21, 9.5650e+22, 3.3968e+23, 7.4119e+22,
        6.0869e+22, 3.8418e+23, 3.6736e+22, 6.4457e+22, 7.1425e+22, 9.9505e+22,
        4.8424e+23, 9.5687e+22, 7.2591e+22, 3.1810e+22, 2.4946e+23, 7.7756e+22,
        2.1383e+24, 7.4783e+22, 2.8774e+24, 2.9404e+23, 5.0740e+22, 5.6463e+22,
        9.7073e+21, 2.7939e+23, 1.7093e+23, 4.8116e+22, 9.1063e+21, 4.4478e+23,
        4.0084e+22, 1.9096e+24, 2.2219e+23, 8.6602e+22, 4.6576e+22, 5.4615e+22,
        3.1412e+22, 1.1138e+24, 1.5670e+23, 1.7278e+22, 1.1826e+23, 2.3909e+23,
        9.9874e+22, 1.2524e+23, 2.9083e+22, 2.8673e+22, 2.0992e+23, 4.4773e+24,
        3.7942e+22, 4.3697e+23, 4.3306e+22, 7.7272e+22, 5.3402e+23, 6.4955e+22,
        7.5062e+22, 1.4822e+22, 2.2840e+23, 2.6181e+22, 9.1629e+22, 7.2705e+22,
        8.2281e+22, 1.7060e+22, 2.5129e+22, 3.5864e+23, 2.8623e+22, 7.0308e+22,
        4.3450e+22, 1.8834e+22, 1.1217e+23, 6.5164e+22, 7.0450e+22, 1.1510e+23,
        9.4106e+22, 3.5301e+23, 1.3780e+23, 1.9525e+23, 2.4756e+22, 2.7610e+23,
        5.9100e+22, 2.8287e+23, 6.1970e+22, 8.0398e+22, 8.0467e+22, 1.0721e+23,
        2.8578e+22, 5.8197e+22, 5.6315e+22, 1.0342e+23, 2.2587e+23, 4.9763e+22,
        2.2267e+22, 1.7534e+22, 1.9183e+22, 3.0044e+22, 1.5556e+23, 2.7213e+23,
        1.8537e+22, 5.0684e+22, 1.2981e+24, 1.5408e+23, 1.7746e+22, 4.6342e+22,
        6.5924e+22, 1.7535e+23, 1.8353e+23, 1.7977e+22, 1.7486e+24, 1.0359e+23,
        7.1420e+22, 2.6293e+23, 5.7828e+22, 3.6664e+22, 2.2588e+22, 1.0322e+23,
        2.1693e+22, 9.9219e+21, 4.5538e+22, 9.9779e+22, 3.0717e+23, 1.2506e+23,
        9.0902e+22, 7.3405e+23, 2.4890e+24, 3.5814e+22, 4.1655e+22, 9.7201e+23,
        5.6574e+22, 8.5901e+22, 7.7401e+22, 3.5144e+23, 4.5173e+23, 7.6862e+22,
        9.3971e+22, 7.5800e+21, 1.0856e+22, 1.0275e+22, 2.2578e+22, 2.7572e+22,
        1.4174e+23, 1.9950e+23, 2.1956e+22, 1.8755e+23, 4.0607e+22, 5.9373e+23,
        2.8241e+22, 7.7224e+22, 6.4459e+22, 6.4327e+22, 3.6853e+24, 1.3960e+23,
        9.7217e+23, 1.8199e+23, 9.0932e+22, 9.3046e+22, 8.5968e+21, 6.2847e+22,
        2.4380e+22, 6.0057e+22, 3.3068e+22, 6.0094e+22, 2.8513e+23, 2.3592e+22,
        2.8944e+23, 5.5609e+23, 2.7652e+22, 7.3851e+22, 1.3177e+23, 5.9246e+21,
        4.2660e+22, 1.4506e+23, 1.1758e+24, 8.1880e+22, 8.4551e+22, 2.8413e+23,
        1.1407e+23, 2.4186e+24, 1.7796e+22, 5.2622e+23, 2.8566e+22, 2.2164e+23,
        6.8808e+22, 2.3036e+23, 2.4737e+22, 5.0003e+22, 9.6856e+22, 8.2008e+21,
        8.9074e+22, 4.0284e+22, 6.5053e+22, 1.2377e+22, 1.4856e+22, 4.4304e+22,
        1.2846e+22, 1.2382e+23, 1.1200e+23, 6.2428e+22, 1.1107e+23, 1.4729e+23,
        2.0713e+23, 1.6081e+22, 2.3627e+23, 5.3378e+22, 4.7523e+23, 2.8643e+22,
        2.4319e+23, 1.2324e+23, 9.0039e+22, 1.9608e+22, 3.1968e+22, 2.5131e+25,
        2.7009e+22, 3.0706e+22, 9.1217e+23, 1.5227e+23, 1.2186e+23, 4.7342e+22,
        4.6242e+22, 1.7919e+22, 8.9223e+22, 2.6591e+23, 1.8359e+24, 2.2215e+23,
        1.2242e+23, 1.0290e+23, 1.1263e+23, 1.5388e+23, 2.7300e+22, 7.8874e+22,
        3.5290e+22, 6.7397e+22, 5.7175e+22, 1.6388e+23, 2.3855e+22, 8.3756e+23,
        2.4361e+23, 2.9845e+22, 1.7203e+22, 2.5646e+22, 4.7887e+23, 1.5354e+22,
        4.3161e+22, 9.6314e+22])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.8501e+23, 1.8127e+23, 1.4449e+23, 1.9007e+23, 1.7115e+23, 1.8097e+23,
        9.2711e+22, 2.1052e+23, 1.8333e+23, 1.4842e+23, 2.2796e+23, 9.7502e+22,
        2.5454e+23, 9.5522e+22, 1.6754e+23, 1.5304e+23, 1.2358e+23, 9.7104e+22,
        1.8511e+23, 1.9804e+23, 9.9239e+22, 1.5198e+23, 9.5428e+22, 1.4470e+23,
        8.5247e+22, 1.7747e+23, 1.4345e+23, 1.5360e+23, 1.3407e+23, 1.6207e+23,
        1.5609e+23, 1.9226e+23, 1.5017e+23, 8.5415e+22, 1.9892e+23, 2.4779e+23,
        2.0188e+23, 1.7284e+23, 8.9859e+22, 9.5452e+22, 1.4946e+23, 1.0651e+23,
        1.1693e+23, 1.9420e+23, 1.1409e+23, 1.1334e+23, 1.9903e+23, 1.8834e+23,
        1.0025e+23, 1.2222e+23, 2.0251e+23, 2.6209e+23, 2.2758e+23, 1.6493e+23,
        2.0270e+23, 2.6372e+23, 1.0218e+23, 1.0650e+23, 1.7327e+23, 1.5061e+23,
        9.0068e+22, 2.1367e+23, 1.3443e+23, 1.2636e+23, 1.5268e+23, 1.2454e+23,
        8.9893e+22, 1.0824e+23, 2.0086e+23, 1.6081e+23, 2.1420e+23, 2.1021e+23,
        1.3617e+23, 1.0347e+23, 1.9522e+23, 1.1726e+23, 8.9995e+22, 6.8842e+22,
        1.8269e+23, 1.1825e+23, 1.3062e+23, 2.9777e+23, 1.1235e+23, 1.5395e+23,
        1.1433e+23, 1.4084e+23, 2.7696e+23, 1.5542e+23, 2.0774e+23, 1.8098e+23,
        2.3581e+23, 1.4041e+23, 2.4963e+23, 1.3379e+23, 1.8038e+23, 1.7057e+23,
        1.5312e+23, 1.3604e+23, 1.0832e+23, 1.2906e+23, 1.2111e+23, 1.3807e+23,
        1.3977e+23, 1.8755e+23, 1.4302e+23, 1.4503e+23, 1.8251e+23, 1.7494e+23,
        1.6753e+23, 1.2320e+23, 1.2552e+23, 1.3816e+23, 1.5544e+23, 1.7371e+23,
        1.0510e+23, 2.5275e+23, 1.6336e+23, 1.4777e+23, 1.2788e+23, 1.7985e+23,
        2.2127e+23, 1.5994e+23, 1.5028e+23, 1.6082e+23, 1.8980e+23, 1.2915e+23,
        1.7620e+23, 1.3311e+23])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0711e+23, 1.1080e+23, 1.1035e+23, 1.5881e+23, 1.8291e+23, 1.5042e+23,
        1.6004e+23, 1.5848e+23, 2.3570e+23, 1.0119e+23, 1.1181e+23, 2.0556e+23,
        1.1925e+23, 1.4161e+23, 9.5362e+22, 2.3137e+23, 1.3652e+23, 1.8877e+23,
        1.9078e+23, 1.3902e+23, 1.9046e+23, 9.4190e+22, 1.2463e+23, 1.4839e+23,
        1.1772e+23, 1.4499e+23, 7.5854e+22, 8.8699e+22, 1.6080e+23, 1.1091e+23,
        1.1314e+23, 1.3410e+23, 1.1184e+23, 1.1992e+23, 1.6251e+23, 8.4739e+22,
        1.9349e+23, 1.6684e+23, 7.8290e+22, 1.8534e+23, 1.7018e+23, 1.2916e+23,
        1.6633e+23, 1.5770e+23, 1.9553e+23, 1.3490e+23, 1.4971e+23, 1.4386e+23,
        6.1907e+22, 2.5478e+23, 1.0697e+23, 1.5535e+23, 1.2541e+23, 1.5693e+23,
        8.7062e+22, 1.3373e+23, 1.3930e+23, 1.7423e+23, 1.3632e+23, 1.6349e+23,
        1.5950e+23, 1.1313e+23, 1.8704e+23, 2.3301e+23, 1.1485e+23, 1.0925e+23,
        1.2733e+23, 1.8516e+23, 1.6453e+23, 1.1360e+23, 1.1488e+23, 1.7479e+23,
        1.3332e+23, 1.3552e+23, 1.6963e+23, 8.4948e+22, 1.3502e+23, 2.1482e+23,
        1.4627e+23, 1.1887e+23, 1.3543e+23, 1.2336e+23, 1.9535e+23, 1.6170e+23,
        1.3529e+23, 1.3178e+23, 9.1257e+22, 1.8176e+23, 1.7931e+23, 1.0620e+23,
        1.6283e+23, 1.2765e+23, 1.1070e+23, 1.5663e+23, 1.4259e+23, 1.6634e+23,
        1.4846e+23, 2.0135e+23, 1.5640e+23, 2.3578e+23, 1.8214e+23, 7.4985e+22,
        1.3989e+23, 1.3603e+23, 8.4419e+22, 1.4677e+23, 9.6755e+22, 2.3613e+23,
        9.3423e+22, 8.6466e+22, 1.0627e+23, 1.7499e+23, 1.6290e+23, 1.3445e+23,
        1.1155e+23, 1.3909e+23, 1.3510e+23, 1.2550e+23, 1.4914e+23, 1.4970e+23,
        1.3820e+23, 1.3262e+23, 9.7060e+22, 2.6794e+23, 2.8292e+23, 1.5908e+23,
        2.0000e+23, 1.3532e+23])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.9599e+20, 2.6049e+21, 4.6155e+21, 1.3949e+21, 2.8226e+21, 3.3662e+21,
        1.1796e+21, 9.0833e+21, 1.7241e+21, 2.7653e+21, 3.6417e+21, 2.8513e+21,
        4.2972e+21, 1.8981e+21, 2.2065e+22, 7.5854e+22, 1.9619e+21, 2.0193e+21,
        4.5781e+21, 2.7549e+21, 5.1742e+21, 6.6597e+21, 3.6952e+21, 1.2422e+21,
        3.2958e+20, 5.4904e+21, 1.8219e+21, 1.3321e+21, 2.9512e+21, 1.3037e+21,
        1.9983e+21, 1.3701e+21, 7.3243e+20, 4.8590e+21, 3.1603e+21, 1.3746e+21,
        1.7639e+21, 8.6859e+21, 4.5260e+21, 5.0061e+21, 2.5416e+21, 3.0997e+21,
        2.0478e+21, 2.2396e+21, 1.7979e+21, 2.0115e+21, 1.0998e+21, 1.3759e+21,
        2.0934e+21, 2.4675e+21, 2.2354e+22, 3.1708e+21, 1.1821e+21, 2.3213e+21,
        1.8082e+21, 1.6511e+21, 4.3956e+21, 7.0007e+21, 6.7002e+21, 3.5823e+21,
        2.6906e+21, 2.0270e+21, 3.1397e+21, 3.3036e+21, 1.1873e+21, 3.1474e+22,
        2.1001e+21, 1.6513e+21, 1.4664e+21, 3.2318e+21, 9.3960e+20, 7.4605e+21,
        9.4930e+20, 2.6814e+21, 9.7377e+20, 2.3657e+21, 3.5008e+21, 4.0532e+21,
        1.9795e+21, 3.3833e+21, 2.5877e+21, 3.5136e+21, 3.2226e+21, 2.3395e+21,
        9.4438e+21, 1.1634e+21, 4.6023e+21, 3.6326e+21, 4.5950e+21, 4.5829e+21,
        2.6039e+21, 4.3926e+21, 3.6266e+21, 3.2453e+21, 2.2551e+21, 1.5071e+22,
        3.3000e+21, 1.7631e+21, 7.7959e+21, 3.2370e+21, 5.1404e+21, 1.9072e+21,
        1.2940e+21, 2.0883e+21, 3.4995e+21, 4.2786e+21, 2.8836e+21, 3.2215e+21,
        5.4244e+21, 3.7358e+21, 2.6978e+21, 1.4411e+21, 2.2331e+21, 3.3569e+21,
        2.2098e+21, 5.2318e+21, 1.9020e+21, 3.2798e+21, 4.3923e+22, 5.9719e+21,
        3.8007e+21, 2.4784e+21, 1.1031e+21, 1.4464e+21, 2.5292e+21, 7.8192e+21,
        2.0339e+21, 2.6857e+21, 4.4081e+21, 2.5799e+21, 3.7350e+21, 7.0809e+21,
        3.3158e+22, 3.5476e+22, 3.6774e+21, 4.2892e+21, 3.2533e+22, 2.8306e+21,
        2.9002e+21, 3.8645e+21, 1.5160e+21, 2.6404e+21, 4.2048e+21, 3.3188e+21,
        5.0113e+21, 1.2949e+21, 3.2193e+21, 2.5148e+21, 3.0720e+21, 4.1927e+21,
        3.7927e+21, 7.2428e+21, 1.3601e+21, 1.7700e+21, 5.1594e+21, 2.7811e+21,
        8.6312e+20, 4.2748e+21, 1.2057e+21, 1.4516e+21, 1.2416e+22, 7.0261e+21,
        3.5523e+21, 1.7706e+21, 2.6785e+21, 6.7155e+21, 1.4992e+21, 1.0004e+22,
        1.8870e+21, 4.0341e+21, 9.5043e+20, 3.2691e+20, 1.8747e+21, 1.5203e+22,
        4.8862e+21, 2.6938e+21, 1.7277e+21, 4.5981e+21, 4.0074e+21, 2.9897e+21,
        1.9377e+21, 5.4215e+21, 3.4195e+21, 8.1073e+21, 5.8824e+21, 6.6030e+20,
        1.5058e+21, 4.0410e+21, 2.4062e+21, 1.1077e+22, 2.4556e+21, 4.4205e+21,
        4.4066e+20, 2.5698e+21, 3.9490e+21, 2.6099e+21, 2.9668e+21, 3.8010e+21,
        6.6248e+21, 3.7103e+21, 1.3444e+21, 8.1451e+21, 1.0319e+22, 3.1148e+21,
        3.3748e+21, 3.3643e+21, 1.5933e+21, 2.2453e+21, 1.7405e+21, 3.0241e+21,
        2.5261e+21, 4.1367e+21, 3.7164e+21, 3.9818e+21, 5.8574e+21, 2.5542e+21,
        3.3309e+21, 4.4681e+21, 2.9014e+21, 4.3956e+21, 2.7900e+21, 1.5529e+21,
        3.1658e+21, 2.8512e+22, 2.9326e+21, 1.2598e+21, 4.6755e+21, 2.3750e+21,
        3.1115e+21, 2.3926e+21, 2.1930e+21, 3.9073e+21, 1.9287e+21, 2.3481e+21,
        1.6155e+21, 5.8023e+21, 4.4758e+21, 1.6110e+22, 3.9887e+21, 1.5042e+21,
        4.8182e+21, 3.1686e+21, 4.8997e+21, 2.2516e+21, 3.1606e+21, 2.6650e+21,
        5.0581e+21, 1.7678e+21, 2.6396e+21, 4.0733e+21, 3.0237e+21, 5.0847e+21,
        2.6429e+21, 2.6031e+21, 4.5234e+21, 9.2880e+21, 2.1845e+21, 3.0281e+21,
        3.6598e+21, 2.9649e+21, 4.8438e+21, 3.6454e+21, 3.0451e+21, 1.7263e+21,
        3.7194e+21, 7.2227e+21, 4.0478e+21, 4.5165e+21, 2.2151e+21, 3.0004e+21,
        7.2126e+21, 2.4350e+21, 1.2258e+21, 4.2100e+21, 2.0575e+21, 1.8968e+21,
        4.4051e+21, 3.5221e+21, 6.1720e+20, 1.5434e+21, 2.4472e+21, 3.2910e+21,
        8.6639e+22, 3.3862e+21, 2.2181e+21, 4.9311e+21, 2.0641e+21, 1.8278e+21,
        4.1642e+21, 3.5911e+21, 2.8766e+21, 7.5638e+21, 1.4878e+22, 1.7541e+22,
        1.3475e+21, 1.1189e+22, 4.3379e+22, 1.5121e+21, 2.0891e+21, 1.2624e+22,
        5.9454e+21, 4.6727e+21, 1.3096e+22, 3.2730e+21, 1.6694e+21, 7.0525e+22,
        9.2535e+20, 3.0210e+21, 5.5066e+21, 4.5687e+21, 5.2524e+21, 2.5497e+22,
        5.6376e+21, 1.5764e+22, 1.1564e+21, 7.9633e+21, 1.0194e+23, 3.0645e+21,
        3.5175e+21, 1.3881e+21, 1.2419e+22, 3.6517e+21, 2.9244e+21, 2.2508e+21,
        5.1729e+21, 9.4940e+21, 5.0919e+21, 1.5802e+22, 3.2927e+21, 1.8243e+21,
        3.6557e+21, 4.6084e+21, 7.0977e+21, 1.9920e+21, 4.1007e+21, 3.5914e+21,
        1.7318e+21, 2.3386e+21, 3.3045e+21, 2.1104e+21, 3.1122e+21, 1.3327e+21,
        1.6436e+21, 2.1657e+21, 2.7102e+21, 2.3716e+21, 1.9911e+21, 2.7471e+21,
        5.2854e+21, 2.3399e+21, 3.6063e+21, 2.7234e+21, 4.2570e+21, 2.1817e+21,
        3.5906e+21, 2.7578e+21, 2.3141e+22, 6.6415e+21, 4.2247e+21, 2.5060e+21,
        5.6455e+21, 1.9237e+21, 1.0906e+21, 3.6676e+21, 9.7173e+20, 1.4681e+21,
        9.7486e+20, 3.8815e+21, 2.8880e+21, 2.1021e+21, 5.3322e+21, 2.2681e+21,
        6.1608e+21, 1.0162e+21, 1.4380e+22, 5.2421e+21, 2.9524e+21, 1.7329e+21,
        2.9881e+21, 4.7131e+21, 2.1544e+21, 2.9496e+21, 3.6091e+21, 2.9937e+21,
        2.4812e+21, 1.2637e+21, 3.5942e+21, 2.8884e+21, 7.0171e+21, 1.6399e+21,
        1.4544e+21, 3.4676e+21, 7.3779e+21, 1.3474e+21, 2.9657e+21, 2.1120e+21,
        9.8522e+20, 6.9230e+21, 4.5620e+22, 3.3841e+21, 7.2262e+21, 1.9796e+21,
        2.7509e+21, 2.2527e+21, 2.9658e+21, 7.4386e+21, 2.1242e+21, 2.1085e+21,
        3.2526e+21, 7.5391e+20, 1.0378e+22, 2.9890e+21, 1.0601e+22, 4.3926e+21,
        1.8740e+21, 1.4941e+21, 2.7349e+21, 2.2433e+21, 2.2562e+21, 2.6177e+21,
        1.2436e+22, 3.5134e+21, 4.1729e+22, 4.3318e+21, 3.4924e+21, 3.1672e+21,
        1.2532e+21, 1.9405e+21, 1.7285e+21, 9.8301e+20, 2.1436e+21, 1.5413e+21,
        4.5696e+21, 6.9611e+21, 2.2499e+21, 1.1000e+21, 2.2026e+21, 2.6072e+21,
        1.4455e+22, 5.3977e+21, 6.7590e+21, 7.4509e+21, 7.3837e+21, 2.4349e+21,
        1.9502e+21, 1.6218e+21, 1.5287e+21, 2.8196e+21, 2.0510e+21, 3.0168e+21,
        1.0525e+22, 2.2609e+21, 2.3515e+21, 1.5975e+21, 1.6191e+21, 1.8492e+21,
        2.0051e+21, 3.8800e+21, 3.9415e+21, 1.1378e+21, 2.7610e+21, 1.3171e+21,
        1.2208e+22, 1.6975e+21, 3.1619e+21, 1.4850e+22, 4.1059e+21, 1.6518e+22,
        2.0706e+21, 2.3319e+22, 3.4692e+21, 4.4622e+21, 4.5186e+21, 1.6607e+21,
        2.5837e+21, 4.1337e+21, 2.4105e+21, 6.5147e+20, 4.3252e+21, 3.2996e+21,
        5.5343e+22, 5.1093e+21, 5.0784e+21, 1.7030e+21, 4.0801e+21, 1.9831e+21,
        1.5127e+22, 1.8123e+21, 4.0771e+21, 1.7374e+21, 2.9663e+21, 2.1361e+21,
        3.2607e+21, 8.3272e+21, 3.1743e+21, 3.0593e+21, 1.1466e+21, 3.7310e+21,
        1.4155e+21, 2.0291e+22, 3.2642e+21, 5.1116e+21, 2.3715e+21, 1.0783e+22,
        2.1910e+21, 2.7479e+21, 2.0528e+21, 1.7219e+21, 2.3655e+21, 2.1953e+21,
        4.2244e+21, 2.2859e+21])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2248e+21, 5.2928e+21, 5.9908e+21, 3.4264e+21, 1.0399e+22, 7.4103e+21,
        3.0064e+21, 9.8825e+21, 7.8445e+21, 3.4972e+21, 3.3136e+21, 4.3341e+21,
        5.6037e+21, 5.1700e+21, 7.6081e+21, 5.1412e+21, 7.8126e+21, 8.5337e+21,
        5.2996e+21, 9.0482e+21, 1.5938e+22, 3.4935e+21, 4.9580e+21, 7.9344e+21,
        5.8273e+21, 6.5079e+21, 6.6381e+21, 5.1452e+21, 7.8867e+21, 4.0782e+21,
        9.5240e+21, 9.4276e+21, 6.0874e+21, 1.0897e+22, 7.8186e+21, 3.6937e+21,
        2.6664e+21, 6.2400e+21, 6.4450e+21, 4.2398e+21, 2.7811e+21, 4.7292e+21,
        5.9191e+21, 6.4200e+21, 1.0136e+22, 6.6414e+21, 5.7911e+21, 8.0717e+21,
        6.8767e+21, 5.5701e+21, 2.6197e+21, 6.5211e+21, 5.5263e+21, 4.2898e+21,
        2.6384e+21, 5.3078e+21, 4.8912e+21, 2.5374e+21, 5.2778e+21, 6.6249e+21,
        2.8584e+21, 5.4848e+21, 7.0138e+21, 4.0195e+21, 3.9449e+21, 1.0595e+22,
        7.8182e+21, 3.7970e+21, 6.5396e+21, 2.8291e+21, 2.8528e+21, 3.6575e+21,
        4.6070e+21, 1.1959e+22, 5.3518e+21, 3.9600e+21, 2.8477e+21, 5.5050e+21,
        6.6894e+21, 6.2261e+21, 7.3831e+21, 3.4935e+21, 8.2198e+21, 3.1756e+21,
        4.2662e+21, 5.6070e+21, 4.4220e+21, 7.5240e+21, 5.6500e+21, 5.6377e+21,
        7.3564e+21, 8.9777e+21, 8.2900e+21, 3.2690e+21, 2.5326e+21, 5.7581e+21,
        6.1821e+21, 5.5888e+21, 4.9183e+21, 6.3413e+21, 6.4578e+21, 2.3371e+21,
        7.5607e+21, 4.0251e+21, 5.0842e+21, 3.8741e+21, 7.1467e+21, 7.8206e+21,
        3.3351e+21, 3.4350e+21, 5.6821e+21, 6.6744e+21, 5.8153e+21, 3.5952e+21,
        3.0358e+21, 5.5306e+21, 4.7782e+21, 5.0999e+21, 4.1399e+21, 8.0018e+21,
        4.8447e+21, 5.1714e+21, 6.0505e+21, 6.0547e+21, 7.3263e+21, 5.6318e+21,
        8.8612e+21, 9.0584e+21])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.9407e+21, 3.0517e+21, 2.6109e+21, 3.0265e+21, 2.5305e+21, 3.9702e+21,
        4.9871e+21, 5.9110e+21, 6.7212e+21, 7.1522e+21, 6.8326e+21, 2.9076e+21,
        6.0050e+21, 2.2791e+21, 5.7509e+21, 5.5105e+21, 4.5748e+21, 6.6723e+21,
        7.1447e+21, 8.1871e+21, 3.0989e+21, 8.1449e+21, 5.1503e+21, 6.1906e+21,
        5.4984e+21, 4.8165e+21, 9.0363e+21, 3.3790e+21, 6.2008e+21, 2.3766e+21,
        7.4164e+21, 7.8859e+21, 4.0740e+21, 5.7428e+21, 4.6581e+21, 3.5049e+21,
        6.2874e+21, 3.3516e+21, 8.5964e+21, 4.0121e+21, 5.0846e+21, 1.8849e+21,
        6.6351e+21, 1.9002e+21, 6.4501e+21, 1.1184e+22, 6.2376e+21, 3.4907e+21,
        4.7525e+21, 7.5420e+21, 5.4153e+21, 5.8483e+21, 4.7008e+21, 5.5882e+21,
        1.0685e+22, 1.3257e+22, 2.7578e+21, 7.2213e+21, 2.1711e+21, 2.4885e+21,
        4.0457e+21, 6.0038e+21, 2.7895e+21, 6.2488e+21, 5.5976e+21, 7.4702e+21,
        5.7839e+21, 4.4017e+21, 4.1774e+21, 7.9702e+21, 4.9542e+21, 4.1400e+21,
        5.1070e+21, 3.4146e+21, 2.5139e+21, 3.3361e+21, 8.1005e+21, 2.1011e+21,
        6.5222e+21, 5.0685e+21, 6.6252e+21, 4.5411e+21, 4.5479e+21, 4.0419e+21,
        1.1018e+22, 4.2263e+21, 4.2169e+21, 6.2059e+21, 6.1408e+21, 5.1145e+21,
        5.5619e+21, 5.9510e+21, 5.8934e+21, 6.3140e+21, 3.8844e+21, 5.0786e+21,
        8.0006e+21, 3.1164e+21, 4.9755e+21, 5.8097e+21, 4.3463e+21, 3.6996e+21,
        5.0165e+21, 4.4522e+21, 6.3896e+21, 6.9074e+21, 7.2189e+21, 3.1681e+21,
        4.3815e+21, 6.2945e+21, 3.4342e+21, 3.8521e+21, 4.6572e+21, 4.5961e+21,
        4.2427e+21, 3.1959e+21, 2.2281e+21, 3.0610e+21, 3.1414e+21, 6.2048e+21,
        7.8323e+21, 2.3436e+21, 2.9507e+21, 1.0372e+22, 3.0581e+21, 7.9223e+21,
        4.7163e+21, 7.3696e+21])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.4229e+18, 1.1894e+19, 1.2847e+19, 1.6217e+19, 3.4499e+20, 2.1358e+19,
        2.7805e+19, 2.2007e+19, 4.2148e+18, 1.3942e+19, 4.0415e+19, 1.6857e+19,
        2.2974e+19, 6.5583e+18, 2.1529e+19, 4.5095e+19, 3.0684e+19, 1.6891e+19,
        1.8146e+21, 1.2606e+19, 1.5815e+20, 4.0657e+19, 3.3542e+19, 3.8255e+19,
        9.5052e+19, 2.8117e+19, 2.7774e+19, 1.0592e+20, 2.5521e+19, 5.6400e+19,
        7.5297e+19, 1.0355e+20, 1.3661e+19, 1.1750e+19, 7.8972e+18, 3.1515e+19,
        2.4714e+19, 4.7680e+19, 2.3671e+19, 2.4125e+19, 1.6375e+19, 1.3115e+20,
        3.1415e+19, 1.5691e+19, 3.1368e+19, 2.1718e+19, 2.8377e+19, 6.2435e+20,
        9.9771e+18, 2.8117e+20, 7.6837e+19, 5.1040e+19, 2.0027e+19, 5.6832e+18,
        6.1360e+18, 1.6333e+19, 1.7553e+19, 1.0253e+19, 2.0032e+19, 2.3286e+19,
        3.8270e+21, 4.4720e+19, 1.8565e+19, 1.4853e+20, 1.2244e+19, 1.4042e+19,
        1.6432e+20, 5.4039e+19, 1.5690e+19, 3.5362e+20, 2.0600e+19, 3.0932e+19,
        2.2725e+19, 6.4709e+19, 2.2796e+19, 3.2971e+19, 4.6841e+19, 5.6944e+19,
        1.8866e+19, 2.2938e+19, 4.1708e+19, 1.6485e+19, 3.1491e+19, 4.1954e+19,
        2.4492e+19, 2.2149e+19, 1.6442e+19, 2.7788e+19, 2.0190e+19, 1.7219e+19,
        4.3149e+19, 3.0002e+19, 1.6949e+19, 2.1364e+19, 2.3195e+19, 1.8432e+19,
        2.4692e+19, 1.4991e+19, 7.3836e+18, 4.9356e+19, 1.9337e+19, 4.1631e+19,
        5.9994e+19, 1.9584e+19, 4.7618e+19, 1.9341e+19, 9.8767e+20, 2.8565e+20,
        2.0416e+19, 2.6256e+19, 2.5140e+19, 1.5783e+19, 2.4372e+19, 2.6748e+19,
        2.1863e+19, 2.7132e+19, 1.2806e+19, 1.7521e+19, 2.9395e+19, 1.5913e+20,
        3.5878e+19, 1.5661e+20, 1.2332e+19, 1.3425e+21, 2.6571e+19, 3.0109e+19,
        2.9471e+19, 1.5656e+19, 1.2298e+19, 4.0073e+19, 3.5257e+19, 1.6907e+19,
        3.2751e+19, 1.6179e+19, 3.1591e+20, 2.9496e+19, 1.1535e+20, 1.5175e+19,
        1.5962e+20, 2.9216e+19, 2.1436e+19, 2.4919e+19, 1.8791e+19, 1.7295e+19,
        3.5938e+19, 3.8003e+19, 8.5917e+18, 2.7062e+19, 1.6973e+19, 2.1431e+19,
        1.3967e+19, 8.4488e+18, 3.8344e+19, 7.1541e+18, 1.8019e+19, 3.7447e+19,
        2.1686e+19, 1.2371e+19, 5.5203e+19, 2.7764e+19, 1.6458e+19, 3.6644e+19,
        1.1445e+19, 7.1012e+18, 2.9635e+19, 1.1179e+19, 2.7461e+19, 1.9784e+19,
        1.1732e+19, 1.8410e+19, 7.2830e+20, 4.1056e+20, 1.8712e+19, 2.3893e+19,
        1.7010e+19, 1.4687e+19, 1.0952e+19, 3.6643e+19, 3.3585e+20, 3.1977e+19,
        1.5947e+19, 2.2084e+19, 1.1841e+19, 1.4561e+20, 8.4716e+19, 3.1585e+18,
        1.7442e+19, 1.2071e+19, 2.7643e+19, 1.3677e+19, 9.0913e+18, 1.4636e+19,
        2.5296e+19, 3.6520e+19, 1.4999e+19, 3.4810e+19, 1.3572e+19, 3.0945e+19,
        3.6596e+19, 1.6106e+19, 2.1942e+19, 2.2836e+19, 2.1541e+19, 2.9414e+19,
        1.8925e+19, 6.8466e+18, 3.8141e+19, 2.2923e+19, 1.0203e+19, 1.9221e+19,
        1.4288e+19, 8.8349e+18, 9.5900e+18, 2.6473e+19, 2.9786e+19, 1.5193e+20,
        1.3698e+20, 1.3081e+19, 1.1678e+19, 2.4257e+19, 2.9099e+20, 3.7861e+19,
        9.5857e+18, 1.5210e+19, 2.1881e+19, 1.1727e+19, 3.1809e+19, 2.8808e+19,
        1.4485e+19, 5.7879e+19, 1.5113e+20, 1.0295e+19, 8.9900e+18, 1.2192e+19,
        2.1881e+19, 5.6707e+19, 3.3050e+19, 2.6558e+19, 2.0307e+19, 3.3619e+20,
        1.9580e+19, 3.4459e+19, 1.1610e+20, 1.5506e+19, 4.2773e+19, 4.2094e+19,
        1.7570e+19, 2.1006e+19, 4.7309e+20, 3.6919e+19, 1.5752e+19, 9.2168e+21,
        2.9398e+19, 2.0085e+19, 2.7550e+19, 3.8460e+19, 3.1241e+19, 1.9045e+19,
        1.2491e+19, 1.4332e+19, 3.7093e+19, 2.0089e+19, 3.5344e+19, 2.0143e+19,
        7.9596e+18, 2.7511e+19, 2.3042e+19, 1.3724e+19, 4.1843e+19, 1.7088e+19,
        3.6892e+19, 4.2987e+20, 7.6419e+19, 2.7760e+19, 1.0070e+19, 9.1073e+19,
        4.1174e+19, 3.3610e+19, 3.0034e+19, 3.3913e+19, 1.4833e+19, 1.6229e+19,
        1.5116e+19, 8.7183e+19, 2.1034e+19, 1.4529e+19, 5.2783e+20, 1.9591e+19,
        4.0076e+20, 1.4182e+19, 1.0934e+19, 4.1037e+20, 8.5243e+19, 1.6480e+19,
        3.4508e+19, 4.0349e+19, 4.1183e+19, 1.9356e+19, 1.6352e+19, 3.3742e+19,
        3.4154e+19, 1.3364e+19, 4.8682e+19, 2.0910e+19, 1.0523e+21, 5.3507e+19,
        7.4321e+18, 2.2783e+19, 3.7859e+19, 1.5036e+19, 2.1060e+19, 8.9325e+18,
        2.3569e+19, 1.0215e+20, 1.6703e+19, 1.4015e+19, 2.0510e+19, 2.2924e+19,
        1.0229e+20, 8.3984e+19, 1.4101e+19, 2.9365e+19, 9.4682e+18, 2.1374e+19,
        3.0718e+19, 1.2021e+19, 2.0441e+19, 4.6070e+19, 3.2128e+19, 3.1404e+19,
        3.4933e+19, 2.8357e+19, 3.9550e+19, 1.5350e+19, 4.6768e+19, 4.3533e+19,
        3.2873e+19, 6.5418e+18, 3.8620e+19, 1.3765e+19, 1.2789e+19, 3.6022e+19,
        1.1969e+19, 5.0566e+19, 1.0858e+20, 1.6634e+19, 3.6914e+19, 1.8293e+19,
        1.3866e+19, 1.1066e+19, 3.6056e+19, 2.1523e+19, 1.8394e+19, 3.5695e+19,
        5.7693e+18, 1.7439e+19, 2.0732e+19, 1.8015e+19, 8.9406e+19, 1.6244e+20,
        3.3242e+19, 8.2014e+19, 1.4660e+19, 2.0251e+20, 1.3305e+19, 1.8714e+19,
        8.6695e+18, 2.7345e+19, 2.3700e+19, 2.4729e+19, 2.6473e+19, 2.3079e+19,
        2.0119e+19, 2.0814e+19, 1.5179e+19, 1.7469e+19, 1.3190e+19, 4.2523e+19,
        1.0871e+20, 7.2750e+20, 3.9656e+19, 5.0953e+18, 1.5711e+19, 2.0765e+19,
        1.6597e+19, 7.8998e+18, 2.2515e+19, 3.9499e+19, 9.2352e+18, 6.5777e+18,
        3.4551e+19, 2.3473e+19, 2.8714e+19, 1.0854e+19, 2.0696e+19, 2.6805e+19,
        2.4258e+20, 1.1732e+19, 8.6507e+18, 1.3132e+19, 1.2898e+19, 2.8103e+19,
        1.0459e+20, 7.2038e+19, 9.9732e+19, 2.3342e+19, 1.1544e+19, 1.4535e+19,
        6.4385e+20, 1.8517e+19, 1.5299e+19, 1.7589e+19, 2.2862e+19, 1.5977e+19,
        3.7805e+19, 1.1689e+19, 3.3814e+19, 3.2081e+19, 3.0467e+19, 1.5969e+19,
        1.4579e+19, 1.5398e+19, 2.0794e+19, 2.7614e+19, 1.2715e+19, 4.2651e+18,
        3.1435e+19, 6.9584e+19, 2.7793e+19, 1.0405e+19, 1.8347e+19, 9.4031e+18,
        3.0751e+19, 3.7892e+19, 4.4371e+19, 1.2461e+19, 1.1872e+19, 6.0724e+18,
        2.3885e+19, 1.7530e+19, 1.9033e+19, 1.8343e+19, 1.8420e+19, 1.7581e+20,
        6.9579e+18, 1.6865e+19, 1.4911e+20, 7.3976e+19, 7.0508e+18, 1.4260e+19,
        1.5073e+19, 2.0711e+19, 9.4978e+18, 4.1009e+18, 2.1397e+19, 1.2301e+19,
        1.5428e+19, 2.8514e+19, 5.6170e+19, 3.1111e+19, 1.6996e+20, 1.4134e+19,
        4.5531e+20, 7.5237e+19, 3.5580e+20, 1.3566e+20, 3.9593e+19, 9.6199e+18,
        1.6268e+19, 1.2421e+20, 2.3033e+19, 3.3678e+19, 2.2195e+19, 1.9711e+19,
        1.5107e+19, 4.4624e+19, 1.8654e+19, 2.6284e+19, 1.9436e+19, 1.5234e+19,
        3.4794e+19, 2.3475e+19, 1.6128e+19, 1.3933e+19, 2.2819e+19, 3.3392e+19,
        3.0406e+19, 9.5272e+20, 6.3883e+19, 1.5300e+19, 2.2699e+19, 7.2927e+19,
        1.1447e+20, 2.2950e+19, 1.4154e+19, 1.1442e+19, 1.7984e+19, 7.1856e+19,
        1.2033e+19, 6.9826e+19, 1.1057e+20, 1.4484e+19, 1.1142e+19, 1.5494e+19,
        9.0989e+18, 1.1053e+19, 3.8223e+19, 1.7424e+19, 7.6693e+19, 1.8635e+19,
        1.2370e+19, 3.2965e+19])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.5010e+19, 7.7759e+19, 1.1055e+20, 7.8533e+19, 3.4547e+19, 4.5407e+19,
        4.8096e+19, 9.5075e+19, 5.4574e+19, 9.4749e+19, 2.8984e+19, 5.4558e+19,
        8.5982e+19, 5.6814e+19, 5.8321e+19, 6.5031e+19, 5.7853e+19, 2.9062e+19,
        8.6348e+19, 9.7328e+19, 1.0356e+20, 6.1808e+19, 9.3778e+19, 6.3805e+19,
        4.7743e+19, 5.6024e+19, 8.0904e+19, 4.3731e+19, 8.5829e+19, 4.8438e+19,
        4.6177e+19, 3.5317e+19, 4.2157e+19, 7.8344e+19, 3.5626e+19, 2.6335e+19,
        5.3097e+19, 5.5872e+19, 3.3576e+19, 3.6276e+19, 5.3187e+19, 4.6269e+19,
        3.3399e+19, 4.1889e+19, 6.1685e+19, 4.8981e+19, 3.8887e+19, 4.7299e+19,
        4.7636e+19, 5.2291e+19, 6.5347e+19, 6.0720e+19, 7.0741e+19, 6.7367e+19,
        9.6818e+19, 3.8395e+19, 4.3135e+19, 2.1840e+19, 3.7993e+19, 4.2219e+19,
        4.2623e+19, 3.9596e+19, 3.0362e+19, 6.4204e+19, 2.2193e+19, 4.3684e+19,
        9.5054e+19, 5.7034e+19, 3.4773e+19, 4.6146e+19, 6.2088e+19, 8.6425e+19,
        2.6588e+19, 5.7456e+19, 5.3932e+19, 3.3967e+19, 7.3200e+19, 3.7158e+19,
        4.0909e+19, 5.4964e+19, 2.2961e+19, 4.0595e+19, 7.4577e+19, 4.8472e+19,
        4.2709e+19, 5.9189e+19, 6.8776e+19, 2.3431e+19, 5.2886e+19, 7.3279e+19,
        4.7719e+19, 3.5506e+19, 6.6548e+19, 3.3900e+19, 5.1126e+19, 4.0338e+19,
        5.3541e+19, 7.2827e+19, 2.7269e+19, 5.1865e+19, 3.7030e+19, 3.4138e+19,
        8.1634e+19, 7.8921e+19, 3.3435e+19, 5.6623e+19, 6.7362e+19, 3.4844e+19,
        4.5625e+19, 6.0501e+19, 6.8922e+19, 2.7206e+19, 6.0321e+19, 6.2545e+19,
        3.9535e+19, 4.9299e+19, 6.7737e+19, 4.4202e+19, 4.5320e+19, 4.3951e+19,
        5.5657e+19, 3.7077e+19, 6.5366e+19, 2.7453e+19, 4.1717e+19, 6.7639e+19,
        6.8507e+19, 3.9120e+19])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.8344e+19, 3.9297e+19, 3.7411e+19, 2.0515e+19, 4.6861e+19, 2.5124e+19,
        2.1839e+19, 2.6714e+19, 4.3811e+19, 6.0868e+19, 4.5579e+19, 2.8656e+19,
        3.5255e+19, 5.4488e+19, 4.5047e+19, 3.1951e+19, 2.2026e+19, 4.0839e+19,
        1.0834e+20, 3.4240e+19, 6.0852e+19, 4.2016e+19, 3.6231e+19, 2.1505e+19,
        5.1425e+19, 5.2831e+19, 9.4435e+19, 5.3759e+19, 3.4764e+19, 5.6480e+19,
        3.7282e+19, 6.1700e+19, 6.0541e+19, 3.8637e+19, 4.2185e+19, 6.6093e+19,
        7.8958e+19, 6.9120e+19, 3.6818e+19, 2.4571e+19, 6.9147e+19, 5.6387e+19,
        3.1956e+19, 5.7981e+19, 1.1834e+20, 6.8448e+19, 3.4385e+19, 2.0068e+19,
        5.5889e+19, 2.6544e+19, 5.6004e+19, 5.8042e+19, 3.8789e+19, 1.6643e+19,
        7.0637e+19, 2.5300e+19, 6.0476e+19, 7.1983e+19, 3.4666e+19, 4.2610e+19,
        5.2978e+19, 3.8820e+19, 8.4402e+19, 6.3247e+19, 3.2852e+19, 3.0612e+19,
        5.8263e+19, 7.2428e+19, 5.8584e+19, 3.2812e+19, 3.3074e+19, 3.2346e+19,
        4.1184e+19, 6.9176e+19, 9.4446e+19, 5.2164e+19, 3.6923e+19, 7.1223e+19,
        9.0379e+19, 4.2823e+19, 4.2934e+19, 6.0928e+19, 5.8518e+19, 4.9541e+19,
        3.4465e+19, 6.8408e+19, 5.5545e+19, 6.1238e+19, 6.1450e+19, 3.2618e+19,
        5.6766e+19, 9.4058e+19, 3.9871e+19, 5.1616e+19, 1.0087e+20, 4.2933e+19,
        4.5495e+19, 8.3170e+19, 3.3976e+19, 4.7753e+19, 3.6243e+19, 3.8768e+19,
        3.6779e+19, 4.2953e+19, 4.7233e+19, 3.9369e+19, 6.4349e+19, 2.7334e+19,
        4.2354e+19, 3.5873e+19, 6.2107e+19, 5.6129e+19, 6.5835e+19, 7.5856e+19,
        2.6949e+19, 2.6154e+19, 4.8234e+19, 5.1572e+19, 7.3476e+19, 5.4707e+19,
        7.3036e+19, 2.4734e+19, 9.2417e+19, 4.6149e+19, 2.6287e+19, 5.1568e+19,
        4.6023e+19, 1.8213e+19])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2797e+17, 4.6357e+17, 2.4396e+17, 6.6938e+17, 9.7272e+17, 1.0486e+18,
        1.8915e+18, 4.0704e+18, 5.4103e+18, 1.0476e+18, 4.5522e+17, 7.5843e+16,
        1.2247e+18, 5.1131e+17, 1.5806e+18, 3.1354e+18, 5.2693e+17, 5.5066e+17,
        8.5636e+17, 7.6713e+17, 5.8247e+17, 2.1701e+18, 2.2841e+17, 1.0484e+18,
        9.7431e+17, 4.3985e+17, 1.7830e+18, 1.1748e+17, 6.5571e+17, 5.8512e+17,
        1.1197e+18, 8.7235e+17, 4.0102e+17, 6.1371e+17, 4.2775e+18, 2.5749e+18,
        4.2590e+18, 1.2533e+18, 1.9240e+18, 3.9829e+19, 6.6745e+17, 7.7095e+17,
        1.1154e+18, 1.3001e+18, 1.0495e+18, 1.4578e+18, 5.0721e+17, 7.1988e+17,
        3.8491e+17, 1.1562e+18, 4.8019e+17, 3.7112e+19, 4.0896e+18, 9.7665e+17,
        8.5039e+16, 6.6188e+17, 1.5857e+18, 3.2041e+18, 2.8215e+17, 1.3734e+19,
        2.8974e+17, 1.3895e+18, 2.5951e+17, 6.1203e+17, 3.8157e+17, 1.1030e+18,
        4.1275e+17, 3.6824e+17, 4.8442e+18, 5.3919e+17, 9.1955e+17, 2.3208e+17,
        3.3831e+17, 4.7864e+17, 6.5575e+17, 1.7930e+18, 3.5188e+17, 5.2597e+17,
        3.5398e+17, 8.8842e+17, 1.1022e+18, 1.2382e+18, 2.2727e+17, 3.5931e+17,
        1.1712e+18, 3.4944e+17, 1.0377e+18, 5.2381e+17, 1.0373e+18, 1.2007e+18,
        3.0018e+17, 1.3390e+18, 6.1045e+18, 4.5218e+18, 6.5353e+17, 9.2717e+17,
        9.5125e+17, 7.4094e+17, 1.9435e+18, 9.9790e+17, 8.9007e+17, 6.6213e+17,
        3.9193e+18, 1.4123e+18, 6.8187e+17, 2.4950e+17, 7.1758e+17, 3.5792e+17,
        5.7624e+17, 4.1718e+18, 3.7615e+17, 8.3051e+17, 3.3245e+17, 3.4027e+17,
        2.6354e+17, 7.4696e+17, 6.6435e+17, 9.4096e+17, 5.7365e+17, 8.6714e+17,
        1.9230e+17, 1.0078e+18, 1.3520e+18, 1.1108e+18, 8.1067e+17, 3.6118e+17,
        6.5945e+17, 6.9981e+17, 1.9220e+17, 1.0031e+18, 8.6242e+16, 1.8410e+17,
        1.2984e+18, 1.9854e+17, 3.2355e+18, 7.8300e+17, 9.9158e+17, 2.1167e+17,
        3.2067e+18, 7.3417e+17, 4.5889e+17, 2.3935e+18, 7.9422e+17, 1.7156e+17,
        1.3387e+18, 8.8183e+17, 1.2228e+18, 7.3226e+16, 2.2359e+17, 2.3252e+18,
        7.6043e+17, 4.3523e+17, 7.7942e+17, 8.7976e+17, 4.4543e+18, 1.0044e+18,
        9.4269e+17, 3.8074e+17, 1.4314e+18, 1.4887e+18, 9.7899e+17, 3.8294e+17,
        2.9485e+17, 2.7536e+18, 4.9693e+17, 7.2889e+17, 1.9618e+17, 5.1586e+17,
        2.3749e+18, 5.8924e+17, 8.9606e+17, 1.0780e+18, 2.5556e+18, 3.4408e+17,
        3.9754e+17, 2.9481e+18, 8.2971e+18, 3.8983e+17, 4.7155e+17, 3.4336e+17,
        2.2059e+17, 5.7027e+17, 1.4401e+18, 5.8720e+17, 2.2646e+17, 4.8629e+17,
        1.7811e+18, 4.6307e+17, 6.1226e+17, 8.8449e+18, 2.8834e+18, 3.7036e+17,
        1.1170e+18, 5.2243e+17, 1.4675e+18, 5.1978e+17, 8.7394e+17, 4.2800e+17,
        4.9685e+17, 7.7761e+17, 3.9099e+17, 2.0510e+17, 4.0374e+18, 1.7629e+18,
        1.8523e+18, 5.7930e+17, 6.4852e+17, 3.7748e+17, 4.3474e+17, 7.6289e+17,
        6.1886e+17, 2.1098e+17, 8.7436e+17, 1.2511e+18, 8.6577e+17, 3.3687e+17,
        8.9619e+17, 3.4504e+17, 5.9442e+17, 4.7791e+18, 1.1362e+18, 9.9166e+17,
        5.7147e+17, 5.9352e+17, 3.3135e+17, 6.7014e+16, 2.4192e+17, 1.7445e+18,
        3.9042e+17, 1.8862e+18, 7.8231e+17, 9.0456e+17, 7.5709e+17, 1.7659e+18,
        2.9878e+18, 4.3134e+17, 2.5427e+18, 8.7805e+17, 2.3263e+17, 9.4436e+17,
        1.0830e+18, 6.5247e+17, 3.8723e+17, 3.7031e+17, 3.7914e+17, 7.3827e+18,
        7.5464e+17, 2.9926e+17, 8.4568e+17, 1.9609e+17, 2.1249e+17, 3.1835e+17,
        3.5207e+17, 2.5216e+17, 9.7835e+17, 1.1726e+18, 3.7177e+18, 1.7376e+18,
        1.7101e+18, 6.8953e+18, 1.0045e+18, 1.2754e+18, 1.2603e+18, 3.0072e+17,
        5.6509e+17, 6.5271e+17, 1.3510e+18, 1.1617e+18, 1.2717e+18, 8.0154e+17,
        1.2490e+18, 1.8691e+18, 1.0389e+19, 3.2422e+17, 1.8825e+18, 6.2306e+19,
        3.9720e+17, 3.2204e+17, 4.3392e+17, 6.7451e+17, 9.8845e+17, 5.3980e+17,
        7.1384e+17, 1.3677e+18, 1.5169e+17, 7.5240e+17, 7.8325e+17, 3.7212e+17,
        6.0042e+17, 5.4133e+17, 6.4574e+18, 7.1597e+17, 8.8633e+17, 2.2950e+17,
        7.4262e+17, 8.6574e+17, 2.8962e+17, 1.0562e+19, 1.2641e+17, 1.9942e+17,
        9.1461e+17, 4.3304e+17, 6.2428e+17, 1.0429e+17, 9.1676e+17, 9.4053e+17,
        4.4754e+17, 9.8976e+16, 4.5958e+17, 4.4436e+18, 3.7746e+17, 9.7596e+17,
        3.0559e+17, 6.2416e+17, 5.1204e+18, 2.9667e+17, 4.2321e+17, 1.5456e+19,
        4.2136e+18, 3.1253e+17, 2.9629e+18, 6.6843e+17, 9.2960e+17, 1.8453e+18,
        4.1923e+17, 5.9617e+17, 6.9946e+17, 1.2913e+18, 1.4675e+18, 1.3045e+18,
        1.0645e+18, 1.8026e+17, 1.5512e+18, 1.1967e+18, 4.9908e+17, 6.1492e+17,
        5.2927e+17, 4.5430e+17, 7.3338e+17, 1.6133e+17, 6.1367e+17, 1.9723e+18,
        2.7490e+17, 1.0994e+18, 5.5106e+17, 1.1055e+18, 1.2471e+18, 2.8859e+17,
        1.4485e+18, 7.5919e+17, 8.3738e+17, 1.4562e+18, 1.5620e+18, 1.2741e+18,
        2.8174e+17, 1.3623e+18, 4.8591e+17, 1.2454e+18, 1.3606e+18, 2.4158e+18,
        1.3428e+18, 7.2404e+17, 1.9624e+18, 2.4023e+17, 6.5736e+17, 4.2800e+18,
        1.2169e+18, 1.5395e+17, 3.3733e+17, 1.4208e+18, 6.3283e+17, 7.4365e+17,
        8.0512e+17, 3.4353e+18, 1.4606e+18, 5.8833e+18, 1.5673e+18, 3.8549e+17,
        1.1398e+18, 7.1073e+17, 1.7548e+17, 4.2503e+17, 3.3923e+17, 9.6221e+17,
        2.0275e+18, 2.0679e+17, 2.7132e+17, 1.0005e+18, 6.1539e+17, 1.6818e+18,
        4.1840e+17, 4.9938e+17, 6.5952e+17, 1.9825e+17, 2.1023e+18, 8.4152e+17,
        7.0911e+17, 1.6519e+18, 6.2289e+17, 3.9597e+17, 2.6617e+17, 1.2725e+18,
        1.1183e+18, 6.5509e+17, 5.3622e+17, 9.3500e+17, 7.3186e+17, 1.6130e+18,
        2.2720e+17, 2.1744e+18, 4.2602e+17, 1.7325e+17, 2.1997e+17, 2.8576e+17,
        1.2574e+18, 2.9497e+17, 1.0653e+18, 1.1188e+18, 4.9020e+17, 7.8967e+16,
        7.0331e+17, 8.9310e+17, 1.1493e+18, 1.0271e+18, 1.1444e+18, 6.5126e+18,
        6.6108e+17, 6.0026e+17, 1.5428e+17, 2.4001e+17, 2.6670e+17, 2.1161e+17,
        7.6733e+17, 3.2630e+17, 1.0989e+18, 6.6953e+17, 2.8989e+18, 5.7066e+17,
        2.1287e+17, 3.4514e+17, 6.2252e+18, 8.2117e+17, 4.7904e+17, 1.9236e+17,
        4.8140e+18, 1.0009e+19, 2.0703e+17, 9.4380e+17, 4.6481e+17, 1.6046e+18,
        5.2975e+17, 1.2236e+17, 3.0845e+18, 4.7499e+17, 8.4924e+17, 1.4933e+17,
        1.5351e+17, 4.0457e+17, 8.4789e+17, 3.4716e+17, 1.5270e+18, 5.7144e+17,
        1.4773e+18, 1.0367e+18, 1.4730e+18, 7.1794e+17, 7.5158e+17, 1.6008e+18,
        4.7128e+17, 1.2114e+18, 1.8551e+18, 1.1112e+18, 4.4295e+18, 1.1806e+18,
        9.9983e+17, 4.9962e+17, 1.1027e+18, 3.1769e+17, 2.7132e+17, 1.3625e+18,
        3.1569e+18, 6.6493e+18, 5.2170e+17, 3.3125e+17, 1.2073e+18, 1.7720e+18,
        3.2512e+17, 4.5794e+18, 2.7496e+17, 9.4278e+17, 8.7998e+17, 3.6863e+19,
        1.0319e+18, 6.2257e+17, 7.2605e+17, 1.5821e+17, 4.8349e+17, 3.6387e+17,
        3.0211e+17, 4.3154e+17, 7.2181e+17, 6.3311e+17, 8.0917e+17, 2.5939e+17,
        6.8044e+17, 5.9935e+17, 3.6599e+17, 6.4750e+17, 6.0103e+17, 1.1669e+18,
        2.2090e+17, 8.2021e+17])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.2854e+17, 1.0424e+18, 1.7256e+18, 1.8836e+18, 1.1880e+18, 5.4510e+17,
        4.5774e+17, 7.5874e+17, 4.1151e+17, 9.1663e+17, 6.0286e+17, 1.9036e+18,
        5.8935e+17, 1.4731e+18, 1.7162e+18, 5.6406e+17, 6.7601e+17, 7.4239e+17,
        1.5650e+18, 1.2823e+18, 2.0923e+18, 1.7441e+18, 1.5117e+18, 6.6125e+17,
        5.0091e+17, 1.2595e+18, 1.0353e+18, 1.1452e+18, 4.8412e+17, 4.7042e+17,
        1.0120e+18, 2.6239e+18, 9.0484e+17, 1.2661e+18, 1.5054e+18, 5.8738e+17,
        3.6623e+17, 8.7826e+17, 5.9894e+17, 1.9623e+18, 3.0425e+17, 1.0245e+18,
        2.9623e+18, 7.8264e+17, 5.7632e+17, 2.7106e+17, 2.8220e+17, 8.1820e+17,
        1.2452e+18, 3.5495e+17, 1.0596e+18, 1.8544e+18, 8.2921e+17, 1.2730e+18,
        1.1782e+18, 6.4097e+17, 1.9351e+18, 2.6500e+18, 1.2542e+18, 2.4014e+18,
        1.0306e+18, 5.4354e+17, 1.5814e+18, 8.2661e+17, 2.2399e+18, 1.3539e+18,
        1.4614e+18, 9.1622e+17, 3.0718e+17, 2.0736e+18, 3.0353e+18, 2.5819e+18,
        5.6157e+17, 1.3186e+18, 5.6618e+17, 1.9890e+18, 9.3240e+17, 4.3241e+17,
        1.8093e+18, 4.6973e+17, 1.1585e+18, 7.9467e+17, 1.3826e+18, 4.8151e+17,
        4.9749e+17, 1.5579e+18, 8.2740e+17, 9.9602e+17, 2.7878e+17, 1.2176e+18,
        3.8715e+17, 7.1614e+17, 1.3406e+18, 1.0669e+18, 1.9024e+18, 1.7697e+18,
        5.3938e+17, 1.0363e+18, 1.7794e+18, 1.0776e+18, 6.4296e+17, 5.2333e+17,
        4.1651e+17, 1.8636e+18, 8.2706e+17, 1.1530e+18, 1.0477e+18, 4.8549e+17,
        8.7405e+17, 5.6627e+17, 1.2083e+18, 3.7692e+17, 4.1329e+17, 8.3998e+17,
        1.0631e+18, 1.2614e+18, 7.9086e+17, 1.1319e+18, 1.1043e+18, 1.5709e+18,
        8.5194e+17, 1.4544e+18, 1.7322e+18, 7.3115e+17, 2.0957e+18, 9.1171e+17,
        1.0854e+18, 5.7818e+17, 5.5782e+17, 1.6076e+18, 6.3475e+17, 5.9989e+17,
        8.7165e+17, 5.0984e+17, 4.0903e+17, 9.7581e+17, 6.6782e+17, 3.5555e+17,
        1.2306e+18, 6.5402e+17, 5.7413e+17, 3.7206e+17, 1.3340e+18, 7.6849e+17,
        7.0833e+17, 6.5967e+17, 6.7982e+17, 9.0192e+17, 1.3047e+18, 1.4354e+18,
        1.4316e+18, 9.7410e+17, 4.5514e+17, 9.3693e+17, 1.2148e+18, 5.1562e+17,
        3.4681e+17, 2.9387e+18, 1.8316e+18, 1.8169e+18, 8.8507e+17, 7.1139e+17,
        4.5452e+17, 1.4150e+18, 1.0490e+18, 1.0740e+18, 5.7699e+17, 5.6434e+17,
        4.0782e+17, 9.0657e+17, 1.5809e+18, 7.2322e+17, 5.6312e+17, 2.1554e+18,
        6.4894e+17, 1.1952e+18, 2.3270e+18, 1.0406e+18, 1.3637e+18, 4.1973e+17,
        1.3573e+18, 2.0263e+18, 1.1974e+18, 1.0891e+18, 1.2857e+18, 2.8766e+17,
        6.1259e+17, 6.5746e+17, 7.1949e+17, 2.1021e+18, 1.4751e+18, 1.4574e+18,
        2.1618e+18, 3.3048e+17, 9.4063e+17, 1.3855e+18, 1.0341e+18, 1.2339e+18,
        7.8984e+17, 1.2803e+18, 7.4601e+17, 2.2850e+17, 1.0319e+18, 6.3667e+17,
        2.0061e+18, 6.6352e+17, 1.2220e+18, 7.6717e+17, 7.6302e+17, 9.5489e+17,
        4.5374e+17, 6.8183e+17, 8.8017e+17, 8.1636e+17, 2.3296e+18, 5.4837e+17,
        1.8315e+18, 1.4267e+18, 1.6718e+18, 1.0841e+18, 1.7231e+18, 5.7698e+17,
        1.4141e+18, 5.0607e+17, 5.7924e+17, 3.0436e+17, 5.4987e+17, 1.6248e+18,
        1.3707e+18, 8.9246e+17, 1.5694e+18, 6.0934e+17, 4.0060e+17, 5.7368e+17,
        7.0976e+17, 4.9085e+17, 1.0664e+18, 6.0491e+17, 7.0482e+17, 9.6331e+17,
        1.0950e+18, 1.6484e+18, 1.8628e+18, 1.3176e+18, 5.3622e+17, 1.9683e+18,
        4.9035e+17, 6.7533e+17, 7.5282e+17, 8.6258e+17, 7.3718e+17, 6.8000e+17,
        1.5567e+18, 4.3450e+17, 6.2233e+17, 2.3425e+17])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.5572e+17, 8.8218e+17, 4.5799e+17, 1.0746e+18, 3.7337e+17, 4.3301e+17,
        1.0477e+18, 3.7920e+17, 5.6954e+17, 1.0717e+18, 6.3596e+17, 4.9281e+17,
        1.2489e+18, 2.6768e+18, 1.6948e+18, 2.7634e+17, 7.7258e+17, 6.5228e+17,
        9.5864e+17, 5.3648e+17, 6.3675e+17, 3.1337e+17, 4.3054e+17, 4.4054e+17,
        8.8125e+17, 6.2759e+17, 1.7190e+18, 1.1709e+18, 4.5020e+17, 4.8306e+17,
        9.3564e+17, 1.5679e+18, 1.3408e+18, 7.4520e+17, 9.2098e+17, 3.1413e+17,
        1.4997e+18, 6.7456e+17, 4.6351e+17, 5.1728e+17, 9.9364e+17, 1.3292e+18,
        1.2519e+18, 1.1297e+18, 1.1033e+18, 9.8796e+17, 1.4655e+18, 4.4546e+17,
        6.1130e+17, 4.4099e+17, 9.2450e+17, 1.6269e+18, 3.3787e+17, 1.6822e+18,
        2.0003e+18, 3.4494e+17, 1.1538e+18, 1.0640e+18, 5.0416e+17, 1.7356e+18,
        9.4689e+17, 1.3026e+18, 8.4083e+17, 1.3483e+18, 2.1846e+17, 4.1950e+17,
        2.1984e+18, 9.5161e+17, 1.0228e+18, 8.8704e+17, 8.3234e+17, 6.7263e+17,
        1.6003e+18, 8.4171e+17, 4.0452e+17, 5.1488e+17, 1.6189e+18, 1.5494e+18,
        6.0030e+17, 5.1725e+17, 1.8295e+18, 1.0610e+18, 1.2364e+18, 1.6467e+18,
        1.1265e+18, 1.0703e+18, 1.6385e+18, 5.7030e+17, 1.4568e+18, 1.0614e+18,
        4.5084e+17, 9.3955e+17, 1.4833e+18, 1.0712e+18, 1.3734e+18, 4.2917e+17,
        4.3985e+17, 7.4357e+17, 6.4753e+17, 5.9291e+17, 1.4066e+18, 1.5562e+18,
        4.9514e+17, 1.0738e+18, 3.0907e+17, 8.0641e+17, 7.6872e+17, 8.2439e+17,
        2.0026e+18, 1.0975e+18, 2.0090e+18, 1.1321e+18, 1.2776e+18, 6.0628e+17,
        8.1756e+17, 7.8013e+17, 5.6519e+17, 5.5932e+17, 6.4857e+17, 1.4340e+18,
        1.3677e+18, 1.6379e+18, 9.0151e+17, 8.3380e+17, 8.5831e+17, 4.7052e+17,
        8.3739e+17, 1.0999e+18, 7.6992e+17, 8.6683e+17, 9.2157e+17, 1.6656e+18,
        1.6451e+18, 3.4470e+17, 1.9501e+18, 1.9436e+18, 1.6453e+18, 4.4334e+17,
        1.2393e+18, 7.2430e+17, 1.4811e+18, 1.5982e+18, 5.7563e+17, 1.9302e+18,
        7.0983e+17, 7.6949e+17, 1.1468e+18, 2.1382e+18, 3.3100e+17, 1.3921e+18,
        1.0859e+18, 3.6598e+18, 2.0703e+18, 3.8087e+17, 7.2517e+17, 1.1113e+18,
        1.0244e+18, 3.7013e+17, 3.6816e+17, 1.3156e+18, 1.7133e+18, 1.0830e+18,
        3.5746e+18, 4.1142e+17, 1.4814e+18, 1.0893e+18, 2.5918e+17, 1.4942e+18,
        7.9337e+17, 4.4362e+17, 2.4472e+18, 4.3186e+17, 1.8253e+18, 1.0717e+18,
        1.2612e+18, 6.3798e+17, 5.2982e+17, 1.0189e+18, 4.6501e+17, 1.4082e+18,
        1.2269e+18, 2.8222e+17, 7.3164e+17, 1.2516e+18, 9.2274e+17, 1.5643e+18,
        7.6851e+17, 1.6579e+18, 7.7834e+17, 1.3064e+18, 6.1495e+17, 8.2160e+17,
        1.0379e+18, 1.1633e+18, 1.2308e+18, 4.9643e+17, 3.8939e+17, 5.1064e+17,
        1.1984e+18, 2.1107e+18, 9.9857e+17, 5.3161e+17, 7.3381e+17, 5.0002e+17,
        1.8557e+18, 1.7447e+18, 3.4043e+17, 6.3344e+17, 9.4088e+17, 3.4625e+17,
        1.2220e+18, 2.2757e+18, 7.7875e+17, 3.4787e+17, 9.6966e+17, 1.1499e+18,
        2.3355e+18, 2.3011e+18, 1.7583e+18, 3.0764e+17, 2.3442e+17, 2.4943e+17,
        1.2979e+18, 1.5830e+18, 7.2304e+17, 9.9127e+17, 7.1130e+17, 1.4396e+18,
        1.2353e+18, 1.3476e+18, 9.7950e+17, 1.8037e+18, 3.8345e+17, 7.7750e+17,
        2.8819e+17, 5.1298e+17, 7.6505e+17, 1.4638e+18, 5.8616e+17, 9.4989e+17,
        9.5002e+17, 4.6397e+17, 3.6640e+17, 6.6659e+17, 1.9470e+18, 2.7210e+17,
        9.3644e+17, 1.6126e+18, 1.1952e+18, 6.2957e+17, 8.2980e+17, 7.8113e+17,
        1.3571e+18, 1.1976e+18, 1.7893e+18, 5.8813e+17])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.5621e+14, 8.9465e+14, 1.8259e+15,  ..., 2.7282e+15, 2.4809e+15,
        6.4622e+14])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0752e+15, 2.5113e+15, 5.5581e+14,  ..., 7.3951e+14, 2.2349e+15,
        2.5576e+15])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.7114e+15, 1.8689e+15, 2.9009e+15, 1.7825e+15, 6.8954e+15, 4.1965e+15,
        2.7098e+15, 5.3366e+15, 4.6462e+15, 3.7868e+15, 2.1179e+15, 1.5430e+15,
        1.6268e+15, 2.1449e+15, 2.7503e+15, 2.0483e+15, 4.7187e+15, 3.4318e+15,
        4.1728e+15, 4.3342e+15, 1.6781e+15, 5.9147e+15, 3.0106e+15, 5.5602e+15,
        3.8822e+15, 2.0872e+15, 1.9208e+15, 3.9633e+15, 2.0258e+15, 6.7157e+15,
        5.0331e+15, 2.2307e+15, 6.0459e+15, 3.7320e+15, 3.6995e+15, 2.5731e+15,
        4.3730e+15, 3.9644e+15, 6.7583e+15, 4.5605e+15, 4.0787e+15, 4.8343e+15,
        1.9281e+15, 1.9734e+15, 2.6963e+15, 6.4681e+15, 7.7145e+15, 3.7047e+15,
        6.2986e+15, 6.2161e+15, 3.6617e+15, 2.3633e+15, 4.0838e+15, 2.3651e+15,
        5.2979e+15, 2.6218e+15, 6.8849e+15, 4.9073e+15, 2.1715e+15, 3.7004e+15,
        3.0021e+15, 4.1905e+15, 9.6512e+15, 5.0991e+15, 6.2730e+15, 5.2271e+15,
        2.6681e+15, 2.7669e+15, 3.8644e+15, 2.9721e+15, 5.5828e+15, 4.4492e+15,
        7.5108e+15, 1.6617e+15, 2.4460e+15, 3.6161e+15, 7.7245e+15, 6.5962e+15,
        2.7372e+15, 2.7410e+15, 3.1674e+15, 4.4661e+15, 4.8662e+15, 3.4374e+15,
        3.9983e+15, 1.9804e+15, 8.4567e+15, 2.1304e+15, 7.3791e+15, 2.5997e+15,
        1.7291e+15, 6.6606e+15, 6.9185e+15, 6.2739e+15, 1.7878e+15, 6.6103e+15,
        8.4457e+15, 4.7453e+15, 4.6125e+15, 3.5907e+15, 2.9585e+15, 3.6568e+15,
        4.1576e+15, 6.6360e+15, 2.8033e+15, 4.2176e+15, 1.3929e+15, 1.9872e+15,
        4.0073e+15, 2.9646e+15, 1.7727e+15, 3.5993e+15, 1.8273e+15, 4.3985e+15,
        2.5026e+15, 2.9554e+15, 4.9086e+15, 1.7449e+15, 2.0273e+15, 5.3427e+15,
        2.6060e+15, 3.3247e+15, 2.8303e+15, 2.6464e+15, 4.4775e+15, 4.6968e+15,
        4.1271e+15, 4.8847e+15, 1.9762e+15, 3.8279e+15, 2.8490e+15, 5.8568e+15,
        5.7590e+15, 1.3690e+15, 1.9450e+15, 4.1208e+15, 1.9331e+15, 6.7382e+15,
        3.9999e+15, 6.2608e+15, 9.1032e+15, 4.2752e+15, 5.3394e+15, 5.6659e+15,
        2.9980e+15, 3.8619e+15, 5.7217e+15, 1.8852e+15, 8.2769e+15, 4.2743e+15,
        5.0541e+15, 2.3701e+15, 9.6209e+15, 1.8989e+15, 5.0087e+15, 3.1867e+15,
        3.9429e+15, 4.3186e+15, 5.2860e+15, 5.3392e+15, 2.2636e+15, 3.4606e+15,
        2.5484e+15, 3.6193e+15, 1.9015e+15, 4.0998e+15, 4.3925e+15, 4.3261e+15,
        3.3922e+15, 8.2420e+15, 2.9239e+15, 8.5910e+15, 5.4701e+15, 2.4537e+15,
        2.9388e+15, 3.7456e+15, 2.3893e+15, 3.2503e+15, 3.3890e+15, 1.2246e+16,
        3.8159e+15, 4.8655e+15, 3.6852e+15, 4.2694e+15, 3.1922e+15, 4.0262e+15,
        3.6450e+15, 4.6730e+15, 2.2758e+15, 6.1573e+15, 3.1581e+15, 2.7316e+15,
        1.1049e+16, 4.1819e+15, 5.9416e+15, 2.0340e+15, 6.9693e+15, 8.1437e+15,
        6.2320e+15, 1.0367e+16, 4.1859e+15, 2.4169e+15, 7.2219e+15, 2.8888e+15,
        4.6600e+15, 2.6482e+15, 2.2342e+15, 4.6340e+15, 4.1508e+15, 3.3835e+15,
        1.9922e+15, 3.1331e+15, 1.6395e+15, 4.1932e+15, 4.9969e+15, 8.0163e+15,
        4.6826e+15, 2.7561e+15, 1.8179e+15, 7.4224e+15, 4.2401e+15, 4.4241e+15,
        6.9967e+15, 2.8049e+15, 1.9241e+15, 6.8781e+15, 2.0994e+15, 6.8586e+15,
        1.9331e+15, 5.0057e+15, 2.5402e+15, 1.9539e+15, 2.7642e+15, 2.1727e+15,
        2.2818e+15, 2.4074e+15, 3.1721e+15, 4.0037e+15, 7.7763e+15, 4.2421e+15,
        2.9364e+15, 2.6102e+15, 4.5002e+15, 3.9851e+15, 3.1133e+15, 6.0544e+15,
        2.6370e+15, 4.5310e+15, 4.3263e+15, 3.4016e+15, 6.3891e+15, 7.1975e+15,
        2.4100e+15, 2.7314e+15, 3.4954e+15, 1.7914e+15])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.1762e+15, 1.0574e+16, 9.6395e+15, 2.2422e+15, 5.3668e+15, 4.0629e+15,
        3.4609e+15, 3.9264e+15, 7.1218e+15, 3.1736e+15, 1.0934e+16, 3.5011e+15,
        4.5049e+15, 2.7813e+15, 3.8344e+15, 8.3026e+15, 5.0532e+15, 3.5914e+15,
        1.0166e+16, 8.3956e+15, 1.3085e+16, 3.4927e+15, 7.8353e+15, 5.4906e+15,
        8.5171e+15, 8.5099e+15, 1.1071e+16, 4.3289e+15, 4.1701e+15, 1.0542e+16,
        3.2171e+15, 1.1888e+16, 5.8281e+15, 1.0066e+16, 2.5125e+15, 2.3495e+15,
        3.3969e+15, 8.6811e+15, 1.0110e+16, 8.3713e+15, 2.5828e+15, 7.0556e+15,
        1.0280e+16, 3.2717e+15, 7.0039e+15, 9.0622e+15, 6.4745e+15, 3.8123e+15,
        3.2221e+15, 4.0041e+15, 7.3140e+15, 8.8024e+15, 5.8103e+15, 4.8230e+15,
        1.0182e+16, 3.2797e+15, 2.6616e+15, 4.7075e+15, 6.6418e+15, 6.2427e+15,
        8.3725e+15, 3.6303e+15, 5.0485e+15, 6.3543e+15, 3.6003e+15, 4.2453e+15,
        5.6241e+15, 6.4387e+15, 8.2166e+15, 3.6552e+15, 3.1167e+15, 6.6475e+15,
        2.9195e+15, 3.6887e+15, 6.7125e+15, 2.4694e+15, 5.2765e+15, 6.1953e+15,
        5.4432e+15, 3.9086e+15, 4.8140e+15, 5.9261e+15, 1.1518e+16, 2.7337e+15,
        5.3301e+15, 6.8028e+15, 6.0335e+15, 8.7018e+15, 3.4327e+15, 8.7909e+15,
        4.4004e+15, 5.8352e+15, 7.1360e+15, 2.3998e+15, 5.8264e+15, 9.4190e+15,
        1.2908e+16, 8.0817e+15, 8.4228e+15, 6.5463e+15, 2.6312e+15, 6.8453e+15,
        9.9238e+15, 5.3887e+15, 7.1967e+15, 6.8072e+15, 9.2602e+15, 5.6015e+15,
        3.1174e+15, 8.4423e+15, 5.4186e+15, 3.7863e+15, 2.8104e+15, 6.1619e+15,
        5.1602e+15, 2.3839e+15, 4.4136e+15, 3.8763e+15, 7.8882e+15, 5.8693e+15,
        3.6456e+15, 8.7325e+15, 5.9587e+15, 2.2859e+15, 6.8179e+15, 6.7557e+15,
        3.5170e+15, 3.8147e+15, 9.8745e+15, 5.2566e+15, 5.9475e+15, 4.5675e+15,
        8.1893e+15, 5.4284e+15, 6.7368e+15, 8.2767e+15, 4.8896e+15, 3.1118e+15,
        6.4831e+15, 3.5957e+15, 7.5146e+15, 5.2209e+15, 4.2252e+15, 2.9393e+15,
        4.4861e+15, 5.4174e+15, 6.5015e+15, 1.3420e+16, 1.3389e+16, 4.9859e+15,
        8.1536e+15, 3.7378e+15, 3.7461e+15, 6.3710e+15, 2.8324e+15, 4.0815e+15,
        2.1019e+15, 5.6772e+15, 3.9170e+15, 4.9902e+15, 6.2661e+15, 4.2619e+15,
        4.6592e+15, 5.4100e+15, 6.8952e+15, 4.3039e+15, 3.0837e+15, 1.0614e+16,
        3.1777e+15, 6.5343e+15, 3.1670e+15, 6.3063e+15, 2.5143e+15, 9.1612e+15,
        3.6448e+15, 5.4160e+15, 1.0432e+16, 5.4393e+15, 3.6574e+15, 7.6461e+15,
        5.7032e+15, 2.3902e+15, 7.5095e+15, 2.6131e+15, 7.1039e+15, 4.9300e+15,
        7.9757e+15, 4.7176e+15, 4.9522e+15, 3.4233e+15, 4.7577e+15, 6.1213e+15,
        1.8104e+15, 5.1102e+15, 1.4012e+16, 4.6922e+15, 2.3845e+15, 6.9492e+15,
        3.5756e+15, 8.2863e+15, 2.4803e+15, 5.3061e+15, 6.4018e+15, 8.0636e+15,
        3.5130e+15, 9.6917e+15, 4.3583e+15, 4.7793e+15, 1.0705e+16, 5.9029e+15,
        6.5883e+15, 3.2614e+15, 3.1582e+15, 3.6418e+15, 3.4094e+15, 8.1276e+15,
        6.7061e+15, 5.6737e+15, 1.0190e+16, 4.1487e+15, 4.1516e+15, 5.5316e+15,
        3.8681e+15, 2.4406e+15, 5.4976e+15, 2.5356e+15, 5.5652e+15, 4.7508e+15,
        3.4473e+15, 4.6984e+15, 5.1456e+15, 5.8518e+15, 3.1816e+15, 3.4687e+15,
        7.4398e+15, 4.9923e+15, 6.4578e+15, 2.8516e+15, 5.9111e+15, 2.2210e+15,
        1.0964e+16, 2.9781e+15, 3.5636e+15, 9.4401e+15, 2.8341e+15, 3.4097e+15,
        4.2469e+15, 1.3782e+16, 3.4193e+15, 2.4386e+15, 4.1509e+15, 1.1236e+16,
        4.7541e+15, 3.5283e+15, 3.3382e+15, 6.3581e+15])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.2118e+14, 2.1966e+13, 2.9650e+13,  ..., 5.6905e+12, 2.6540e+13,
        3.6569e+13])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.0700e+13, 7.5048e+13, 3.9702e+13, 7.3459e+13, 6.4524e+13, 5.7801e+13,
        6.6037e+13, 5.5702e+13, 5.1546e+13, 5.0628e+13, 4.3319e+13, 4.3984e+13,
        6.9292e+13, 4.6413e+13, 5.3952e+13, 4.1089e+13, 6.7178e+13, 5.6984e+13,
        4.3682e+13, 7.3177e+13, 7.0654e+13, 5.1011e+13, 7.2931e+13, 5.3564e+13,
        4.6219e+13, 5.3956e+13, 6.8848e+13, 4.1197e+13, 6.5244e+13, 4.1683e+13,
        7.5619e+13, 6.4479e+13, 6.0565e+13, 6.3781e+13, 4.9678e+13, 3.9096e+13,
        5.3579e+13, 8.1551e+13, 5.0898e+13, 5.2256e+13, 6.4592e+13, 6.0262e+13,
        6.1435e+13, 7.7844e+13, 6.5923e+13, 6.8378e+13, 6.0385e+13, 6.1322e+13,
        4.2418e+13, 5.4443e+13, 4.9407e+13, 7.1317e+13, 5.0470e+13, 6.1675e+13,
        4.1315e+13, 5.1190e+13, 4.3144e+13, 6.2553e+13, 6.1776e+13, 5.3482e+13,
        4.1190e+13, 4.2713e+13, 4.4264e+13, 5.4949e+13, 4.0319e+13, 5.2873e+13,
        5.2978e+13, 4.1408e+13, 4.1083e+13, 4.9080e+13, 5.8501e+13, 7.4608e+13,
        4.5207e+13, 4.6523e+13, 7.5643e+13, 6.3088e+13, 6.1516e+13, 5.1757e+13,
        5.4435e+13, 5.2510e+13, 4.9227e+13, 4.3753e+13, 5.9599e+13, 7.5742e+13,
        7.2264e+13, 7.3328e+13, 5.0989e+13, 5.1379e+13, 4.0001e+13, 4.2014e+13,
        4.4706e+13, 7.1255e+13, 7.1541e+13, 5.6557e+13, 6.1316e+13, 5.9637e+13,
        5.0514e+13, 5.7131e+13, 4.7976e+13, 6.5525e+13, 6.3393e+13, 4.3468e+13,
        4.8376e+13, 6.1823e+13, 6.5435e+13, 4.7228e+13, 5.6456e+13, 6.3006e+13,
        6.2629e+13, 6.3409e+13, 4.2911e+13, 5.6398e+13, 5.3646e+13, 8.6391e+13,
        6.3336e+13, 6.3396e+13, 4.2201e+13, 6.4715e+13, 7.0193e+13, 5.4214e+13,
        4.9377e+13, 4.4493e+13, 4.6054e+13, 6.5164e+13, 5.3579e+13, 7.8687e+13,
        4.6092e+13, 5.2745e+13, 6.8235e+13, 4.0804e+13, 5.4622e+13, 5.3610e+13,
        5.4702e+13, 4.6812e+13, 6.5254e+13, 6.0992e+13, 7.8415e+13, 6.5615e+13,
        7.2784e+13, 5.2694e+13, 5.3376e+13, 4.9258e+13, 4.8474e+13, 5.4289e+13,
        5.2452e+13, 5.0612e+13, 5.5605e+13, 5.4375e+13, 4.4159e+13, 5.1221e+13,
        5.5722e+13, 4.8158e+13, 5.0786e+13, 5.1564e+13, 5.2270e+13, 4.8175e+13,
        5.1716e+13, 5.0200e+13, 5.2035e+13, 8.5258e+13, 5.0821e+13, 4.2506e+13,
        4.8301e+13, 5.1652e+13, 4.9072e+13, 4.8396e+13, 4.2647e+13, 5.9420e+13,
        6.3327e+13, 5.9382e+13, 4.3823e+13, 6.4791e+13, 7.3145e+13, 4.7276e+13,
        6.7338e+13, 4.8875e+13, 4.3165e+13, 4.7557e+13, 4.4876e+13, 6.5016e+13,
        5.5552e+13, 4.9778e+13, 5.6073e+13, 7.1219e+13, 6.4604e+13, 4.7619e+13,
        6.2713e+13, 5.4543e+13, 5.5287e+13, 5.7494e+13, 5.0692e+13, 6.2389e+13,
        8.4193e+13, 4.2977e+13, 4.1580e+13, 3.4904e+13, 4.9543e+13, 6.5788e+13,
        6.2746e+13, 5.0391e+13, 7.5758e+13, 5.5852e+13, 7.5330e+13, 7.0913e+13,
        4.2127e+13, 6.4224e+13, 4.5857e+13, 8.5425e+13, 4.4334e+13, 4.8454e+13,
        4.7374e+13, 5.3912e+13, 6.7204e+13, 5.1065e+13, 5.4882e+13, 4.4642e+13,
        4.2228e+13, 7.1503e+13, 4.8124e+13, 4.7019e+13, 4.8387e+13, 7.7318e+13,
        8.4485e+13, 4.3385e+13, 7.2189e+13, 8.2780e+13, 4.1498e+13, 5.4798e+13,
        4.5276e+13, 4.9167e+13, 7.5923e+13, 5.2112e+13, 4.9907e+13, 8.2670e+13,
        4.3656e+13, 3.9636e+13, 5.6228e+13, 6.6912e+13, 5.2855e+13, 4.3752e+13,
        4.3000e+13, 4.7964e+13, 5.8054e+13, 5.8270e+13, 5.6553e+13, 6.0042e+13,
        4.2411e+13, 5.7692e+13, 6.2105e+13, 3.7312e+13, 6.2043e+13, 5.9419e+13,
        5.0483e+13, 3.6385e+13, 5.2727e+13, 5.1522e+13])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.8841e+13, 8.8319e+13, 7.4124e+13, 8.5960e+13, 1.0670e+14, 9.9219e+13,
        8.3177e+13, 4.6581e+13, 6.2775e+13, 7.5015e+13, 8.7134e+13, 6.2766e+13,
        5.5880e+13, 8.5690e+13, 8.9171e+13, 8.0728e+13, 1.2495e+14, 1.1567e+14,
        6.3169e+13, 6.3741e+13, 7.0925e+13, 1.0762e+14, 5.3863e+13, 6.5518e+13,
        8.4424e+13, 6.3813e+13, 6.0324e+13, 5.8091e+13, 7.2752e+13, 6.4848e+13,
        5.8656e+13, 1.3254e+14, 9.6463e+13, 6.1870e+13, 9.6158e+13, 7.9895e+13,
        6.1792e+13, 1.1523e+14, 5.7842e+13, 6.8535e+13, 6.7650e+13, 9.2676e+13,
        5.5199e+13, 7.3186e+13, 6.1993e+13, 8.6993e+13, 7.4386e+13, 1.0247e+14,
        8.7062e+13, 6.9850e+13, 8.3330e+13, 8.0870e+13, 8.3821e+13, 5.4622e+13,
        1.1667e+14, 7.3591e+13, 9.0593e+13, 9.3188e+13, 8.8477e+13, 8.0283e+13,
        8.2972e+13, 1.0304e+14, 7.9656e+13, 7.9941e+13, 6.2363e+13, 1.2053e+14,
        7.9643e+13, 8.2662e+13, 7.2772e+13, 8.7857e+13, 1.0664e+14, 5.5506e+13,
        1.1780e+14, 7.0035e+13, 7.5213e+13, 8.4050e+13, 6.0327e+13, 1.0956e+14,
        9.0043e+13, 8.0220e+13, 1.1041e+14, 5.7586e+13, 7.5675e+13, 7.7962e+13,
        7.3597e+13, 7.5623e+13, 8.6955e+13, 6.8282e+13, 5.5450e+13, 6.8843e+13,
        7.4451e+13, 4.7775e+13, 8.4902e+13, 7.6409e+13, 7.4711e+13, 7.5425e+13,
        9.6072e+13, 6.8549e+13, 6.1339e+13, 7.7058e+13, 9.3833e+13, 8.0516e+13,
        9.7835e+13, 1.0613e+14, 9.1929e+13, 7.9778e+13, 4.9262e+13, 6.8763e+13,
        6.3215e+13, 7.8322e+13, 6.9260e+13, 9.5398e+13, 7.6603e+13, 6.5056e+13,
        8.2422e+13, 6.0377e+13, 6.6504e+13, 6.5857e+13, 6.1074e+13, 8.0958e+13,
        5.8027e+13, 8.3865e+13, 7.0967e+13, 7.1180e+13, 9.2027e+13, 1.0845e+14,
        5.7455e+13, 6.4030e+13, 5.4205e+13, 8.4969e+13, 9.4160e+13, 1.1341e+14,
        6.7003e+13, 6.8025e+13, 7.0463e+13, 7.1136e+13, 6.5326e+13, 7.6618e+13,
        9.7388e+13, 7.8020e+13, 7.4354e+13, 9.0410e+13, 7.5858e+13, 9.3887e+13,
        9.5319e+13, 7.4641e+13, 9.4613e+13, 6.2716e+13, 8.1404e+13, 4.7682e+13,
        9.6856e+13, 6.9267e+13, 9.1574e+13, 9.4674e+13, 9.0095e+13, 9.4073e+13,
        1.2972e+14, 6.7189e+13, 7.9767e+13, 9.0568e+13, 6.8159e+13, 8.8293e+13,
        5.4680e+13, 5.9194e+13, 8.2088e+13, 5.8349e+13, 5.7974e+13, 5.6178e+13,
        9.3013e+13, 5.9013e+13, 6.4463e+13, 8.4173e+13, 7.2931e+13, 8.1449e+13,
        4.7351e+13, 6.2536e+13, 8.1075e+13, 7.2773e+13, 8.7395e+13, 6.8445e+13,
        6.3655e+13, 1.1006e+14, 9.6939e+13, 6.5202e+13, 7.0916e+13, 6.3726e+13,
        7.3943e+13, 6.8290e+13, 7.0990e+13, 7.2854e+13, 7.6677e+13, 9.3409e+13,
        7.8860e+13, 7.9325e+13, 1.2052e+14, 7.3181e+13, 9.1707e+13, 8.5856e+13,
        8.5800e+13, 8.4452e+13, 8.2605e+13, 7.8389e+13, 8.1003e+13, 7.6698e+13,
        1.1013e+14, 5.5843e+13, 7.8086e+13, 8.1332e+13, 7.0318e+13, 7.2504e+13,
        6.7127e+13, 1.0756e+14, 7.6801e+13, 7.8680e+13, 5.6523e+13, 6.3417e+13,
        1.0831e+14, 6.2412e+13, 6.2173e+13, 5.8022e+13, 7.6264e+13, 8.5763e+13,
        6.8143e+13, 6.6659e+13, 5.8925e+13, 7.1281e+13, 5.8582e+13, 9.3268e+13,
        6.3080e+13, 6.7181e+13, 9.8468e+13, 9.3220e+13, 9.0553e+13, 8.3035e+13,
        6.6083e+13, 5.5752e+13, 6.2296e+13, 7.3018e+13, 6.4263e+13, 8.2284e+13,
        5.1537e+13, 5.4288e+13, 6.5037e+13, 9.4779e+13, 6.8157e+13, 9.5121e+13,
        1.0074e+14, 1.0238e+14, 5.8284e+13, 6.5258e+13, 1.2953e+14, 6.5234e+13,
        5.4500e+13, 7.4494e+13, 7.7664e+13, 7.7841e+13])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.6678e+11, 8.6350e+11, 9.9181e+11,  ..., 6.7481e+12, 8.6473e+11,
        5.7765e+11])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6621e+12, 2.2501e+12, 2.3295e+12, 2.6347e+12, 1.9455e+12, 2.2791e+12,
        1.6253e+12, 1.3548e+12, 1.0787e+12, 1.8541e+12, 1.6085e+12, 1.7376e+12,
        1.2847e+12, 1.5749e+12, 2.2018e+12, 1.8193e+12, 1.1020e+12, 2.2974e+12,
        1.5642e+12, 1.1857e+12, 1.2644e+12, 1.4215e+12, 8.7667e+11, 1.2782e+12,
        1.8574e+12, 1.5887e+12, 1.7716e+12, 1.7769e+12, 1.8674e+12, 1.9367e+12,
        1.7626e+12, 1.3221e+12, 1.3066e+12, 1.9082e+12, 2.2274e+12, 1.8188e+12,
        2.0208e+12, 1.1624e+12, 1.2540e+12, 2.0540e+12, 1.1752e+12, 1.8076e+12,
        1.4661e+12, 1.7470e+12, 1.3846e+12, 1.6837e+12, 1.2830e+12, 1.3138e+12,
        1.9430e+12, 1.6757e+12, 2.3460e+12, 2.2049e+12, 1.2647e+12, 1.8673e+12,
        1.9613e+12, 1.9039e+12, 1.2155e+12, 1.5469e+12, 1.3315e+12, 1.7166e+12,
        1.4232e+12, 1.5579e+12, 1.5688e+12, 1.8402e+12, 1.1753e+12, 2.2367e+12,
        1.3470e+12, 1.0518e+12, 1.5713e+12, 1.2206e+12, 2.2574e+12, 1.4721e+12,
        1.8931e+12, 1.6342e+12, 1.6746e+12, 1.5808e+12, 1.3939e+12, 1.3163e+12,
        1.6128e+12, 1.4363e+12, 1.1649e+12, 1.7991e+12, 1.8444e+12, 1.1410e+12,
        1.8484e+12, 2.0300e+12, 1.5412e+12, 1.4896e+12, 1.7969e+12, 1.3898e+12,
        2.0610e+12, 1.4614e+12, 1.2491e+12, 1.7774e+12, 1.5983e+12, 1.2203e+12,
        1.7750e+12, 1.9272e+12, 2.2315e+12, 2.3474e+12, 1.7666e+12, 1.8869e+12,
        2.0880e+12, 1.5503e+12, 2.0312e+12, 1.9073e+12, 1.3465e+12, 1.5494e+12,
        2.0637e+12, 1.3219e+12, 2.0211e+12, 1.7661e+12, 2.3800e+12, 1.7683e+12,
        1.9775e+12, 1.9841e+12, 1.8447e+12, 1.2049e+12, 1.7041e+12, 1.7009e+12,
        1.9434e+12, 1.0608e+12, 1.1687e+12, 1.4377e+12, 1.5603e+12, 1.9929e+12,
        1.3679e+12, 2.8125e+12, 1.9220e+12, 2.0407e+12, 1.2589e+12, 1.4672e+12,
        1.4942e+12, 1.3862e+12, 1.7078e+12, 1.2387e+12, 2.2407e+12, 1.0846e+12,
        1.4674e+12, 1.6511e+12, 1.4939e+12, 1.7377e+12, 1.6662e+12, 1.4613e+12,
        1.3761e+12, 1.7583e+12, 1.1906e+12, 1.8892e+12, 1.4558e+12, 1.6509e+12,
        2.5124e+12, 1.4379e+12, 1.8647e+12, 1.3997e+12, 2.0228e+12, 1.8117e+12,
        1.3720e+12, 1.6562e+12, 1.4299e+12, 1.1039e+12, 1.7739e+12, 1.8893e+12,
        1.2098e+12, 1.0437e+12, 1.6872e+12, 2.0372e+12, 1.7742e+12, 1.9388e+12,
        2.0973e+12, 1.5868e+12, 1.4522e+12, 1.3264e+12, 1.4281e+12, 1.1825e+12,
        1.5911e+12, 1.4972e+12, 1.5285e+12, 1.6240e+12, 1.9287e+12, 1.1211e+12,
        1.6745e+12, 1.8903e+12, 1.4802e+12, 1.7772e+12, 1.7610e+12, 1.9614e+12,
        1.4796e+12, 2.5625e+12, 1.9876e+12, 1.6220e+12, 1.4917e+12, 1.9053e+12,
        1.6794e+12, 1.9270e+12, 1.6252e+12, 1.4026e+12, 1.7876e+12, 1.9952e+12,
        1.3855e+12, 1.3457e+12, 1.9741e+12, 2.1660e+12, 1.7261e+12, 1.2568e+12,
        1.1394e+12, 2.1547e+12, 2.1120e+12, 1.0532e+12, 1.5448e+12, 1.3841e+12,
        2.0424e+12, 1.1978e+12, 1.8839e+12, 1.4797e+12, 1.4215e+12, 2.3819e+12,
        2.0967e+12, 1.4474e+12, 1.3518e+12, 2.3834e+12, 1.5823e+12, 1.8351e+12,
        1.4198e+12, 1.5987e+12, 1.5194e+12, 1.3025e+12, 1.5283e+12, 1.8889e+12,
        1.0366e+12, 1.3039e+12, 1.5820e+12, 2.0954e+12, 1.5224e+12, 1.3318e+12,
        1.9246e+12, 1.9065e+12, 1.6503e+12, 1.8646e+12, 1.2231e+12, 2.2429e+12,
        1.6907e+12, 1.5806e+12, 1.4306e+12, 1.3187e+12, 1.9115e+12, 1.7832e+12,
        1.6998e+12, 1.4199e+12, 1.9676e+12, 1.4745e+12, 1.8641e+12, 1.2376e+12,
        1.5519e+12, 1.4198e+12, 2.2515e+12, 1.8372e+12])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2584e+12, 2.2452e+12, 2.7636e+12, 3.8510e+12, 2.7181e+12, 2.6894e+12,
        2.8428e+12, 2.7335e+12, 1.6914e+12, 1.8814e+12, 3.1346e+12, 2.5924e+12,
        2.1221e+12, 2.4380e+12, 2.5415e+12, 3.2187e+12, 2.9009e+12, 3.4459e+12,
        2.4562e+12, 1.9430e+12, 2.8387e+12, 2.4424e+12, 1.7836e+12, 1.7403e+12,
        2.1067e+12, 2.7506e+12, 1.7248e+12, 2.3211e+12, 2.0134e+12, 2.4956e+12,
        2.1601e+12, 2.0209e+12, 1.8518e+12, 2.5737e+12, 1.7519e+12, 2.4988e+12,
        2.0383e+12, 3.6134e+12, 2.3239e+12, 2.2027e+12, 2.4376e+12, 3.1143e+12,
        2.0159e+12, 2.3254e+12, 2.6465e+12, 2.2974e+12, 2.6156e+12, 2.2016e+12,
        2.0581e+12, 3.0661e+12, 2.1358e+12, 2.7729e+12, 3.1655e+12, 2.5676e+12,
        2.3027e+12, 1.8277e+12, 2.2547e+12, 2.5020e+12, 2.4708e+12, 2.8367e+12,
        2.9020e+12, 2.0903e+12, 2.9693e+12, 2.2163e+12, 2.2385e+12, 3.1532e+12,
        3.1114e+12, 2.5857e+12, 2.6174e+12, 3.3540e+12, 2.6634e+12, 2.8015e+12,
        3.0721e+12, 3.1145e+12, 2.7873e+12, 1.7382e+12, 2.6165e+12, 2.1262e+12,
        2.4451e+12, 2.8282e+12, 1.9394e+12, 2.9971e+12, 2.5204e+12, 2.6000e+12,
        2.7818e+12, 3.2023e+12, 2.9907e+12, 2.1226e+12, 2.2630e+12, 2.2007e+12,
        4.1866e+12, 2.9302e+12, 2.3276e+12, 2.1971e+12, 2.7635e+12, 2.0294e+12,
        2.6893e+12, 2.0927e+12, 2.2847e+12, 2.9578e+12, 3.2579e+12, 2.2229e+12,
        2.6402e+12, 2.9792e+12, 2.8768e+12, 4.7143e+12, 2.9686e+12, 2.3161e+12,
        2.3397e+12, 3.4185e+12, 1.8176e+12, 1.8198e+12, 2.2638e+12, 2.1587e+12,
        2.3133e+12, 1.8278e+12, 2.1680e+12, 2.1206e+12, 2.0187e+12, 1.9790e+12,
        2.9489e+12, 2.5041e+12, 2.5667e+12, 2.9534e+12, 2.2452e+12, 2.7069e+12,
        1.5943e+12, 1.4713e+12, 2.0756e+12, 2.1784e+12, 2.7727e+12, 3.0557e+12,
        2.4357e+12, 3.6302e+12, 2.4301e+12, 2.5719e+12, 2.9539e+12, 2.3198e+12,
        2.4528e+12, 1.9449e+12, 2.6640e+12, 2.0828e+12, 2.0719e+12, 3.1772e+12,
        2.9783e+12, 2.3375e+12, 2.4794e+12, 2.8013e+12, 3.1883e+12, 2.9433e+12,
        2.3289e+12, 2.1747e+12, 3.0479e+12, 2.5965e+12, 2.3906e+12, 2.0729e+12,
        2.3991e+12, 1.7991e+12, 2.5577e+12, 2.3346e+12, 1.9383e+12, 2.3567e+12,
        2.2155e+12, 1.9514e+12, 2.3555e+12, 2.3370e+12, 2.0239e+12, 2.5999e+12,
        2.8336e+12, 1.5202e+12, 2.8047e+12, 2.9544e+12, 2.2760e+12, 1.6528e+12,
        2.9851e+12, 2.8600e+12, 2.4103e+12, 2.0772e+12, 2.7765e+12, 2.0158e+12,
        1.9688e+12, 2.9024e+12, 2.4143e+12, 1.8520e+12, 2.2107e+12, 2.5338e+12,
        1.7504e+12, 2.0169e+12, 3.0933e+12, 2.4712e+12, 1.9422e+12, 2.4187e+12,
        2.1199e+12, 2.4246e+12, 2.4604e+12, 2.4195e+12, 2.8042e+12, 2.3851e+12,
        3.0457e+12, 2.9374e+12, 2.8166e+12, 3.4922e+12, 2.6145e+12, 2.5865e+12,
        2.5621e+12, 2.2847e+12, 2.3355e+12, 3.7675e+12, 1.8311e+12, 2.7543e+12,
        2.6507e+12, 3.2633e+12, 2.1435e+12, 2.7056e+12, 2.7405e+12, 2.7974e+12,
        2.1424e+12, 2.3937e+12, 2.3504e+12, 2.8064e+12, 2.8246e+12, 2.6824e+12,
        2.5725e+12, 2.0181e+12, 2.8080e+12, 2.0516e+12, 2.5326e+12, 2.8874e+12,
        2.0150e+12, 3.2279e+12, 1.8957e+12, 1.4686e+12, 2.8888e+12, 2.5327e+12,
        2.6698e+12, 2.1462e+12, 2.7810e+12, 2.1344e+12, 2.8675e+12, 2.3248e+12,
        2.9388e+12, 1.9606e+12, 1.8675e+12, 1.9919e+12, 2.0900e+12, 2.6592e+12,
        2.1819e+12, 2.3769e+12, 3.0997e+12, 2.5351e+12, 2.1493e+12, 1.6345e+12,
        2.6407e+12, 2.1163e+12, 3.5458e+12, 2.7108e+12])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.3302e+10, 1.3447e+11, 2.6182e+11,  ..., 1.7243e+10, 2.7025e+10,
        2.2725e+10])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.4367e+10, 4.4963e+10, 5.7871e+10, 5.5470e+10, 5.4933e+10, 5.0834e+10,
        5.3663e+10, 8.2565e+10, 5.4177e+10, 5.6907e+10, 6.7351e+10, 4.5069e+10,
        5.4510e+10, 5.9624e+10, 5.2991e+10, 5.2190e+10, 5.5974e+10, 4.2311e+10,
        5.1305e+10, 4.6827e+10, 4.1773e+10, 4.5511e+10, 4.3492e+10, 5.4125e+10,
        6.4792e+10, 5.7393e+10, 4.2458e+10, 8.6587e+10, 3.9688e+10, 6.1656e+10,
        5.4181e+10, 4.4888e+10, 5.7140e+10, 5.0760e+10, 4.0918e+10, 5.0123e+10,
        6.6564e+10, 8.2366e+10, 5.2991e+10, 6.4931e+10, 3.7068e+10, 5.0652e+10,
        5.5493e+10, 7.0549e+10, 4.4086e+10, 6.7918e+10, 5.2924e+10, 5.5737e+10,
        7.0211e+10, 8.2715e+10, 3.9948e+10, 6.4472e+10, 4.6237e+10, 6.4068e+10,
        5.6201e+10, 5.2851e+10, 7.8440e+10, 4.9590e+10, 4.4769e+10, 3.7326e+10,
        6.2582e+10, 7.3299e+10, 3.2519e+10, 3.1683e+10, 5.8534e+10, 7.3730e+10,
        6.1000e+10, 6.9827e+10, 5.0506e+10, 6.2454e+10, 4.6997e+10, 5.9451e+10,
        5.6505e+10, 5.3900e+10, 6.4055e+10, 6.3996e+10, 8.7577e+10, 4.7094e+10,
        7.5896e+10, 6.3819e+10, 7.9627e+10, 4.7280e+10, 5.9819e+10, 4.1289e+10,
        6.0248e+10, 3.6906e+10, 3.6520e+10, 7.7923e+10, 7.1760e+10, 4.6282e+10,
        6.2042e+10, 4.9387e+10, 4.9094e+10, 6.7813e+10, 3.5329e+10, 5.1783e+10,
        5.4862e+10, 5.1621e+10, 3.6350e+10, 5.5954e+10, 6.8046e+10, 7.8674e+10,
        6.5192e+10, 5.7010e+10, 6.9071e+10, 5.8368e+10, 7.7308e+10, 4.6207e+10,
        6.2662e+10, 6.5825e+10, 6.0384e+10, 5.0674e+10, 4.3093e+10, 4.6901e+10,
        4.7129e+10, 5.2389e+10, 5.2990e+10, 4.3784e+10, 6.6872e+10, 3.6834e+10,
        5.4978e+10, 8.9528e+10, 5.0293e+10, 8.3378e+10, 5.5317e+10, 4.9298e+10,
        5.6653e+10, 5.1387e+10, 3.4131e+10, 5.7510e+10, 5.9975e+10, 6.3957e+10,
        5.1139e+10, 3.8700e+10, 6.2592e+10, 4.6279e+10, 6.2474e+10, 8.0128e+10,
        5.4788e+10, 7.2437e+10, 5.0571e+10, 4.7007e+10, 4.2130e+10, 6.1720e+10,
        4.8062e+10, 7.0858e+10, 7.6821e+10, 5.4184e+10, 7.5668e+10, 5.2623e+10,
        4.4092e+10, 6.0524e+10, 7.4105e+10, 5.7730e+10, 7.3801e+10, 4.5766e+10,
        6.0657e+10, 4.7731e+10, 3.1591e+10, 7.4720e+10, 7.7836e+10, 8.4625e+10,
        4.1319e+10, 3.8056e+10, 5.7598e+10, 3.4300e+10, 4.9179e+10, 4.8895e+10,
        3.9265e+10, 3.2064e+10, 5.1962e+10, 5.8774e+10, 5.3946e+10, 6.1977e+10,
        6.4182e+10, 4.8977e+10, 4.6363e+10, 4.9410e+10, 5.4395e+10, 5.0948e+10,
        6.3451e+10, 5.8354e+10, 7.4548e+10, 5.3147e+10, 6.5197e+10, 5.4602e+10,
        7.5502e+10, 6.9833e+10, 5.6350e+10, 4.5124e+10, 4.2096e+10, 5.6786e+10,
        7.4019e+10, 6.8780e+10, 6.9137e+10, 7.0283e+10, 7.6048e+10, 7.2281e+10,
        7.4438e+10, 7.8622e+10, 4.0460e+10, 5.4033e+10, 5.3543e+10, 6.5212e+10,
        6.0756e+10, 9.0783e+10, 6.4216e+10, 4.8553e+10, 5.9845e+10, 5.8309e+10,
        8.1294e+10, 7.3497e+10, 7.8552e+10, 7.0554e+10, 4.9646e+10, 5.5282e+10,
        6.1203e+10, 3.9936e+10, 4.2300e+10, 5.4933e+10, 5.7235e+10, 6.0140e+10,
        5.4165e+10, 5.3783e+10, 5.3241e+10, 4.8383e+10, 4.9963e+10, 5.6148e+10,
        5.1372e+10, 5.9902e+10, 5.0544e+10, 4.0045e+10, 3.6896e+10, 6.3872e+10,
        4.2460e+10, 5.3498e+10, 5.4686e+10, 7.5858e+10, 9.2169e+10, 6.8294e+10,
        5.7320e+10, 5.3192e+10, 7.5181e+10, 5.3407e+10, 7.4199e+10, 4.3864e+10,
        3.6589e+10, 5.8440e+10, 5.4817e+10, 7.4441e+10, 4.8538e+10, 3.2974e+10,
        3.9138e+10, 6.1268e+10, 3.2581e+10, 5.9995e+10])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.2061e+10, 7.0247e+10, 8.4536e+10, 6.9028e+10, 7.9084e+10, 8.7318e+10,
        6.9588e+10, 8.9119e+10, 7.4904e+10, 9.7028e+10, 6.2347e+10, 7.2380e+10,
        1.1934e+11, 7.5047e+10, 5.3621e+10, 1.2815e+11, 1.6623e+11, 1.2148e+11,
        1.1112e+11, 7.8313e+10, 9.9532e+10, 8.8279e+10, 7.0658e+10, 7.2798e+10,
        5.9053e+10, 9.1590e+10, 1.1842e+11, 7.7470e+10, 9.2264e+10, 6.8290e+10,
        7.5714e+10, 7.1258e+10, 7.3525e+10, 8.5492e+10, 1.0313e+11, 1.0232e+11,
        7.5558e+10, 1.0196e+11, 1.0418e+11, 8.8743e+10, 6.3683e+10, 1.1023e+11,
        6.8418e+10, 8.0893e+10, 7.8246e+10, 6.9957e+10, 7.4620e+10, 7.2982e+10,
        8.5841e+10, 6.6185e+10, 1.1548e+11, 1.0537e+11, 8.2324e+10, 7.8087e+10,
        8.0417e+10, 7.6809e+10, 1.1119e+11, 8.7393e+10, 8.2700e+10, 6.7223e+10,
        5.6187e+10, 6.7761e+10, 9.6399e+10, 5.4249e+10, 6.4599e+10, 7.5933e+10,
        6.9663e+10, 1.0205e+11, 4.9606e+10, 7.6309e+10, 1.1310e+11, 6.8201e+10,
        7.7029e+10, 1.0376e+11, 7.0204e+10, 9.6124e+10, 8.6278e+10, 1.2182e+11,
        1.1217e+11, 6.3929e+10, 8.9065e+10, 9.4343e+10, 8.3379e+10, 1.0046e+11,
        8.7367e+10, 7.7496e+10, 5.7654e+10, 1.0740e+11, 6.9541e+10, 7.1135e+10,
        1.0465e+11, 4.9713e+10, 1.0213e+11, 1.0298e+11, 1.1953e+11, 8.5977e+10,
        6.5276e+10, 1.1382e+11, 6.8049e+10, 7.9096e+10, 1.0174e+11, 1.0537e+11,
        8.7007e+10, 8.5129e+10, 9.6695e+10, 8.1465e+10, 7.7807e+10, 7.0171e+10,
        1.1362e+11, 1.0695e+11, 1.0310e+11, 9.9728e+10, 6.9487e+10, 6.5965e+10,
        1.1361e+11, 1.1833e+11, 8.9787e+10, 5.8187e+10, 8.6121e+10, 9.4505e+10,
        1.1833e+11, 9.0165e+10, 7.2600e+10, 7.2220e+10, 9.7340e+10, 8.7287e+10,
        7.8015e+10, 1.1622e+11, 1.2172e+11, 8.5334e+10, 9.5665e+10, 6.3527e+10,
        8.7938e+10, 8.1033e+10, 7.1231e+10, 5.6719e+10, 6.9940e+10, 9.4507e+10,
        1.4030e+11, 1.2582e+11, 1.1032e+11, 1.0160e+11, 9.7550e+10, 8.0183e+10,
        1.0356e+11, 7.6803e+10, 1.0201e+11, 1.1039e+11, 6.3154e+10, 6.5127e+10,
        6.8899e+10, 7.2141e+10, 9.5493e+10, 9.2601e+10, 6.7860e+10, 9.9680e+10,
        5.4692e+10, 8.1914e+10, 7.7003e+10, 9.1048e+10, 8.8685e+10, 7.2476e+10,
        1.0800e+11, 9.0006e+10, 5.6617e+10, 9.0562e+10, 1.1635e+11, 1.0549e+11,
        8.7810e+10, 7.9501e+10, 5.4178e+10, 6.6521e+10, 8.5413e+10, 7.3297e+10,
        9.7748e+10, 7.6006e+10, 5.1589e+10, 8.6495e+10, 6.8560e+10, 9.1943e+10,
        6.3054e+10, 1.1116e+11, 9.5608e+10, 8.2234e+10, 1.1007e+11, 7.3134e+10,
        7.2067e+10, 1.2027e+11, 9.1395e+10, 6.9474e+10, 6.3063e+10, 8.1982e+10,
        9.7056e+10, 7.6215e+10, 6.8964e+10, 6.8934e+10, 6.2112e+10, 9.2893e+10,
        7.2660e+10, 8.2630e+10, 9.1394e+10, 1.0695e+11, 1.0203e+11, 1.0664e+11,
        1.1763e+11, 8.6172e+10, 8.3359e+10, 8.0242e+10, 1.1670e+11, 7.2093e+10,
        7.3533e+10, 1.1122e+11, 8.5590e+10, 1.0346e+11, 8.2493e+10, 9.4535e+10,
        1.1853e+11, 8.9207e+10, 7.0161e+10, 7.9812e+10, 1.2826e+11, 1.1574e+11,
        8.1261e+10, 1.0122e+11, 9.4682e+10, 1.0000e+11, 1.0855e+11, 1.0404e+11,
        1.1196e+11, 6.1078e+10, 7.8747e+10, 1.4499e+11, 1.1114e+11, 8.2021e+10,
        1.2147e+11, 1.0534e+11, 8.5278e+10, 8.4387e+10, 1.1681e+11, 8.5661e+10,
        9.4524e+10, 1.2862e+11, 5.9715e+10, 6.4795e+10, 1.5986e+11, 6.4034e+10,
        8.1921e+10, 1.0632e+11, 1.3212e+11, 1.0753e+11, 9.6722e+10, 1.0644e+11,
        7.7624e+10, 7.0641e+10, 1.0246e+11, 7.7661e+10])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.1890e+08, 5.4463e+08, 6.3755e+08,  ..., 6.8446e+08, 1.0670e+10,
        4.1880e+08])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.8074e+08, 1.6589e+09, 1.1884e+09, 1.6310e+09, 1.2897e+09, 1.1564e+09,
        1.6369e+09, 1.4468e+09, 1.9661e+09, 1.8444e+09, 2.2500e+09, 1.3350e+09,
        1.5485e+09, 1.6341e+09, 1.6640e+09, 1.4093e+09, 1.3659e+09, 1.6440e+09,
        1.3247e+09, 2.3387e+09, 9.9468e+08, 1.1744e+09, 1.1318e+09, 1.7053e+09,
        1.3534e+09, 1.9048e+09, 1.1217e+09, 2.2898e+09, 9.6836e+08, 1.5559e+09,
        1.5308e+09, 1.4794e+09, 1.5950e+09, 1.1146e+09, 1.1150e+09, 1.3090e+09,
        2.2514e+09, 1.3920e+09, 1.2345e+09, 1.6202e+09, 1.2037e+09, 1.9230e+09,
        1.6610e+09, 1.3104e+09, 1.7557e+09, 1.7200e+09, 1.6008e+09, 1.7457e+09,
        1.3352e+09, 2.3974e+09, 1.0948e+09, 1.0338e+09, 1.5500e+09, 1.5648e+09,
        1.4576e+09, 1.8102e+09, 1.4733e+09, 1.3734e+09, 1.3332e+09, 9.9141e+08,
        1.1659e+09, 1.4414e+09, 1.2121e+09, 1.1548e+09, 1.1795e+09, 1.2962e+09,
        1.5135e+09, 1.3589e+09, 1.2907e+09, 1.5745e+09, 1.8755e+09, 1.6279e+09,
        1.8877e+09, 1.5738e+09, 1.0185e+09, 1.3444e+09, 1.5176e+09, 1.4156e+09,
        1.3885e+09, 1.6064e+09, 1.0047e+09, 1.4683e+09, 1.8067e+09, 1.3420e+09,
        1.5875e+09, 1.4342e+09, 1.4307e+09, 1.3735e+09, 1.1064e+09, 1.1933e+09,
        1.0358e+09, 1.3846e+09, 1.5892e+09, 1.6629e+09, 1.3934e+09, 1.0973e+09,
        1.8778e+09, 1.8327e+09, 1.1715e+09, 1.7295e+09, 2.3772e+09, 1.3943e+09,
        1.0207e+09, 1.6667e+09, 1.6444e+09, 1.4730e+09, 1.4741e+09, 1.0354e+09,
        9.2464e+08, 8.8267e+08, 1.4801e+09, 1.3980e+09, 1.3994e+09, 1.5640e+09,
        1.0744e+09, 1.6572e+09, 1.5266e+09, 1.3627e+09, 1.4236e+09, 1.7975e+09,
        1.0089e+09, 1.2010e+09, 9.2380e+08, 1.3943e+09, 1.3952e+09, 1.6808e+09,
        1.8482e+09, 1.5387e+09, 1.7616e+09, 1.2848e+09, 1.0666e+09, 1.5287e+09,
        1.2635e+09, 1.0193e+09, 1.3960e+09, 1.1342e+09, 1.8310e+09, 1.2944e+09,
        1.2703e+09, 1.1616e+09, 1.3867e+09, 1.5874e+09, 1.6835e+09, 1.4793e+09,
        1.2198e+09, 1.6214e+09, 1.3185e+09, 1.2187e+09, 1.4521e+09, 1.8708e+09,
        1.5051e+09, 1.9577e+09, 1.6647e+09, 1.5923e+09, 1.3440e+09, 1.4215e+09,
        1.6767e+09, 1.3930e+09, 1.7839e+09, 1.0051e+09, 1.4781e+09, 9.9903e+08,
        1.5854e+09, 1.3280e+09, 1.5128e+09, 2.1307e+09, 1.1632e+09, 1.0930e+09,
        2.1542e+09, 1.2220e+09, 1.6663e+09, 1.5224e+09, 2.1285e+09, 1.3839e+09,
        1.0591e+09, 1.6484e+09, 1.0559e+09, 1.5583e+09, 1.7202e+09, 1.8445e+09,
        1.6831e+09, 1.7251e+09, 1.3797e+09, 1.3687e+09, 1.2776e+09, 1.5931e+09,
        1.4808e+09, 1.5544e+09, 1.5783e+09, 1.5712e+09, 1.7912e+09, 1.2544e+09,
        1.3296e+09, 1.1445e+09, 9.1899e+08, 1.3340e+09, 1.9492e+09, 1.5205e+09,
        1.9506e+09, 2.1291e+09, 1.5317e+09, 1.2635e+09, 1.2390e+09, 1.6491e+09,
        1.7016e+09, 1.3033e+09, 1.5175e+09, 1.7106e+09, 1.4471e+09, 1.4420e+09,
        2.0958e+09, 1.2856e+09, 1.6997e+09, 1.1469e+09, 1.4198e+09, 1.8838e+09,
        1.1182e+09, 1.2571e+09, 1.3710e+09, 1.5022e+09, 1.3844e+09, 1.2208e+09,
        1.4025e+09, 1.2148e+09, 1.2615e+09, 8.6883e+08, 9.0216e+08, 9.5218e+08,
        1.6531e+09, 1.7280e+09, 1.2100e+09, 1.6950e+09, 1.3951e+09, 2.1166e+09,
        1.5406e+09, 2.2014e+09, 1.6728e+09, 1.5830e+09, 1.7327e+09, 1.2091e+09,
        1.0991e+09, 1.4452e+09, 1.3600e+09, 2.0550e+09, 2.0175e+09, 1.2374e+09,
        1.5036e+09, 1.2581e+09, 1.5509e+09, 1.5414e+09, 1.4549e+09, 2.0061e+09,
        1.8963e+09, 1.5987e+09, 1.6998e+09, 1.7032e+09])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.7906e+09, 2.1440e+09, 2.7407e+09, 1.9501e+09, 2.2027e+09, 2.4276e+09,
        3.1086e+09, 2.1501e+09, 2.3486e+09, 2.2869e+09, 1.6768e+09, 2.5428e+09,
        2.0813e+09, 1.6964e+09, 2.7017e+09, 3.0176e+09, 3.3067e+09, 2.5436e+09,
        1.5575e+09, 3.0702e+09, 2.3708e+09, 2.7435e+09, 2.3353e+09, 2.2130e+09,
        1.8687e+09, 3.0806e+09, 2.4656e+09, 2.8189e+09, 3.0499e+09, 2.3466e+09,
        2.0553e+09, 2.4723e+09, 2.5311e+09, 2.4162e+09, 1.8101e+09, 1.7253e+09,
        2.1810e+09, 2.1137e+09, 2.0254e+09, 1.9768e+09, 3.6989e+09, 2.4980e+09,
        2.3436e+09, 2.6011e+09, 2.5327e+09, 2.1063e+09, 1.8167e+09, 1.4975e+09,
        2.0630e+09, 1.9960e+09, 2.7769e+09, 2.1401e+09, 2.1092e+09, 1.7699e+09,
        3.1352e+09, 2.7059e+09, 1.8052e+09, 2.5399e+09, 2.3017e+09, 2.1944e+09,
        1.5705e+09, 2.1552e+09, 1.8343e+09, 2.3637e+09, 1.6539e+09, 2.2479e+09,
        3.2457e+09, 2.6436e+09, 2.8920e+09, 2.3176e+09, 2.2943e+09, 1.3408e+09,
        2.1994e+09, 2.1352e+09, 3.3577e+09, 2.7330e+09, 3.4899e+09, 1.7728e+09,
        1.9136e+09, 2.7306e+09, 2.3360e+09, 3.1247e+09, 1.7304e+09, 1.7615e+09,
        1.9296e+09, 3.1779e+09, 2.0001e+09, 1.8890e+09, 2.0872e+09, 2.6644e+09,
        1.6713e+09, 2.2020e+09, 2.2704e+09, 2.6121e+09, 3.1868e+09, 1.8765e+09,
        1.9806e+09, 2.9937e+09, 2.5477e+09, 2.1776e+09, 1.8791e+09, 1.8317e+09,
        3.3402e+09, 1.9024e+09, 1.8594e+09, 2.5094e+09, 3.1738e+09, 1.4921e+09,
        1.9190e+09, 2.0419e+09, 2.3896e+09, 2.7879e+09, 2.1902e+09, 2.5749e+09,
        2.8166e+09, 2.4607e+09, 2.4569e+09, 1.7010e+09, 2.4969e+09, 2.6732e+09,
        2.4547e+09, 3.5476e+09, 2.1346e+09, 2.5332e+09, 1.8623e+09, 2.3789e+09,
        1.9805e+09, 2.7751e+09, 1.8982e+09, 2.4765e+09, 2.7308e+09, 2.0494e+09,
        1.6409e+09, 2.1436e+09, 3.2059e+09, 2.6498e+09, 2.5440e+09, 2.9225e+09,
        1.8094e+09, 2.6972e+09, 2.1312e+09, 2.5588e+09, 1.9880e+09, 1.8499e+09,
        2.5981e+09, 2.7422e+09, 1.8879e+09, 2.9798e+09, 2.3252e+09, 2.8967e+09,
        1.9756e+09, 2.6716e+09, 1.8931e+09, 1.5139e+09, 2.1569e+09, 2.1382e+09,
        1.5009e+09, 2.7146e+09, 1.8130e+09, 1.9321e+09, 2.4148e+09, 2.4725e+09,
        2.9003e+09, 2.3159e+09, 1.7261e+09, 3.0922e+09, 2.3492e+09, 2.0036e+09,
        2.3252e+09, 2.3363e+09, 2.5130e+09, 3.0233e+09, 2.5743e+09, 3.5814e+09,
        2.5359e+09, 2.4256e+09, 2.1587e+09, 2.2915e+09, 2.5237e+09, 3.1732e+09,
        2.0749e+09, 2.4318e+09, 1.4779e+09, 2.5180e+09, 2.1050e+09, 1.7398e+09,
        2.0232e+09, 2.5901e+09, 2.0044e+09, 1.7751e+09, 1.4260e+09, 2.1706e+09,
        3.1274e+09, 2.3493e+09, 1.8145e+09, 2.1404e+09, 1.6726e+09, 2.5541e+09,
        2.7170e+09, 2.9023e+09, 2.0149e+09, 2.2133e+09, 1.8535e+09, 1.8428e+09,
        2.2842e+09, 1.8843e+09, 2.5483e+09, 2.0202e+09, 2.3793e+09, 1.8804e+09,
        1.9733e+09, 2.5943e+09, 2.0083e+09, 3.1780e+09, 2.7754e+09, 2.0947e+09,
        1.6130e+09, 2.0492e+09, 2.3562e+09, 1.8431e+09, 2.8683e+09, 2.2752e+09,
        2.1737e+09, 2.7666e+09, 2.0618e+09, 1.8041e+09, 2.8109e+09, 2.0738e+09,
        2.3230e+09, 1.9907e+09, 2.3741e+09, 1.5284e+09, 2.5384e+09, 2.3789e+09,
        2.8460e+09, 1.8547e+09, 1.4683e+09, 1.9005e+09, 2.2069e+09, 2.0232e+09,
        2.7406e+09, 2.1494e+09, 2.7696e+09, 2.6505e+09, 2.5172e+09, 2.5859e+09,
        3.6737e+09, 2.1602e+09, 1.8026e+09, 2.9114e+09, 2.9185e+09, 2.8196e+09,
        1.3337e+09, 2.2491e+09, 2.1344e+09, 1.7957e+09])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([11112175.0000,  9218644.0000, 15739761.0000,  ...,
        37689940.0000,  4886068.5000, 15263272.0000])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([20354842., 26364048., 23819586., 17852650., 16559430., 16616542.,
        22224534., 30760552., 23679542., 20781858., 15862683., 20774128.,
        22052112., 19511850., 23289562., 25767590., 20061304., 17572314.,
        18927104., 17909642., 24590982., 17495256., 23931576., 24046982.,
        20517090., 14240789., 20278154., 16044435., 23295304., 21681046.,
        21491220., 23917752., 21144166., 22524646., 15633050., 25194216.,
        22055254., 23360570., 22284694., 19799668., 16305517., 23440272.,
        20149088., 28848008., 25415798., 24504152., 22189158., 20319988.,
        20722578., 18841892., 20888132., 19367730., 17882704., 17067096.,
        21292326., 27298382., 19766302., 16741193., 21433538., 18389046.,
        15563395., 24092022., 19103922., 20471312., 19878030., 21219908.,
        19127344., 23778972., 22643702., 17725724., 16074859., 23562040.,
        23039344., 19911978., 18521330., 30643470., 25605322., 16521081.,
        26051444., 18357634., 15562853., 20248014., 18703262., 21956164.,
        23667042., 21970038., 22373764., 21381340., 22424016., 23228290.,
        21561120., 20447858., 18340712., 29695750., 18397366., 21285432.,
        21548718., 19085870., 22429606., 24518294., 26924468., 16928386.,
        16591884., 19574602., 19343398., 22409786., 26320686., 18414076.,
        20315632., 17005554., 23858282., 16851116., 17802898., 17309950.,
        24668576., 16797612., 16263734., 13633601., 18823866., 23801084.,
        17658338., 19031064., 15357836., 16396264., 11659475., 17544154.,
        14614073., 19070350., 17842482., 24623830., 23091080., 27876194.,
        23201250., 24608868., 19352212., 18074652., 18098186., 25013100.,
        20888800., 16068266., 20990470., 21664284., 22104624., 27266936.,
        22596690., 21118160., 22728012., 30796302., 24472328., 26852918.,
        26568660., 19363806., 24598578., 21323398., 20754728., 28461744.,
        22146958., 17104924., 19684120., 21611504., 20612564., 30441744.,
        24989514., 26793238., 15927588., 21055132., 17982476., 21125046.,
        18532476., 22586234., 22952904., 31481446., 14934318., 17388108.,
        24354988., 20253118., 17725526., 27519206., 18048200., 17684112.,
        22535130., 34646436., 20188182., 17267062., 21272276., 22139364.,
        16352955., 16249378., 24714282., 20855442., 21174648., 16041609.,
        18181024., 16796690., 20355710., 20336952., 18131288., 16039022.,
        19768628., 19315084., 20443520., 22456218., 29953792., 20899624.,
        21386212., 22987608., 29390680., 23614714., 18717726., 14350367.,
        24894836., 16161968., 18971280., 24075772., 23587520., 21847700.,
        21126324., 16764856., 20630778., 20771970., 16875428., 24954224.,
        24275634., 18871690., 19244316., 22219692., 16215433., 16810848.,
        20119016., 17887238., 18473824., 21914962., 20525406., 18680142.,
        19028784., 20395570., 23596706., 18716548., 17597392., 29128160.,
        15932677., 25051848., 23705134., 27552910., 19318572., 23014360.,
        20018108., 24502628., 19770256., 25753170., 24982046., 24565180.,
        22161858., 17632874., 13266780., 19168398., 22998620., 25198188.,
        23115214., 20472804., 25531884., 17685792., 25830162., 18747548.,
        16822274., 17288542., 16828378., 20457910., 18462130., 16660748.,
        25179674., 20175048., 20468860., 23448628., 18196022., 16160011.,
        18891588., 17930860., 13314164., 19225116., 17323470., 19631602.,
        24978498., 26722804., 18783278., 20692482., 18550992., 29264148.,
        15334585., 19731950., 19285422., 14991041., 20770476., 21150680.,
        20744778., 20945736., 23438308., 28331136., 19729066., 14282649.,
        19595774., 18760884., 23670950., 17657366., 19611230., 24252694.,
        19169620., 18464106., 21887656., 21577192., 28982426., 22283378.,
        18622484., 12690840., 17826406., 19028544., 20707612., 21932438.,
        22248316., 18149320., 24050160., 22342268., 20003366., 19400828.,
        18795542., 22886202., 25351146., 19211052., 28341104., 16258774.,
        23542888., 21768240., 31972942., 18216180., 18146254., 25100770.,
        22532122., 19429994., 22727806., 28242706., 18246122., 24821832.,
        18308348., 20148378., 15766987., 21270998., 21306252., 16196494.,
        20942332., 20190674., 18091486., 26023868., 31570188., 18258590.,
        19245710., 20352640., 21126762., 23620214., 24681300., 18750840.,
        17756018., 24306544., 18079370., 22893208., 21385228., 16565857.,
        19981300., 28675634., 17985476., 22066810., 25678584., 21211636.,
        18642330., 15203610., 27345466., 19364766., 28112180., 24915718.,
        25009500., 24835534., 21810078., 20172366., 16437120., 21256026.,
        20562232., 25727914., 19904994., 23744608., 30208880., 22206098.,
        18905068., 17869358., 19934304., 20838368., 16020531., 16325219.,
        29220756., 20035438., 20105136., 23946044., 34712144., 15818794.,
        21467512., 21289284., 23835782., 17070676., 22920864., 19318142.,
        19992256., 14880493., 18583072., 18029814., 20211164., 19652550.,
        25532862., 22665240., 16536681., 15458116., 24700650., 16572630.,
        22537892., 19614332., 30371926., 20782126., 19827456., 18355456.,
        21893208., 19184336., 28150886., 18426922., 29630584., 20576950.,
        27916478., 24155200., 27926266., 19895168., 17315216., 19896438.,
        17173486., 22630524., 23614578., 18704780., 16972702., 18907486.,
        19582208., 13929074., 18791186., 21635152., 19136164., 22662906.,
        15924085., 26495238., 21229072., 24511694., 23051800., 25855420.,
        24533696., 18471730., 22051514., 21190516., 28385918., 17084292.,
        20370150., 19852588., 18363250., 21489752., 20380686., 29944682.,
        17848254., 19343672., 17840080., 17478362., 24643884., 31695544.,
        21362082., 26229914., 19902612., 23064130., 24093764., 18795100.,
        22315426., 24229936., 18957440., 19652428., 24115834., 18570722.,
        24216882., 16819162., 22638636., 20041964., 25660834., 17533254.,
        27392120., 22749990., 20445816., 20656752., 16309829., 21252828.,
        24868158., 21427222., 19689398., 20507528., 31018540., 26889134.,
        22371324., 21999344., 22506322., 26443860., 18045662., 22383118.,
        18923748., 21601978.])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([40266476., 50740788., 51383544., 44738756., 37336692., 63399828.,
        58997736., 58085424., 57717040., 32409406., 44216736., 50334928.,
        55250952., 63087792., 54652008., 44149872., 45526524., 50817388.,
        53426920., 52160548., 56632264., 49566092., 58188876., 36935984.,
        54369824., 41726852., 48690576., 44565296., 42833352., 42072668.,
        39434712., 72898536., 47446612., 65411396., 37534612., 41333724.,
        49356724., 51326260., 53436580., 56501364., 41291044., 56468928.,
        57989448., 47266392., 45924264., 58567232., 49661080., 51491392.,
        57528184., 45438904., 80075560., 69643240., 46285328., 49881924.,
        32013764., 44501460., 43215216., 52041260., 37752048., 48886732.,
        38995204., 61305332., 43128656., 46637444., 58917424., 60443300.,
        53783904., 52730032., 62911152., 45280704., 48667616., 50158536.,
        51904332., 51628388., 44778568., 42440316., 43300968., 40906220.,
        43963460., 58970896., 56555596., 54571368., 40916320., 50470344.,
        43522532., 53652312., 48609748., 49234588., 64792672., 34604156.,
        48326112., 48238976., 47759428., 44386884., 53576940., 46635176.,
        38845456., 51000140., 45221720., 47904696., 35198960., 40580064.,
        58660664., 67018376., 61168056., 55613756., 44861320., 51169192.,
        47065912., 62854852., 57270888., 45407696., 66448320., 39315892.,
        36580760., 43157408., 46255772., 43272656., 53610864., 49223448.,
        52677724., 42992204., 45717412., 62094344., 56223168., 48538196.,
        57691576., 54274184., 52205564., 65431576., 43260252., 53626048.,
        63255476., 38101816., 37121532., 52236152., 47963640., 51516860.,
        48333980., 36204432., 44966084., 33951732., 61507580., 52174164.,
        50646932., 47909404., 49059048., 41665012., 46840952., 52176736.,
        32155326., 37391616., 44541960., 61013088., 46445748., 61097828.,
        39364956., 45965060., 57312948., 52136564., 57956200., 52695972.,
        45010436., 50363208., 36565664., 46742904., 36144208., 50552724.,
        45140056., 43974440., 41357064., 54719708., 57963328., 45823388.,
        52993896., 46589696., 49933652., 38510656., 46237796., 62920572.,
        46753604., 41224788., 41900784., 51573372., 39564824., 42868364.,
        58115400., 37565688., 38254488., 39820592., 38763372., 33604080.,
        43298260., 43256040., 35228320., 54488784., 54109928., 54175256.,
        46236940., 33794368., 58884980., 46595688., 47431108., 54389644.,
        50794484., 56051368., 47947808., 54075000., 49093352., 56088712.,
        43272932., 55653408., 69235312., 56858452., 35146692., 55958868.,
        48227476., 39255908., 60296124., 65236368., 45039740., 54328860.,
        45366264., 75227616., 41293120., 52115776., 72543408., 51604804.,
        52128684., 52260072., 56996920., 34262164., 56220368., 46138752.,
        52656564., 68880672., 78555520., 45138112., 50561328., 52196588.,
        49600928., 53379124., 66420084., 44380908., 52671260., 44345592.,
        39345908., 45977340., 36240188., 45011884., 52165976., 64762684.,
        46142136., 50612184., 39749956., 29504626., 44615392., 53930152.,
        41587852., 64027588., 40916644., 41383000., 43826284., 40941068.,
        58409008., 46910676., 60413028., 43811524., 51930564., 48426532.,
        54770660., 47667840., 42270372., 48119556., 40145836., 40259572.,
        36817196., 54026676., 61162560., 49675068., 53923980., 48548736.,
        47892264., 52171496., 39433376., 60197260., 46254120., 52796524.,
        41723460., 49535340., 55584504., 51497756., 55512440., 68763496.,
        67292120., 52888572., 45960176., 56491104., 45287992., 62266612.,
        47584576., 34814396., 52137064., 50757660., 44878936., 38311256.,
        47418048., 37969552., 43243976., 43797596., 49300508., 50714872.,
        52587252., 39381076., 54887696., 53436304., 42859940., 45567592.,
        41194472., 46222344., 66017860., 52821716., 57702632., 49585060.,
        49015356., 43371260., 53260876., 47328408., 43043592., 59636608.,
        65985504., 43958980., 48702144., 40447840., 43107312., 47641480.,
        50867040., 33407862., 45458716., 71605344., 40613684., 53680664.,
        67632096., 40278836., 50808424., 56495896., 60242828., 62489692.,
        48797956., 35027692., 51228828., 40674416., 49697760., 55164864.,
        43328140., 47329556., 56847292., 50661852., 48284328., 49069596.,
        48574776., 65953872., 46044288., 39200260., 54134968., 35413280.,
        50119044., 54811876., 41770000., 58045340., 37962876., 55439792.,
        46397384., 57340368., 59194388., 64436300., 53453348., 62717440.,
        41089480., 53526248., 66883340., 72719144., 48307128., 48401312.,
        35913780., 36645940., 54645812., 37668744., 52635720., 52053304.,
        57009704., 44993428., 37379588., 47616056., 60194936., 55890832.,
        51102996., 56372740., 43009864., 46851480., 64653552., 48140620.,
        55376708., 49437448., 52003104., 45670884., 43861728., 45019664.,
        46814104., 49053028., 63838388., 47734092., 37814368., 61297524.,
        58098652., 51832852., 51440264., 69737600., 53333864., 41514628.,
        42367376., 43495704., 42340036., 43068852., 46519764., 50332952.,
        57657856., 49050124., 37671880., 42643972., 60339856., 34610692.,
        46738164., 70816336., 51427720., 68861576., 43423952., 39060280.,
        58436608., 50713156., 38510220., 42262120., 52657900., 62063416.,
        50494216., 45454604., 44391148., 58564516., 60006684., 40758348.,
        48198076., 48802880., 47289864., 43834872., 32071464., 57367344.,
        53739048., 37898552., 56754192., 62598784., 56281076., 51768152.,
        52718548., 52796396., 79027944., 46651508., 37028972., 47658276.,
        61058624., 43478604., 58484992., 54447244., 51905516., 43544984.,
        45885948., 56265416., 41896848., 57051864., 48848504., 43906516.,
        55253932., 56085172., 44150444., 49120904., 56309052., 51109264.,
        61338016., 54495836., 62543524., 44866884., 48838240., 45489412.,
        49709408., 60095964., 38742040., 45819636., 42052908., 35793524.,
        46603892., 62796260., 50344484., 51637684., 46833404., 52363504.,
        43120296., 54274572., 58914180., 56077240., 47074820., 46734428.,
        48639124., 53505436.])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([443871.0938, 159351.1250, 729325.6250,  ...,  57460.8633,
         37484.1211,  49613.7539])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([  56491.3633,   21254.2500, 1045521.1875,  ...,  113777.7031,
         164001.0625,  228889.0000])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([391206.8750, 398385.7812, 320943.5625, 460844.1875, 325956.4375,
        280501.8750, 260209.8125, 251447.5312, 414583.1250, 357271.6562,
        423465.5938, 273597.7188, 341069.0938, 441195.7812, 480396.9375,
        272186.6875, 363991.8750, 367835.6562, 541399.3750, 434847.5625,
        480117.2188, 378925.1250, 435251.7500, 433980.5938, 429037.9375,
        270148.3125, 389949.2812, 659008.9375, 297025.6250, 362307.7500,
        237685.5625, 255554.0625, 302098.0938, 400785.8125, 599431.8125,
        363728.1562, 370824.5312, 395081.3438, 643852.0625, 309117.9062,
        346198.3125, 421168.0000, 487160.5000, 415385.4062, 318401.5000,
        314604.1875, 650324.1250, 330843.2188, 327223.1875, 433238.6250,
        403804.2188, 354085.9375, 472016.0000, 425797.0312, 270730.0625,
        344973.4062, 355769.0312, 363014.9062, 297497.8438, 444907.9062,
        362237.9375, 256740.3750, 181803.5625, 245193.4375, 384639.5312,
        536303.1250, 372766.4062, 394321.9688, 446363.1250, 500133.0625,
        512997.1875, 311662.0938, 383641.1562, 384592.8750, 510651.8750,
        588308.7500, 514531.5312, 405386.9062, 590927.4375, 350138.0312,
        321782.6562, 605426.5625, 461155.4688, 329964.2500, 381984.7500,
        255040.4375, 458248.0312, 508092.2188, 419126.4688, 540502.8125,
        403034.2812, 363487.7188, 469111.8125, 462700.8125, 548887.5000,
        297953.1875, 498907.5625, 276659.1250, 460662.4688, 579835.9375,
        469893.0000, 360042.0938, 324404.7812, 289127.9062, 354272.8438,
        366033.7500, 526856.8750, 311293.1875, 408002.3438, 355031.1875,
        471952.6562, 518505.9375, 371755.2188, 363761.6875, 512207.1562,
        324453.3438, 516761.2500, 564495.6875, 513395.8438, 350397.4375,
        630118.6250, 421429.0312, 432898.4688, 504556.9062, 540161.0000,
        354053.0000, 269440.6250, 452010.6250, 355888.2188, 303679.9062,
        477042.4062, 276935.5938, 334508.4375, 568171.8125, 291304.2188,
        413380.2188, 369578.5938, 269303.4375, 516807.2500, 307372.4688,
        363516.7188, 572359.0000, 417051.6875, 380037.7812, 302812.7812,
        483326.8438, 439915.4062, 331834.1250, 249128.1406, 408372.4062,
        419833.7812, 285404.1250, 250690.4062, 480516.8750, 293247.7500,
        462871.3750, 428047.5938, 601326.5000, 249306.7031, 378918.5312,
        528641.1250, 284732.2500, 250000.4688, 276801.8438, 305852.3438,
        343000.5625, 360338.2500, 429420.1250, 288351.3750, 448822.8125,
        301776.2500, 433126.9688, 416760.1875, 460788.3438, 336640.9688,
        451168.9688, 541877.8125, 288675.7812, 294606.0625, 291670.0312,
        239766.6562, 433280.0312, 319398.6250, 261118.5000, 447186.1250,
        502194.9688, 342057.4062, 288206.5000, 451326.0312, 419331.9688,
        453352.8125, 296911.9062, 414534.3438, 384817.1250, 336202.3438,
        501449.4062, 326612.8750, 440978.5000, 479590.8750, 263149.3438,
        565014.0625, 492511.4375, 423974.6875, 271025.4688, 329662.7188,
        286923.8750, 344933.2500, 427222.8750, 434781.4375, 287637.8125,
        432372.3438, 417729.8750, 445768.8750, 460610.5312, 507212.8125,
        358841.1875, 395827.9688, 468481.6562, 257013.4219, 366354.8750,
        585039.8750, 537111.5625, 264493.7188, 316414.6562, 528262.1250,
        328859.4688, 378783.9062, 188778.7656, 408215.0938, 277309.7500,
        310837.2812, 304987.0625, 264360.0625, 400764.9062, 245753.3750,
        358468.0312, 363959.8438, 438954.6875, 305680.4688, 457692.1875,
        338866.6875, 444268.3750, 545125.4375, 337473.6562, 346496.2812,
        332813.5312, 369990.9375, 432958.2500, 284785.0938, 698625.0625,
        410250.9375, 365908.3125, 341226.6562, 425551.5938, 327930.0000,
        311184.8125, 504076.5312, 429245.0312, 244685.0156, 513119.9375,
        260654.3906, 447361.7500, 367234.2812, 359491.4375, 270051.5938,
        368801.4375, 313104.2188, 357435.4688, 601545.1250, 339015.4688,
        323138.9688, 451111.0000, 476592.0312, 262114.8906, 264579.2500,
        305512.0000, 266054.1250, 401097.8438, 371953.6250, 489054.6875,
        533017.8750, 248071.8906, 204234.3438, 310289.2500, 397946.2812,
        305845.7188, 491041.4062, 326611.5938, 459310.9062, 542642.5000,
        474799.6875, 428560.6250, 450669.7500, 422752.4375, 377192.8125,
        432098.1562, 352196.5625, 349294.0625, 343943.0000, 691816.5625,
        397248.8125, 492609.3750, 301863.1875, 326109.3125, 274192.8125,
        294186.8438, 601463.9375, 241847.9688, 474317.8750, 529054.6250,
        512445.7188, 352114.5312, 381367.4062, 462288.6562, 407983.3750,
        713279.5625, 479581.2500, 560238.5625, 330520.8438, 349301.6250,
        335782.8750, 626910.4375, 538299.1250, 439589.6250, 246510.4375,
        343741.3750, 387747.5312, 531224.5625, 282939.6250, 284831.8125,
        368327.1875, 390773.1562, 278391.6875, 357094.5000, 346557.2500,
        279919.2188, 388938.3750, 324798.0938, 502800.7812, 270830.1250,
        371517.3438, 512607.6250, 629530.8125, 444180.1562, 307766.1875,
        579658.3125, 361467.9375, 342115.3438, 349202.9688, 387786.3125,
        439786.3750, 429464.4688, 291937.4688, 317051.4688, 250592.7656,
        417612.2812, 356742.2812, 612047.4375, 434909.8438, 350656.2812,
        341916.7500, 385214.4375, 248476.7656, 349118.3438, 395569.0312,
        311063.0938, 285274.0312, 418648.1875, 326686.0312, 298159.6562,
        426198.5625, 440916.9062, 240082.3438, 549802.4375, 479713.9375,
        517268.0938, 343926.2812, 398786.1562, 302768.5000, 414618.8438,
        348989.8125, 334813.7188, 418512.5000, 520980.9688, 310850.0000,
        283300.8438, 327169.5625, 571906.9375, 449240.4062, 501776.6562,
        475437.8750, 345385.7188, 308135.9062, 419540.2812, 232676.1406,
        414514.9062, 275909.7188, 265938.4062, 483969.0312, 561530.4375,
        398321.0625, 295568.8125, 325156.0938, 373213.0000, 469706.9375,
        380101.7812, 469693.2500, 408283.1250, 426420.7812, 290946.8750,
        341424.5000, 299754.5938, 438194.4688, 365745.0625, 269421.8438,
        447651.8750, 618284.3125, 402203.3125, 362699.0938, 398339.4375,
        397841.2500, 446784.5312, 453665.0000, 332683.5312, 423564.0938,
        508701.4375, 522581.1562, 427709.0312, 374048.8750, 249502.9844,
        450910.8438, 346380.8125, 425553.4375, 446764.0938, 422978.1250,
        400563.0625, 328322.2188, 401057.7812, 329591.6250, 292825.0312,
        391543.1562, 345826.7188, 392931.7188, 301494.9062, 506491.0312,
        460352.0000, 367218.6562, 352065.2500, 271338.2500, 300293.5000,
        354454.7500, 420001.7812, 444183.5000, 351098.0625, 416314.2812,
        495304.0312, 433819.2500, 368886.9688, 417439.9375, 339970.4375,
        577569.1875, 369256.4062, 353221.6875, 215290.6562, 296110.4062,
        347380.3125, 421004.3750, 334135.0938, 435483.8750, 357442.5625,
        382095.7812, 350892.7500, 326701.0938, 257044.7656, 368744.1875,
        423744.1875, 366023.7188, 312218.3125, 288214.8750, 331431.7188,
        302056.7812, 321766.0312, 391284.6875, 283455.6250, 362603.6562,
        544758.4375, 262430.9688, 442294.1250, 342370.1250, 346926.8438,
        373470.7812, 387092.7188, 628254.0000, 456349.2188, 267308.5000,
        243499.7500, 344148.3125, 379940.8438, 526613.1250, 382238.5938,
        506669.4062, 332926.9062, 346599.2500, 444507.6875, 389116.8125,
        362951.8125, 362078.5938, 311472.5312, 310518.3750, 498096.6875,
        528089.1875, 330557.7812])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 955704.6250, 1023140.3750,  751879.4375, 1163493.8750,  643793.2500,
        1164188.8750,  572427.1875,  458341.8750, 1478980.8750, 1148371.6250,
        1366544.3750,  616301.4375, 1269882.8750,  917204.5625,  725450.3750,
         599352.3750,  723708.0000, 1121058.2500,  545660.4375, 1105369.1250,
         837954.9375, 1246213.1250,  645037.8750,  732946.6875, 1360137.6250,
        1309646.8750,  759221.0000,  800332.9375,  817420.3750, 1044803.8750,
         770049.9375,  740056.1875,  714019.0000,  862422.0625,  884810.5000,
         839091.9375,  547772.1250,  995002.9375,  832834.8750,  878018.3750,
         952884.8125,  646276.3750, 1032103.8750,  566233.2500,  884203.7500,
         708891.6250,  927294.0625,  979272.5000,  945197.1875,  814645.8750,
         917792.3750,  799794.3125,  777836.0625,  589787.3750,  598344.7500,
         899619.3125,  931411.3125,  938035.4375, 1153123.6250, 1143433.2500,
         573124.8750,  952890.1875,  742818.0625, 1142801.8750, 1175892.7500,
         732055.8125,  675017.5000,  832619.1250,  915342.7500, 1123055.0000,
         593841.2500,  882481.6250,  731261.6250,  606552.6875, 1186684.8750,
         747190.7500,  730917.3750, 1118781.3750, 1044380.3125,  889003.6250,
         616110.7500, 1180173.5000,  952751.9375,  705105.5625,  971809.8750,
         689723.6250,  921398.8125,  755554.9375, 1133978.2500,  922465.3125,
        1222521.2500, 1617230.2500,  943419.7500, 1132927.7500, 1059852.2500,
         764407.2500, 1187414.7500, 1079121.2500,  517083.9688,  594599.3125,
         641421.8125,  741988.0000,  737776.3125, 1024670.0000, 1020491.4375,
         786276.5000,  755125.6875, 1192447.6250,  642742.8750,  621570.6250,
         954095.8750,  618981.5000, 1124773.3750,  615890.8750,  741612.6250,
         894869.0000, 1105843.7500,  705807.1875,  846893.1875, 1009344.5625,
         718214.8750, 1000501.1875, 1242243.1250,  730856.8125,  719515.3750,
         656786.2500, 1094285.3750, 1017308.2500, 1036116.1250, 1107619.0000,
         888313.0625,  512611.1562, 1280680.1250,  831338.5000, 1128224.7500,
         759099.7500,  705858.8125,  982208.0000,  659712.2500, 1476290.0000,
        1305827.6250,  893363.8750,  938189.7500, 1015868.3750,  855299.9375,
        1088101.5000, 1013509.5625,  986140.8125,  911794.0625,  656273.2500,
         597026.2500,  761651.6250,  856309.0625,  954698.5625,  691260.5625,
        1056215.7500,  953687.0000,  564998.2500,  959026.4375,  562555.0625,
         779039.8750,  847165.6875,  777280.1250,  895532.7500,  810028.0000,
        1044592.1250,  853333.1875, 1055568.1250,  847278.9375,  627003.6875,
         822572.1875, 1769215.5000,  814336.8750,  925125.3750,  656954.8125,
         562295.8750,  812986.8750,  898863.9375,  892809.3750, 1071186.7500,
         871317.6875,  851507.7500,  742078.4375,  798952.7500,  998077.5000,
         906665.5625,  682484.9375,  945902.0625,  913232.2500,  677192.3125,
        1809968.2500,  937999.9375, 1085821.5000,  825898.1250,  631976.4375,
         691310.1875, 1119778.6250,  845977.0000, 1304475.2500,  516247.7812,
         863487.6250,  947337.6875, 1107222.2500,  727948.5000,  731986.1875,
         891367.3125,  761915.1250,  942434.4375,  898846.2500,  798583.1875,
         785023.1875,  726408.2500,  690406.3125,  806799.6875,  892066.0000,
         786967.3125,  587315.5000,  880725.5000, 1052673.8750,  863092.6875,
         951228.5000, 1472150.2500,  871814.6250,  987370.6250,  512943.3438,
         767976.7500,  744034.8750,  798421.8125, 1021972.3125,  913729.9375,
         844687.5000,  851058.1250, 1209563.7500,  994513.3125,  749803.6250,
         955022.3125,  948689.7500, 1126122.0000, 1019922.5625,  739968.8125,
         884197.9375, 1165551.5000,  700122.1875,  995820.8750,  658862.4375,
         860010.8750, 1058179.0000,  703710.5625,  819370.9375,  879626.4375,
        1191419.1250,  715389.4375,  593331.9375, 1165476.2500,  913045.7500,
         860287.3750,  554171.5625,  851688.0625,  740654.9375,  873297.8750,
         597082.1250,  975849.1250, 1218747.3750,  901973.2500, 1406884.1250,
        1044382.1250, 1363706.1250, 1230132.3750, 1051956.1250, 1113497.3750,
         793596.4375,  867006.6250,  996913.8750, 1095861.3750,  698787.6250,
         679043.8125,  655143.6250, 1262503.6250,  871538.1875,  690834.9375,
         702062.5625,  637865.1875,  952833.8125, 1293580.0000,  798126.7500,
         855681.9375,  800094.4375,  629703.5000,  897447.0625,  955486.8750,
         791759.0625,  994624.1875, 1048475.3125,  774013.1875,  836301.8750,
        1029699.1875, 1375870.8750, 1068691.2500,  863128.4375, 1088753.1250,
         989475.6250,  982896.4375, 1165839.8750,  537443.2500, 1007566.3750,
         618041.5000,  460612.4688,  771067.2500,  815566.8750,  611231.5625,
        1332656.3750,  817309.0625,  828332.5625,  918532.8750,  625399.3125,
         933445.8750,  834859.8125, 1114117.5000,  958574.1250,  794438.3125,
        1000821.8750, 1300807.1250,  827805.5625,  724423.6875,  666526.2500,
         721929.3125, 1330513.3750,  871521.3125,  807276.4375,  926803.0625,
         741347.3125,  641374.5000,  774478.6875,  697040.3125,  811924.7500,
        1241187.5000,  706462.1875, 1048353.6875,  899781.7500,  945385.6875,
         834090.0625,  993025.7500,  748853.5625,  811968.8125,  697625.1250,
         624079.0000, 1380704.5000,  818612.1250, 1024978.0000,  570022.0000,
         817338.5625,  945600.6875,  872154.8750, 1028660.9375,  690960.7500,
         827398.0625,  750962.8750, 1032380.9375, 1148777.5000,  843008.1250,
         999973.3125,  965474.0000, 1074393.3750,  655297.6875, 1236629.0000,
        1036058.0625, 1326868.0000, 1433471.5000,  676611.2500,  732808.1875,
        1180452.1250,  918793.6250,  857497.9375, 1062022.5000,  831219.5000,
        1052125.5000, 1155274.8750,  814905.4375,  797477.7500, 1166010.3750,
         850287.0625,  821433.8750,  840365.1875,  768198.7500,  900032.2500,
         940621.5000,  998530.3125, 1039498.1250,  688985.8750,  589115.1875,
         764671.6250,  959779.6875,  800123.7500, 1131552.3750, 1029991.5000,
         933856.5000,  801674.3125,  776898.8750,  540885.5000, 1221690.3750,
         803361.6250,  898987.1875,  826723.0000,  856713.1875,  812287.2500,
         815767.2500,  767336.0625,  725590.0000,  923080.3750,  645726.1875,
         856162.1875,  622063.1250,  732755.0625,  781831.0000,  975483.3125,
         847341.5000,  725455.3750,  843980.0625,  584530.8125,  869798.6875,
         939910.9375,  659577.9375,  729040.2500,  752968.8750, 1045779.0625,
         989640.8750,  836327.4375, 1032974.2500,  796118.0000,  600614.6875,
         874612.8750,  886444.0625,  489132.3438,  972200.7500,  954910.8750,
        1172514.8750, 1101946.2500,  659108.3125,  642455.5000,  911338.4375,
         799878.5625,  686339.6250,  650925.6250,  795166.9375,  629694.8125,
         770511.4375,  835583.1250,  554933.5625, 1049149.5000,  970292.7500,
        1022046.2500,  710617.4375, 1080602.3750,  881860.7500,  896190.7500,
         850682.0625,  746208.7500,  500084.0312,  815377.1250,  825859.8750,
        1218072.6250,  627514.5625, 1194077.2500,  521990.1875, 1441413.1250,
         712997.4375,  714700.0625,  622554.3125,  807793.4375,  843572.0000,
         976455.3750, 1011506.4375, 1134483.7500, 1274966.2500,  994683.3125,
         957066.6875, 1137259.8750,  907881.5625,  519673.6875,  814546.8750,
         808423.0625,  702658.6875, 1100259.8750,  837951.3750,  747407.8125,
         768969.3750,  893443.5625, 1044793.0000,  803021.4375, 1015322.9375,
         935475.5000, 1256925.6250,  852509.7500,  872820.4375,  608648.2500,
         600107.9375,  819318.3125,  739515.8125, 1103512.8750,  891736.9375,
         734760.4375, 1153789.7500,  914749.7500,  673005.1875, 1170769.8750,
         675698.8125,  993674.7500,  780759.9375, 1099280.6250, 1051034.2500,
         786763.5625,  745562.6250])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([76685.6719,  1192.3740,  4507.1069,  ...,  3401.4666,  4711.3652,
         1089.5573])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([13177.6064, 11576.5977, 11697.4941, 18168.5742,  9933.1074, 11190.1172,
         9081.0781, 15300.2285, 13980.7412, 11830.0869, 11573.9180, 11303.4482,
        15723.6475, 12297.8516, 15226.2344, 10077.0205, 11554.7119, 10627.8008,
        17207.3770, 14164.4248, 13624.0830, 16365.7148, 13403.3691, 10968.4785,
        19417.7520, 13722.6309, 15937.4600, 15559.1104, 16512.9316, 10830.7539,
        24836.7891, 13025.0723, 11056.0176, 11616.4912, 17580.8105, 11580.5645,
        11579.4414, 14183.1074, 14751.0205, 11836.0928, 15968.5488, 15170.5195,
        10200.7520, 13494.3125, 18336.9023, 14030.1279, 13690.5410, 10802.7139,
        12481.6309, 12669.8213, 14443.4180, 10968.5352, 13176.4824, 13404.2803,
        11598.4893, 11281.6143, 12188.8467,  9720.4600, 15161.6807, 16957.0449,
        14069.0439, 12639.8975, 14851.4570, 11998.2432, 13334.4014, 12140.7891,
        14544.7910, 13549.8066, 10079.9707, 12509.3994,  8955.9609, 16976.5703,
        12689.5312, 12453.1738, 15758.2852, 19490.9375, 12360.2861, 14141.7480,
        11074.3232, 17917.6816, 11690.9434, 15225.2793, 17343.7520, 12510.7061,
        12864.7285, 14277.9355, 10144.8906, 17491.4160, 10226.2139, 10734.7822,
        13868.4131, 10870.8262, 14081.8096, 12280.8730,  8887.4902, 16926.8379,
        13920.7207, 14470.8467, 12935.8604, 20370.2383, 11633.2217, 11954.4150,
        19112.4492, 14074.0068, 12195.8975, 13410.1895, 12962.2588, 16261.8262,
        11930.4326, 15496.0137, 13527.4453, 10581.8779, 13294.3906, 11972.1445,
        14305.7852, 12353.8525, 17955.7031, 16259.4082,  9790.9795, 11400.8730,
        13005.6094, 11862.4668, 10660.3955, 14462.7002, 13821.4023, 14832.0840,
        14591.6641,  8903.6885, 12985.6465, 12341.6553, 17981.9863, 14206.4639,
        15558.0264, 12023.8779, 18503.0586, 12925.1963, 15457.0684, 10785.2256,
        15518.7422, 11241.8555, 11450.3535, 10528.9668, 12575.0547, 17521.1328,
        11802.1992, 13759.0029, 10560.0098, 12149.8818, 12398.7109, 12856.8721,
        11904.1416, 12549.9697, 16573.8555, 16559.2344, 12506.5342,  9139.8965,
        12169.8652, 10770.7812,  8766.8750, 13595.1328, 15424.0371, 15785.4912,
        15903.8408, 17165.2891,  9226.4912, 13781.1826, 13651.0967, 12293.1191,
        13254.9619, 12333.6289, 17792.2031, 13769.8496, 13106.6650, 13647.9336,
        15514.9365, 14029.7188, 12930.2158, 12050.6758, 10371.2158, 13655.0605,
        15385.5186, 12098.6758, 16239.3369, 13145.7842, 13927.3350, 12382.0566,
        10353.7109, 13858.1182, 11058.4326, 11188.9844, 18182.3340, 14756.6787,
         9872.9092, 11324.7236, 19462.7188, 13172.9014, 19084.1367, 17150.6348,
        15416.0840, 14920.6328, 11538.0264, 11409.1387, 12534.3076, 14761.3936,
        13660.6230, 13790.4873, 12619.5947, 15427.6494, 13673.2676, 15857.3027,
        13401.6924, 11682.3574, 11739.7158, 12448.8955, 12306.4443, 16494.2930,
        10075.1660, 13715.2744, 12449.0059, 17594.1973, 14207.3340, 17372.0645,
         9188.9219, 18077.8008, 13278.7490, 15202.1484, 15296.6777, 18390.8320,
        12063.0859, 13203.3740, 13089.2197, 13116.5342, 10118.2314, 13217.4043,
        14788.5996,  9916.2480, 12366.1191, 14464.4395, 15614.0977, 15346.4004,
         9149.0791, 13441.5156, 17909.1172, 11970.3281, 14553.6699, 10169.8555,
        15552.1562, 16195.4521, 13359.0977,  9059.1396, 14279.8223, 12843.3535,
        16480.1113, 21790.0488, 13128.0537, 14012.2988, 12255.1055,  9723.4932,
        14695.7461, 11826.6211, 14218.6514, 14339.6104, 14333.5078,  9882.7061,
        10764.1484, 15847.8301, 10755.7588, 11354.2578, 16692.8887, 12836.0420,
        18653.1387, 15345.1963, 16695.2070, 13985.4766, 11697.3467, 16229.8936,
        10900.2822, 16022.6914, 15275.4072, 12735.8711, 12264.2998, 11550.7119,
        14273.6631, 14956.1133, 11035.0908, 12507.4814, 14870.3135, 16276.4092,
        10010.8301, 10526.1680, 18396.1562, 12268.4941, 16707.7949, 11095.9180,
        10587.8203, 10894.2188, 12063.7227, 11466.6748, 14309.3906, 15825.9141,
        17312.8223, 13659.4150, 11674.5186, 14261.0527, 10639.7773, 18996.4336,
        15089.9268, 12923.0859, 10912.8213, 12847.5889, 13674.0459, 12966.2881,
        15695.9521, 10584.6387, 16122.6738, 10065.6348, 14485.6934, 13713.5508,
        20878.1426, 14727.0312, 16387.8105, 11200.4170, 14449.8799, 15125.0928,
        13846.1465, 11212.2568, 13985.7051, 13215.8193, 10860.1660, 13681.9473,
        12835.5088, 15838.5400, 13237.0645, 13509.1377, 17403.3418, 13767.1699,
        16373.2773, 16727.4922, 12293.3955, 18356.4355, 17979.4160, 14388.9521,
        11131.8584, 14123.1680, 11213.9785, 18203.5371, 11649.3438, 11859.0078,
        14427.9912, 10678.6660, 10033.7012, 12654.1689, 10816.6797, 12826.5713,
        12870.3633, 14739.2891, 11853.0029, 15101.7832, 14548.0645, 13057.7256,
        12525.2480,  9566.6592, 13555.3965, 11890.8418, 19485.5918, 14805.4268,
        11485.3164, 12673.9326, 11704.7500, 15332.0332, 11821.7451, 14618.7676,
        14224.2568, 11534.3311, 12424.8213, 17499.4766, 13496.9072, 13292.0723,
        11426.8252, 15312.0264, 17242.0879, 13501.1172, 14332.0928, 11203.5957,
        17451.6094, 14752.7480, 14605.4951,  9855.1816, 16133.9883,  9346.6348,
        12407.3594, 10690.4775, 11245.1494, 12229.7559, 14228.9619, 16314.4863,
        12109.5898, 13633.5557, 13556.8906, 17095.8184, 11490.7266, 12494.6543,
        15519.1602, 14643.4766, 10870.6240, 12095.6260, 21008.7500, 17307.4336,
         9023.3818, 15032.9141, 12873.2051, 14066.4443, 20536.4707, 15226.8447,
        13437.0820, 11677.5107, 15142.3135, 14308.7979, 14974.7295, 13489.6943,
        10402.3398, 16033.0654, 15287.8213, 14120.4590, 10271.2520, 12448.8066,
        14888.6924,  8789.3438, 16232.7793, 13733.2354, 13822.1465,  9383.6299,
        14945.0635, 12072.2295, 13587.9512, 13911.6133, 10407.5381, 11384.0801,
        11532.0244, 17393.1777, 10288.5645, 12528.8027, 17966.7324, 13411.0732,
        16110.8027, 14960.4385, 16335.9014, 10371.3477, 12424.5352, 14785.3867,
        14708.7588, 14698.1191, 15174.0156, 15638.0762, 11621.1992,  9729.7119,
        10522.0664, 15137.7637, 14741.3672, 12721.4033, 16840.9062, 12786.9492,
         8808.0332, 12678.1582,  9798.9092, 16625.0078, 13250.2158, 16403.8789,
        10084.4336, 12527.6182,  7900.8262, 12743.6006, 10321.5430,  9055.9844,
        13469.5391, 10389.1484,  9184.0000, 13447.2891, 11421.6660, 12602.6758,
        11913.6582, 16821.1152, 12123.4141, 14525.9043, 15372.3613, 14185.9688,
        14383.2803, 10796.0107, 12019.6230, 15144.4111, 10272.6602, 15119.7920,
        12378.3096, 12028.7529, 15806.3457, 10816.1455, 15597.6904, 10045.7930,
        10989.8340, 12413.2129, 11429.0479,  8947.2910, 13781.1045, 15462.8721,
        11932.6152, 12293.8115, 13457.4697, 10926.0352, 14928.2217, 14773.5430,
        11987.9795, 11521.4141])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([47967.8125, 39115.0000, 28035.2695, 40225.6211, 37873.6016, 36714.6602,
        41835.4180, 29940.3770, 37364.8477, 39284.1953, 52538.2734, 34612.5312,
        38905.5859, 29896.0410, 30200.7988, 27828.3516, 28532.1875, 27311.6270,
        43742.6445, 35061.2773, 39629.7578, 39093.1641, 27874.5293, 36352.7031,
        30273.0488, 56852.5859, 45442.5391, 33312.4219, 30940.4648, 34186.8047,
        29392.1113, 24954.7793, 48161.5117, 34867.2852, 40430.4961, 40548.8516,
        34427.6406, 44972.3516, 29371.6973, 41014.4180, 51750.5703, 28092.8574,
        35280.3867, 41503.9102, 36605.7070, 48663.8242, 36011.4922, 44022.4023,
        33054.4023, 35838.2461, 33087.2812, 40993.8008, 30231.0840, 33621.4453,
        44584.8828, 28020.7207, 45739.3672, 23116.9824, 34986.7227, 48820.5469,
        34190.3008, 36528.5391, 45524.4570, 36151.2930, 38630.9023, 42236.3711,
        48527.7305, 31760.0352, 25038.6621, 29032.9219, 37205.3125, 44074.2578,
        28021.5645, 39055.4570, 44362.8672, 34120.2812, 46931.5664, 32673.6465,
        37099.2578, 31786.8613, 35953.6523, 31836.0820, 30368.4805, 41443.9023,
        34297.1172, 47307.4961, 47647.1562, 50438.0156, 33559.7578, 26641.0547,
        46387.4102, 43774.8633, 35898.0898, 28614.0234, 41208.0156, 38434.7773,
        31100.9199, 36445.9961, 27875.3672, 27033.3086, 27955.4668, 32795.3828,
        35262.7266, 57659.4570, 32303.4766, 44207.1914, 29922.0918, 39320.3906,
        33178.6211, 35141.7266, 38545.6406, 41803.1953, 46171.9453, 39086.2422,
        27424.3320, 36494.1055, 32643.8594, 46715.2969, 30461.1328, 25423.2461,
        43541.7852, 31489.7852, 33872.0625, 34963.5703, 39045.1719, 45367.2617,
        31905.5625, 43960.4219, 38798.4961, 38471.0039, 33602.2695, 39580.4414,
        31099.1016, 40113.8203, 38790.5352, 35744.2930, 35136.5586, 33886.5117,
        35184.7383, 42601.7070, 37658.3086, 37183.5938, 24824.1309, 41407.8750,
        31748.3320, 28932.4883, 38055.0312, 47254.7852, 28614.5352, 34929.3750,
        35661.3242, 30891.8477, 29854.4062, 28941.8809, 37941.2930, 34569.2695,
        33968.8672, 49284.4961, 40965.3672, 48483.5156, 30340.0762, 38211.2500,
        32027.4883, 26297.2266, 33163.2266, 37427.9570, 44133.8945, 40548.7070,
        42551.8594, 29687.0703, 48694.1836, 28985.5605, 50727.3906, 36772.4648,
        31477.7969, 29655.4727, 31068.6777, 41386.9375, 33615.5742, 38266.3281,
        42384.8438, 31207.0156, 37594.7852, 50780.5273, 46722.8242, 47332.6484,
        37513.5820, 27755.1523, 38968.2461, 51015.1562, 30604.4844, 42715.5625,
        49745.3203, 56947.0938, 43271.3789, 23281.1367, 38682.7305, 27886.8750,
        37578.3086, 44802.5430, 38328.6367, 35098.6523, 29667.5137, 46391.9570,
        45120.9102, 30026.7559, 32700.0195, 36032.5273, 33158.2852, 29852.1191,
        44338.9062, 34639.6250, 28449.4629, 27937.3301, 36377.9336, 38640.8398,
        37070.2578, 32739.2324, 39295.3320, 35706.6602, 44967.4570, 39190.3711,
        41403.6953, 45739.9727, 27918.0820, 39775.7812, 28402.4531, 32423.0234,
        42136.2734, 40232.2070, 30437.1191, 39258.2422, 32166.2129, 45879.8867,
        39592.8398, 33153.3359, 42487.5820, 26364.9590, 41551.5781, 38970.0898,
        51162.7695, 49613.8867, 31578.9688, 37683.1719, 39653.2852, 43558.3008,
        29590.1348, 27877.8848, 43557.6562, 32352.1113, 34332.0039, 54426.9609,
        38502.9102, 31078.3691, 28257.2598, 37541.5391, 43248.2383, 32811.9805,
        45293.6680, 36840.9492, 35782.4883, 41405.2148, 26543.0352, 27349.3457,
        53528.8672, 26757.7402, 28887.3086, 27771.5020, 34505.3516, 33638.2461,
        33672.3047, 32790.9336, 37753.7773, 35727.3281, 35324.1172, 45616.7305,
        35930.3242, 37493.7422, 42066.2734, 31577.6738, 45220.5391, 48289.2773,
        35518.6367, 38622.3320, 36687.1758, 35136.0508, 41770.7422, 39045.8242,
        32187.2852, 30434.3809, 37928.0117, 41035.5156, 33580.6562, 31760.8887,
        32838.8359, 26126.1445, 31427.9258, 36904.1211, 29599.5078, 36960.2773,
        26528.5684, 40990.2500, 32114.9941, 28744.6641, 38797.4141, 32876.7461,
        36602.0391, 30015.0508, 27437.6113, 36469.1328, 39503.4297, 38237.9531,
        43492.2383, 34263.8203, 35244.0859, 35190.7148, 47679.6172, 45415.9141,
        33250.6641, 36089.1836, 49864.7891, 31408.2090, 36996.3711, 40240.2695,
        31509.0723, 43224.5195, 42488.9688, 38478.9727, 49755.8281, 52042.0625,
        33105.8984, 28314.1348, 27543.6484, 39587.2617, 47110.0234, 43378.7148,
        32097.0039, 24279.7422, 47864.6914, 36470.0938, 37850.2344, 52134.4102,
        25541.4238, 38271.5625, 36981.6289, 46161.3750, 38967.3828, 37904.3750,
        35998.2461, 30749.3652, 35390.1875, 36685.5742, 34057.4609, 32665.3887,
        36295.7695, 31711.5547, 33542.1328, 26289.8281, 44890.7266, 27115.4531,
        34151.1719, 40601.1289, 34499.8945, 29634.7598, 33587.0430, 37033.2148,
        31833.8105, 44150.3203, 44625.2031, 33889.3789, 49809.6953, 34292.4648,
        33310.9766, 28856.1543, 37695.6133, 46245.9844, 33482.9062, 38580.6133,
        31263.2148, 28026.1348, 26635.7969, 39019.1719, 46629.1758, 24653.5977,
        28988.5957, 45045.8906, 48428.3477, 28287.5469, 39155.1641, 37638.8320,
        36672.4375, 34477.5508, 42232.8242, 39900.5000, 40739.0430, 36053.1836,
        39501.7695, 46550.8047, 36262.8047, 34215.2930, 33139.9062, 42907.8477,
        41989.1875, 25209.9277, 33337.3359, 35888.6562, 47986.1289, 40255.6055,
        36678.4766, 33703.7109, 27839.6680, 38342.3164, 35067.8711, 29121.6035,
        28180.2676, 30587.1582, 37708.8984, 34238.4609, 45903.7227, 37765.8086,
        45415.9023, 54758.0156, 32427.1699, 31728.7051, 47447.2148, 46695.2109,
        43401.0859, 42350.8906, 46610.3633, 40817.4727, 34937.1875, 26351.4980,
        41730.7500, 39090.7031, 30129.4590, 38476.9023, 35014.5195, 34891.1250,
        39437.6953, 46924.9883, 42528.9023, 40215.3125, 49071.9141, 31139.4922,
        40461.1055, 28163.8320, 36392.5039, 30572.4883, 40367.4414, 35396.3633,
        45362.6797, 31238.9707, 41707.6133, 42975.8047, 50044.4805, 35629.8789,
        31391.9531, 31401.8359, 39329.3711, 32065.1758, 48507.0469, 38479.1992,
        35776.5547, 29430.7344, 31581.1348, 40210.0664, 38649.4180, 33582.2422,
        34152.9609, 34929.2773, 33602.9102, 41283.5273, 36973.7031, 32822.9062,
        41057.3750, 33080.7656, 53042.5586, 26600.7676, 42096.5430, 27916.9258,
        53837.2383, 43414.9766, 43089.8477, 36182.3828, 36928.8281, 33697.7344,
        28516.3906, 44825.7031, 38464.3164, 32101.4785, 38114.7422, 33174.5117,
        46256.0664, 30984.8125, 36290.8281, 31224.2676, 32599.8340, 50977.7070,
        38215.4648, 39487.9961, 28479.9844, 33613.7422, 42999.1094, 48134.4805,
        27560.5176, 36724.8867, 32125.5273, 39464.1328, 28566.8125, 37880.8359,
        28605.4922, 27680.5449])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1291.2964,   30.1408,  201.5727,  ...,    6.7101,  192.7522,
          84.8660])
Calling generate_global_pruning_mask with pruning_rate: 0.3
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.6730e+31, 7.8170e+31, 5.7895e+31, 6.9606e+31, 1.1035e+32, 1.2733e+32,
        8.7101e+31, 1.0066e+32, 1.0239e+32, 1.0723e+32, 5.4556e+31, 1.2842e+32,
        6.8186e+31, 7.4394e+31, 1.7539e+32, 4.1352e+31, 1.2586e+32, 4.9257e+31,
        5.1011e+31, 1.0975e+32, 3.5889e+31, 8.7373e+31, 6.2949e+31, 5.8583e+31,
        9.7922e+31, 5.6798e+31, 1.0040e+32, 8.4782e+31, 7.3021e+31, 8.3380e+31,
        5.1298e+31, 1.2069e+32, 6.3236e+31, 7.3901e+31, 9.3889e+31, 1.3815e+32,
        7.3349e+31, 1.4949e+32, 1.0049e+32, 1.3848e+32, 1.4875e+32, 6.5424e+31,
        5.8275e+31, 5.3738e+31, 6.1101e+31, 6.2715e+31, 7.5853e+31, 8.5179e+31,
        7.3475e+31, 9.5972e+31, 7.1046e+31, 1.3348e+32, 1.8825e+32, 8.1764e+31,
        5.5137e+31, 8.2113e+31, 1.6142e+32, 4.9947e+31, 6.0951e+31, 7.4240e+31,
        3.7655e+31, 8.0619e+31, 3.7195e+31, 7.2040e+31])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.9891e+31, 3.6307e+31, 4.1234e+31, 4.2382e+31, 1.8721e+31, 4.0730e+31,
        4.4953e+31, 4.2507e+31, 2.8337e+31, 2.8139e+31, 2.6328e+31, 5.7071e+31,
        6.9353e+31, 2.4479e+31, 3.3791e+31, 4.1921e+31, 3.0585e+31, 2.3812e+31,
        2.6242e+31, 7.1573e+31, 2.2314e+31, 1.9015e+31, 2.0252e+31, 4.9245e+31,
        6.0370e+31, 3.9780e+31, 4.2746e+31, 5.5910e+31, 4.8387e+31, 3.1746e+31,
        3.0613e+31, 5.0816e+31, 5.2354e+31, 4.6914e+31, 6.3460e+31, 2.1447e+31,
        8.4243e+31, 2.2079e+31, 8.9588e+31, 2.1566e+31, 4.4417e+31, 4.5512e+31,
        4.1611e+31, 2.3432e+31, 3.5767e+31, 5.4134e+31, 5.6335e+31, 3.2811e+31,
        5.1234e+31, 2.4466e+31, 4.7396e+31, 6.2142e+31, 4.7643e+31, 2.0518e+31,
        5.9901e+31, 6.7335e+30, 2.0815e+31, 3.3294e+31, 3.8343e+31, 2.4035e+31,
        8.6024e+31, 7.8925e+31, 2.4566e+31, 3.3934e+31])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.4133e+31, 3.4893e+31, 2.2617e+31, 4.0074e+31, 4.8595e+31, 3.4065e+31,
        2.0381e+31, 1.9189e+31, 3.6077e+31, 1.8551e+31, 5.6272e+31, 5.6095e+31,
        4.4646e+31, 1.9894e+31, 1.7566e+31, 4.3444e+31, 3.4901e+31, 1.8654e+31,
        1.7699e+31, 2.6060e+31, 4.3548e+31, 2.8370e+31, 6.3712e+31, 2.8706e+31,
        1.6506e+31, 2.3320e+31, 3.7079e+31, 3.7660e+31, 3.1942e+31, 2.0232e+31,
        1.5782e+31, 2.6650e+31, 4.0864e+31, 2.3797e+31, 4.3334e+31, 1.7863e+31,
        1.6925e+31, 2.1089e+31, 1.6824e+31, 3.0453e+31, 3.7107e+31, 2.1139e+31,
        1.8078e+31, 3.5745e+31, 1.4315e+31, 5.6880e+31, 5.2890e+31, 2.7835e+31,
        4.3573e+31, 3.7662e+31, 1.6470e+31, 2.1098e+31, 4.1249e+31, 2.7499e+31,
        5.3407e+31, 2.3579e+31, 2.2085e+31, 1.8478e+31, 4.2216e+31, 2.1889e+31,
        2.4288e+31, 1.8995e+31, 2.9583e+31, 3.0679e+31])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0126e+29, 4.1093e+30, 1.2852e+30, 1.5201e+30, 7.8759e+29, 3.2145e+31,
        1.4700e+29, 1.4212e+29, 5.1815e+28, 2.8912e+29, 6.2690e+28, 1.8576e+30,
        1.3163e+29, 8.4292e+29, 8.4287e+29, 2.5330e+29, 1.3486e+29, 2.4264e+29,
        8.7951e+29, 5.1713e+28, 6.1063e+28, 5.6766e+29, 2.0628e+29, 8.0050e+28,
        2.1815e+29, 1.1083e+30, 4.6981e+29, 1.9381e+30, 4.7268e+29, 2.0903e+29,
        2.9799e+29, 8.4553e+28, 8.0395e+29, 4.7007e+30, 1.9080e+29, 1.6254e+29,
        2.3591e+29, 3.7561e+29, 3.5551e+29, 3.7930e+29, 8.2090e+28, 5.3769e+30,
        7.4152e+28, 1.3541e+30, 1.0839e+29, 6.8606e+29, 5.5631e+29, 1.1308e+30,
        1.8317e+29, 1.1407e+30, 1.7162e+29, 4.3039e+29, 2.8547e+30, 3.6928e+28,
        3.8922e+30, 8.7667e+30, 7.3793e+29, 1.9551e+29, 1.1566e+29, 3.8587e+28,
        3.8642e+29, 5.3873e+28, 6.1832e+28, 5.7129e+29, 2.4121e+30, 5.1832e+30,
        8.7209e+28, 1.0015e+30, 6.9924e+30, 4.2733e+29, 7.9479e+29, 1.6880e+29,
        6.4132e+30, 2.9318e+29, 3.2348e+29, 6.7031e+28, 5.0918e+28, 5.7786e+29,
        6.0399e+29, 6.3524e+29, 7.3837e+29, 6.7568e+29, 9.2898e+28, 2.6614e+29,
        7.6161e+28, 1.5800e+29, 4.3513e+29, 9.2802e+29, 9.0378e+29, 4.0055e+28,
        6.3420e+28, 1.7486e+29, 7.2831e+29, 1.7620e+29, 8.9167e+28, 2.1608e+28,
        1.3167e+29, 4.0690e+29, 3.6588e+28, 8.9256e+28, 1.0289e+29, 4.2453e+29,
        1.6111e+29, 1.6485e+29, 1.1804e+30, 4.7507e+29, 1.1221e+30, 1.3177e+29,
        3.0985e+28, 1.9801e+29, 1.1969e+29, 4.3982e+28, 2.7772e+29, 1.7201e+29,
        2.4296e+29, 3.7809e+29, 9.0137e+28, 1.7213e+30, 1.3316e+29, 6.6590e+29,
        6.1287e+29, 3.0793e+29, 1.8556e+31, 6.7493e+29, 1.5620e+29, 3.2279e+29,
        5.3572e+29, 4.4077e+29, 1.2503e+29, 4.7145e+29, 4.1850e+29, 5.9843e+29,
        9.4980e+29, 1.0572e+30, 2.0013e+30, 2.3016e+29, 1.5111e+29, 1.7325e+29,
        7.8752e+28, 6.1727e+29, 1.4271e+30, 4.1855e+29, 5.6132e+30, 3.7402e+29,
        4.9109e+30, 2.6007e+28, 2.1019e+30, 3.6817e+29, 9.7251e+28, 7.6198e+29,
        1.6235e+29, 3.8883e+29, 5.8539e+29, 3.7705e+29, 1.4206e+29, 7.0555e+28,
        1.6728e+29, 3.2750e+29, 9.6411e+29, 8.7282e+28, 1.7211e+30, 1.0861e+29,
        6.1090e+29, 1.0132e+29, 1.6821e+30, 4.1897e+29, 1.7752e+29, 1.8658e+28,
        4.7500e+29, 9.3377e+29, 2.4990e+29, 3.2072e+31, 6.6686e+29, 4.0119e+29,
        2.3657e+30, 2.6310e+28, 2.7033e+29, 1.4817e+29, 4.7723e+29, 4.1352e+29,
        9.3506e+29, 2.9211e+29, 2.2123e+29, 3.2891e+29, 6.8289e+28, 6.5179e+30,
        1.8331e+29, 2.1037e+29, 6.9725e+29, 1.4842e+30, 5.8638e+29, 2.3170e+29,
        9.9443e+28, 5.4859e+29, 5.8716e+29, 1.8389e+29, 2.8164e+29, 1.8698e+29,
        1.9075e+29, 5.2896e+28, 1.9664e+29, 3.6343e+29, 1.7980e+29, 5.6761e+29,
        4.2730e+30, 3.5072e+29, 1.6358e+30, 1.2946e+29, 2.3525e+30, 3.9008e+30,
        1.9985e+29, 4.9861e+28, 6.4147e+29, 2.4147e+29, 1.1884e+29, 1.9765e+29,
        3.9959e+30, 8.7693e+29, 1.2360e+29, 9.4665e+28, 1.2585e+29, 2.1854e+28,
        2.6335e+29, 2.3204e+29, 7.9435e+28, 2.3353e+29, 5.5138e+29, 2.0976e+29,
        8.7797e+29, 1.3708e+29, 5.6279e+29, 1.6117e+30, 2.5937e+30, 3.1557e+29,
        5.2797e+29, 6.0502e+28, 3.0071e+30, 1.3812e+29, 2.1721e+29, 4.0976e+28,
        9.2617e+28, 1.0921e+29, 2.9950e+29, 9.4624e+29, 2.7006e+29, 2.7060e+29,
        3.0608e+29, 4.0551e+29, 2.3804e+29, 2.9893e+28, 9.3402e+29, 8.2464e+29,
        2.7879e+28, 2.1390e+30, 2.5118e+29, 1.3399e+29])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3768e+29, 3.8773e+30, 1.2508e+30, 1.6507e+30, 6.3409e+29, 3.2126e+31,
        1.9643e+29, 5.8191e+28, 1.7031e+29, 2.6354e+29, 1.3015e+29, 2.0507e+30,
        1.5679e+29, 9.4369e+29, 6.5535e+29, 5.8745e+29, 1.8024e+29, 3.3178e+29,
        8.9885e+29, 3.1239e+29, 2.9903e+29, 5.3252e+29, 9.7199e+27, 5.4730e+28,
        1.7321e+29, 1.0246e+30, 5.7441e+29, 1.7973e+30, 2.3541e+29, 1.7520e+29,
        2.2281e+29, 2.0374e+29, 7.7240e+29, 4.6232e+30, 4.2266e+28, 1.7917e+29,
        9.0320e+28, 2.0764e+29, 4.4971e+29, 5.7146e+29, 2.1396e+29, 5.2860e+30,
        1.3445e+29, 9.5286e+29, 1.2021e+29, 7.1241e+29, 3.5633e+29, 1.1621e+30,
        3.9781e+29, 1.2127e+30, 3.8066e+29, 3.2660e+29, 2.8038e+30, 3.3150e+29,
        4.0142e+30, 8.5807e+30, 6.1025e+29, 5.3674e+28, 4.7337e+28, 1.7530e+29,
        4.0446e+29, 3.1246e+29, 1.6613e+29, 3.6083e+29, 2.4336e+30, 5.0447e+30,
        1.0203e+29, 7.6504e+29, 6.9492e+30, 6.7725e+29, 6.9178e+29, 1.5493e+28,
        6.7857e+30, 1.9393e+29, 1.9715e+29, 2.8073e+29, 8.6131e+28, 5.5192e+29,
        4.6815e+29, 6.8502e+29, 6.3912e+29, 6.6040e+29, 5.7801e+28, 1.9730e+29,
        7.8382e+28, 4.5626e+28, 3.9560e+29, 9.0394e+29, 9.4457e+29, 4.6383e+28,
        1.0214e+29, 1.2365e+29, 4.1326e+29, 9.0330e+28, 1.4884e+29, 1.2859e+29,
        1.2413e+29, 3.2132e+29, 6.0795e+28, 3.8108e+28, 1.6764e+29, 5.2573e+29,
        1.4959e+29, 3.4233e+29, 1.3508e+30, 5.9361e+29, 1.3186e+30, 7.5760e+28,
        2.0196e+29, 8.3019e+28, 6.7067e+28, 1.1319e+29, 2.2984e+29, 2.1144e+28,
        3.5846e+29, 2.9513e+29, 9.0678e+28, 1.6457e+30, 2.4542e+29, 5.8610e+29,
        5.7852e+29, 2.3556e+29, 1.8575e+31, 7.3462e+29, 4.7740e+29, 4.0752e+29,
        2.2496e+29, 4.2634e+29, 8.0742e+28, 7.1629e+29, 3.0933e+29, 3.4037e+29,
        6.4338e+29, 8.7748e+29, 1.6089e+30, 9.5244e+28, 6.0284e+28, 2.9073e+29,
        1.1813e+29, 7.8338e+29, 1.3018e+30, 2.0951e+29, 5.7244e+30, 2.0997e+29,
        4.8289e+30, 2.2382e+29, 1.9238e+30, 4.9169e+29, 1.3586e+29, 7.4235e+29,
        3.5854e+29, 4.7888e+29, 5.6754e+29, 4.2385e+29, 8.9224e+28, 2.2099e+29,
        2.5413e+29, 3.5382e+29, 1.2112e+30, 9.6718e+28, 1.6273e+30, 1.4287e+29,
        3.9588e+29, 7.8627e+28, 1.6591e+30, 4.4476e+29, 1.9637e+29, 1.4613e+29,
        3.3364e+29, 1.0685e+30, 5.0925e+28, 3.1982e+31, 6.5928e+29, 9.6076e+28,
        2.3506e+30, 1.3566e+29, 3.1709e+29, 5.5208e+28, 2.7237e+29, 2.8408e+29,
        7.4285e+29, 3.8033e+29, 8.5897e+28, 1.2571e+29, 2.8969e+28, 6.4285e+30,
        1.4262e+29, 6.5010e+28, 9.9669e+29, 1.0956e+30, 4.4343e+29, 4.7776e+29,
        1.2629e+29, 5.4619e+29, 3.4839e+29, 1.1800e+29, 1.2171e+29, 3.3255e+28,
        1.0389e+29, 3.9418e+29, 3.3309e+28, 1.4241e+29, 1.2978e+29, 6.8387e+29,
        4.2383e+30, 2.5390e+29, 1.4867e+30, 9.4125e+28, 2.2825e+30, 3.9915e+30,
        8.7245e+28, 7.7607e+28, 5.1958e+29, 3.3431e+29, 2.3454e+29, 1.9117e+29,
        4.0953e+30, 7.0159e+29, 3.3180e+29, 2.7327e+29, 6.6868e+28, 1.0373e+29,
        7.8217e+28, 2.6779e+29, 7.5817e+28, 3.5230e+29, 5.4757e+29, 2.8115e+29,
        8.3245e+29, 3.2089e+29, 8.1661e+29, 1.3818e+30, 2.5194e+30, 2.2511e+29,
        3.4998e+29, 8.1029e+28, 3.1314e+30, 2.1424e+29, 3.4820e+29, 9.1828e+28,
        2.8132e+29, 6.7480e+28, 4.7007e+29, 8.0585e+29, 9.3409e+28, 1.7749e+29,
        3.4647e+29, 2.3997e+29, 3.5673e+29, 1.4182e+29, 8.5803e+29, 8.3025e+29,
        1.4976e+29, 2.1612e+30, 3.0180e+29, 6.4285e+28])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5402e+29, 4.7006e+29, 2.8736e+29, 1.8313e+29, 7.2432e+29, 2.2061e+29,
        4.9745e+29, 2.7766e+29, 2.8617e+29, 5.4960e+29, 4.7765e+29, 6.9849e+29,
        2.1194e+29, 1.7856e+29, 4.2438e+29, 1.8894e+29, 3.1269e+29, 2.5105e+29,
        3.0552e+29, 2.4437e+29, 4.1771e+29, 3.5293e+29, 4.0808e+29, 2.7670e+29,
        2.7847e+29, 4.0573e+29, 1.5535e+29, 4.7168e+29, 2.8509e+29, 6.0830e+29,
        3.4440e+29, 4.1623e+29, 2.3040e+29, 5.5401e+29, 2.8040e+29, 3.2065e+29,
        3.8813e+29, 3.3412e+29, 5.2691e+29, 5.7715e+29, 1.8341e+29, 2.5408e+29,
        3.5795e+29, 2.1881e+29, 2.6503e+29, 1.5367e+29, 1.6283e+29, 4.3074e+29,
        2.4803e+29, 8.2753e+28, 3.5584e+29, 2.3182e+29, 3.1857e+29, 3.7709e+29,
        4.4465e+29, 5.2835e+29, 2.2630e+29, 2.0439e+29, 3.0407e+29, 2.9664e+29,
        7.5762e+29, 2.5811e+29, 1.2160e+29, 2.1816e+29])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5830e+29, 2.2988e+29, 1.2126e+29, 1.9582e+29, 2.4124e+29, 1.8679e+29,
        3.4271e+29, 3.0943e+29, 1.1793e+29, 9.5724e+28, 3.0486e+29, 1.3644e+29,
        2.0083e+29, 1.2014e+29, 2.2625e+29, 4.4476e+29, 1.1978e+29, 1.5404e+29,
        3.3979e+29, 2.6305e+29, 3.9084e+29, 3.1250e+29, 2.5509e+29, 1.7463e+29,
        3.6149e+29, 3.7508e+29, 2.1905e+29, 3.0036e+29, 2.0985e+29, 2.8081e+29,
        3.1587e+29, 2.5362e+29, 2.9719e+29, 2.9706e+29, 1.7033e+29, 2.8385e+29,
        3.1655e+29, 5.5525e+29, 2.2639e+29, 1.5361e+29, 3.6086e+29, 2.2132e+29,
        1.0929e+29, 2.9429e+29, 1.8079e+29, 1.4219e+29, 3.6536e+29, 1.2218e+29,
        1.9291e+29, 4.4291e+29, 4.3380e+29, 1.8284e+29, 1.2585e+29, 5.1954e+29,
        1.9769e+29, 4.3362e+29, 1.3830e+29, 2.7536e+29, 2.4199e+29, 2.6934e+29,
        3.4444e+29, 1.5928e+29, 2.3196e+29, 2.5092e+29])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.7637e+26, 4.9067e+27, 6.3126e+27, 3.9695e+27, 2.4428e+29, 6.8938e+27,
        7.3038e+27, 1.2136e+27, 3.0865e+27, 2.3467e+27, 2.1751e+27, 3.2879e+27,
        1.5112e+27, 8.4126e+27, 1.3351e+27, 1.9500e+29, 2.8761e+27, 2.3621e+27,
        1.4472e+27, 3.7530e+27, 1.8848e+28, 6.5492e+27, 3.4574e+27, 3.8212e+27,
        1.1557e+27, 7.6126e+27, 2.5603e+27, 3.8558e+27, 3.4654e+27, 8.5916e+27,
        2.6484e+27, 3.6981e+27, 4.7198e+27, 5.8461e+27, 3.5784e+27, 4.8885e+27,
        2.8638e+29, 8.8319e+27, 4.4974e+27, 1.7952e+27, 3.5347e+27, 3.4063e+27,
        5.0484e+28, 4.9142e+27, 2.2085e+27, 2.6300e+27, 3.4919e+27, 1.7785e+28,
        1.2267e+27, 1.5247e+28, 1.6980e+27, 2.2996e+27, 1.4558e+27, 1.4445e+28,
        2.3904e+27, 2.8728e+27, 2.4999e+27, 2.6331e+27, 2.3892e+27, 9.2890e+28,
        9.9871e+27, 1.7870e+27, 4.6116e+27, 1.2351e+27, 4.7075e+27, 4.4710e+27,
        5.0222e+27, 2.0887e+27, 2.1740e+27, 3.5713e+27, 3.3542e+27, 2.2724e+28,
        4.1972e+27, 3.1321e+27, 5.7386e+27, 1.3319e+27, 4.6151e+26, 1.1005e+27,
        3.2591e+27, 1.5693e+27, 3.4189e+27, 2.7975e+27, 2.8582e+27, 7.0640e+27,
        6.2369e+27, 3.0936e+28, 5.0274e+27, 2.9366e+27, 2.8616e+27, 2.0873e+28,
        2.8954e+27, 4.7552e+27, 4.0096e+27, 1.7985e+27, 1.5454e+27, 2.3062e+27,
        2.8557e+27, 2.4804e+27, 1.2225e+27, 2.2984e+28, 1.3901e+28, 7.1625e+27,
        3.2712e+27, 3.2188e+27, 3.5562e+27, 3.4774e+27, 2.7674e+27, 1.0715e+28,
        6.9933e+27, 5.6223e+27, 2.3227e+27, 6.0882e+26, 3.9780e+27, 9.8168e+27,
        4.6403e+27, 2.6845e+27, 2.6964e+27, 2.5823e+27, 3.3855e+27, 5.1712e+27,
        4.0568e+27, 5.8156e+27, 2.6039e+27, 3.4049e+27, 1.8528e+27, 2.8941e+27,
        5.7309e+27, 2.8292e+27, 2.9253e+27, 9.0321e+27, 3.1187e+27, 3.6386e+27,
        1.1000e+28, 5.0759e+27, 5.4469e+27, 2.6314e+27, 8.1697e+28, 3.7668e+27,
        1.0603e+28, 1.6285e+27, 2.8497e+27, 2.8731e+27, 3.0862e+27, 1.8766e+27,
        2.9587e+27, 2.1364e+27, 5.9074e+27, 2.0718e+27, 2.1367e+28, 4.7636e+27,
        4.0558e+27, 1.4278e+28, 4.2943e+27, 2.9027e+27, 4.1488e+27, 3.1213e+27,
        7.7987e+26, 3.3286e+27, 3.2551e+27, 8.8063e+27, 2.8732e+27, 6.1647e+27,
        3.0317e+27, 1.8981e+27, 3.0809e+27, 5.1026e+27, 2.6248e+27, 4.0268e+27,
        1.6177e+27, 4.2497e+27, 4.0219e+27, 2.7596e+27, 4.6085e+27, 3.0615e+27,
        2.7253e+27, 2.7202e+27, 2.4659e+27, 2.2265e+27, 4.9737e+27, 4.2694e+27,
        3.5574e+27, 4.8116e+27, 4.7647e+27, 2.9180e+27, 1.3307e+27, 1.9574e+27,
        2.5127e+27, 1.5774e+27, 8.7033e+27, 2.2745e+27, 1.8356e+28, 2.4109e+27,
        1.3584e+27, 6.1626e+27, 1.6875e+27, 1.5383e+28, 3.2379e+27, 1.9705e+28,
        3.3821e+27, 7.2100e+26, 8.3282e+27, 1.1231e+27, 2.7257e+27, 2.7954e+27,
        3.8495e+27, 3.0040e+27, 4.9000e+27, 9.7620e+26, 1.9677e+27, 2.4935e+27,
        5.7274e+27, 5.2043e+27, 5.9219e+27, 4.1896e+27, 3.8812e+27, 5.7168e+27,
        3.5948e+27, 1.4351e+28, 1.0995e+27, 1.6818e+27, 3.1131e+27, 2.3107e+27,
        2.1307e+27, 7.9488e+27, 3.1214e+27, 3.4114e+27, 3.4467e+27, 7.4693e+27,
        5.0680e+27, 6.0537e+27, 2.2503e+27, 4.1504e+27, 5.3025e+27, 1.0518e+28,
        2.6241e+27, 3.0589e+27, 4.0326e+27, 2.5741e+27, 5.2973e+27, 1.4472e+27,
        3.8513e+27, 2.5239e+27, 2.5503e+27, 2.3779e+27, 1.7364e+27, 4.5522e+27,
        5.1972e+27, 5.8226e+27, 5.3160e+27, 2.4478e+28, 1.5076e+28, 2.6494e+27,
        2.3494e+27, 5.8783e+27, 4.8202e+27, 7.2578e+27])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.9429e+27, 4.2232e+27, 3.3307e+27, 3.5122e+27, 2.7788e+27, 8.4201e+27,
        5.7356e+27, 5.6275e+27, 8.1555e+27, 4.8676e+27, 2.9163e+27, 4.4392e+27,
        8.2031e+27, 6.4837e+27, 4.0715e+27, 5.2621e+27, 7.9513e+27, 4.4489e+27,
        6.6550e+27, 3.8092e+27, 5.4209e+27, 6.9476e+27, 9.5962e+27, 9.9254e+27,
        8.0301e+27, 5.2239e+27, 2.3434e+27, 8.1750e+27, 3.6898e+27, 7.1051e+27,
        9.3439e+27, 8.6005e+27, 6.3482e+27, 4.1508e+27, 4.6072e+27, 5.9448e+27,
        8.7469e+27, 5.4784e+27, 8.1314e+27, 6.8398e+27, 7.0587e+27, 4.5961e+27,
        5.0760e+27, 8.2780e+27, 3.4123e+27, 5.5867e+27, 3.7153e+27, 6.3732e+27,
        4.8485e+27, 1.0009e+28, 4.7576e+27, 5.8915e+27, 3.7678e+27, 2.6817e+27,
        4.7892e+27, 4.6405e+27, 6.4809e+27, 1.0079e+28, 4.1682e+27, 9.2937e+27,
        2.3893e+27, 6.4243e+27, 9.0057e+27, 5.5456e+27])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.1574e+27, 5.8044e+27, 3.9983e+27, 5.1338e+27, 2.6469e+27, 7.4280e+27,
        5.3013e+27, 4.7218e+27, 3.9540e+27, 6.5708e+27, 7.7095e+27, 4.6121e+27,
        4.9280e+27, 3.0948e+27, 7.9942e+27, 3.0164e+27, 6.4045e+27, 5.4337e+27,
        4.4658e+27, 6.5059e+27, 2.0696e+27, 2.8848e+27, 3.7498e+27, 5.2377e+27,
        5.4297e+27, 5.1734e+27, 4.0516e+27, 6.1175e+27, 4.4445e+27, 6.7913e+27,
        7.2875e+27, 4.9004e+27, 7.1676e+27, 3.7865e+27, 4.7085e+27, 6.0094e+27,
        8.2803e+27, 4.6333e+27, 5.4517e+27, 6.0246e+27, 3.2628e+27, 5.8277e+27,
        7.1767e+27, 4.4084e+27, 4.5336e+27, 7.1652e+27, 2.4721e+27, 8.2739e+27,
        5.6025e+27, 2.4100e+27, 3.5149e+27, 5.4860e+27, 5.6545e+27, 7.1033e+27,
        4.2832e+27, 6.7544e+27, 8.6971e+27, 5.2552e+27, 3.3017e+27, 4.8172e+27,
        6.0445e+27, 7.5129e+27, 6.7405e+27, 2.2561e+27])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0986e+25, 3.7634e+25, 2.7887e+25, 1.7993e+25, 5.2021e+25, 4.5893e+25,
        1.7762e+25, 4.3808e+25, 3.9382e+24, 1.6056e+25, 3.1809e+25, 2.3447e+25,
        7.0160e+25, 3.1259e+25, 1.5309e+26, 5.4394e+25, 3.4968e+25, 5.5074e+25,
        1.3293e+25, 2.1353e+25, 1.6924e+26, 2.2426e+25, 1.4370e+25, 1.2862e+26,
        2.2166e+25, 1.8316e+25, 5.2549e+24, 2.0722e+25, 1.4780e+25, 2.7211e+25,
        3.9074e+25, 1.7276e+25, 3.8398e+25, 2.4577e+25, 5.4561e+26, 1.2117e+25,
        2.1631e+25, 1.7052e+25, 2.1692e+25, 6.3431e+24, 2.0089e+25, 1.5757e+25,
        3.9787e+25, 1.0602e+25, 8.0460e+24, 1.7859e+25, 1.7415e+25, 1.8977e+25,
        3.3046e+25, 2.8480e+25, 2.2901e+25, 3.9822e+25, 5.2760e+25, 1.7024e+25,
        2.4593e+25, 2.6458e+25, 1.9489e+25, 1.1993e+25, 4.2304e+26, 7.5027e+25,
        2.7607e+25, 5.0667e+24, 5.2218e+25, 1.1089e+25, 1.5218e+25, 2.4511e+25,
        2.3153e+25, 3.5051e+25, 1.4979e+25, 1.6457e+25, 3.8331e+25, 1.5688e+25,
        1.9562e+25, 2.6597e+25, 9.3148e+24, 1.9904e+27, 1.3038e+26, 2.6587e+25,
        2.8023e+25, 6.7756e+24, 1.6127e+25, 1.0579e+25, 2.3807e+25, 2.4527e+27,
        1.4615e+25, 5.3116e+25, 3.0371e+25, 2.7918e+25, 2.1785e+25, 2.0990e+25,
        2.2923e+25, 2.0786e+25, 3.1475e+25, 2.2250e+25, 2.6623e+26, 1.2635e+25,
        2.7091e+25, 2.1286e+25, 1.5496e+25, 4.8371e+24, 9.2681e+26, 2.0193e+25,
        1.0872e+25, 3.8799e+25, 2.5737e+25, 1.4723e+25, 1.2479e+25, 6.0877e+25,
        4.5770e+25, 1.6950e+25, 1.8732e+25, 1.2134e+26, 1.8259e+25, 7.5401e+25,
        9.9022e+24, 7.9986e+24, 1.2616e+25, 3.2856e+24, 3.2376e+25, 1.5513e+25,
        1.8292e+25, 3.0681e+25, 1.3123e+25, 1.2622e+26, 2.6586e+25, 6.6216e+25,
        2.5502e+25, 2.7409e+26, 3.1617e+25, 2.0464e+25, 3.4468e+25, 1.0296e+25,
        3.5779e+25, 2.9488e+25, 2.3197e+25, 2.4055e+25, 1.9307e+25, 3.5366e+25,
        2.1182e+25, 5.5435e+25, 2.7143e+25, 4.0572e+25, 3.0478e+25, 4.3051e+25,
        4.1757e+27, 2.2496e+25, 3.2579e+25, 9.2150e+25, 1.9533e+25, 3.9221e+26,
        1.6748e+25, 3.4862e+25, 1.7604e+25, 1.5731e+25, 1.0158e+26, 3.0837e+25,
        3.0216e+25, 1.4467e+25, 2.8547e+25, 4.1915e+25, 8.5581e+25, 1.4450e+25,
        1.2395e+25, 6.9871e+25, 1.8140e+26, 2.3955e+26, 1.9423e+26, 2.7039e+25,
        2.2526e+25, 3.2676e+25, 2.2522e+25, 2.5442e+25, 9.4464e+25, 1.3991e+25,
        5.5408e+26, 3.2648e+25, 9.2782e+25, 5.7350e+25, 1.3718e+25, 8.3959e+24,
        2.9589e+25, 6.0738e+25, 2.2543e+27, 1.8644e+25, 2.0930e+25, 1.8816e+25,
        8.8901e+24, 6.5076e+24, 4.8139e+26, 4.9954e+25, 3.3654e+25, 7.2690e+24,
        6.6950e+24, 4.0794e+25, 9.7726e+25, 3.0169e+25, 1.9182e+25, 3.9036e+25,
        1.6990e+26, 1.3568e+25, 6.1891e+25, 4.7716e+26, 2.7773e+25, 1.6559e+25,
        2.1406e+25, 5.5075e+24, 3.8403e+25, 3.6739e+26, 5.9076e+25, 1.9660e+25,
        1.5261e+25, 1.2572e+25, 2.1862e+25, 2.9711e+25, 1.1949e+26, 5.8253e+25,
        1.9019e+25, 8.8818e+25, 2.8516e+25, 1.1188e+25, 4.3415e+25, 1.4091e+26,
        1.9008e+25, 2.3878e+25, 2.8643e+25, 1.2528e+25, 1.0985e+26, 2.6172e+25,
        7.3583e+24, 1.4381e+25, 4.8024e+25, 3.6715e+25, 1.7085e+25, 2.1933e+25,
        2.3819e+25, 1.8016e+26, 3.0151e+25, 1.1231e+25, 2.9919e+25, 3.3744e+25,
        1.2657e+25, 1.7910e+25, 2.1341e+25, 2.1017e+25, 3.1266e+25, 2.3187e+25,
        1.4393e+25, 2.4229e+25, 1.1296e+25, 4.2485e+26, 3.8544e+25, 3.4640e+25,
        2.1413e+25, 3.4885e+25, 1.6748e+25, 2.3004e+25])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.5588e+25, 3.9527e+25, 1.4049e+25, 2.7833e+25, 2.3976e+25, 2.3618e+25,
        4.9097e+25, 2.1731e+25, 3.8978e+25, 2.4478e+25, 3.0146e+25, 4.3695e+25,
        4.6369e+25, 1.8897e+25, 2.6970e+25, 1.6691e+25, 3.8694e+25, 4.9313e+25,
        3.4726e+25, 4.8937e+25, 1.5984e+25, 3.3568e+25, 1.2939e+25, 2.5520e+25,
        2.2770e+25, 1.5753e+25, 4.7165e+25, 2.0483e+25, 4.2156e+25, 2.4666e+25,
        3.5115e+25, 2.3677e+25, 1.6777e+25, 1.7829e+25, 3.0044e+25, 2.0458e+25,
        2.4602e+25, 2.1716e+25, 1.5007e+25, 6.3012e+25, 2.9859e+25, 3.1115e+25,
        2.8212e+25, 3.4721e+25, 2.0976e+25, 3.4483e+25, 2.7924e+25, 2.9029e+25,
        3.3648e+25, 3.2049e+25, 4.1963e+25, 3.2757e+25, 1.4227e+25, 3.8608e+25,
        3.3492e+25, 2.4380e+25, 2.3677e+25, 2.1556e+25, 3.3438e+25, 2.1363e+25,
        3.5044e+25, 4.8829e+25, 5.1446e+25, 2.9139e+25, 2.0215e+25, 3.3405e+25,
        2.8082e+25, 4.0038e+25, 4.7809e+25, 2.3600e+25, 3.1099e+25, 2.7210e+25,
        4.1604e+25, 1.6656e+25, 3.9247e+25, 4.3848e+25, 2.6168e+25, 3.3552e+25,
        2.7043e+25, 1.8974e+25, 3.4571e+25, 3.7563e+25, 4.1107e+25, 2.3693e+25,
        6.6788e+25, 4.6900e+25, 2.9262e+25, 2.0851e+25, 2.4638e+25, 3.0170e+25,
        1.9975e+25, 2.4188e+25, 1.4607e+25, 2.3250e+25, 3.6692e+25, 3.5374e+25,
        2.9199e+25, 2.2479e+25, 3.8372e+25, 3.3543e+25, 3.4126e+25, 2.4127e+25,
        2.0163e+25, 2.4416e+25, 1.9852e+25, 5.6321e+25, 3.4053e+25, 2.3486e+25,
        3.2330e+25, 3.2547e+25, 3.1630e+25, 3.2169e+25, 2.4105e+25, 3.0007e+25,
        3.7187e+25, 2.6974e+25, 2.2525e+25, 3.7529e+25, 3.9197e+25, 2.5973e+25,
        3.7527e+25, 3.3293e+25, 2.9731e+25, 4.0745e+25, 3.3257e+25, 3.6348e+25,
        2.4244e+25, 4.9885e+25])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.4632e+25, 2.2883e+25, 2.0781e+25, 1.8753e+25, 2.3850e+25, 2.1918e+25,
        2.1599e+25, 2.8427e+25, 2.2923e+25, 1.5430e+25, 3.3483e+25, 2.3962e+25,
        1.9509e+25, 1.8057e+25, 3.3968e+25, 2.3113e+25, 2.4377e+25, 1.6567e+25,
        1.4008e+25, 2.8348e+25, 2.3571e+25, 2.3026e+25, 1.0885e+25, 3.2149e+25,
        2.4621e+25, 2.2607e+25, 4.5174e+25, 1.9758e+25, 3.5458e+25, 1.6091e+25,
        2.4463e+25, 3.1708e+25, 2.0300e+25, 2.6922e+25, 2.0838e+25, 3.0033e+25,
        3.1772e+25, 3.3970e+25, 2.5605e+25, 2.4837e+25, 1.0654e+25, 2.2773e+25,
        3.8450e+25, 2.4043e+25, 2.7152e+25, 2.4063e+25, 2.8331e+25, 2.5021e+25,
        1.7387e+25, 2.7329e+25, 1.3148e+25, 3.9027e+25, 2.5076e+25, 2.7989e+25,
        1.0853e+25, 2.4273e+25, 2.4401e+25, 3.6121e+25, 3.9288e+25, 4.5008e+25,
        1.9980e+25, 2.5529e+25, 1.8000e+25, 1.5277e+25, 2.1708e+25, 3.3202e+25,
        4.6049e+25, 1.7702e+25, 2.9952e+25, 2.1814e+25, 2.2292e+25, 2.1379e+25,
        3.6644e+25, 1.3590e+25, 1.8181e+25, 3.0558e+25, 2.6565e+25, 1.4329e+25,
        1.2436e+25, 1.9549e+25, 2.9904e+25, 2.6773e+25, 2.6464e+25, 2.0916e+25,
        3.5231e+25, 2.9421e+25, 3.4641e+25, 2.4587e+25, 2.9873e+25, 3.4463e+25,
        2.5739e+25, 3.6857e+25, 1.9178e+25, 3.4755e+25, 2.6332e+25, 1.7634e+25,
        3.3058e+25, 1.9884e+25, 1.9834e+25, 2.0174e+25, 2.3106e+25, 1.4988e+25,
        2.8006e+25, 3.6960e+25, 1.1966e+25, 3.4081e+25, 2.1043e+25, 3.6330e+25,
        2.7150e+25, 2.7659e+25, 3.2786e+25, 2.9115e+25, 2.9390e+25, 1.8595e+25,
        1.7814e+25, 3.6357e+25, 2.8081e+25, 3.4859e+25, 2.6151e+25, 2.7311e+25,
        2.4365e+25, 3.5319e+25, 2.1889e+25, 1.9376e+25, 2.5767e+25, 2.3781e+25,
        2.2638e+25, 1.8428e+25])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0758e+23, 1.2735e+24, 7.0005e+22, 1.9617e+23, 4.4424e+23, 1.8186e+23,
        6.2489e+23, 5.6484e+24, 1.9466e+23, 5.5190e+23, 2.5707e+23, 6.0032e+23,
        2.0248e+23, 6.3708e+22, 4.0997e+22, 6.0207e+22, 1.5127e+23, 2.8136e+23,
        9.6373e+23, 1.2465e+24, 2.2698e+23, 2.1850e+23, 5.7566e+23, 1.8946e+23,
        1.0843e+23, 1.0841e+23, 4.4668e+23, 7.0750e+22, 6.9717e+22, 5.6017e+23,
        5.1940e+22, 9.7044e+22, 7.8961e+22, 3.0879e+22, 1.1339e+23, 8.4567e+22,
        2.2948e+23, 1.5463e+23, 3.3898e+23, 4.7890e+22, 4.0031e+23, 7.0452e+22,
        2.9317e+23, 3.1623e+22, 2.6852e+22, 6.7065e+23, 1.5759e+23, 6.1350e+22,
        9.4487e+22, 7.4349e+22, 3.5794e+23, 6.1450e+22, 9.8566e+22, 6.1207e+22,
        8.4327e+22, 2.5383e+23, 1.3373e+23, 6.2261e+23, 4.5670e+22, 6.4400e+22,
        1.8070e+23, 5.0287e+23, 2.6269e+23, 3.2123e+23, 9.4728e+23, 2.1316e+23,
        4.8063e+23, 4.4642e+23, 3.5685e+23, 8.6224e+22, 1.7272e+23, 1.8744e+23,
        1.3278e+23, 3.7261e+23, 2.4893e+23, 8.6331e+22, 1.3258e+23, 2.7533e+24,
        2.1218e+23, 1.7034e+23, 7.7543e+22, 5.9098e+22, 2.8805e+23, 6.1335e+22,
        2.0427e+23, 7.6050e+22, 2.7009e+23, 5.2990e+23, 2.6310e+25, 8.0323e+22,
        5.6551e+22, 8.1620e+22, 1.4811e+23, 1.2771e+23, 3.6071e+23, 6.9505e+22,
        5.4970e+23, 2.2071e+23, 2.4085e+22, 8.6450e+22, 1.1688e+23, 2.1938e+23,
        7.0307e+22, 8.0683e+22, 3.0820e+23, 9.6133e+22, 5.8184e+23, 9.3814e+22,
        1.7563e+23, 3.4124e+23, 2.0008e+25, 1.0495e+23, 2.8021e+23, 1.0604e+23,
        7.8592e+22, 5.2039e+23, 5.6787e+23, 3.0394e+22, 1.3610e+23, 1.4180e+24,
        4.8179e+22, 9.8431e+22, 2.9495e+22, 7.8613e+22, 1.5364e+23, 6.7814e+23,
        1.9421e+24, 2.1764e+23, 8.0230e+22, 1.2858e+24, 1.2678e+23, 2.0796e+23,
        4.3179e+22, 1.0889e+23, 9.6548e+22, 1.7184e+23, 1.2718e+23, 7.7035e+22,
        2.8229e+23, 1.7896e+24, 7.7914e+22, 9.3653e+23, 1.2250e+23, 1.6952e+23,
        2.1617e+23, 2.1217e+22, 2.0923e+24, 1.3913e+23, 8.3318e+22, 7.5449e+22,
        6.4944e+22, 7.0190e+22, 2.3253e+23, 6.9548e+22, 1.0695e+23, 5.6139e+22,
        3.9551e+23, 6.6298e+23, 6.7802e+22, 3.3466e+22, 1.6411e+23, 3.2727e+23,
        9.8994e+22, 5.3956e+23, 6.4408e+22, 3.4042e+23, 1.8950e+23, 4.1213e+23,
        3.2743e+22, 2.1845e+23, 9.1121e+22, 4.2988e+22, 7.1270e+23, 1.3549e+23,
        7.5049e+22, 4.8841e+23, 1.3753e+23, 1.0358e+23, 9.6306e+22, 5.5527e+22,
        7.5826e+22, 1.8679e+23, 2.8045e+23, 6.3549e+22, 3.2205e+23, 8.7431e+22,
        5.8140e+22, 1.4804e+23, 7.6555e+23, 5.4336e+22, 9.0013e+22, 8.4701e+23,
        1.1216e+23, 7.2113e+23, 1.0028e+23, 2.1421e+23, 2.6052e+23, 1.5393e+23,
        8.9101e+22, 2.1518e+23, 1.5537e+23, 6.7934e+22, 3.5522e+23, 8.1521e+23,
        9.4578e+22, 8.0526e+22, 5.7839e+23, 2.0487e+23, 7.4340e+22, 6.1989e+22,
        1.8730e+23, 5.7172e+22, 1.0884e+23, 1.7766e+23, 7.7048e+22, 2.0581e+24,
        1.7817e+23, 7.9241e+22, 8.1590e+22, 4.7490e+23, 1.7545e+23, 1.4307e+23,
        1.0228e+23, 2.1900e+24, 3.3196e+23, 9.6889e+22, 4.3135e+22, 1.5812e+23,
        1.0108e+24, 1.2996e+23, 9.9608e+22, 7.2230e+22, 1.2219e+23, 6.4197e+23,
        1.4466e+23, 7.4007e+22, 3.4448e+22, 7.4749e+22, 9.7857e+22, 2.1986e+23,
        1.1872e+24, 1.0420e+23, 6.9494e+23, 7.3391e+22, 3.4089e+23, 1.3683e+23,
        9.5030e+22, 1.9494e+23, 3.4944e+23, 2.3501e+23, 4.5288e+22, 5.0788e+23,
        5.5618e+23, 3.6070e+23, 1.5601e+23, 2.5012e+24, 4.8510e+22, 1.3431e+25,
        8.8932e+22, 7.8118e+23, 3.0006e+23, 2.3242e+23, 4.6425e+22, 2.2985e+23,
        1.4772e+23, 2.4696e+23, 6.1984e+22, 8.0409e+22, 5.7398e+22, 9.3640e+22,
        5.1991e+23, 3.0710e+23, 1.0419e+23, 1.7450e+22, 3.0304e+23, 8.4227e+22,
        6.2895e+22, 3.4679e+23, 9.5776e+22, 1.1022e+23, 1.2340e+23, 9.4560e+22,
        3.8315e+23, 8.7929e+22, 5.4104e+22, 9.3349e+22, 2.5173e+23, 3.7686e+22,
        2.1695e+24, 4.3966e+22, 2.9371e+24, 3.0888e+23, 7.2984e+22, 2.1343e+22,
        6.1747e+22, 3.0982e+23, 3.8335e+22, 2.3868e+22, 6.6368e+22, 5.2517e+23,
        1.0379e+23, 1.7877e+24, 1.6983e+23, 1.6235e+23, 6.9683e+22, 1.0200e+23,
        5.7824e+22, 1.1163e+24, 1.9509e+23, 9.5570e+22, 1.9274e+23, 4.2414e+23,
        8.9309e+22, 1.3948e+23, 6.6164e+22, 6.5470e+22, 2.9713e+23, 4.4568e+24,
        1.0785e+23, 4.7588e+23, 6.7831e+22, 8.6631e+22, 5.6862e+23, 4.2028e+22,
        1.9261e+23, 5.8401e+22, 2.8531e+23, 1.3338e+23, 3.5111e+22, 4.4745e+22,
        1.5646e+23, 1.1287e+23, 1.1809e+23, 4.3527e+23, 9.8767e+22, 1.6738e+23,
        6.8886e+22, 1.3061e+23, 1.7212e+23, 7.7049e+22, 1.9507e+23, 1.3190e+23,
        6.4028e+22, 5.1999e+23, 1.8990e+23, 2.1607e+23, 1.3467e+23, 2.4635e+23,
        6.3696e+22, 3.3845e+23, 1.7287e+23, 1.4445e+23, 1.3988e+23, 7.4077e+22,
        6.9107e+22, 1.1182e+23, 1.4659e+23, 4.4688e+22, 2.1599e+23, 1.4360e+23,
        1.2335e+23, 6.4562e+22, 1.2941e+23, 1.9415e+23, 1.5489e+23, 2.1284e+23,
        5.5190e+22, 1.0976e+23, 1.2729e+24, 5.4988e+22, 1.1977e+23, 7.7354e+22,
        5.5291e+22, 1.7049e+23, 2.5330e+23, 5.7051e+22, 1.8175e+24, 1.3997e+23,
        7.0079e+22, 3.3113e+23, 3.0973e+22, 6.3326e+22, 3.7493e+22, 5.5036e+22,
        5.5056e+22, 5.5034e+22, 9.1541e+22, 2.6485e+22, 2.0206e+23, 1.5161e+23,
        7.6055e+22, 6.8802e+23, 2.4610e+24, 7.4607e+22, 8.0102e+22, 1.0198e+24,
        7.3231e+22, 1.6363e+23, 2.8953e+23, 3.3428e+23, 4.9038e+23, 1.1004e+23,
        1.9785e+23, 8.7071e+22, 5.3289e+22, 9.9793e+22, 7.2821e+22, 4.5008e+22,
        2.1100e+23, 1.1266e+23, 1.4138e+23, 3.3500e+23, 3.2131e+22, 5.3055e+23,
        1.3992e+23, 2.6613e+22, 6.6933e+22, 1.5864e+23, 3.7135e+24, 6.2952e+22,
        1.0106e+24, 1.7556e+23, 9.2930e+22, 1.1201e+23, 1.6734e+23, 3.7814e+22,
        1.0016e+23, 1.5202e+23, 1.4535e+23, 6.0867e+22, 2.6256e+23, 3.9711e+22,
        3.5301e+23, 6.1146e+23, 7.5174e+22, 9.0399e+22, 1.9073e+23, 1.1337e+23,
        8.5163e+22, 1.4020e+23, 1.1572e+24, 7.0576e+22, 1.4388e+23, 3.5897e+23,
        1.2885e+23, 2.4829e+24, 8.9114e+22, 4.5902e+23, 6.5271e+22, 2.1700e+23,
        3.2853e+22, 2.6737e+23, 1.5565e+23, 4.1497e+22, 3.9548e+22, 4.6555e+22,
        1.0247e+23, 4.8049e+22, 6.1359e+22, 1.0486e+23, 7.3996e+22, 1.8961e+22,
        8.4200e+22, 2.5140e+23, 1.8602e+23, 4.6325e+22, 1.0199e+23, 2.5630e+22,
        1.3152e+23, 1.0330e+23, 2.2797e+23, 5.2313e+22, 4.0656e+23, 5.1594e+22,
        3.3347e+23, 7.8220e+22, 1.1255e+23, 9.3449e+22, 1.3910e+23, 2.5130e+25,
        4.6353e+22, 6.0312e+22, 9.6810e+23, 9.7379e+22, 1.4148e+23, 3.5152e+22,
        1.1983e+23, 9.5273e+22, 2.7661e+22, 3.6995e+23, 1.8870e+24, 2.7978e+23,
        7.9831e+22, 2.0697e+23, 1.6781e+23, 7.7955e+22, 9.6346e+22, 3.8021e+22,
        1.6478e+23, 1.7413e+23, 1.3691e+23, 1.9925e+23, 8.5972e+22, 7.9719e+23,
        3.2462e+23, 6.0186e+22, 7.4312e+22, 7.0880e+22, 4.4768e+23, 9.7351e+22,
        3.0200e+22, 1.7726e+23])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.9351e+21, 1.2731e+24, 6.5930e+22, 1.5206e+23, 3.3745e+23, 1.7199e+23,
        5.2890e+23, 5.6159e+24, 8.5391e+21, 5.2241e+23, 2.1360e+23, 4.9259e+23,
        2.4421e+23, 6.0564e+22, 7.2588e+22, 1.6685e+23, 9.4360e+22, 2.2304e+23,
        9.6935e+23, 1.2546e+24, 1.5751e+23, 2.3545e+23, 5.4050e+23, 1.5240e+23,
        3.6439e+22, 3.3505e+22, 2.9600e+23, 2.6521e+22, 5.9124e+22, 5.8988e+23,
        1.4132e+23, 5.3461e+22, 8.3881e+21, 7.5043e+22, 3.4531e+22, 3.0344e+22,
        1.9726e+23, 4.9586e+22, 3.7917e+23, 2.9631e+22, 3.2045e+23, 3.0672e+22,
        2.3671e+23, 5.8819e+22, 4.0425e+22, 6.6736e+23, 1.1896e+23, 2.3111e+22,
        1.2774e+22, 5.1911e+22, 4.1543e+23, 1.4908e+22, 1.2347e+23, 9.9624e+22,
        1.1502e+23, 2.5758e+23, 6.0142e+22, 4.5037e+23, 7.6976e+22, 4.6635e+22,
        6.0809e+22, 4.3757e+23, 1.9072e+23, 3.0185e+23, 9.0428e+23, 1.5302e+23,
        4.7551e+23, 4.4332e+23, 3.7367e+23, 4.7511e+22, 6.7111e+22, 7.4970e+22,
        1.2228e+23, 4.5368e+23, 1.9373e+23, 9.9621e+22, 6.9842e+22, 2.7206e+24,
        2.0783e+23, 6.8432e+22, 7.5162e+22, 2.7488e+22, 3.7684e+23, 2.0050e+22,
        1.7408e+23, 1.2115e+22, 1.3786e+23, 6.3143e+23, 2.6369e+25, 4.4840e+22,
        4.2863e+22, 6.6724e+22, 1.8910e+23, 3.8891e+22, 3.1076e+23, 1.3980e+22,
        6.0022e+23, 1.0816e+23, 5.2830e+22, 1.0355e+23, 1.2089e+23, 1.9628e+23,
        1.0819e+22, 2.1667e+22, 3.0017e+23, 5.3983e+22, 5.0641e+23, 2.1765e+22,
        8.9551e+22, 3.3259e+23, 2.0046e+25, 1.2854e+22, 2.6167e+23, 5.3894e+22,
        4.9451e+22, 5.1425e+23, 5.9369e+23, 2.0161e+22, 8.3716e+22, 1.4170e+24,
        2.0614e+22, 4.4355e+22, 1.0187e+23, 1.1272e+23, 4.5363e+22, 6.5255e+23,
        1.9174e+24, 3.3889e+23, 1.6807e+23, 1.2350e+24, 2.1370e+22, 1.8364e+23,
        3.4767e+22, 2.8552e+22, 1.5350e+22, 5.2529e+22, 4.2998e+22, 4.0957e+22,
        2.2481e+23, 1.7725e+24, 4.7331e+22, 9.3218e+23, 4.8021e+22, 1.5708e+22,
        2.2316e+23, 5.7888e+22, 2.0278e+24, 2.0522e+22, 4.0247e+22, 1.9659e+22,
        4.2349e+22, 2.0977e+22, 2.7165e+23, 2.2215e+22, 4.9087e+22, 1.1660e+23,
        4.1365e+23, 7.8829e+23, 1.2245e+22, 5.2402e+22, 1.7793e+23, 2.7520e+23,
        2.1270e+22, 5.5736e+23, 5.6581e+22, 2.8399e+23, 2.0126e+23, 3.7058e+23,
        4.4252e+22, 2.0181e+23, 2.1312e+22, 2.5722e+22, 7.2064e+23, 9.8356e+22,
        7.6134e+22, 4.5628e+23, 8.7632e+21, 4.7327e+22, 1.5401e+22, 5.5005e+22,
        8.7258e+22, 1.1248e+23, 2.8154e+23, 3.9728e+22, 2.8799e+23, 8.0620e+21,
        4.0137e+22, 3.0167e+23, 7.1284e+23, 5.1699e+22, 3.7505e+22, 7.6192e+23,
        3.4127e+22, 6.5778e+23, 3.1505e+22, 1.9013e+23, 2.1262e+23, 6.4150e+22,
        3.4240e+22, 2.3773e+23, 2.3379e+23, 2.8977e+22, 3.4200e+23, 7.7744e+23,
        2.8002e+22, 2.8863e+22, 5.5408e+23, 1.7826e+23, 2.6781e+22, 1.1335e+22,
        1.7130e+23, 2.7043e+22, 2.0349e+22, 1.5177e+23, 7.4229e+22, 2.0623e+24,
        1.0595e+23, 7.9718e+22, 3.3604e+22, 5.4218e+23, 1.1062e+23, 1.3360e+23,
        5.6211e+21, 2.1500e+24, 2.4403e+23, 1.8961e+22, 5.6649e+22, 2.2646e+22,
        9.9786e+23, 2.1348e+23, 9.0300e+22, 1.5115e+23, 3.8106e+22, 6.1209e+23,
        1.2382e+23, 1.3836e+22, 4.9610e+22, 1.8241e+22, 3.5150e+22, 7.4551e+22,
        1.1354e+24, 3.7352e+22, 7.4934e+23, 1.4804e+23, 4.0700e+23, 1.5195e+23,
        3.7820e+22, 3.6232e+22, 2.5172e+23, 1.6489e+23, 8.3601e+22, 4.8001e+23,
        6.0883e+23, 2.6264e+23, 3.0309e+23, 2.5092e+24, 1.7557e+23, 1.3477e+25,
        3.7541e+22, 6.9965e+23, 2.5861e+23, 2.1771e+23, 3.1313e+22, 3.0361e+23,
        1.5908e+23, 2.7304e+23, 4.8476e+22, 4.6762e+22, 7.5813e+22, 3.7564e+22,
        4.7677e+23, 3.1880e+23, 9.7482e+21, 9.5650e+22, 3.3968e+23, 7.4119e+22,
        6.0869e+22, 3.8418e+23, 3.6736e+22, 6.4457e+22, 7.1425e+22, 9.9505e+22,
        4.8424e+23, 9.5687e+22, 7.2591e+22, 3.1810e+22, 2.4946e+23, 7.7756e+22,
        2.1383e+24, 7.4783e+22, 2.8774e+24, 2.9404e+23, 5.0740e+22, 5.6463e+22,
        9.7073e+21, 2.7939e+23, 1.7093e+23, 4.8116e+22, 9.1063e+21, 4.4478e+23,
        4.0084e+22, 1.9096e+24, 2.2219e+23, 8.6602e+22, 4.6576e+22, 5.4615e+22,
        3.1412e+22, 1.1138e+24, 1.5670e+23, 1.7278e+22, 1.1826e+23, 2.3909e+23,
        9.9874e+22, 1.2524e+23, 2.9083e+22, 2.8673e+22, 2.0992e+23, 4.4773e+24,
        3.7942e+22, 4.3697e+23, 4.3306e+22, 7.7272e+22, 5.3402e+23, 6.4955e+22,
        7.5062e+22, 1.4822e+22, 2.2840e+23, 2.6181e+22, 9.1629e+22, 7.2705e+22,
        8.2281e+22, 1.7060e+22, 2.5129e+22, 3.5864e+23, 2.8623e+22, 7.0308e+22,
        4.3450e+22, 1.8834e+22, 1.1217e+23, 6.5164e+22, 7.0450e+22, 1.1510e+23,
        9.4106e+22, 3.5301e+23, 1.3780e+23, 1.9525e+23, 2.4756e+22, 2.7610e+23,
        5.9100e+22, 2.8287e+23, 6.1970e+22, 8.0398e+22, 8.0467e+22, 1.0721e+23,
        2.8578e+22, 5.8197e+22, 5.6315e+22, 1.0342e+23, 2.2587e+23, 4.9763e+22,
        2.2267e+22, 1.7534e+22, 1.9183e+22, 3.0044e+22, 1.5556e+23, 2.7213e+23,
        1.8537e+22, 5.0684e+22, 1.2981e+24, 1.5408e+23, 1.7746e+22, 4.6342e+22,
        6.5924e+22, 1.7535e+23, 1.8353e+23, 1.7977e+22, 1.7486e+24, 1.0359e+23,
        7.1420e+22, 2.6293e+23, 5.7828e+22, 3.6664e+22, 2.2588e+22, 1.0322e+23,
        2.1693e+22, 9.9219e+21, 4.5538e+22, 9.9779e+22, 3.0717e+23, 1.2506e+23,
        9.0902e+22, 7.3405e+23, 2.4890e+24, 3.5814e+22, 4.1655e+22, 9.7201e+23,
        5.6574e+22, 8.5901e+22, 7.7401e+22, 3.5144e+23, 4.5173e+23, 7.6862e+22,
        9.3971e+22, 7.5800e+21, 1.0856e+22, 1.0275e+22, 2.2578e+22, 2.7572e+22,
        1.4174e+23, 1.9950e+23, 2.1956e+22, 1.8755e+23, 4.0607e+22, 5.9373e+23,
        2.8241e+22, 7.7224e+22, 6.4459e+22, 6.4327e+22, 3.6853e+24, 1.3960e+23,
        9.7217e+23, 1.8199e+23, 9.0932e+22, 9.3046e+22, 8.5968e+21, 6.2847e+22,
        2.4380e+22, 6.0057e+22, 3.3068e+22, 6.0094e+22, 2.8513e+23, 2.3592e+22,
        2.8944e+23, 5.5609e+23, 2.7652e+22, 7.3851e+22, 1.3177e+23, 5.9246e+21,
        4.2660e+22, 1.4506e+23, 1.1758e+24, 8.1880e+22, 8.4551e+22, 2.8413e+23,
        1.1407e+23, 2.4186e+24, 1.7796e+22, 5.2622e+23, 2.8566e+22, 2.2164e+23,
        6.8808e+22, 2.3036e+23, 2.4737e+22, 5.0003e+22, 9.6856e+22, 8.2008e+21,
        8.9074e+22, 4.0284e+22, 6.5053e+22, 1.2377e+22, 1.4856e+22, 4.4304e+22,
        1.2846e+22, 1.2382e+23, 1.1200e+23, 6.2428e+22, 1.1107e+23, 1.4729e+23,
        2.0713e+23, 1.6081e+22, 2.3627e+23, 5.3378e+22, 4.7523e+23, 2.8643e+22,
        2.4319e+23, 1.2324e+23, 9.0039e+22, 1.9608e+22, 3.1968e+22, 2.5131e+25,
        2.7009e+22, 3.0706e+22, 9.1217e+23, 1.5227e+23, 1.2186e+23, 4.7342e+22,
        4.6242e+22, 1.7919e+22, 8.9223e+22, 2.6591e+23, 1.8359e+24, 2.2215e+23,
        1.2242e+23, 1.0290e+23, 1.1263e+23, 1.5388e+23, 2.7300e+22, 7.8874e+22,
        3.5290e+22, 6.7397e+22, 5.7175e+22, 1.6388e+23, 2.3855e+22, 8.3756e+23,
        2.4361e+23, 2.9845e+22, 1.7203e+22, 2.5646e+22, 4.7887e+23, 1.5354e+22,
        4.3161e+22, 9.6314e+22])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.8501e+23, 1.8127e+23, 1.4449e+23, 1.9007e+23, 1.7115e+23, 1.8097e+23,
        9.2711e+22, 2.1052e+23, 1.8333e+23, 1.4842e+23, 2.2796e+23, 9.7502e+22,
        2.5454e+23, 9.5522e+22, 1.6754e+23, 1.5304e+23, 1.2358e+23, 9.7104e+22,
        1.8511e+23, 1.9804e+23, 9.9239e+22, 1.5198e+23, 9.5428e+22, 1.4470e+23,
        8.5247e+22, 1.7747e+23, 1.4345e+23, 1.5360e+23, 1.3407e+23, 1.6207e+23,
        1.5609e+23, 1.9226e+23, 1.5017e+23, 8.5415e+22, 1.9892e+23, 2.4779e+23,
        2.0188e+23, 1.7284e+23, 8.9859e+22, 9.5452e+22, 1.4946e+23, 1.0651e+23,
        1.1693e+23, 1.9420e+23, 1.1409e+23, 1.1334e+23, 1.9903e+23, 1.8834e+23,
        1.0025e+23, 1.2222e+23, 2.0251e+23, 2.6209e+23, 2.2758e+23, 1.6493e+23,
        2.0270e+23, 2.6372e+23, 1.0218e+23, 1.0650e+23, 1.7327e+23, 1.5061e+23,
        9.0068e+22, 2.1367e+23, 1.3443e+23, 1.2636e+23, 1.5268e+23, 1.2454e+23,
        8.9893e+22, 1.0824e+23, 2.0086e+23, 1.6081e+23, 2.1420e+23, 2.1021e+23,
        1.3617e+23, 1.0347e+23, 1.9522e+23, 1.1726e+23, 8.9995e+22, 6.8842e+22,
        1.8269e+23, 1.1825e+23, 1.3062e+23, 2.9777e+23, 1.1235e+23, 1.5395e+23,
        1.1433e+23, 1.4084e+23, 2.7696e+23, 1.5542e+23, 2.0774e+23, 1.8098e+23,
        2.3581e+23, 1.4041e+23, 2.4963e+23, 1.3379e+23, 1.8038e+23, 1.7057e+23,
        1.5312e+23, 1.3604e+23, 1.0832e+23, 1.2906e+23, 1.2111e+23, 1.3807e+23,
        1.3977e+23, 1.8755e+23, 1.4302e+23, 1.4503e+23, 1.8251e+23, 1.7494e+23,
        1.6753e+23, 1.2320e+23, 1.2552e+23, 1.3816e+23, 1.5544e+23, 1.7371e+23,
        1.0510e+23, 2.5275e+23, 1.6336e+23, 1.4777e+23, 1.2788e+23, 1.7985e+23,
        2.2127e+23, 1.5994e+23, 1.5028e+23, 1.6082e+23, 1.8980e+23, 1.2915e+23,
        1.7620e+23, 1.3311e+23])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0711e+23, 1.1080e+23, 1.1035e+23, 1.5881e+23, 1.8291e+23, 1.5042e+23,
        1.6004e+23, 1.5848e+23, 2.3570e+23, 1.0119e+23, 1.1181e+23, 2.0556e+23,
        1.1925e+23, 1.4161e+23, 9.5362e+22, 2.3137e+23, 1.3652e+23, 1.8877e+23,
        1.9078e+23, 1.3902e+23, 1.9046e+23, 9.4190e+22, 1.2463e+23, 1.4839e+23,
        1.1772e+23, 1.4499e+23, 7.5854e+22, 8.8699e+22, 1.6080e+23, 1.1091e+23,
        1.1314e+23, 1.3410e+23, 1.1184e+23, 1.1992e+23, 1.6251e+23, 8.4739e+22,
        1.9349e+23, 1.6684e+23, 7.8290e+22, 1.8534e+23, 1.7018e+23, 1.2916e+23,
        1.6633e+23, 1.5770e+23, 1.9553e+23, 1.3490e+23, 1.4971e+23, 1.4386e+23,
        6.1907e+22, 2.5478e+23, 1.0697e+23, 1.5535e+23, 1.2541e+23, 1.5693e+23,
        8.7062e+22, 1.3373e+23, 1.3930e+23, 1.7423e+23, 1.3632e+23, 1.6349e+23,
        1.5950e+23, 1.1313e+23, 1.8704e+23, 2.3301e+23, 1.1485e+23, 1.0925e+23,
        1.2733e+23, 1.8516e+23, 1.6453e+23, 1.1360e+23, 1.1488e+23, 1.7479e+23,
        1.3332e+23, 1.3552e+23, 1.6963e+23, 8.4948e+22, 1.3502e+23, 2.1482e+23,
        1.4627e+23, 1.1887e+23, 1.3543e+23, 1.2336e+23, 1.9535e+23, 1.6170e+23,
        1.3529e+23, 1.3178e+23, 9.1257e+22, 1.8176e+23, 1.7931e+23, 1.0620e+23,
        1.6283e+23, 1.2765e+23, 1.1070e+23, 1.5663e+23, 1.4259e+23, 1.6634e+23,
        1.4846e+23, 2.0135e+23, 1.5640e+23, 2.3578e+23, 1.8214e+23, 7.4985e+22,
        1.3989e+23, 1.3603e+23, 8.4419e+22, 1.4677e+23, 9.6755e+22, 2.3613e+23,
        9.3423e+22, 8.6466e+22, 1.0627e+23, 1.7499e+23, 1.6290e+23, 1.3445e+23,
        1.1155e+23, 1.3909e+23, 1.3510e+23, 1.2550e+23, 1.4914e+23, 1.4970e+23,
        1.3820e+23, 1.3262e+23, 9.7060e+22, 2.6794e+23, 2.8292e+23, 1.5908e+23,
        2.0000e+23, 1.3532e+23])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.9599e+20, 2.6049e+21, 4.6155e+21, 1.3949e+21, 2.8226e+21, 3.3662e+21,
        1.1796e+21, 9.0833e+21, 1.7241e+21, 2.7653e+21, 3.6417e+21, 2.8513e+21,
        4.2972e+21, 1.8981e+21, 2.2065e+22, 7.5854e+22, 1.9619e+21, 2.0193e+21,
        4.5781e+21, 2.7549e+21, 5.1742e+21, 6.6597e+21, 3.6952e+21, 1.2422e+21,
        3.2958e+20, 5.4904e+21, 1.8219e+21, 1.3321e+21, 2.9512e+21, 1.3037e+21,
        1.9983e+21, 1.3701e+21, 7.3243e+20, 4.8590e+21, 3.1603e+21, 1.3746e+21,
        1.7639e+21, 8.6859e+21, 4.5260e+21, 5.0061e+21, 2.5416e+21, 3.0997e+21,
        2.0478e+21, 2.2396e+21, 1.7979e+21, 2.0115e+21, 1.0998e+21, 1.3759e+21,
        2.0934e+21, 2.4675e+21, 2.2354e+22, 3.1708e+21, 1.1821e+21, 2.3213e+21,
        1.8082e+21, 1.6511e+21, 4.3956e+21, 7.0007e+21, 6.7002e+21, 3.5823e+21,
        2.6906e+21, 2.0270e+21, 3.1397e+21, 3.3036e+21, 1.1873e+21, 3.1474e+22,
        2.1001e+21, 1.6513e+21, 1.4664e+21, 3.2318e+21, 9.3960e+20, 7.4605e+21,
        9.4930e+20, 2.6814e+21, 9.7377e+20, 2.3657e+21, 3.5008e+21, 4.0532e+21,
        1.9795e+21, 3.3833e+21, 2.5877e+21, 3.5136e+21, 3.2226e+21, 2.3395e+21,
        9.4438e+21, 1.1634e+21, 4.6023e+21, 3.6326e+21, 4.5950e+21, 4.5829e+21,
        2.6039e+21, 4.3926e+21, 3.6266e+21, 3.2453e+21, 2.2551e+21, 1.5071e+22,
        3.3000e+21, 1.7631e+21, 7.7959e+21, 3.2370e+21, 5.1404e+21, 1.9072e+21,
        1.2940e+21, 2.0883e+21, 3.4995e+21, 4.2786e+21, 2.8836e+21, 3.2215e+21,
        5.4244e+21, 3.7358e+21, 2.6978e+21, 1.4411e+21, 2.2331e+21, 3.3569e+21,
        2.2098e+21, 5.2318e+21, 1.9020e+21, 3.2798e+21, 4.3923e+22, 5.9719e+21,
        3.8007e+21, 2.4784e+21, 1.1031e+21, 1.4464e+21, 2.5292e+21, 7.8192e+21,
        2.0339e+21, 2.6857e+21, 4.4081e+21, 2.5799e+21, 3.7350e+21, 7.0809e+21,
        3.3158e+22, 3.5476e+22, 3.6774e+21, 4.2892e+21, 3.2533e+22, 2.8306e+21,
        2.9002e+21, 3.8645e+21, 1.5160e+21, 2.6404e+21, 4.2048e+21, 3.3188e+21,
        5.0113e+21, 1.2949e+21, 3.2193e+21, 2.5148e+21, 3.0720e+21, 4.1927e+21,
        3.7927e+21, 7.2428e+21, 1.3601e+21, 1.7700e+21, 5.1594e+21, 2.7811e+21,
        8.6312e+20, 4.2748e+21, 1.2057e+21, 1.4516e+21, 1.2416e+22, 7.0261e+21,
        3.5523e+21, 1.7706e+21, 2.6785e+21, 6.7155e+21, 1.4992e+21, 1.0004e+22,
        1.8870e+21, 4.0341e+21, 9.5043e+20, 3.2691e+20, 1.8747e+21, 1.5203e+22,
        4.8862e+21, 2.6938e+21, 1.7277e+21, 4.5981e+21, 4.0074e+21, 2.9897e+21,
        1.9377e+21, 5.4215e+21, 3.4195e+21, 8.1073e+21, 5.8824e+21, 6.6030e+20,
        1.5058e+21, 4.0410e+21, 2.4062e+21, 1.1077e+22, 2.4556e+21, 4.4205e+21,
        4.4066e+20, 2.5698e+21, 3.9490e+21, 2.6099e+21, 2.9668e+21, 3.8010e+21,
        6.6248e+21, 3.7103e+21, 1.3444e+21, 8.1451e+21, 1.0319e+22, 3.1148e+21,
        3.3748e+21, 3.3643e+21, 1.5933e+21, 2.2453e+21, 1.7405e+21, 3.0241e+21,
        2.5261e+21, 4.1367e+21, 3.7164e+21, 3.9818e+21, 5.8574e+21, 2.5542e+21,
        3.3309e+21, 4.4681e+21, 2.9014e+21, 4.3956e+21, 2.7900e+21, 1.5529e+21,
        3.1658e+21, 2.8512e+22, 2.9326e+21, 1.2598e+21, 4.6755e+21, 2.3750e+21,
        3.1115e+21, 2.3926e+21, 2.1930e+21, 3.9073e+21, 1.9287e+21, 2.3481e+21,
        1.6155e+21, 5.8023e+21, 4.4758e+21, 1.6110e+22, 3.9887e+21, 1.5042e+21,
        4.8182e+21, 3.1686e+21, 4.8997e+21, 2.2516e+21, 3.1606e+21, 2.6650e+21,
        5.0581e+21, 1.7678e+21, 2.6396e+21, 4.0733e+21, 3.0237e+21, 5.0847e+21,
        2.6429e+21, 2.6031e+21, 4.5234e+21, 9.2880e+21, 2.1845e+21, 3.0281e+21,
        3.6598e+21, 2.9649e+21, 4.8438e+21, 3.6454e+21, 3.0451e+21, 1.7263e+21,
        3.7194e+21, 7.2227e+21, 4.0478e+21, 4.5165e+21, 2.2151e+21, 3.0004e+21,
        7.2126e+21, 2.4350e+21, 1.2258e+21, 4.2100e+21, 2.0575e+21, 1.8968e+21,
        4.4051e+21, 3.5221e+21, 6.1720e+20, 1.5434e+21, 2.4472e+21, 3.2910e+21,
        8.6639e+22, 3.3862e+21, 2.2181e+21, 4.9311e+21, 2.0641e+21, 1.8278e+21,
        4.1642e+21, 3.5911e+21, 2.8766e+21, 7.5638e+21, 1.4878e+22, 1.7541e+22,
        1.3475e+21, 1.1189e+22, 4.3379e+22, 1.5121e+21, 2.0891e+21, 1.2624e+22,
        5.9454e+21, 4.6727e+21, 1.3096e+22, 3.2730e+21, 1.6694e+21, 7.0525e+22,
        9.2535e+20, 3.0210e+21, 5.5066e+21, 4.5687e+21, 5.2524e+21, 2.5497e+22,
        5.6376e+21, 1.5764e+22, 1.1564e+21, 7.9633e+21, 1.0194e+23, 3.0645e+21,
        3.5175e+21, 1.3881e+21, 1.2419e+22, 3.6517e+21, 2.9244e+21, 2.2508e+21,
        5.1729e+21, 9.4940e+21, 5.0919e+21, 1.5802e+22, 3.2927e+21, 1.8243e+21,
        3.6557e+21, 4.6084e+21, 7.0977e+21, 1.9920e+21, 4.1007e+21, 3.5914e+21,
        1.7318e+21, 2.3386e+21, 3.3045e+21, 2.1104e+21, 3.1122e+21, 1.3327e+21,
        1.6436e+21, 2.1657e+21, 2.7102e+21, 2.3716e+21, 1.9911e+21, 2.7471e+21,
        5.2854e+21, 2.3399e+21, 3.6063e+21, 2.7234e+21, 4.2570e+21, 2.1817e+21,
        3.5906e+21, 2.7578e+21, 2.3141e+22, 6.6415e+21, 4.2247e+21, 2.5060e+21,
        5.6455e+21, 1.9237e+21, 1.0906e+21, 3.6676e+21, 9.7173e+20, 1.4681e+21,
        9.7486e+20, 3.8815e+21, 2.8880e+21, 2.1021e+21, 5.3322e+21, 2.2681e+21,
        6.1608e+21, 1.0162e+21, 1.4380e+22, 5.2421e+21, 2.9524e+21, 1.7329e+21,
        2.9881e+21, 4.7131e+21, 2.1544e+21, 2.9496e+21, 3.6091e+21, 2.9937e+21,
        2.4812e+21, 1.2637e+21, 3.5942e+21, 2.8884e+21, 7.0171e+21, 1.6399e+21,
        1.4544e+21, 3.4676e+21, 7.3779e+21, 1.3474e+21, 2.9657e+21, 2.1120e+21,
        9.8522e+20, 6.9230e+21, 4.5620e+22, 3.3841e+21, 7.2262e+21, 1.9796e+21,
        2.7509e+21, 2.2527e+21, 2.9658e+21, 7.4386e+21, 2.1242e+21, 2.1085e+21,
        3.2526e+21, 7.5391e+20, 1.0378e+22, 2.9890e+21, 1.0601e+22, 4.3926e+21,
        1.8740e+21, 1.4941e+21, 2.7349e+21, 2.2433e+21, 2.2562e+21, 2.6177e+21,
        1.2436e+22, 3.5134e+21, 4.1729e+22, 4.3318e+21, 3.4924e+21, 3.1672e+21,
        1.2532e+21, 1.9405e+21, 1.7285e+21, 9.8301e+20, 2.1436e+21, 1.5413e+21,
        4.5696e+21, 6.9611e+21, 2.2499e+21, 1.1000e+21, 2.2026e+21, 2.6072e+21,
        1.4455e+22, 5.3977e+21, 6.7590e+21, 7.4509e+21, 7.3837e+21, 2.4349e+21,
        1.9502e+21, 1.6218e+21, 1.5287e+21, 2.8196e+21, 2.0510e+21, 3.0168e+21,
        1.0525e+22, 2.2609e+21, 2.3515e+21, 1.5975e+21, 1.6191e+21, 1.8492e+21,
        2.0051e+21, 3.8800e+21, 3.9415e+21, 1.1378e+21, 2.7610e+21, 1.3171e+21,
        1.2208e+22, 1.6975e+21, 3.1619e+21, 1.4850e+22, 4.1059e+21, 1.6518e+22,
        2.0706e+21, 2.3319e+22, 3.4692e+21, 4.4622e+21, 4.5186e+21, 1.6607e+21,
        2.5837e+21, 4.1337e+21, 2.4105e+21, 6.5147e+20, 4.3252e+21, 3.2996e+21,
        5.5343e+22, 5.1093e+21, 5.0784e+21, 1.7030e+21, 4.0801e+21, 1.9831e+21,
        1.5127e+22, 1.8123e+21, 4.0771e+21, 1.7374e+21, 2.9663e+21, 2.1361e+21,
        3.2607e+21, 8.3272e+21, 3.1743e+21, 3.0593e+21, 1.1466e+21, 3.7310e+21,
        1.4155e+21, 2.0291e+22, 3.2642e+21, 5.1116e+21, 2.3715e+21, 1.0783e+22,
        2.1910e+21, 2.7479e+21, 2.0528e+21, 1.7219e+21, 2.3655e+21, 2.1953e+21,
        4.2244e+21, 2.2859e+21])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2248e+21, 5.2928e+21, 5.9908e+21, 3.4264e+21, 1.0399e+22, 7.4103e+21,
        3.0064e+21, 9.8825e+21, 7.8445e+21, 3.4972e+21, 3.3136e+21, 4.3341e+21,
        5.6037e+21, 5.1700e+21, 7.6081e+21, 5.1412e+21, 7.8126e+21, 8.5337e+21,
        5.2996e+21, 9.0482e+21, 1.5938e+22, 3.4935e+21, 4.9580e+21, 7.9344e+21,
        5.8273e+21, 6.5079e+21, 6.6381e+21, 5.1452e+21, 7.8867e+21, 4.0782e+21,
        9.5240e+21, 9.4276e+21, 6.0874e+21, 1.0897e+22, 7.8186e+21, 3.6937e+21,
        2.6664e+21, 6.2400e+21, 6.4450e+21, 4.2398e+21, 2.7811e+21, 4.7292e+21,
        5.9191e+21, 6.4200e+21, 1.0136e+22, 6.6414e+21, 5.7911e+21, 8.0717e+21,
        6.8767e+21, 5.5701e+21, 2.6197e+21, 6.5211e+21, 5.5263e+21, 4.2898e+21,
        2.6384e+21, 5.3078e+21, 4.8912e+21, 2.5374e+21, 5.2778e+21, 6.6249e+21,
        2.8584e+21, 5.4848e+21, 7.0138e+21, 4.0195e+21, 3.9449e+21, 1.0595e+22,
        7.8182e+21, 3.7970e+21, 6.5396e+21, 2.8291e+21, 2.8528e+21, 3.6575e+21,
        4.6070e+21, 1.1959e+22, 5.3518e+21, 3.9600e+21, 2.8477e+21, 5.5050e+21,
        6.6894e+21, 6.2261e+21, 7.3831e+21, 3.4935e+21, 8.2198e+21, 3.1756e+21,
        4.2662e+21, 5.6070e+21, 4.4220e+21, 7.5240e+21, 5.6500e+21, 5.6377e+21,
        7.3564e+21, 8.9777e+21, 8.2900e+21, 3.2690e+21, 2.5326e+21, 5.7581e+21,
        6.1821e+21, 5.5888e+21, 4.9183e+21, 6.3413e+21, 6.4578e+21, 2.3371e+21,
        7.5607e+21, 4.0251e+21, 5.0842e+21, 3.8741e+21, 7.1467e+21, 7.8206e+21,
        3.3351e+21, 3.4350e+21, 5.6821e+21, 6.6744e+21, 5.8153e+21, 3.5952e+21,
        3.0358e+21, 5.5306e+21, 4.7782e+21, 5.0999e+21, 4.1399e+21, 8.0018e+21,
        4.8447e+21, 5.1714e+21, 6.0505e+21, 6.0547e+21, 7.3263e+21, 5.6318e+21,
        8.8612e+21, 9.0584e+21])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.9407e+21, 3.0517e+21, 2.6109e+21, 3.0265e+21, 2.5305e+21, 3.9702e+21,
        4.9871e+21, 5.9110e+21, 6.7212e+21, 7.1522e+21, 6.8326e+21, 2.9076e+21,
        6.0050e+21, 2.2791e+21, 5.7509e+21, 5.5105e+21, 4.5748e+21, 6.6723e+21,
        7.1447e+21, 8.1871e+21, 3.0989e+21, 8.1449e+21, 5.1503e+21, 6.1906e+21,
        5.4984e+21, 4.8165e+21, 9.0363e+21, 3.3790e+21, 6.2008e+21, 2.3766e+21,
        7.4164e+21, 7.8859e+21, 4.0740e+21, 5.7428e+21, 4.6581e+21, 3.5049e+21,
        6.2874e+21, 3.3516e+21, 8.5964e+21, 4.0121e+21, 5.0846e+21, 1.8849e+21,
        6.6351e+21, 1.9002e+21, 6.4501e+21, 1.1184e+22, 6.2376e+21, 3.4907e+21,
        4.7525e+21, 7.5420e+21, 5.4153e+21, 5.8483e+21, 4.7008e+21, 5.5882e+21,
        1.0685e+22, 1.3257e+22, 2.7578e+21, 7.2213e+21, 2.1711e+21, 2.4885e+21,
        4.0457e+21, 6.0038e+21, 2.7895e+21, 6.2488e+21, 5.5976e+21, 7.4702e+21,
        5.7839e+21, 4.4017e+21, 4.1774e+21, 7.9702e+21, 4.9542e+21, 4.1400e+21,
        5.1070e+21, 3.4146e+21, 2.5139e+21, 3.3361e+21, 8.1005e+21, 2.1011e+21,
        6.5222e+21, 5.0685e+21, 6.6252e+21, 4.5411e+21, 4.5479e+21, 4.0419e+21,
        1.1018e+22, 4.2263e+21, 4.2169e+21, 6.2059e+21, 6.1408e+21, 5.1145e+21,
        5.5619e+21, 5.9510e+21, 5.8934e+21, 6.3140e+21, 3.8844e+21, 5.0786e+21,
        8.0006e+21, 3.1164e+21, 4.9755e+21, 5.8097e+21, 4.3463e+21, 3.6996e+21,
        5.0165e+21, 4.4522e+21, 6.3896e+21, 6.9074e+21, 7.2189e+21, 3.1681e+21,
        4.3815e+21, 6.2945e+21, 3.4342e+21, 3.8521e+21, 4.6572e+21, 4.5961e+21,
        4.2427e+21, 3.1959e+21, 2.2281e+21, 3.0610e+21, 3.1414e+21, 6.2048e+21,
        7.8323e+21, 2.3436e+21, 2.9507e+21, 1.0372e+22, 3.0581e+21, 7.9223e+21,
        4.7163e+21, 7.3696e+21])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.4229e+18, 1.1894e+19, 1.2847e+19, 1.6217e+19, 3.4499e+20, 2.1358e+19,
        2.7805e+19, 2.2007e+19, 4.2148e+18, 1.3942e+19, 4.0415e+19, 1.6857e+19,
        2.2974e+19, 6.5583e+18, 2.1529e+19, 4.5095e+19, 3.0684e+19, 1.6891e+19,
        1.8146e+21, 1.2606e+19, 1.5815e+20, 4.0657e+19, 3.3542e+19, 3.8255e+19,
        9.5052e+19, 2.8117e+19, 2.7774e+19, 1.0592e+20, 2.5521e+19, 5.6400e+19,
        7.5297e+19, 1.0355e+20, 1.3661e+19, 1.1750e+19, 7.8972e+18, 3.1515e+19,
        2.4714e+19, 4.7680e+19, 2.3671e+19, 2.4125e+19, 1.6375e+19, 1.3115e+20,
        3.1415e+19, 1.5691e+19, 3.1368e+19, 2.1718e+19, 2.8377e+19, 6.2435e+20,
        9.9771e+18, 2.8117e+20, 7.6837e+19, 5.1040e+19, 2.0027e+19, 5.6832e+18,
        6.1360e+18, 1.6333e+19, 1.7553e+19, 1.0253e+19, 2.0032e+19, 2.3286e+19,
        3.8270e+21, 4.4720e+19, 1.8565e+19, 1.4853e+20, 1.2244e+19, 1.4042e+19,
        1.6432e+20, 5.4039e+19, 1.5690e+19, 3.5362e+20, 2.0600e+19, 3.0932e+19,
        2.2725e+19, 6.4709e+19, 2.2796e+19, 3.2971e+19, 4.6841e+19, 5.6944e+19,
        1.8866e+19, 2.2938e+19, 4.1708e+19, 1.6485e+19, 3.1491e+19, 4.1954e+19,
        2.4492e+19, 2.2149e+19, 1.6442e+19, 2.7788e+19, 2.0190e+19, 1.7219e+19,
        4.3149e+19, 3.0002e+19, 1.6949e+19, 2.1364e+19, 2.3195e+19, 1.8432e+19,
        2.4692e+19, 1.4991e+19, 7.3836e+18, 4.9356e+19, 1.9337e+19, 4.1631e+19,
        5.9994e+19, 1.9584e+19, 4.7618e+19, 1.9341e+19, 9.8767e+20, 2.8565e+20,
        2.0416e+19, 2.6256e+19, 2.5140e+19, 1.5783e+19, 2.4372e+19, 2.6748e+19,
        2.1863e+19, 2.7132e+19, 1.2806e+19, 1.7521e+19, 2.9395e+19, 1.5913e+20,
        3.5878e+19, 1.5661e+20, 1.2332e+19, 1.3425e+21, 2.6571e+19, 3.0109e+19,
        2.9471e+19, 1.5656e+19, 1.2298e+19, 4.0073e+19, 3.5257e+19, 1.6907e+19,
        3.2751e+19, 1.6179e+19, 3.1591e+20, 2.9496e+19, 1.1535e+20, 1.5175e+19,
        1.5962e+20, 2.9216e+19, 2.1436e+19, 2.4919e+19, 1.8791e+19, 1.7295e+19,
        3.5938e+19, 3.8003e+19, 8.5917e+18, 2.7062e+19, 1.6973e+19, 2.1431e+19,
        1.3967e+19, 8.4488e+18, 3.8344e+19, 7.1541e+18, 1.8019e+19, 3.7447e+19,
        2.1686e+19, 1.2371e+19, 5.5203e+19, 2.7764e+19, 1.6458e+19, 3.6644e+19,
        1.1445e+19, 7.1012e+18, 2.9635e+19, 1.1179e+19, 2.7461e+19, 1.9784e+19,
        1.1732e+19, 1.8410e+19, 7.2830e+20, 4.1056e+20, 1.8712e+19, 2.3893e+19,
        1.7010e+19, 1.4687e+19, 1.0952e+19, 3.6643e+19, 3.3585e+20, 3.1977e+19,
        1.5947e+19, 2.2084e+19, 1.1841e+19, 1.4561e+20, 8.4716e+19, 3.1585e+18,
        1.7442e+19, 1.2071e+19, 2.7643e+19, 1.3677e+19, 9.0913e+18, 1.4636e+19,
        2.5296e+19, 3.6520e+19, 1.4999e+19, 3.4810e+19, 1.3572e+19, 3.0945e+19,
        3.6596e+19, 1.6106e+19, 2.1942e+19, 2.2836e+19, 2.1541e+19, 2.9414e+19,
        1.8925e+19, 6.8466e+18, 3.8141e+19, 2.2923e+19, 1.0203e+19, 1.9221e+19,
        1.4288e+19, 8.8349e+18, 9.5900e+18, 2.6473e+19, 2.9786e+19, 1.5193e+20,
        1.3698e+20, 1.3081e+19, 1.1678e+19, 2.4257e+19, 2.9099e+20, 3.7861e+19,
        9.5857e+18, 1.5210e+19, 2.1881e+19, 1.1727e+19, 3.1809e+19, 2.8808e+19,
        1.4485e+19, 5.7879e+19, 1.5113e+20, 1.0295e+19, 8.9900e+18, 1.2192e+19,
        2.1881e+19, 5.6707e+19, 3.3050e+19, 2.6558e+19, 2.0307e+19, 3.3619e+20,
        1.9580e+19, 3.4459e+19, 1.1610e+20, 1.5506e+19, 4.2773e+19, 4.2094e+19,
        1.7570e+19, 2.1006e+19, 4.7309e+20, 3.6919e+19, 1.5752e+19, 9.2168e+21,
        2.9398e+19, 2.0085e+19, 2.7550e+19, 3.8460e+19, 3.1241e+19, 1.9045e+19,
        1.2491e+19, 1.4332e+19, 3.7093e+19, 2.0089e+19, 3.5344e+19, 2.0143e+19,
        7.9596e+18, 2.7511e+19, 2.3042e+19, 1.3724e+19, 4.1843e+19, 1.7088e+19,
        3.6892e+19, 4.2987e+20, 7.6419e+19, 2.7760e+19, 1.0070e+19, 9.1073e+19,
        4.1174e+19, 3.3610e+19, 3.0034e+19, 3.3913e+19, 1.4833e+19, 1.6229e+19,
        1.5116e+19, 8.7183e+19, 2.1034e+19, 1.4529e+19, 5.2783e+20, 1.9591e+19,
        4.0076e+20, 1.4182e+19, 1.0934e+19, 4.1037e+20, 8.5243e+19, 1.6480e+19,
        3.4508e+19, 4.0349e+19, 4.1183e+19, 1.9356e+19, 1.6352e+19, 3.3742e+19,
        3.4154e+19, 1.3364e+19, 4.8682e+19, 2.0910e+19, 1.0523e+21, 5.3507e+19,
        7.4321e+18, 2.2783e+19, 3.7859e+19, 1.5036e+19, 2.1060e+19, 8.9325e+18,
        2.3569e+19, 1.0215e+20, 1.6703e+19, 1.4015e+19, 2.0510e+19, 2.2924e+19,
        1.0229e+20, 8.3984e+19, 1.4101e+19, 2.9365e+19, 9.4682e+18, 2.1374e+19,
        3.0718e+19, 1.2021e+19, 2.0441e+19, 4.6070e+19, 3.2128e+19, 3.1404e+19,
        3.4933e+19, 2.8357e+19, 3.9550e+19, 1.5350e+19, 4.6768e+19, 4.3533e+19,
        3.2873e+19, 6.5418e+18, 3.8620e+19, 1.3765e+19, 1.2789e+19, 3.6022e+19,
        1.1969e+19, 5.0566e+19, 1.0858e+20, 1.6634e+19, 3.6914e+19, 1.8293e+19,
        1.3866e+19, 1.1066e+19, 3.6056e+19, 2.1523e+19, 1.8394e+19, 3.5695e+19,
        5.7693e+18, 1.7439e+19, 2.0732e+19, 1.8015e+19, 8.9406e+19, 1.6244e+20,
        3.3242e+19, 8.2014e+19, 1.4660e+19, 2.0251e+20, 1.3305e+19, 1.8714e+19,
        8.6695e+18, 2.7345e+19, 2.3700e+19, 2.4729e+19, 2.6473e+19, 2.3079e+19,
        2.0119e+19, 2.0814e+19, 1.5179e+19, 1.7469e+19, 1.3190e+19, 4.2523e+19,
        1.0871e+20, 7.2750e+20, 3.9656e+19, 5.0953e+18, 1.5711e+19, 2.0765e+19,
        1.6597e+19, 7.8998e+18, 2.2515e+19, 3.9499e+19, 9.2352e+18, 6.5777e+18,
        3.4551e+19, 2.3473e+19, 2.8714e+19, 1.0854e+19, 2.0696e+19, 2.6805e+19,
        2.4258e+20, 1.1732e+19, 8.6507e+18, 1.3132e+19, 1.2898e+19, 2.8103e+19,
        1.0459e+20, 7.2038e+19, 9.9732e+19, 2.3342e+19, 1.1544e+19, 1.4535e+19,
        6.4385e+20, 1.8517e+19, 1.5299e+19, 1.7589e+19, 2.2862e+19, 1.5977e+19,
        3.7805e+19, 1.1689e+19, 3.3814e+19, 3.2081e+19, 3.0467e+19, 1.5969e+19,
        1.4579e+19, 1.5398e+19, 2.0794e+19, 2.7614e+19, 1.2715e+19, 4.2651e+18,
        3.1435e+19, 6.9584e+19, 2.7793e+19, 1.0405e+19, 1.8347e+19, 9.4031e+18,
        3.0751e+19, 3.7892e+19, 4.4371e+19, 1.2461e+19, 1.1872e+19, 6.0724e+18,
        2.3885e+19, 1.7530e+19, 1.9033e+19, 1.8343e+19, 1.8420e+19, 1.7581e+20,
        6.9579e+18, 1.6865e+19, 1.4911e+20, 7.3976e+19, 7.0508e+18, 1.4260e+19,
        1.5073e+19, 2.0711e+19, 9.4978e+18, 4.1009e+18, 2.1397e+19, 1.2301e+19,
        1.5428e+19, 2.8514e+19, 5.6170e+19, 3.1111e+19, 1.6996e+20, 1.4134e+19,
        4.5531e+20, 7.5237e+19, 3.5580e+20, 1.3566e+20, 3.9593e+19, 9.6199e+18,
        1.6268e+19, 1.2421e+20, 2.3033e+19, 3.3678e+19, 2.2195e+19, 1.9711e+19,
        1.5107e+19, 4.4624e+19, 1.8654e+19, 2.6284e+19, 1.9436e+19, 1.5234e+19,
        3.4794e+19, 2.3475e+19, 1.6128e+19, 1.3933e+19, 2.2819e+19, 3.3392e+19,
        3.0406e+19, 9.5272e+20, 6.3883e+19, 1.5300e+19, 2.2699e+19, 7.2927e+19,
        1.1447e+20, 2.2950e+19, 1.4154e+19, 1.1442e+19, 1.7984e+19, 7.1856e+19,
        1.2033e+19, 6.9826e+19, 1.1057e+20, 1.4484e+19, 1.1142e+19, 1.5494e+19,
        9.0989e+18, 1.1053e+19, 3.8223e+19, 1.7424e+19, 7.6693e+19, 1.8635e+19,
        1.2370e+19, 3.2965e+19])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.5010e+19, 7.7759e+19, 1.1055e+20, 7.8533e+19, 3.4547e+19, 4.5407e+19,
        4.8096e+19, 9.5075e+19, 5.4574e+19, 9.4749e+19, 2.8984e+19, 5.4558e+19,
        8.5982e+19, 5.6814e+19, 5.8321e+19, 6.5031e+19, 5.7853e+19, 2.9062e+19,
        8.6348e+19, 9.7328e+19, 1.0356e+20, 6.1808e+19, 9.3778e+19, 6.3805e+19,
        4.7743e+19, 5.6024e+19, 8.0904e+19, 4.3731e+19, 8.5829e+19, 4.8438e+19,
        4.6177e+19, 3.5317e+19, 4.2157e+19, 7.8344e+19, 3.5626e+19, 2.6335e+19,
        5.3097e+19, 5.5872e+19, 3.3576e+19, 3.6276e+19, 5.3187e+19, 4.6269e+19,
        3.3399e+19, 4.1889e+19, 6.1685e+19, 4.8981e+19, 3.8887e+19, 4.7299e+19,
        4.7636e+19, 5.2291e+19, 6.5347e+19, 6.0720e+19, 7.0741e+19, 6.7367e+19,
        9.6818e+19, 3.8395e+19, 4.3135e+19, 2.1840e+19, 3.7993e+19, 4.2219e+19,
        4.2623e+19, 3.9596e+19, 3.0362e+19, 6.4204e+19, 2.2193e+19, 4.3684e+19,
        9.5054e+19, 5.7034e+19, 3.4773e+19, 4.6146e+19, 6.2088e+19, 8.6425e+19,
        2.6588e+19, 5.7456e+19, 5.3932e+19, 3.3967e+19, 7.3200e+19, 3.7158e+19,
        4.0909e+19, 5.4964e+19, 2.2961e+19, 4.0595e+19, 7.4577e+19, 4.8472e+19,
        4.2709e+19, 5.9189e+19, 6.8776e+19, 2.3431e+19, 5.2886e+19, 7.3279e+19,
        4.7719e+19, 3.5506e+19, 6.6548e+19, 3.3900e+19, 5.1126e+19, 4.0338e+19,
        5.3541e+19, 7.2827e+19, 2.7269e+19, 5.1865e+19, 3.7030e+19, 3.4138e+19,
        8.1634e+19, 7.8921e+19, 3.3435e+19, 5.6623e+19, 6.7362e+19, 3.4844e+19,
        4.5625e+19, 6.0501e+19, 6.8922e+19, 2.7206e+19, 6.0321e+19, 6.2545e+19,
        3.9535e+19, 4.9299e+19, 6.7737e+19, 4.4202e+19, 4.5320e+19, 4.3951e+19,
        5.5657e+19, 3.7077e+19, 6.5366e+19, 2.7453e+19, 4.1717e+19, 6.7639e+19,
        6.8507e+19, 3.9120e+19])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.8344e+19, 3.9297e+19, 3.7411e+19, 2.0515e+19, 4.6861e+19, 2.5124e+19,
        2.1839e+19, 2.6714e+19, 4.3811e+19, 6.0868e+19, 4.5579e+19, 2.8656e+19,
        3.5255e+19, 5.4488e+19, 4.5047e+19, 3.1951e+19, 2.2026e+19, 4.0839e+19,
        1.0834e+20, 3.4240e+19, 6.0852e+19, 4.2016e+19, 3.6231e+19, 2.1505e+19,
        5.1425e+19, 5.2831e+19, 9.4435e+19, 5.3759e+19, 3.4764e+19, 5.6480e+19,
        3.7282e+19, 6.1700e+19, 6.0541e+19, 3.8637e+19, 4.2185e+19, 6.6093e+19,
        7.8958e+19, 6.9120e+19, 3.6818e+19, 2.4571e+19, 6.9147e+19, 5.6387e+19,
        3.1956e+19, 5.7981e+19, 1.1834e+20, 6.8448e+19, 3.4385e+19, 2.0068e+19,
        5.5889e+19, 2.6544e+19, 5.6004e+19, 5.8042e+19, 3.8789e+19, 1.6643e+19,
        7.0637e+19, 2.5300e+19, 6.0476e+19, 7.1983e+19, 3.4666e+19, 4.2610e+19,
        5.2978e+19, 3.8820e+19, 8.4402e+19, 6.3247e+19, 3.2852e+19, 3.0612e+19,
        5.8263e+19, 7.2428e+19, 5.8584e+19, 3.2812e+19, 3.3074e+19, 3.2346e+19,
        4.1184e+19, 6.9176e+19, 9.4446e+19, 5.2164e+19, 3.6923e+19, 7.1223e+19,
        9.0379e+19, 4.2823e+19, 4.2934e+19, 6.0928e+19, 5.8518e+19, 4.9541e+19,
        3.4465e+19, 6.8408e+19, 5.5545e+19, 6.1238e+19, 6.1450e+19, 3.2618e+19,
        5.6766e+19, 9.4058e+19, 3.9871e+19, 5.1616e+19, 1.0087e+20, 4.2933e+19,
        4.5495e+19, 8.3170e+19, 3.3976e+19, 4.7753e+19, 3.6243e+19, 3.8768e+19,
        3.6779e+19, 4.2953e+19, 4.7233e+19, 3.9369e+19, 6.4349e+19, 2.7334e+19,
        4.2354e+19, 3.5873e+19, 6.2107e+19, 5.6129e+19, 6.5835e+19, 7.5856e+19,
        2.6949e+19, 2.6154e+19, 4.8234e+19, 5.1572e+19, 7.3476e+19, 5.4707e+19,
        7.3036e+19, 2.4734e+19, 9.2417e+19, 4.6149e+19, 2.6287e+19, 5.1568e+19,
        4.6023e+19, 1.8213e+19])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2797e+17, 4.6357e+17, 2.4396e+17, 6.6938e+17, 9.7272e+17, 1.0486e+18,
        1.8915e+18, 4.0704e+18, 5.4103e+18, 1.0476e+18, 4.5522e+17, 7.5843e+16,
        1.2247e+18, 5.1131e+17, 1.5806e+18, 3.1354e+18, 5.2693e+17, 5.5066e+17,
        8.5636e+17, 7.6713e+17, 5.8247e+17, 2.1701e+18, 2.2841e+17, 1.0484e+18,
        9.7431e+17, 4.3985e+17, 1.7830e+18, 1.1748e+17, 6.5571e+17, 5.8512e+17,
        1.1197e+18, 8.7235e+17, 4.0102e+17, 6.1371e+17, 4.2775e+18, 2.5749e+18,
        4.2590e+18, 1.2533e+18, 1.9240e+18, 3.9829e+19, 6.6745e+17, 7.7095e+17,
        1.1154e+18, 1.3001e+18, 1.0495e+18, 1.4578e+18, 5.0721e+17, 7.1988e+17,
        3.8491e+17, 1.1562e+18, 4.8019e+17, 3.7112e+19, 4.0896e+18, 9.7665e+17,
        8.5039e+16, 6.6188e+17, 1.5857e+18, 3.2041e+18, 2.8215e+17, 1.3734e+19,
        2.8974e+17, 1.3895e+18, 2.5951e+17, 6.1203e+17, 3.8157e+17, 1.1030e+18,
        4.1275e+17, 3.6824e+17, 4.8442e+18, 5.3919e+17, 9.1955e+17, 2.3208e+17,
        3.3831e+17, 4.7864e+17, 6.5575e+17, 1.7930e+18, 3.5188e+17, 5.2597e+17,
        3.5398e+17, 8.8842e+17, 1.1022e+18, 1.2382e+18, 2.2727e+17, 3.5931e+17,
        1.1712e+18, 3.4944e+17, 1.0377e+18, 5.2381e+17, 1.0373e+18, 1.2007e+18,
        3.0018e+17, 1.3390e+18, 6.1045e+18, 4.5218e+18, 6.5353e+17, 9.2717e+17,
        9.5125e+17, 7.4094e+17, 1.9435e+18, 9.9790e+17, 8.9007e+17, 6.6213e+17,
        3.9193e+18, 1.4123e+18, 6.8187e+17, 2.4950e+17, 7.1758e+17, 3.5792e+17,
        5.7624e+17, 4.1718e+18, 3.7615e+17, 8.3051e+17, 3.3245e+17, 3.4027e+17,
        2.6354e+17, 7.4696e+17, 6.6435e+17, 9.4096e+17, 5.7365e+17, 8.6714e+17,
        1.9230e+17, 1.0078e+18, 1.3520e+18, 1.1108e+18, 8.1067e+17, 3.6118e+17,
        6.5945e+17, 6.9981e+17, 1.9220e+17, 1.0031e+18, 8.6242e+16, 1.8410e+17,
        1.2984e+18, 1.9854e+17, 3.2355e+18, 7.8300e+17, 9.9158e+17, 2.1167e+17,
        3.2067e+18, 7.3417e+17, 4.5889e+17, 2.3935e+18, 7.9422e+17, 1.7156e+17,
        1.3387e+18, 8.8183e+17, 1.2228e+18, 7.3226e+16, 2.2359e+17, 2.3252e+18,
        7.6043e+17, 4.3523e+17, 7.7942e+17, 8.7976e+17, 4.4543e+18, 1.0044e+18,
        9.4269e+17, 3.8074e+17, 1.4314e+18, 1.4887e+18, 9.7899e+17, 3.8294e+17,
        2.9485e+17, 2.7536e+18, 4.9693e+17, 7.2889e+17, 1.9618e+17, 5.1586e+17,
        2.3749e+18, 5.8924e+17, 8.9606e+17, 1.0780e+18, 2.5556e+18, 3.4408e+17,
        3.9754e+17, 2.9481e+18, 8.2971e+18, 3.8983e+17, 4.7155e+17, 3.4336e+17,
        2.2059e+17, 5.7027e+17, 1.4401e+18, 5.8720e+17, 2.2646e+17, 4.8629e+17,
        1.7811e+18, 4.6307e+17, 6.1226e+17, 8.8449e+18, 2.8834e+18, 3.7036e+17,
        1.1170e+18, 5.2243e+17, 1.4675e+18, 5.1978e+17, 8.7394e+17, 4.2800e+17,
        4.9685e+17, 7.7761e+17, 3.9099e+17, 2.0510e+17, 4.0374e+18, 1.7629e+18,
        1.8523e+18, 5.7930e+17, 6.4852e+17, 3.7748e+17, 4.3474e+17, 7.6289e+17,
        6.1886e+17, 2.1098e+17, 8.7436e+17, 1.2511e+18, 8.6577e+17, 3.3687e+17,
        8.9619e+17, 3.4504e+17, 5.9442e+17, 4.7791e+18, 1.1362e+18, 9.9166e+17,
        5.7147e+17, 5.9352e+17, 3.3135e+17, 6.7014e+16, 2.4192e+17, 1.7445e+18,
        3.9042e+17, 1.8862e+18, 7.8231e+17, 9.0456e+17, 7.5709e+17, 1.7659e+18,
        2.9878e+18, 4.3134e+17, 2.5427e+18, 8.7805e+17, 2.3263e+17, 9.4436e+17,
        1.0830e+18, 6.5247e+17, 3.8723e+17, 3.7031e+17, 3.7914e+17, 7.3827e+18,
        7.5464e+17, 2.9926e+17, 8.4568e+17, 1.9609e+17, 2.1249e+17, 3.1835e+17,
        3.5207e+17, 2.5216e+17, 9.7835e+17, 1.1726e+18, 3.7177e+18, 1.7376e+18,
        1.7101e+18, 6.8953e+18, 1.0045e+18, 1.2754e+18, 1.2603e+18, 3.0072e+17,
        5.6509e+17, 6.5271e+17, 1.3510e+18, 1.1617e+18, 1.2717e+18, 8.0154e+17,
        1.2490e+18, 1.8691e+18, 1.0389e+19, 3.2422e+17, 1.8825e+18, 6.2306e+19,
        3.9720e+17, 3.2204e+17, 4.3392e+17, 6.7451e+17, 9.8845e+17, 5.3980e+17,
        7.1384e+17, 1.3677e+18, 1.5169e+17, 7.5240e+17, 7.8325e+17, 3.7212e+17,
        6.0042e+17, 5.4133e+17, 6.4574e+18, 7.1597e+17, 8.8633e+17, 2.2950e+17,
        7.4262e+17, 8.6574e+17, 2.8962e+17, 1.0562e+19, 1.2641e+17, 1.9942e+17,
        9.1461e+17, 4.3304e+17, 6.2428e+17, 1.0429e+17, 9.1676e+17, 9.4053e+17,
        4.4754e+17, 9.8976e+16, 4.5958e+17, 4.4436e+18, 3.7746e+17, 9.7596e+17,
        3.0559e+17, 6.2416e+17, 5.1204e+18, 2.9667e+17, 4.2321e+17, 1.5456e+19,
        4.2136e+18, 3.1253e+17, 2.9629e+18, 6.6843e+17, 9.2960e+17, 1.8453e+18,
        4.1923e+17, 5.9617e+17, 6.9946e+17, 1.2913e+18, 1.4675e+18, 1.3045e+18,
        1.0645e+18, 1.8026e+17, 1.5512e+18, 1.1967e+18, 4.9908e+17, 6.1492e+17,
        5.2927e+17, 4.5430e+17, 7.3338e+17, 1.6133e+17, 6.1367e+17, 1.9723e+18,
        2.7490e+17, 1.0994e+18, 5.5106e+17, 1.1055e+18, 1.2471e+18, 2.8859e+17,
        1.4485e+18, 7.5919e+17, 8.3738e+17, 1.4562e+18, 1.5620e+18, 1.2741e+18,
        2.8174e+17, 1.3623e+18, 4.8591e+17, 1.2454e+18, 1.3606e+18, 2.4158e+18,
        1.3428e+18, 7.2404e+17, 1.9624e+18, 2.4023e+17, 6.5736e+17, 4.2800e+18,
        1.2169e+18, 1.5395e+17, 3.3733e+17, 1.4208e+18, 6.3283e+17, 7.4365e+17,
        8.0512e+17, 3.4353e+18, 1.4606e+18, 5.8833e+18, 1.5673e+18, 3.8549e+17,
        1.1398e+18, 7.1073e+17, 1.7548e+17, 4.2503e+17, 3.3923e+17, 9.6221e+17,
        2.0275e+18, 2.0679e+17, 2.7132e+17, 1.0005e+18, 6.1539e+17, 1.6818e+18,
        4.1840e+17, 4.9938e+17, 6.5952e+17, 1.9825e+17, 2.1023e+18, 8.4152e+17,
        7.0911e+17, 1.6519e+18, 6.2289e+17, 3.9597e+17, 2.6617e+17, 1.2725e+18,
        1.1183e+18, 6.5509e+17, 5.3622e+17, 9.3500e+17, 7.3186e+17, 1.6130e+18,
        2.2720e+17, 2.1744e+18, 4.2602e+17, 1.7325e+17, 2.1997e+17, 2.8576e+17,
        1.2574e+18, 2.9497e+17, 1.0653e+18, 1.1188e+18, 4.9020e+17, 7.8967e+16,
        7.0331e+17, 8.9310e+17, 1.1493e+18, 1.0271e+18, 1.1444e+18, 6.5126e+18,
        6.6108e+17, 6.0026e+17, 1.5428e+17, 2.4001e+17, 2.6670e+17, 2.1161e+17,
        7.6733e+17, 3.2630e+17, 1.0989e+18, 6.6953e+17, 2.8989e+18, 5.7066e+17,
        2.1287e+17, 3.4514e+17, 6.2252e+18, 8.2117e+17, 4.7904e+17, 1.9236e+17,
        4.8140e+18, 1.0009e+19, 2.0703e+17, 9.4380e+17, 4.6481e+17, 1.6046e+18,
        5.2975e+17, 1.2236e+17, 3.0845e+18, 4.7499e+17, 8.4924e+17, 1.4933e+17,
        1.5351e+17, 4.0457e+17, 8.4789e+17, 3.4716e+17, 1.5270e+18, 5.7144e+17,
        1.4773e+18, 1.0367e+18, 1.4730e+18, 7.1794e+17, 7.5158e+17, 1.6008e+18,
        4.7128e+17, 1.2114e+18, 1.8551e+18, 1.1112e+18, 4.4295e+18, 1.1806e+18,
        9.9983e+17, 4.9962e+17, 1.1027e+18, 3.1769e+17, 2.7132e+17, 1.3625e+18,
        3.1569e+18, 6.6493e+18, 5.2170e+17, 3.3125e+17, 1.2073e+18, 1.7720e+18,
        3.2512e+17, 4.5794e+18, 2.7496e+17, 9.4278e+17, 8.7998e+17, 3.6863e+19,
        1.0319e+18, 6.2257e+17, 7.2605e+17, 1.5821e+17, 4.8349e+17, 3.6387e+17,
        3.0211e+17, 4.3154e+17, 7.2181e+17, 6.3311e+17, 8.0917e+17, 2.5939e+17,
        6.8044e+17, 5.9935e+17, 3.6599e+17, 6.4750e+17, 6.0103e+17, 1.1669e+18,
        2.2090e+17, 8.2021e+17])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.2854e+17, 1.0424e+18, 1.7256e+18, 1.8836e+18, 1.1880e+18, 5.4510e+17,
        4.5774e+17, 7.5874e+17, 4.1151e+17, 9.1663e+17, 6.0286e+17, 1.9036e+18,
        5.8935e+17, 1.4731e+18, 1.7162e+18, 5.6406e+17, 6.7601e+17, 7.4239e+17,
        1.5650e+18, 1.2823e+18, 2.0923e+18, 1.7441e+18, 1.5117e+18, 6.6125e+17,
        5.0091e+17, 1.2595e+18, 1.0353e+18, 1.1452e+18, 4.8412e+17, 4.7042e+17,
        1.0120e+18, 2.6239e+18, 9.0484e+17, 1.2661e+18, 1.5054e+18, 5.8738e+17,
        3.6623e+17, 8.7826e+17, 5.9894e+17, 1.9623e+18, 3.0425e+17, 1.0245e+18,
        2.9623e+18, 7.8264e+17, 5.7632e+17, 2.7106e+17, 2.8220e+17, 8.1820e+17,
        1.2452e+18, 3.5495e+17, 1.0596e+18, 1.8544e+18, 8.2921e+17, 1.2730e+18,
        1.1782e+18, 6.4097e+17, 1.9351e+18, 2.6500e+18, 1.2542e+18, 2.4014e+18,
        1.0306e+18, 5.4354e+17, 1.5814e+18, 8.2661e+17, 2.2399e+18, 1.3539e+18,
        1.4614e+18, 9.1622e+17, 3.0718e+17, 2.0736e+18, 3.0353e+18, 2.5819e+18,
        5.6157e+17, 1.3186e+18, 5.6618e+17, 1.9890e+18, 9.3240e+17, 4.3241e+17,
        1.8093e+18, 4.6973e+17, 1.1585e+18, 7.9467e+17, 1.3826e+18, 4.8151e+17,
        4.9749e+17, 1.5579e+18, 8.2740e+17, 9.9602e+17, 2.7878e+17, 1.2176e+18,
        3.8715e+17, 7.1614e+17, 1.3406e+18, 1.0669e+18, 1.9024e+18, 1.7697e+18,
        5.3938e+17, 1.0363e+18, 1.7794e+18, 1.0776e+18, 6.4296e+17, 5.2333e+17,
        4.1651e+17, 1.8636e+18, 8.2706e+17, 1.1530e+18, 1.0477e+18, 4.8549e+17,
        8.7405e+17, 5.6627e+17, 1.2083e+18, 3.7692e+17, 4.1329e+17, 8.3998e+17,
        1.0631e+18, 1.2614e+18, 7.9086e+17, 1.1319e+18, 1.1043e+18, 1.5709e+18,
        8.5194e+17, 1.4544e+18, 1.7322e+18, 7.3115e+17, 2.0957e+18, 9.1171e+17,
        1.0854e+18, 5.7818e+17, 5.5782e+17, 1.6076e+18, 6.3475e+17, 5.9989e+17,
        8.7165e+17, 5.0984e+17, 4.0903e+17, 9.7581e+17, 6.6782e+17, 3.5555e+17,
        1.2306e+18, 6.5402e+17, 5.7413e+17, 3.7206e+17, 1.3340e+18, 7.6849e+17,
        7.0833e+17, 6.5967e+17, 6.7982e+17, 9.0192e+17, 1.3047e+18, 1.4354e+18,
        1.4316e+18, 9.7410e+17, 4.5514e+17, 9.3693e+17, 1.2148e+18, 5.1562e+17,
        3.4681e+17, 2.9387e+18, 1.8316e+18, 1.8169e+18, 8.8507e+17, 7.1139e+17,
        4.5452e+17, 1.4150e+18, 1.0490e+18, 1.0740e+18, 5.7699e+17, 5.6434e+17,
        4.0782e+17, 9.0657e+17, 1.5809e+18, 7.2322e+17, 5.6312e+17, 2.1554e+18,
        6.4894e+17, 1.1952e+18, 2.3270e+18, 1.0406e+18, 1.3637e+18, 4.1973e+17,
        1.3573e+18, 2.0263e+18, 1.1974e+18, 1.0891e+18, 1.2857e+18, 2.8766e+17,
        6.1259e+17, 6.5746e+17, 7.1949e+17, 2.1021e+18, 1.4751e+18, 1.4574e+18,
        2.1618e+18, 3.3048e+17, 9.4063e+17, 1.3855e+18, 1.0341e+18, 1.2339e+18,
        7.8984e+17, 1.2803e+18, 7.4601e+17, 2.2850e+17, 1.0319e+18, 6.3667e+17,
        2.0061e+18, 6.6352e+17, 1.2220e+18, 7.6717e+17, 7.6302e+17, 9.5489e+17,
        4.5374e+17, 6.8183e+17, 8.8017e+17, 8.1636e+17, 2.3296e+18, 5.4837e+17,
        1.8315e+18, 1.4267e+18, 1.6718e+18, 1.0841e+18, 1.7231e+18, 5.7698e+17,
        1.4141e+18, 5.0607e+17, 5.7924e+17, 3.0436e+17, 5.4987e+17, 1.6248e+18,
        1.3707e+18, 8.9246e+17, 1.5694e+18, 6.0934e+17, 4.0060e+17, 5.7368e+17,
        7.0976e+17, 4.9085e+17, 1.0664e+18, 6.0491e+17, 7.0482e+17, 9.6331e+17,
        1.0950e+18, 1.6484e+18, 1.8628e+18, 1.3176e+18, 5.3622e+17, 1.9683e+18,
        4.9035e+17, 6.7533e+17, 7.5282e+17, 8.6258e+17, 7.3718e+17, 6.8000e+17,
        1.5567e+18, 4.3450e+17, 6.2233e+17, 2.3425e+17])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.5572e+17, 8.8218e+17, 4.5799e+17, 1.0746e+18, 3.7337e+17, 4.3301e+17,
        1.0477e+18, 3.7920e+17, 5.6954e+17, 1.0717e+18, 6.3596e+17, 4.9281e+17,
        1.2489e+18, 2.6768e+18, 1.6948e+18, 2.7634e+17, 7.7258e+17, 6.5228e+17,
        9.5864e+17, 5.3648e+17, 6.3675e+17, 3.1337e+17, 4.3054e+17, 4.4054e+17,
        8.8125e+17, 6.2759e+17, 1.7190e+18, 1.1709e+18, 4.5020e+17, 4.8306e+17,
        9.3564e+17, 1.5679e+18, 1.3408e+18, 7.4520e+17, 9.2098e+17, 3.1413e+17,
        1.4997e+18, 6.7456e+17, 4.6351e+17, 5.1728e+17, 9.9364e+17, 1.3292e+18,
        1.2519e+18, 1.1297e+18, 1.1033e+18, 9.8796e+17, 1.4655e+18, 4.4546e+17,
        6.1130e+17, 4.4099e+17, 9.2450e+17, 1.6269e+18, 3.3787e+17, 1.6822e+18,
        2.0003e+18, 3.4494e+17, 1.1538e+18, 1.0640e+18, 5.0416e+17, 1.7356e+18,
        9.4689e+17, 1.3026e+18, 8.4083e+17, 1.3483e+18, 2.1846e+17, 4.1950e+17,
        2.1984e+18, 9.5161e+17, 1.0228e+18, 8.8704e+17, 8.3234e+17, 6.7263e+17,
        1.6003e+18, 8.4171e+17, 4.0452e+17, 5.1488e+17, 1.6189e+18, 1.5494e+18,
        6.0030e+17, 5.1725e+17, 1.8295e+18, 1.0610e+18, 1.2364e+18, 1.6467e+18,
        1.1265e+18, 1.0703e+18, 1.6385e+18, 5.7030e+17, 1.4568e+18, 1.0614e+18,
        4.5084e+17, 9.3955e+17, 1.4833e+18, 1.0712e+18, 1.3734e+18, 4.2917e+17,
        4.3985e+17, 7.4357e+17, 6.4753e+17, 5.9291e+17, 1.4066e+18, 1.5562e+18,
        4.9514e+17, 1.0738e+18, 3.0907e+17, 8.0641e+17, 7.6872e+17, 8.2439e+17,
        2.0026e+18, 1.0975e+18, 2.0090e+18, 1.1321e+18, 1.2776e+18, 6.0628e+17,
        8.1756e+17, 7.8013e+17, 5.6519e+17, 5.5932e+17, 6.4857e+17, 1.4340e+18,
        1.3677e+18, 1.6379e+18, 9.0151e+17, 8.3380e+17, 8.5831e+17, 4.7052e+17,
        8.3739e+17, 1.0999e+18, 7.6992e+17, 8.6683e+17, 9.2157e+17, 1.6656e+18,
        1.6451e+18, 3.4470e+17, 1.9501e+18, 1.9436e+18, 1.6453e+18, 4.4334e+17,
        1.2393e+18, 7.2430e+17, 1.4811e+18, 1.5982e+18, 5.7563e+17, 1.9302e+18,
        7.0983e+17, 7.6949e+17, 1.1468e+18, 2.1382e+18, 3.3100e+17, 1.3921e+18,
        1.0859e+18, 3.6598e+18, 2.0703e+18, 3.8087e+17, 7.2517e+17, 1.1113e+18,
        1.0244e+18, 3.7013e+17, 3.6816e+17, 1.3156e+18, 1.7133e+18, 1.0830e+18,
        3.5746e+18, 4.1142e+17, 1.4814e+18, 1.0893e+18, 2.5918e+17, 1.4942e+18,
        7.9337e+17, 4.4362e+17, 2.4472e+18, 4.3186e+17, 1.8253e+18, 1.0717e+18,
        1.2612e+18, 6.3798e+17, 5.2982e+17, 1.0189e+18, 4.6501e+17, 1.4082e+18,
        1.2269e+18, 2.8222e+17, 7.3164e+17, 1.2516e+18, 9.2274e+17, 1.5643e+18,
        7.6851e+17, 1.6579e+18, 7.7834e+17, 1.3064e+18, 6.1495e+17, 8.2160e+17,
        1.0379e+18, 1.1633e+18, 1.2308e+18, 4.9643e+17, 3.8939e+17, 5.1064e+17,
        1.1984e+18, 2.1107e+18, 9.9857e+17, 5.3161e+17, 7.3381e+17, 5.0002e+17,
        1.8557e+18, 1.7447e+18, 3.4043e+17, 6.3344e+17, 9.4088e+17, 3.4625e+17,
        1.2220e+18, 2.2757e+18, 7.7875e+17, 3.4787e+17, 9.6966e+17, 1.1499e+18,
        2.3355e+18, 2.3011e+18, 1.7583e+18, 3.0764e+17, 2.3442e+17, 2.4943e+17,
        1.2979e+18, 1.5830e+18, 7.2304e+17, 9.9127e+17, 7.1130e+17, 1.4396e+18,
        1.2353e+18, 1.3476e+18, 9.7950e+17, 1.8037e+18, 3.8345e+17, 7.7750e+17,
        2.8819e+17, 5.1298e+17, 7.6505e+17, 1.4638e+18, 5.8616e+17, 9.4989e+17,
        9.5002e+17, 4.6397e+17, 3.6640e+17, 6.6659e+17, 1.9470e+18, 2.7210e+17,
        9.3644e+17, 1.6126e+18, 1.1952e+18, 6.2957e+17, 8.2980e+17, 7.8113e+17,
        1.3571e+18, 1.1976e+18, 1.7893e+18, 5.8813e+17])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.5621e+14, 8.9465e+14, 1.8259e+15,  ..., 2.7282e+15, 2.4809e+15,
        6.4622e+14])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0752e+15, 2.5113e+15, 5.5581e+14,  ..., 7.3951e+14, 2.2349e+15,
        2.5576e+15])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.7114e+15, 1.8689e+15, 2.9009e+15, 1.7825e+15, 6.8954e+15, 4.1965e+15,
        2.7098e+15, 5.3366e+15, 4.6462e+15, 3.7868e+15, 2.1179e+15, 1.5430e+15,
        1.6268e+15, 2.1449e+15, 2.7503e+15, 2.0483e+15, 4.7187e+15, 3.4318e+15,
        4.1728e+15, 4.3342e+15, 1.6781e+15, 5.9147e+15, 3.0106e+15, 5.5602e+15,
        3.8822e+15, 2.0872e+15, 1.9208e+15, 3.9633e+15, 2.0258e+15, 6.7157e+15,
        5.0331e+15, 2.2307e+15, 6.0459e+15, 3.7320e+15, 3.6995e+15, 2.5731e+15,
        4.3730e+15, 3.9644e+15, 6.7583e+15, 4.5605e+15, 4.0787e+15, 4.8343e+15,
        1.9281e+15, 1.9734e+15, 2.6963e+15, 6.4681e+15, 7.7145e+15, 3.7047e+15,
        6.2986e+15, 6.2161e+15, 3.6617e+15, 2.3633e+15, 4.0838e+15, 2.3651e+15,
        5.2979e+15, 2.6218e+15, 6.8849e+15, 4.9073e+15, 2.1715e+15, 3.7004e+15,
        3.0021e+15, 4.1905e+15, 9.6512e+15, 5.0991e+15, 6.2730e+15, 5.2271e+15,
        2.6681e+15, 2.7669e+15, 3.8644e+15, 2.9721e+15, 5.5828e+15, 4.4492e+15,
        7.5108e+15, 1.6617e+15, 2.4460e+15, 3.6161e+15, 7.7245e+15, 6.5962e+15,
        2.7372e+15, 2.7410e+15, 3.1674e+15, 4.4661e+15, 4.8662e+15, 3.4374e+15,
        3.9983e+15, 1.9804e+15, 8.4567e+15, 2.1304e+15, 7.3791e+15, 2.5997e+15,
        1.7291e+15, 6.6606e+15, 6.9185e+15, 6.2739e+15, 1.7878e+15, 6.6103e+15,
        8.4457e+15, 4.7453e+15, 4.6125e+15, 3.5907e+15, 2.9585e+15, 3.6568e+15,
        4.1576e+15, 6.6360e+15, 2.8033e+15, 4.2176e+15, 1.3929e+15, 1.9872e+15,
        4.0073e+15, 2.9646e+15, 1.7727e+15, 3.5993e+15, 1.8273e+15, 4.3985e+15,
        2.5026e+15, 2.9554e+15, 4.9086e+15, 1.7449e+15, 2.0273e+15, 5.3427e+15,
        2.6060e+15, 3.3247e+15, 2.8303e+15, 2.6464e+15, 4.4775e+15, 4.6968e+15,
        4.1271e+15, 4.8847e+15, 1.9762e+15, 3.8279e+15, 2.8490e+15, 5.8568e+15,
        5.7590e+15, 1.3690e+15, 1.9450e+15, 4.1208e+15, 1.9331e+15, 6.7382e+15,
        3.9999e+15, 6.2608e+15, 9.1032e+15, 4.2752e+15, 5.3394e+15, 5.6659e+15,
        2.9980e+15, 3.8619e+15, 5.7217e+15, 1.8852e+15, 8.2769e+15, 4.2743e+15,
        5.0541e+15, 2.3701e+15, 9.6209e+15, 1.8989e+15, 5.0087e+15, 3.1867e+15,
        3.9429e+15, 4.3186e+15, 5.2860e+15, 5.3392e+15, 2.2636e+15, 3.4606e+15,
        2.5484e+15, 3.6193e+15, 1.9015e+15, 4.0998e+15, 4.3925e+15, 4.3261e+15,
        3.3922e+15, 8.2420e+15, 2.9239e+15, 8.5910e+15, 5.4701e+15, 2.4537e+15,
        2.9388e+15, 3.7456e+15, 2.3893e+15, 3.2503e+15, 3.3890e+15, 1.2246e+16,
        3.8159e+15, 4.8655e+15, 3.6852e+15, 4.2694e+15, 3.1922e+15, 4.0262e+15,
        3.6450e+15, 4.6730e+15, 2.2758e+15, 6.1573e+15, 3.1581e+15, 2.7316e+15,
        1.1049e+16, 4.1819e+15, 5.9416e+15, 2.0340e+15, 6.9693e+15, 8.1437e+15,
        6.2320e+15, 1.0367e+16, 4.1859e+15, 2.4169e+15, 7.2219e+15, 2.8888e+15,
        4.6600e+15, 2.6482e+15, 2.2342e+15, 4.6340e+15, 4.1508e+15, 3.3835e+15,
        1.9922e+15, 3.1331e+15, 1.6395e+15, 4.1932e+15, 4.9969e+15, 8.0163e+15,
        4.6826e+15, 2.7561e+15, 1.8179e+15, 7.4224e+15, 4.2401e+15, 4.4241e+15,
        6.9967e+15, 2.8049e+15, 1.9241e+15, 6.8781e+15, 2.0994e+15, 6.8586e+15,
        1.9331e+15, 5.0057e+15, 2.5402e+15, 1.9539e+15, 2.7642e+15, 2.1727e+15,
        2.2818e+15, 2.4074e+15, 3.1721e+15, 4.0037e+15, 7.7763e+15, 4.2421e+15,
        2.9364e+15, 2.6102e+15, 4.5002e+15, 3.9851e+15, 3.1133e+15, 6.0544e+15,
        2.6370e+15, 4.5310e+15, 4.3263e+15, 3.4016e+15, 6.3891e+15, 7.1975e+15,
        2.4100e+15, 2.7314e+15, 3.4954e+15, 1.7914e+15])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.1762e+15, 1.0574e+16, 9.6395e+15, 2.2422e+15, 5.3668e+15, 4.0629e+15,
        3.4609e+15, 3.9264e+15, 7.1218e+15, 3.1736e+15, 1.0934e+16, 3.5011e+15,
        4.5049e+15, 2.7813e+15, 3.8344e+15, 8.3026e+15, 5.0532e+15, 3.5914e+15,
        1.0166e+16, 8.3956e+15, 1.3085e+16, 3.4927e+15, 7.8353e+15, 5.4906e+15,
        8.5171e+15, 8.5099e+15, 1.1071e+16, 4.3289e+15, 4.1701e+15, 1.0542e+16,
        3.2171e+15, 1.1888e+16, 5.8281e+15, 1.0066e+16, 2.5125e+15, 2.3495e+15,
        3.3969e+15, 8.6811e+15, 1.0110e+16, 8.3713e+15, 2.5828e+15, 7.0556e+15,
        1.0280e+16, 3.2717e+15, 7.0039e+15, 9.0622e+15, 6.4745e+15, 3.8123e+15,
        3.2221e+15, 4.0041e+15, 7.3140e+15, 8.8024e+15, 5.8103e+15, 4.8230e+15,
        1.0182e+16, 3.2797e+15, 2.6616e+15, 4.7075e+15, 6.6418e+15, 6.2427e+15,
        8.3725e+15, 3.6303e+15, 5.0485e+15, 6.3543e+15, 3.6003e+15, 4.2453e+15,
        5.6241e+15, 6.4387e+15, 8.2166e+15, 3.6552e+15, 3.1167e+15, 6.6475e+15,
        2.9195e+15, 3.6887e+15, 6.7125e+15, 2.4694e+15, 5.2765e+15, 6.1953e+15,
        5.4432e+15, 3.9086e+15, 4.8140e+15, 5.9261e+15, 1.1518e+16, 2.7337e+15,
        5.3301e+15, 6.8028e+15, 6.0335e+15, 8.7018e+15, 3.4327e+15, 8.7909e+15,
        4.4004e+15, 5.8352e+15, 7.1360e+15, 2.3998e+15, 5.8264e+15, 9.4190e+15,
        1.2908e+16, 8.0817e+15, 8.4228e+15, 6.5463e+15, 2.6312e+15, 6.8453e+15,
        9.9238e+15, 5.3887e+15, 7.1967e+15, 6.8072e+15, 9.2602e+15, 5.6015e+15,
        3.1174e+15, 8.4423e+15, 5.4186e+15, 3.7863e+15, 2.8104e+15, 6.1619e+15,
        5.1602e+15, 2.3839e+15, 4.4136e+15, 3.8763e+15, 7.8882e+15, 5.8693e+15,
        3.6456e+15, 8.7325e+15, 5.9587e+15, 2.2859e+15, 6.8179e+15, 6.7557e+15,
        3.5170e+15, 3.8147e+15, 9.8745e+15, 5.2566e+15, 5.9475e+15, 4.5675e+15,
        8.1893e+15, 5.4284e+15, 6.7368e+15, 8.2767e+15, 4.8896e+15, 3.1118e+15,
        6.4831e+15, 3.5957e+15, 7.5146e+15, 5.2209e+15, 4.2252e+15, 2.9393e+15,
        4.4861e+15, 5.4174e+15, 6.5015e+15, 1.3420e+16, 1.3389e+16, 4.9859e+15,
        8.1536e+15, 3.7378e+15, 3.7461e+15, 6.3710e+15, 2.8324e+15, 4.0815e+15,
        2.1019e+15, 5.6772e+15, 3.9170e+15, 4.9902e+15, 6.2661e+15, 4.2619e+15,
        4.6592e+15, 5.4100e+15, 6.8952e+15, 4.3039e+15, 3.0837e+15, 1.0614e+16,
        3.1777e+15, 6.5343e+15, 3.1670e+15, 6.3063e+15, 2.5143e+15, 9.1612e+15,
        3.6448e+15, 5.4160e+15, 1.0432e+16, 5.4393e+15, 3.6574e+15, 7.6461e+15,
        5.7032e+15, 2.3902e+15, 7.5095e+15, 2.6131e+15, 7.1039e+15, 4.9300e+15,
        7.9757e+15, 4.7176e+15, 4.9522e+15, 3.4233e+15, 4.7577e+15, 6.1213e+15,
        1.8104e+15, 5.1102e+15, 1.4012e+16, 4.6922e+15, 2.3845e+15, 6.9492e+15,
        3.5756e+15, 8.2863e+15, 2.4803e+15, 5.3061e+15, 6.4018e+15, 8.0636e+15,
        3.5130e+15, 9.6917e+15, 4.3583e+15, 4.7793e+15, 1.0705e+16, 5.9029e+15,
        6.5883e+15, 3.2614e+15, 3.1582e+15, 3.6418e+15, 3.4094e+15, 8.1276e+15,
        6.7061e+15, 5.6737e+15, 1.0190e+16, 4.1487e+15, 4.1516e+15, 5.5316e+15,
        3.8681e+15, 2.4406e+15, 5.4976e+15, 2.5356e+15, 5.5652e+15, 4.7508e+15,
        3.4473e+15, 4.6984e+15, 5.1456e+15, 5.8518e+15, 3.1816e+15, 3.4687e+15,
        7.4398e+15, 4.9923e+15, 6.4578e+15, 2.8516e+15, 5.9111e+15, 2.2210e+15,
        1.0964e+16, 2.9781e+15, 3.5636e+15, 9.4401e+15, 2.8341e+15, 3.4097e+15,
        4.2469e+15, 1.3782e+16, 3.4193e+15, 2.4386e+15, 4.1509e+15, 1.1236e+16,
        4.7541e+15, 3.5283e+15, 3.3382e+15, 6.3581e+15])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.2118e+14, 2.1966e+13, 2.9650e+13,  ..., 5.6905e+12, 2.6540e+13,
        3.6569e+13])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.0700e+13, 7.5048e+13, 3.9702e+13, 7.3459e+13, 6.4524e+13, 5.7801e+13,
        6.6037e+13, 5.5702e+13, 5.1546e+13, 5.0628e+13, 4.3319e+13, 4.3984e+13,
        6.9292e+13, 4.6413e+13, 5.3952e+13, 4.1089e+13, 6.7178e+13, 5.6984e+13,
        4.3682e+13, 7.3177e+13, 7.0654e+13, 5.1011e+13, 7.2931e+13, 5.3564e+13,
        4.6219e+13, 5.3956e+13, 6.8848e+13, 4.1197e+13, 6.5244e+13, 4.1683e+13,
        7.5619e+13, 6.4479e+13, 6.0565e+13, 6.3781e+13, 4.9678e+13, 3.9096e+13,
        5.3579e+13, 8.1551e+13, 5.0898e+13, 5.2256e+13, 6.4592e+13, 6.0262e+13,
        6.1435e+13, 7.7844e+13, 6.5923e+13, 6.8378e+13, 6.0385e+13, 6.1322e+13,
        4.2418e+13, 5.4443e+13, 4.9407e+13, 7.1317e+13, 5.0470e+13, 6.1675e+13,
        4.1315e+13, 5.1190e+13, 4.3144e+13, 6.2553e+13, 6.1776e+13, 5.3482e+13,
        4.1190e+13, 4.2713e+13, 4.4264e+13, 5.4949e+13, 4.0319e+13, 5.2873e+13,
        5.2978e+13, 4.1408e+13, 4.1083e+13, 4.9080e+13, 5.8501e+13, 7.4608e+13,
        4.5207e+13, 4.6523e+13, 7.5643e+13, 6.3088e+13, 6.1516e+13, 5.1757e+13,
        5.4435e+13, 5.2510e+13, 4.9227e+13, 4.3753e+13, 5.9599e+13, 7.5742e+13,
        7.2264e+13, 7.3328e+13, 5.0989e+13, 5.1379e+13, 4.0001e+13, 4.2014e+13,
        4.4706e+13, 7.1255e+13, 7.1541e+13, 5.6557e+13, 6.1316e+13, 5.9637e+13,
        5.0514e+13, 5.7131e+13, 4.7976e+13, 6.5525e+13, 6.3393e+13, 4.3468e+13,
        4.8376e+13, 6.1823e+13, 6.5435e+13, 4.7228e+13, 5.6456e+13, 6.3006e+13,
        6.2629e+13, 6.3409e+13, 4.2911e+13, 5.6398e+13, 5.3646e+13, 8.6391e+13,
        6.3336e+13, 6.3396e+13, 4.2201e+13, 6.4715e+13, 7.0193e+13, 5.4214e+13,
        4.9377e+13, 4.4493e+13, 4.6054e+13, 6.5164e+13, 5.3579e+13, 7.8687e+13,
        4.6092e+13, 5.2745e+13, 6.8235e+13, 4.0804e+13, 5.4622e+13, 5.3610e+13,
        5.4702e+13, 4.6812e+13, 6.5254e+13, 6.0992e+13, 7.8415e+13, 6.5615e+13,
        7.2784e+13, 5.2694e+13, 5.3376e+13, 4.9258e+13, 4.8474e+13, 5.4289e+13,
        5.2452e+13, 5.0612e+13, 5.5605e+13, 5.4375e+13, 4.4159e+13, 5.1221e+13,
        5.5722e+13, 4.8158e+13, 5.0786e+13, 5.1564e+13, 5.2270e+13, 4.8175e+13,
        5.1716e+13, 5.0200e+13, 5.2035e+13, 8.5258e+13, 5.0821e+13, 4.2506e+13,
        4.8301e+13, 5.1652e+13, 4.9072e+13, 4.8396e+13, 4.2647e+13, 5.9420e+13,
        6.3327e+13, 5.9382e+13, 4.3823e+13, 6.4791e+13, 7.3145e+13, 4.7276e+13,
        6.7338e+13, 4.8875e+13, 4.3165e+13, 4.7557e+13, 4.4876e+13, 6.5016e+13,
        5.5552e+13, 4.9778e+13, 5.6073e+13, 7.1219e+13, 6.4604e+13, 4.7619e+13,
        6.2713e+13, 5.4543e+13, 5.5287e+13, 5.7494e+13, 5.0692e+13, 6.2389e+13,
        8.4193e+13, 4.2977e+13, 4.1580e+13, 3.4904e+13, 4.9543e+13, 6.5788e+13,
        6.2746e+13, 5.0391e+13, 7.5758e+13, 5.5852e+13, 7.5330e+13, 7.0913e+13,
        4.2127e+13, 6.4224e+13, 4.5857e+13, 8.5425e+13, 4.4334e+13, 4.8454e+13,
        4.7374e+13, 5.3912e+13, 6.7204e+13, 5.1065e+13, 5.4882e+13, 4.4642e+13,
        4.2228e+13, 7.1503e+13, 4.8124e+13, 4.7019e+13, 4.8387e+13, 7.7318e+13,
        8.4485e+13, 4.3385e+13, 7.2189e+13, 8.2780e+13, 4.1498e+13, 5.4798e+13,
        4.5276e+13, 4.9167e+13, 7.5923e+13, 5.2112e+13, 4.9907e+13, 8.2670e+13,
        4.3656e+13, 3.9636e+13, 5.6228e+13, 6.6912e+13, 5.2855e+13, 4.3752e+13,
        4.3000e+13, 4.7964e+13, 5.8054e+13, 5.8270e+13, 5.6553e+13, 6.0042e+13,
        4.2411e+13, 5.7692e+13, 6.2105e+13, 3.7312e+13, 6.2043e+13, 5.9419e+13,
        5.0483e+13, 3.6385e+13, 5.2727e+13, 5.1522e+13])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.8841e+13, 8.8319e+13, 7.4124e+13, 8.5960e+13, 1.0670e+14, 9.9219e+13,
        8.3177e+13, 4.6581e+13, 6.2775e+13, 7.5015e+13, 8.7134e+13, 6.2766e+13,
        5.5880e+13, 8.5690e+13, 8.9171e+13, 8.0728e+13, 1.2495e+14, 1.1567e+14,
        6.3169e+13, 6.3741e+13, 7.0925e+13, 1.0762e+14, 5.3863e+13, 6.5518e+13,
        8.4424e+13, 6.3813e+13, 6.0324e+13, 5.8091e+13, 7.2752e+13, 6.4848e+13,
        5.8656e+13, 1.3254e+14, 9.6463e+13, 6.1870e+13, 9.6158e+13, 7.9895e+13,
        6.1792e+13, 1.1523e+14, 5.7842e+13, 6.8535e+13, 6.7650e+13, 9.2676e+13,
        5.5199e+13, 7.3186e+13, 6.1993e+13, 8.6993e+13, 7.4386e+13, 1.0247e+14,
        8.7062e+13, 6.9850e+13, 8.3330e+13, 8.0870e+13, 8.3821e+13, 5.4622e+13,
        1.1667e+14, 7.3591e+13, 9.0593e+13, 9.3188e+13, 8.8477e+13, 8.0283e+13,
        8.2972e+13, 1.0304e+14, 7.9656e+13, 7.9941e+13, 6.2363e+13, 1.2053e+14,
        7.9643e+13, 8.2662e+13, 7.2772e+13, 8.7857e+13, 1.0664e+14, 5.5506e+13,
        1.1780e+14, 7.0035e+13, 7.5213e+13, 8.4050e+13, 6.0327e+13, 1.0956e+14,
        9.0043e+13, 8.0220e+13, 1.1041e+14, 5.7586e+13, 7.5675e+13, 7.7962e+13,
        7.3597e+13, 7.5623e+13, 8.6955e+13, 6.8282e+13, 5.5450e+13, 6.8843e+13,
        7.4451e+13, 4.7775e+13, 8.4902e+13, 7.6409e+13, 7.4711e+13, 7.5425e+13,
        9.6072e+13, 6.8549e+13, 6.1339e+13, 7.7058e+13, 9.3833e+13, 8.0516e+13,
        9.7835e+13, 1.0613e+14, 9.1929e+13, 7.9778e+13, 4.9262e+13, 6.8763e+13,
        6.3215e+13, 7.8322e+13, 6.9260e+13, 9.5398e+13, 7.6603e+13, 6.5056e+13,
        8.2422e+13, 6.0377e+13, 6.6504e+13, 6.5857e+13, 6.1074e+13, 8.0958e+13,
        5.8027e+13, 8.3865e+13, 7.0967e+13, 7.1180e+13, 9.2027e+13, 1.0845e+14,
        5.7455e+13, 6.4030e+13, 5.4205e+13, 8.4969e+13, 9.4160e+13, 1.1341e+14,
        6.7003e+13, 6.8025e+13, 7.0463e+13, 7.1136e+13, 6.5326e+13, 7.6618e+13,
        9.7388e+13, 7.8020e+13, 7.4354e+13, 9.0410e+13, 7.5858e+13, 9.3887e+13,
        9.5319e+13, 7.4641e+13, 9.4613e+13, 6.2716e+13, 8.1404e+13, 4.7682e+13,
        9.6856e+13, 6.9267e+13, 9.1574e+13, 9.4674e+13, 9.0095e+13, 9.4073e+13,
        1.2972e+14, 6.7189e+13, 7.9767e+13, 9.0568e+13, 6.8159e+13, 8.8293e+13,
        5.4680e+13, 5.9194e+13, 8.2088e+13, 5.8349e+13, 5.7974e+13, 5.6178e+13,
        9.3013e+13, 5.9013e+13, 6.4463e+13, 8.4173e+13, 7.2931e+13, 8.1449e+13,
        4.7351e+13, 6.2536e+13, 8.1075e+13, 7.2773e+13, 8.7395e+13, 6.8445e+13,
        6.3655e+13, 1.1006e+14, 9.6939e+13, 6.5202e+13, 7.0916e+13, 6.3726e+13,
        7.3943e+13, 6.8290e+13, 7.0990e+13, 7.2854e+13, 7.6677e+13, 9.3409e+13,
        7.8860e+13, 7.9325e+13, 1.2052e+14, 7.3181e+13, 9.1707e+13, 8.5856e+13,
        8.5800e+13, 8.4452e+13, 8.2605e+13, 7.8389e+13, 8.1003e+13, 7.6698e+13,
        1.1013e+14, 5.5843e+13, 7.8086e+13, 8.1332e+13, 7.0318e+13, 7.2504e+13,
        6.7127e+13, 1.0756e+14, 7.6801e+13, 7.8680e+13, 5.6523e+13, 6.3417e+13,
        1.0831e+14, 6.2412e+13, 6.2173e+13, 5.8022e+13, 7.6264e+13, 8.5763e+13,
        6.8143e+13, 6.6659e+13, 5.8925e+13, 7.1281e+13, 5.8582e+13, 9.3268e+13,
        6.3080e+13, 6.7181e+13, 9.8468e+13, 9.3220e+13, 9.0553e+13, 8.3035e+13,
        6.6083e+13, 5.5752e+13, 6.2296e+13, 7.3018e+13, 6.4263e+13, 8.2284e+13,
        5.1537e+13, 5.4288e+13, 6.5037e+13, 9.4779e+13, 6.8157e+13, 9.5121e+13,
        1.0074e+14, 1.0238e+14, 5.8284e+13, 6.5258e+13, 1.2953e+14, 6.5234e+13,
        5.4500e+13, 7.4494e+13, 7.7664e+13, 7.7841e+13])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.6678e+11, 8.6350e+11, 9.9181e+11,  ..., 6.7481e+12, 8.6473e+11,
        5.7765e+11])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6621e+12, 2.2501e+12, 2.3295e+12, 2.6347e+12, 1.9455e+12, 2.2791e+12,
        1.6253e+12, 1.3548e+12, 1.0787e+12, 1.8541e+12, 1.6085e+12, 1.7376e+12,
        1.2847e+12, 1.5749e+12, 2.2018e+12, 1.8193e+12, 1.1020e+12, 2.2974e+12,
        1.5642e+12, 1.1857e+12, 1.2644e+12, 1.4215e+12, 8.7667e+11, 1.2782e+12,
        1.8574e+12, 1.5887e+12, 1.7716e+12, 1.7769e+12, 1.8674e+12, 1.9367e+12,
        1.7626e+12, 1.3221e+12, 1.3066e+12, 1.9082e+12, 2.2274e+12, 1.8188e+12,
        2.0208e+12, 1.1624e+12, 1.2540e+12, 2.0540e+12, 1.1752e+12, 1.8076e+12,
        1.4661e+12, 1.7470e+12, 1.3846e+12, 1.6837e+12, 1.2830e+12, 1.3138e+12,
        1.9430e+12, 1.6757e+12, 2.3460e+12, 2.2049e+12, 1.2647e+12, 1.8673e+12,
        1.9613e+12, 1.9039e+12, 1.2155e+12, 1.5469e+12, 1.3315e+12, 1.7166e+12,
        1.4232e+12, 1.5579e+12, 1.5688e+12, 1.8402e+12, 1.1753e+12, 2.2367e+12,
        1.3470e+12, 1.0518e+12, 1.5713e+12, 1.2206e+12, 2.2574e+12, 1.4721e+12,
        1.8931e+12, 1.6342e+12, 1.6746e+12, 1.5808e+12, 1.3939e+12, 1.3163e+12,
        1.6128e+12, 1.4363e+12, 1.1649e+12, 1.7991e+12, 1.8444e+12, 1.1410e+12,
        1.8484e+12, 2.0300e+12, 1.5412e+12, 1.4896e+12, 1.7969e+12, 1.3898e+12,
        2.0610e+12, 1.4614e+12, 1.2491e+12, 1.7774e+12, 1.5983e+12, 1.2203e+12,
        1.7750e+12, 1.9272e+12, 2.2315e+12, 2.3474e+12, 1.7666e+12, 1.8869e+12,
        2.0880e+12, 1.5503e+12, 2.0312e+12, 1.9073e+12, 1.3465e+12, 1.5494e+12,
        2.0637e+12, 1.3219e+12, 2.0211e+12, 1.7661e+12, 2.3800e+12, 1.7683e+12,
        1.9775e+12, 1.9841e+12, 1.8447e+12, 1.2049e+12, 1.7041e+12, 1.7009e+12,
        1.9434e+12, 1.0608e+12, 1.1687e+12, 1.4377e+12, 1.5603e+12, 1.9929e+12,
        1.3679e+12, 2.8125e+12, 1.9220e+12, 2.0407e+12, 1.2589e+12, 1.4672e+12,
        1.4942e+12, 1.3862e+12, 1.7078e+12, 1.2387e+12, 2.2407e+12, 1.0846e+12,
        1.4674e+12, 1.6511e+12, 1.4939e+12, 1.7377e+12, 1.6662e+12, 1.4613e+12,
        1.3761e+12, 1.7583e+12, 1.1906e+12, 1.8892e+12, 1.4558e+12, 1.6509e+12,
        2.5124e+12, 1.4379e+12, 1.8647e+12, 1.3997e+12, 2.0228e+12, 1.8117e+12,
        1.3720e+12, 1.6562e+12, 1.4299e+12, 1.1039e+12, 1.7739e+12, 1.8893e+12,
        1.2098e+12, 1.0437e+12, 1.6872e+12, 2.0372e+12, 1.7742e+12, 1.9388e+12,
        2.0973e+12, 1.5868e+12, 1.4522e+12, 1.3264e+12, 1.4281e+12, 1.1825e+12,
        1.5911e+12, 1.4972e+12, 1.5285e+12, 1.6240e+12, 1.9287e+12, 1.1211e+12,
        1.6745e+12, 1.8903e+12, 1.4802e+12, 1.7772e+12, 1.7610e+12, 1.9614e+12,
        1.4796e+12, 2.5625e+12, 1.9876e+12, 1.6220e+12, 1.4917e+12, 1.9053e+12,
        1.6794e+12, 1.9270e+12, 1.6252e+12, 1.4026e+12, 1.7876e+12, 1.9952e+12,
        1.3855e+12, 1.3457e+12, 1.9741e+12, 2.1660e+12, 1.7261e+12, 1.2568e+12,
        1.1394e+12, 2.1547e+12, 2.1120e+12, 1.0532e+12, 1.5448e+12, 1.3841e+12,
        2.0424e+12, 1.1978e+12, 1.8839e+12, 1.4797e+12, 1.4215e+12, 2.3819e+12,
        2.0967e+12, 1.4474e+12, 1.3518e+12, 2.3834e+12, 1.5823e+12, 1.8351e+12,
        1.4198e+12, 1.5987e+12, 1.5194e+12, 1.3025e+12, 1.5283e+12, 1.8889e+12,
        1.0366e+12, 1.3039e+12, 1.5820e+12, 2.0954e+12, 1.5224e+12, 1.3318e+12,
        1.9246e+12, 1.9065e+12, 1.6503e+12, 1.8646e+12, 1.2231e+12, 2.2429e+12,
        1.6907e+12, 1.5806e+12, 1.4306e+12, 1.3187e+12, 1.9115e+12, 1.7832e+12,
        1.6998e+12, 1.4199e+12, 1.9676e+12, 1.4745e+12, 1.8641e+12, 1.2376e+12,
        1.5519e+12, 1.4198e+12, 2.2515e+12, 1.8372e+12])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2584e+12, 2.2452e+12, 2.7636e+12, 3.8510e+12, 2.7181e+12, 2.6894e+12,
        2.8428e+12, 2.7335e+12, 1.6914e+12, 1.8814e+12, 3.1346e+12, 2.5924e+12,
        2.1221e+12, 2.4380e+12, 2.5415e+12, 3.2187e+12, 2.9009e+12, 3.4459e+12,
        2.4562e+12, 1.9430e+12, 2.8387e+12, 2.4424e+12, 1.7836e+12, 1.7403e+12,
        2.1067e+12, 2.7506e+12, 1.7248e+12, 2.3211e+12, 2.0134e+12, 2.4956e+12,
        2.1601e+12, 2.0209e+12, 1.8518e+12, 2.5737e+12, 1.7519e+12, 2.4988e+12,
        2.0383e+12, 3.6134e+12, 2.3239e+12, 2.2027e+12, 2.4376e+12, 3.1143e+12,
        2.0159e+12, 2.3254e+12, 2.6465e+12, 2.2974e+12, 2.6156e+12, 2.2016e+12,
        2.0581e+12, 3.0661e+12, 2.1358e+12, 2.7729e+12, 3.1655e+12, 2.5676e+12,
        2.3027e+12, 1.8277e+12, 2.2547e+12, 2.5020e+12, 2.4708e+12, 2.8367e+12,
        2.9020e+12, 2.0903e+12, 2.9693e+12, 2.2163e+12, 2.2385e+12, 3.1532e+12,
        3.1114e+12, 2.5857e+12, 2.6174e+12, 3.3540e+12, 2.6634e+12, 2.8015e+12,
        3.0721e+12, 3.1145e+12, 2.7873e+12, 1.7382e+12, 2.6165e+12, 2.1262e+12,
        2.4451e+12, 2.8282e+12, 1.9394e+12, 2.9971e+12, 2.5204e+12, 2.6000e+12,
        2.7818e+12, 3.2023e+12, 2.9907e+12, 2.1226e+12, 2.2630e+12, 2.2007e+12,
        4.1866e+12, 2.9302e+12, 2.3276e+12, 2.1971e+12, 2.7635e+12, 2.0294e+12,
        2.6893e+12, 2.0927e+12, 2.2847e+12, 2.9578e+12, 3.2579e+12, 2.2229e+12,
        2.6402e+12, 2.9792e+12, 2.8768e+12, 4.7143e+12, 2.9686e+12, 2.3161e+12,
        2.3397e+12, 3.4185e+12, 1.8176e+12, 1.8198e+12, 2.2638e+12, 2.1587e+12,
        2.3133e+12, 1.8278e+12, 2.1680e+12, 2.1206e+12, 2.0187e+12, 1.9790e+12,
        2.9489e+12, 2.5041e+12, 2.5667e+12, 2.9534e+12, 2.2452e+12, 2.7069e+12,
        1.5943e+12, 1.4713e+12, 2.0756e+12, 2.1784e+12, 2.7727e+12, 3.0557e+12,
        2.4357e+12, 3.6302e+12, 2.4301e+12, 2.5719e+12, 2.9539e+12, 2.3198e+12,
        2.4528e+12, 1.9449e+12, 2.6640e+12, 2.0828e+12, 2.0719e+12, 3.1772e+12,
        2.9783e+12, 2.3375e+12, 2.4794e+12, 2.8013e+12, 3.1883e+12, 2.9433e+12,
        2.3289e+12, 2.1747e+12, 3.0479e+12, 2.5965e+12, 2.3906e+12, 2.0729e+12,
        2.3991e+12, 1.7991e+12, 2.5577e+12, 2.3346e+12, 1.9383e+12, 2.3567e+12,
        2.2155e+12, 1.9514e+12, 2.3555e+12, 2.3370e+12, 2.0239e+12, 2.5999e+12,
        2.8336e+12, 1.5202e+12, 2.8047e+12, 2.9544e+12, 2.2760e+12, 1.6528e+12,
        2.9851e+12, 2.8600e+12, 2.4103e+12, 2.0772e+12, 2.7765e+12, 2.0158e+12,
        1.9688e+12, 2.9024e+12, 2.4143e+12, 1.8520e+12, 2.2107e+12, 2.5338e+12,
        1.7504e+12, 2.0169e+12, 3.0933e+12, 2.4712e+12, 1.9422e+12, 2.4187e+12,
        2.1199e+12, 2.4246e+12, 2.4604e+12, 2.4195e+12, 2.8042e+12, 2.3851e+12,
        3.0457e+12, 2.9374e+12, 2.8166e+12, 3.4922e+12, 2.6145e+12, 2.5865e+12,
        2.5621e+12, 2.2847e+12, 2.3355e+12, 3.7675e+12, 1.8311e+12, 2.7543e+12,
        2.6507e+12, 3.2633e+12, 2.1435e+12, 2.7056e+12, 2.7405e+12, 2.7974e+12,
        2.1424e+12, 2.3937e+12, 2.3504e+12, 2.8064e+12, 2.8246e+12, 2.6824e+12,
        2.5725e+12, 2.0181e+12, 2.8080e+12, 2.0516e+12, 2.5326e+12, 2.8874e+12,
        2.0150e+12, 3.2279e+12, 1.8957e+12, 1.4686e+12, 2.8888e+12, 2.5327e+12,
        2.6698e+12, 2.1462e+12, 2.7810e+12, 2.1344e+12, 2.8675e+12, 2.3248e+12,
        2.9388e+12, 1.9606e+12, 1.8675e+12, 1.9919e+12, 2.0900e+12, 2.6592e+12,
        2.1819e+12, 2.3769e+12, 3.0997e+12, 2.5351e+12, 2.1493e+12, 1.6345e+12,
        2.6407e+12, 2.1163e+12, 3.5458e+12, 2.7108e+12])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.3302e+10, 1.3447e+11, 2.6182e+11,  ..., 1.7243e+10, 2.7025e+10,
        2.2725e+10])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.4367e+10, 4.4963e+10, 5.7871e+10, 5.5470e+10, 5.4933e+10, 5.0834e+10,
        5.3663e+10, 8.2565e+10, 5.4177e+10, 5.6907e+10, 6.7351e+10, 4.5069e+10,
        5.4510e+10, 5.9624e+10, 5.2991e+10, 5.2190e+10, 5.5974e+10, 4.2311e+10,
        5.1305e+10, 4.6827e+10, 4.1773e+10, 4.5511e+10, 4.3492e+10, 5.4125e+10,
        6.4792e+10, 5.7393e+10, 4.2458e+10, 8.6587e+10, 3.9688e+10, 6.1656e+10,
        5.4181e+10, 4.4888e+10, 5.7140e+10, 5.0760e+10, 4.0918e+10, 5.0123e+10,
        6.6564e+10, 8.2366e+10, 5.2991e+10, 6.4931e+10, 3.7068e+10, 5.0652e+10,
        5.5493e+10, 7.0549e+10, 4.4086e+10, 6.7918e+10, 5.2924e+10, 5.5737e+10,
        7.0211e+10, 8.2715e+10, 3.9948e+10, 6.4472e+10, 4.6237e+10, 6.4068e+10,
        5.6201e+10, 5.2851e+10, 7.8440e+10, 4.9590e+10, 4.4769e+10, 3.7326e+10,
        6.2582e+10, 7.3299e+10, 3.2519e+10, 3.1683e+10, 5.8534e+10, 7.3730e+10,
        6.1000e+10, 6.9827e+10, 5.0506e+10, 6.2454e+10, 4.6997e+10, 5.9451e+10,
        5.6505e+10, 5.3900e+10, 6.4055e+10, 6.3996e+10, 8.7577e+10, 4.7094e+10,
        7.5896e+10, 6.3819e+10, 7.9627e+10, 4.7280e+10, 5.9819e+10, 4.1289e+10,
        6.0248e+10, 3.6906e+10, 3.6520e+10, 7.7923e+10, 7.1760e+10, 4.6282e+10,
        6.2042e+10, 4.9387e+10, 4.9094e+10, 6.7813e+10, 3.5329e+10, 5.1783e+10,
        5.4862e+10, 5.1621e+10, 3.6350e+10, 5.5954e+10, 6.8046e+10, 7.8674e+10,
        6.5192e+10, 5.7010e+10, 6.9071e+10, 5.8368e+10, 7.7308e+10, 4.6207e+10,
        6.2662e+10, 6.5825e+10, 6.0384e+10, 5.0674e+10, 4.3093e+10, 4.6901e+10,
        4.7129e+10, 5.2389e+10, 5.2990e+10, 4.3784e+10, 6.6872e+10, 3.6834e+10,
        5.4978e+10, 8.9528e+10, 5.0293e+10, 8.3378e+10, 5.5317e+10, 4.9298e+10,
        5.6653e+10, 5.1387e+10, 3.4131e+10, 5.7510e+10, 5.9975e+10, 6.3957e+10,
        5.1139e+10, 3.8700e+10, 6.2592e+10, 4.6279e+10, 6.2474e+10, 8.0128e+10,
        5.4788e+10, 7.2437e+10, 5.0571e+10, 4.7007e+10, 4.2130e+10, 6.1720e+10,
        4.8062e+10, 7.0858e+10, 7.6821e+10, 5.4184e+10, 7.5668e+10, 5.2623e+10,
        4.4092e+10, 6.0524e+10, 7.4105e+10, 5.7730e+10, 7.3801e+10, 4.5766e+10,
        6.0657e+10, 4.7731e+10, 3.1591e+10, 7.4720e+10, 7.7836e+10, 8.4625e+10,
        4.1319e+10, 3.8056e+10, 5.7598e+10, 3.4300e+10, 4.9179e+10, 4.8895e+10,
        3.9265e+10, 3.2064e+10, 5.1962e+10, 5.8774e+10, 5.3946e+10, 6.1977e+10,
        6.4182e+10, 4.8977e+10, 4.6363e+10, 4.9410e+10, 5.4395e+10, 5.0948e+10,
        6.3451e+10, 5.8354e+10, 7.4548e+10, 5.3147e+10, 6.5197e+10, 5.4602e+10,
        7.5502e+10, 6.9833e+10, 5.6350e+10, 4.5124e+10, 4.2096e+10, 5.6786e+10,
        7.4019e+10, 6.8780e+10, 6.9137e+10, 7.0283e+10, 7.6048e+10, 7.2281e+10,
        7.4438e+10, 7.8622e+10, 4.0460e+10, 5.4033e+10, 5.3543e+10, 6.5212e+10,
        6.0756e+10, 9.0783e+10, 6.4216e+10, 4.8553e+10, 5.9845e+10, 5.8309e+10,
        8.1294e+10, 7.3497e+10, 7.8552e+10, 7.0554e+10, 4.9646e+10, 5.5282e+10,
        6.1203e+10, 3.9936e+10, 4.2300e+10, 5.4933e+10, 5.7235e+10, 6.0140e+10,
        5.4165e+10, 5.3783e+10, 5.3241e+10, 4.8383e+10, 4.9963e+10, 5.6148e+10,
        5.1372e+10, 5.9902e+10, 5.0544e+10, 4.0045e+10, 3.6896e+10, 6.3872e+10,
        4.2460e+10, 5.3498e+10, 5.4686e+10, 7.5858e+10, 9.2169e+10, 6.8294e+10,
        5.7320e+10, 5.3192e+10, 7.5181e+10, 5.3407e+10, 7.4199e+10, 4.3864e+10,
        3.6589e+10, 5.8440e+10, 5.4817e+10, 7.4441e+10, 4.8538e+10, 3.2974e+10,
        3.9138e+10, 6.1268e+10, 3.2581e+10, 5.9995e+10])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.2061e+10, 7.0247e+10, 8.4536e+10, 6.9028e+10, 7.9084e+10, 8.7318e+10,
        6.9588e+10, 8.9119e+10, 7.4904e+10, 9.7028e+10, 6.2347e+10, 7.2380e+10,
        1.1934e+11, 7.5047e+10, 5.3621e+10, 1.2815e+11, 1.6623e+11, 1.2148e+11,
        1.1112e+11, 7.8313e+10, 9.9532e+10, 8.8279e+10, 7.0658e+10, 7.2798e+10,
        5.9053e+10, 9.1590e+10, 1.1842e+11, 7.7470e+10, 9.2264e+10, 6.8290e+10,
        7.5714e+10, 7.1258e+10, 7.3525e+10, 8.5492e+10, 1.0313e+11, 1.0232e+11,
        7.5558e+10, 1.0196e+11, 1.0418e+11, 8.8743e+10, 6.3683e+10, 1.1023e+11,
        6.8418e+10, 8.0893e+10, 7.8246e+10, 6.9957e+10, 7.4620e+10, 7.2982e+10,
        8.5841e+10, 6.6185e+10, 1.1548e+11, 1.0537e+11, 8.2324e+10, 7.8087e+10,
        8.0417e+10, 7.6809e+10, 1.1119e+11, 8.7393e+10, 8.2700e+10, 6.7223e+10,
        5.6187e+10, 6.7761e+10, 9.6399e+10, 5.4249e+10, 6.4599e+10, 7.5933e+10,
        6.9663e+10, 1.0205e+11, 4.9606e+10, 7.6309e+10, 1.1310e+11, 6.8201e+10,
        7.7029e+10, 1.0376e+11, 7.0204e+10, 9.6124e+10, 8.6278e+10, 1.2182e+11,
        1.1217e+11, 6.3929e+10, 8.9065e+10, 9.4343e+10, 8.3379e+10, 1.0046e+11,
        8.7367e+10, 7.7496e+10, 5.7654e+10, 1.0740e+11, 6.9541e+10, 7.1135e+10,
        1.0465e+11, 4.9713e+10, 1.0213e+11, 1.0298e+11, 1.1953e+11, 8.5977e+10,
        6.5276e+10, 1.1382e+11, 6.8049e+10, 7.9096e+10, 1.0174e+11, 1.0537e+11,
        8.7007e+10, 8.5129e+10, 9.6695e+10, 8.1465e+10, 7.7807e+10, 7.0171e+10,
        1.1362e+11, 1.0695e+11, 1.0310e+11, 9.9728e+10, 6.9487e+10, 6.5965e+10,
        1.1361e+11, 1.1833e+11, 8.9787e+10, 5.8187e+10, 8.6121e+10, 9.4505e+10,
        1.1833e+11, 9.0165e+10, 7.2600e+10, 7.2220e+10, 9.7340e+10, 8.7287e+10,
        7.8015e+10, 1.1622e+11, 1.2172e+11, 8.5334e+10, 9.5665e+10, 6.3527e+10,
        8.7938e+10, 8.1033e+10, 7.1231e+10, 5.6719e+10, 6.9940e+10, 9.4507e+10,
        1.4030e+11, 1.2582e+11, 1.1032e+11, 1.0160e+11, 9.7550e+10, 8.0183e+10,
        1.0356e+11, 7.6803e+10, 1.0201e+11, 1.1039e+11, 6.3154e+10, 6.5127e+10,
        6.8899e+10, 7.2141e+10, 9.5493e+10, 9.2601e+10, 6.7860e+10, 9.9680e+10,
        5.4692e+10, 8.1914e+10, 7.7003e+10, 9.1048e+10, 8.8685e+10, 7.2476e+10,
        1.0800e+11, 9.0006e+10, 5.6617e+10, 9.0562e+10, 1.1635e+11, 1.0549e+11,
        8.7810e+10, 7.9501e+10, 5.4178e+10, 6.6521e+10, 8.5413e+10, 7.3297e+10,
        9.7748e+10, 7.6006e+10, 5.1589e+10, 8.6495e+10, 6.8560e+10, 9.1943e+10,
        6.3054e+10, 1.1116e+11, 9.5608e+10, 8.2234e+10, 1.1007e+11, 7.3134e+10,
        7.2067e+10, 1.2027e+11, 9.1395e+10, 6.9474e+10, 6.3063e+10, 8.1982e+10,
        9.7056e+10, 7.6215e+10, 6.8964e+10, 6.8934e+10, 6.2112e+10, 9.2893e+10,
        7.2660e+10, 8.2630e+10, 9.1394e+10, 1.0695e+11, 1.0203e+11, 1.0664e+11,
        1.1763e+11, 8.6172e+10, 8.3359e+10, 8.0242e+10, 1.1670e+11, 7.2093e+10,
        7.3533e+10, 1.1122e+11, 8.5590e+10, 1.0346e+11, 8.2493e+10, 9.4535e+10,
        1.1853e+11, 8.9207e+10, 7.0161e+10, 7.9812e+10, 1.2826e+11, 1.1574e+11,
        8.1261e+10, 1.0122e+11, 9.4682e+10, 1.0000e+11, 1.0855e+11, 1.0404e+11,
        1.1196e+11, 6.1078e+10, 7.8747e+10, 1.4499e+11, 1.1114e+11, 8.2021e+10,
        1.2147e+11, 1.0534e+11, 8.5278e+10, 8.4387e+10, 1.1681e+11, 8.5661e+10,
        9.4524e+10, 1.2862e+11, 5.9715e+10, 6.4795e+10, 1.5986e+11, 6.4034e+10,
        8.1921e+10, 1.0632e+11, 1.3212e+11, 1.0753e+11, 9.6722e+10, 1.0644e+11,
        7.7624e+10, 7.0641e+10, 1.0246e+11, 7.7661e+10])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.1890e+08, 5.4463e+08, 6.3755e+08,  ..., 6.8446e+08, 1.0670e+10,
        4.1880e+08])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([8.8074e+08, 1.6589e+09, 1.1884e+09, 1.6310e+09, 1.2897e+09, 1.1564e+09,
        1.6369e+09, 1.4468e+09, 1.9661e+09, 1.8444e+09, 2.2500e+09, 1.3350e+09,
        1.5485e+09, 1.6341e+09, 1.6640e+09, 1.4093e+09, 1.3659e+09, 1.6440e+09,
        1.3247e+09, 2.3387e+09, 9.9468e+08, 1.1744e+09, 1.1318e+09, 1.7053e+09,
        1.3534e+09, 1.9048e+09, 1.1217e+09, 2.2898e+09, 9.6836e+08, 1.5559e+09,
        1.5308e+09, 1.4794e+09, 1.5950e+09, 1.1146e+09, 1.1150e+09, 1.3090e+09,
        2.2514e+09, 1.3920e+09, 1.2345e+09, 1.6202e+09, 1.2037e+09, 1.9230e+09,
        1.6610e+09, 1.3104e+09, 1.7557e+09, 1.7200e+09, 1.6008e+09, 1.7457e+09,
        1.3352e+09, 2.3974e+09, 1.0948e+09, 1.0338e+09, 1.5500e+09, 1.5648e+09,
        1.4576e+09, 1.8102e+09, 1.4733e+09, 1.3734e+09, 1.3332e+09, 9.9141e+08,
        1.1659e+09, 1.4414e+09, 1.2121e+09, 1.1548e+09, 1.1795e+09, 1.2962e+09,
        1.5135e+09, 1.3589e+09, 1.2907e+09, 1.5745e+09, 1.8755e+09, 1.6279e+09,
        1.8877e+09, 1.5738e+09, 1.0185e+09, 1.3444e+09, 1.5176e+09, 1.4156e+09,
        1.3885e+09, 1.6064e+09, 1.0047e+09, 1.4683e+09, 1.8067e+09, 1.3420e+09,
        1.5875e+09, 1.4342e+09, 1.4307e+09, 1.3735e+09, 1.1064e+09, 1.1933e+09,
        1.0358e+09, 1.3846e+09, 1.5892e+09, 1.6629e+09, 1.3934e+09, 1.0973e+09,
        1.8778e+09, 1.8327e+09, 1.1715e+09, 1.7295e+09, 2.3772e+09, 1.3943e+09,
        1.0207e+09, 1.6667e+09, 1.6444e+09, 1.4730e+09, 1.4741e+09, 1.0354e+09,
        9.2464e+08, 8.8267e+08, 1.4801e+09, 1.3980e+09, 1.3994e+09, 1.5640e+09,
        1.0744e+09, 1.6572e+09, 1.5266e+09, 1.3627e+09, 1.4236e+09, 1.7975e+09,
        1.0089e+09, 1.2010e+09, 9.2380e+08, 1.3943e+09, 1.3952e+09, 1.6808e+09,
        1.8482e+09, 1.5387e+09, 1.7616e+09, 1.2848e+09, 1.0666e+09, 1.5287e+09,
        1.2635e+09, 1.0193e+09, 1.3960e+09, 1.1342e+09, 1.8310e+09, 1.2944e+09,
        1.2703e+09, 1.1616e+09, 1.3867e+09, 1.5874e+09, 1.6835e+09, 1.4793e+09,
        1.2198e+09, 1.6214e+09, 1.3185e+09, 1.2187e+09, 1.4521e+09, 1.8708e+09,
        1.5051e+09, 1.9577e+09, 1.6647e+09, 1.5923e+09, 1.3440e+09, 1.4215e+09,
        1.6767e+09, 1.3930e+09, 1.7839e+09, 1.0051e+09, 1.4781e+09, 9.9903e+08,
        1.5854e+09, 1.3280e+09, 1.5128e+09, 2.1307e+09, 1.1632e+09, 1.0930e+09,
        2.1542e+09, 1.2220e+09, 1.6663e+09, 1.5224e+09, 2.1285e+09, 1.3839e+09,
        1.0591e+09, 1.6484e+09, 1.0559e+09, 1.5583e+09, 1.7202e+09, 1.8445e+09,
        1.6831e+09, 1.7251e+09, 1.3797e+09, 1.3687e+09, 1.2776e+09, 1.5931e+09,
        1.4808e+09, 1.5544e+09, 1.5783e+09, 1.5712e+09, 1.7912e+09, 1.2544e+09,
        1.3296e+09, 1.1445e+09, 9.1899e+08, 1.3340e+09, 1.9492e+09, 1.5205e+09,
        1.9506e+09, 2.1291e+09, 1.5317e+09, 1.2635e+09, 1.2390e+09, 1.6491e+09,
        1.7016e+09, 1.3033e+09, 1.5175e+09, 1.7106e+09, 1.4471e+09, 1.4420e+09,
        2.0958e+09, 1.2856e+09, 1.6997e+09, 1.1469e+09, 1.4198e+09, 1.8838e+09,
        1.1182e+09, 1.2571e+09, 1.3710e+09, 1.5022e+09, 1.3844e+09, 1.2208e+09,
        1.4025e+09, 1.2148e+09, 1.2615e+09, 8.6883e+08, 9.0216e+08, 9.5218e+08,
        1.6531e+09, 1.7280e+09, 1.2100e+09, 1.6950e+09, 1.3951e+09, 2.1166e+09,
        1.5406e+09, 2.2014e+09, 1.6728e+09, 1.5830e+09, 1.7327e+09, 1.2091e+09,
        1.0991e+09, 1.4452e+09, 1.3600e+09, 2.0550e+09, 2.0175e+09, 1.2374e+09,
        1.5036e+09, 1.2581e+09, 1.5509e+09, 1.5414e+09, 1.4549e+09, 2.0061e+09,
        1.8963e+09, 1.5987e+09, 1.6998e+09, 1.7032e+09])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.7906e+09, 2.1440e+09, 2.7407e+09, 1.9501e+09, 2.2027e+09, 2.4276e+09,
        3.1086e+09, 2.1501e+09, 2.3486e+09, 2.2869e+09, 1.6768e+09, 2.5428e+09,
        2.0813e+09, 1.6964e+09, 2.7017e+09, 3.0176e+09, 3.3067e+09, 2.5436e+09,
        1.5575e+09, 3.0702e+09, 2.3708e+09, 2.7435e+09, 2.3353e+09, 2.2130e+09,
        1.8687e+09, 3.0806e+09, 2.4656e+09, 2.8189e+09, 3.0499e+09, 2.3466e+09,
        2.0553e+09, 2.4723e+09, 2.5311e+09, 2.4162e+09, 1.8101e+09, 1.7253e+09,
        2.1810e+09, 2.1137e+09, 2.0254e+09, 1.9768e+09, 3.6989e+09, 2.4980e+09,
        2.3436e+09, 2.6011e+09, 2.5327e+09, 2.1063e+09, 1.8167e+09, 1.4975e+09,
        2.0630e+09, 1.9960e+09, 2.7769e+09, 2.1401e+09, 2.1092e+09, 1.7699e+09,
        3.1352e+09, 2.7059e+09, 1.8052e+09, 2.5399e+09, 2.3017e+09, 2.1944e+09,
        1.5705e+09, 2.1552e+09, 1.8343e+09, 2.3637e+09, 1.6539e+09, 2.2479e+09,
        3.2457e+09, 2.6436e+09, 2.8920e+09, 2.3176e+09, 2.2943e+09, 1.3408e+09,
        2.1994e+09, 2.1352e+09, 3.3577e+09, 2.7330e+09, 3.4899e+09, 1.7728e+09,
        1.9136e+09, 2.7306e+09, 2.3360e+09, 3.1247e+09, 1.7304e+09, 1.7615e+09,
        1.9296e+09, 3.1779e+09, 2.0001e+09, 1.8890e+09, 2.0872e+09, 2.6644e+09,
        1.6713e+09, 2.2020e+09, 2.2704e+09, 2.6121e+09, 3.1868e+09, 1.8765e+09,
        1.9806e+09, 2.9937e+09, 2.5477e+09, 2.1776e+09, 1.8791e+09, 1.8317e+09,
        3.3402e+09, 1.9024e+09, 1.8594e+09, 2.5094e+09, 3.1738e+09, 1.4921e+09,
        1.9190e+09, 2.0419e+09, 2.3896e+09, 2.7879e+09, 2.1902e+09, 2.5749e+09,
        2.8166e+09, 2.4607e+09, 2.4569e+09, 1.7010e+09, 2.4969e+09, 2.6732e+09,
        2.4547e+09, 3.5476e+09, 2.1346e+09, 2.5332e+09, 1.8623e+09, 2.3789e+09,
        1.9805e+09, 2.7751e+09, 1.8982e+09, 2.4765e+09, 2.7308e+09, 2.0494e+09,
        1.6409e+09, 2.1436e+09, 3.2059e+09, 2.6498e+09, 2.5440e+09, 2.9225e+09,
        1.8094e+09, 2.6972e+09, 2.1312e+09, 2.5588e+09, 1.9880e+09, 1.8499e+09,
        2.5981e+09, 2.7422e+09, 1.8879e+09, 2.9798e+09, 2.3252e+09, 2.8967e+09,
        1.9756e+09, 2.6716e+09, 1.8931e+09, 1.5139e+09, 2.1569e+09, 2.1382e+09,
        1.5009e+09, 2.7146e+09, 1.8130e+09, 1.9321e+09, 2.4148e+09, 2.4725e+09,
        2.9003e+09, 2.3159e+09, 1.7261e+09, 3.0922e+09, 2.3492e+09, 2.0036e+09,
        2.3252e+09, 2.3363e+09, 2.5130e+09, 3.0233e+09, 2.5743e+09, 3.5814e+09,
        2.5359e+09, 2.4256e+09, 2.1587e+09, 2.2915e+09, 2.5237e+09, 3.1732e+09,
        2.0749e+09, 2.4318e+09, 1.4779e+09, 2.5180e+09, 2.1050e+09, 1.7398e+09,
        2.0232e+09, 2.5901e+09, 2.0044e+09, 1.7751e+09, 1.4260e+09, 2.1706e+09,
        3.1274e+09, 2.3493e+09, 1.8145e+09, 2.1404e+09, 1.6726e+09, 2.5541e+09,
        2.7170e+09, 2.9023e+09, 2.0149e+09, 2.2133e+09, 1.8535e+09, 1.8428e+09,
        2.2842e+09, 1.8843e+09, 2.5483e+09, 2.0202e+09, 2.3793e+09, 1.8804e+09,
        1.9733e+09, 2.5943e+09, 2.0083e+09, 3.1780e+09, 2.7754e+09, 2.0947e+09,
        1.6130e+09, 2.0492e+09, 2.3562e+09, 1.8431e+09, 2.8683e+09, 2.2752e+09,
        2.1737e+09, 2.7666e+09, 2.0618e+09, 1.8041e+09, 2.8109e+09, 2.0738e+09,
        2.3230e+09, 1.9907e+09, 2.3741e+09, 1.5284e+09, 2.5384e+09, 2.3789e+09,
        2.8460e+09, 1.8547e+09, 1.4683e+09, 1.9005e+09, 2.2069e+09, 2.0232e+09,
        2.7406e+09, 2.1494e+09, 2.7696e+09, 2.6505e+09, 2.5172e+09, 2.5859e+09,
        3.6737e+09, 2.1602e+09, 1.8026e+09, 2.9114e+09, 2.9185e+09, 2.8196e+09,
        1.3337e+09, 2.2491e+09, 2.1344e+09, 1.7957e+09])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([11112175.0000,  9218644.0000, 15739761.0000,  ...,
        37689940.0000,  4886068.5000, 15263272.0000])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([20354842., 26364048., 23819586., 17852650., 16559430., 16616542.,
        22224534., 30760552., 23679542., 20781858., 15862683., 20774128.,
        22052112., 19511850., 23289562., 25767590., 20061304., 17572314.,
        18927104., 17909642., 24590982., 17495256., 23931576., 24046982.,
        20517090., 14240789., 20278154., 16044435., 23295304., 21681046.,
        21491220., 23917752., 21144166., 22524646., 15633050., 25194216.,
        22055254., 23360570., 22284694., 19799668., 16305517., 23440272.,
        20149088., 28848008., 25415798., 24504152., 22189158., 20319988.,
        20722578., 18841892., 20888132., 19367730., 17882704., 17067096.,
        21292326., 27298382., 19766302., 16741193., 21433538., 18389046.,
        15563395., 24092022., 19103922., 20471312., 19878030., 21219908.,
        19127344., 23778972., 22643702., 17725724., 16074859., 23562040.,
        23039344., 19911978., 18521330., 30643470., 25605322., 16521081.,
        26051444., 18357634., 15562853., 20248014., 18703262., 21956164.,
        23667042., 21970038., 22373764., 21381340., 22424016., 23228290.,
        21561120., 20447858., 18340712., 29695750., 18397366., 21285432.,
        21548718., 19085870., 22429606., 24518294., 26924468., 16928386.,
        16591884., 19574602., 19343398., 22409786., 26320686., 18414076.,
        20315632., 17005554., 23858282., 16851116., 17802898., 17309950.,
        24668576., 16797612., 16263734., 13633601., 18823866., 23801084.,
        17658338., 19031064., 15357836., 16396264., 11659475., 17544154.,
        14614073., 19070350., 17842482., 24623830., 23091080., 27876194.,
        23201250., 24608868., 19352212., 18074652., 18098186., 25013100.,
        20888800., 16068266., 20990470., 21664284., 22104624., 27266936.,
        22596690., 21118160., 22728012., 30796302., 24472328., 26852918.,
        26568660., 19363806., 24598578., 21323398., 20754728., 28461744.,
        22146958., 17104924., 19684120., 21611504., 20612564., 30441744.,
        24989514., 26793238., 15927588., 21055132., 17982476., 21125046.,
        18532476., 22586234., 22952904., 31481446., 14934318., 17388108.,
        24354988., 20253118., 17725526., 27519206., 18048200., 17684112.,
        22535130., 34646436., 20188182., 17267062., 21272276., 22139364.,
        16352955., 16249378., 24714282., 20855442., 21174648., 16041609.,
        18181024., 16796690., 20355710., 20336952., 18131288., 16039022.,
        19768628., 19315084., 20443520., 22456218., 29953792., 20899624.,
        21386212., 22987608., 29390680., 23614714., 18717726., 14350367.,
        24894836., 16161968., 18971280., 24075772., 23587520., 21847700.,
        21126324., 16764856., 20630778., 20771970., 16875428., 24954224.,
        24275634., 18871690., 19244316., 22219692., 16215433., 16810848.,
        20119016., 17887238., 18473824., 21914962., 20525406., 18680142.,
        19028784., 20395570., 23596706., 18716548., 17597392., 29128160.,
        15932677., 25051848., 23705134., 27552910., 19318572., 23014360.,
        20018108., 24502628., 19770256., 25753170., 24982046., 24565180.,
        22161858., 17632874., 13266780., 19168398., 22998620., 25198188.,
        23115214., 20472804., 25531884., 17685792., 25830162., 18747548.,
        16822274., 17288542., 16828378., 20457910., 18462130., 16660748.,
        25179674., 20175048., 20468860., 23448628., 18196022., 16160011.,
        18891588., 17930860., 13314164., 19225116., 17323470., 19631602.,
        24978498., 26722804., 18783278., 20692482., 18550992., 29264148.,
        15334585., 19731950., 19285422., 14991041., 20770476., 21150680.,
        20744778., 20945736., 23438308., 28331136., 19729066., 14282649.,
        19595774., 18760884., 23670950., 17657366., 19611230., 24252694.,
        19169620., 18464106., 21887656., 21577192., 28982426., 22283378.,
        18622484., 12690840., 17826406., 19028544., 20707612., 21932438.,
        22248316., 18149320., 24050160., 22342268., 20003366., 19400828.,
        18795542., 22886202., 25351146., 19211052., 28341104., 16258774.,
        23542888., 21768240., 31972942., 18216180., 18146254., 25100770.,
        22532122., 19429994., 22727806., 28242706., 18246122., 24821832.,
        18308348., 20148378., 15766987., 21270998., 21306252., 16196494.,
        20942332., 20190674., 18091486., 26023868., 31570188., 18258590.,
        19245710., 20352640., 21126762., 23620214., 24681300., 18750840.,
        17756018., 24306544., 18079370., 22893208., 21385228., 16565857.,
        19981300., 28675634., 17985476., 22066810., 25678584., 21211636.,
        18642330., 15203610., 27345466., 19364766., 28112180., 24915718.,
        25009500., 24835534., 21810078., 20172366., 16437120., 21256026.,
        20562232., 25727914., 19904994., 23744608., 30208880., 22206098.,
        18905068., 17869358., 19934304., 20838368., 16020531., 16325219.,
        29220756., 20035438., 20105136., 23946044., 34712144., 15818794.,
        21467512., 21289284., 23835782., 17070676., 22920864., 19318142.,
        19992256., 14880493., 18583072., 18029814., 20211164., 19652550.,
        25532862., 22665240., 16536681., 15458116., 24700650., 16572630.,
        22537892., 19614332., 30371926., 20782126., 19827456., 18355456.,
        21893208., 19184336., 28150886., 18426922., 29630584., 20576950.,
        27916478., 24155200., 27926266., 19895168., 17315216., 19896438.,
        17173486., 22630524., 23614578., 18704780., 16972702., 18907486.,
        19582208., 13929074., 18791186., 21635152., 19136164., 22662906.,
        15924085., 26495238., 21229072., 24511694., 23051800., 25855420.,
        24533696., 18471730., 22051514., 21190516., 28385918., 17084292.,
        20370150., 19852588., 18363250., 21489752., 20380686., 29944682.,
        17848254., 19343672., 17840080., 17478362., 24643884., 31695544.,
        21362082., 26229914., 19902612., 23064130., 24093764., 18795100.,
        22315426., 24229936., 18957440., 19652428., 24115834., 18570722.,
        24216882., 16819162., 22638636., 20041964., 25660834., 17533254.,
        27392120., 22749990., 20445816., 20656752., 16309829., 21252828.,
        24868158., 21427222., 19689398., 20507528., 31018540., 26889134.,
        22371324., 21999344., 22506322., 26443860., 18045662., 22383118.,
        18923748., 21601978.])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([40266476., 50740788., 51383544., 44738756., 37336692., 63399828.,
        58997736., 58085424., 57717040., 32409406., 44216736., 50334928.,
        55250952., 63087792., 54652008., 44149872., 45526524., 50817388.,
        53426920., 52160548., 56632264., 49566092., 58188876., 36935984.,
        54369824., 41726852., 48690576., 44565296., 42833352., 42072668.,
        39434712., 72898536., 47446612., 65411396., 37534612., 41333724.,
        49356724., 51326260., 53436580., 56501364., 41291044., 56468928.,
        57989448., 47266392., 45924264., 58567232., 49661080., 51491392.,
        57528184., 45438904., 80075560., 69643240., 46285328., 49881924.,
        32013764., 44501460., 43215216., 52041260., 37752048., 48886732.,
        38995204., 61305332., 43128656., 46637444., 58917424., 60443300.,
        53783904., 52730032., 62911152., 45280704., 48667616., 50158536.,
        51904332., 51628388., 44778568., 42440316., 43300968., 40906220.,
        43963460., 58970896., 56555596., 54571368., 40916320., 50470344.,
        43522532., 53652312., 48609748., 49234588., 64792672., 34604156.,
        48326112., 48238976., 47759428., 44386884., 53576940., 46635176.,
        38845456., 51000140., 45221720., 47904696., 35198960., 40580064.,
        58660664., 67018376., 61168056., 55613756., 44861320., 51169192.,
        47065912., 62854852., 57270888., 45407696., 66448320., 39315892.,
        36580760., 43157408., 46255772., 43272656., 53610864., 49223448.,
        52677724., 42992204., 45717412., 62094344., 56223168., 48538196.,
        57691576., 54274184., 52205564., 65431576., 43260252., 53626048.,
        63255476., 38101816., 37121532., 52236152., 47963640., 51516860.,
        48333980., 36204432., 44966084., 33951732., 61507580., 52174164.,
        50646932., 47909404., 49059048., 41665012., 46840952., 52176736.,
        32155326., 37391616., 44541960., 61013088., 46445748., 61097828.,
        39364956., 45965060., 57312948., 52136564., 57956200., 52695972.,
        45010436., 50363208., 36565664., 46742904., 36144208., 50552724.,
        45140056., 43974440., 41357064., 54719708., 57963328., 45823388.,
        52993896., 46589696., 49933652., 38510656., 46237796., 62920572.,
        46753604., 41224788., 41900784., 51573372., 39564824., 42868364.,
        58115400., 37565688., 38254488., 39820592., 38763372., 33604080.,
        43298260., 43256040., 35228320., 54488784., 54109928., 54175256.,
        46236940., 33794368., 58884980., 46595688., 47431108., 54389644.,
        50794484., 56051368., 47947808., 54075000., 49093352., 56088712.,
        43272932., 55653408., 69235312., 56858452., 35146692., 55958868.,
        48227476., 39255908., 60296124., 65236368., 45039740., 54328860.,
        45366264., 75227616., 41293120., 52115776., 72543408., 51604804.,
        52128684., 52260072., 56996920., 34262164., 56220368., 46138752.,
        52656564., 68880672., 78555520., 45138112., 50561328., 52196588.,
        49600928., 53379124., 66420084., 44380908., 52671260., 44345592.,
        39345908., 45977340., 36240188., 45011884., 52165976., 64762684.,
        46142136., 50612184., 39749956., 29504626., 44615392., 53930152.,
        41587852., 64027588., 40916644., 41383000., 43826284., 40941068.,
        58409008., 46910676., 60413028., 43811524., 51930564., 48426532.,
        54770660., 47667840., 42270372., 48119556., 40145836., 40259572.,
        36817196., 54026676., 61162560., 49675068., 53923980., 48548736.,
        47892264., 52171496., 39433376., 60197260., 46254120., 52796524.,
        41723460., 49535340., 55584504., 51497756., 55512440., 68763496.,
        67292120., 52888572., 45960176., 56491104., 45287992., 62266612.,
        47584576., 34814396., 52137064., 50757660., 44878936., 38311256.,
        47418048., 37969552., 43243976., 43797596., 49300508., 50714872.,
        52587252., 39381076., 54887696., 53436304., 42859940., 45567592.,
        41194472., 46222344., 66017860., 52821716., 57702632., 49585060.,
        49015356., 43371260., 53260876., 47328408., 43043592., 59636608.,
        65985504., 43958980., 48702144., 40447840., 43107312., 47641480.,
        50867040., 33407862., 45458716., 71605344., 40613684., 53680664.,
        67632096., 40278836., 50808424., 56495896., 60242828., 62489692.,
        48797956., 35027692., 51228828., 40674416., 49697760., 55164864.,
        43328140., 47329556., 56847292., 50661852., 48284328., 49069596.,
        48574776., 65953872., 46044288., 39200260., 54134968., 35413280.,
        50119044., 54811876., 41770000., 58045340., 37962876., 55439792.,
        46397384., 57340368., 59194388., 64436300., 53453348., 62717440.,
        41089480., 53526248., 66883340., 72719144., 48307128., 48401312.,
        35913780., 36645940., 54645812., 37668744., 52635720., 52053304.,
        57009704., 44993428., 37379588., 47616056., 60194936., 55890832.,
        51102996., 56372740., 43009864., 46851480., 64653552., 48140620.,
        55376708., 49437448., 52003104., 45670884., 43861728., 45019664.,
        46814104., 49053028., 63838388., 47734092., 37814368., 61297524.,
        58098652., 51832852., 51440264., 69737600., 53333864., 41514628.,
        42367376., 43495704., 42340036., 43068852., 46519764., 50332952.,
        57657856., 49050124., 37671880., 42643972., 60339856., 34610692.,
        46738164., 70816336., 51427720., 68861576., 43423952., 39060280.,
        58436608., 50713156., 38510220., 42262120., 52657900., 62063416.,
        50494216., 45454604., 44391148., 58564516., 60006684., 40758348.,
        48198076., 48802880., 47289864., 43834872., 32071464., 57367344.,
        53739048., 37898552., 56754192., 62598784., 56281076., 51768152.,
        52718548., 52796396., 79027944., 46651508., 37028972., 47658276.,
        61058624., 43478604., 58484992., 54447244., 51905516., 43544984.,
        45885948., 56265416., 41896848., 57051864., 48848504., 43906516.,
        55253932., 56085172., 44150444., 49120904., 56309052., 51109264.,
        61338016., 54495836., 62543524., 44866884., 48838240., 45489412.,
        49709408., 60095964., 38742040., 45819636., 42052908., 35793524.,
        46603892., 62796260., 50344484., 51637684., 46833404., 52363504.,
        43120296., 54274572., 58914180., 56077240., 47074820., 46734428.,
        48639124., 53505436.])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([443871.0938, 159351.1250, 729325.6250,  ...,  57460.8633,
         37484.1211,  49613.7539])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([  56491.3633,   21254.2500, 1045521.1875,  ...,  113777.7031,
         164001.0625,  228889.0000])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([391206.8750, 398385.7812, 320943.5625, 460844.1875, 325956.4375,
        280501.8750, 260209.8125, 251447.5312, 414583.1250, 357271.6562,
        423465.5938, 273597.7188, 341069.0938, 441195.7812, 480396.9375,
        272186.6875, 363991.8750, 367835.6562, 541399.3750, 434847.5625,
        480117.2188, 378925.1250, 435251.7500, 433980.5938, 429037.9375,
        270148.3125, 389949.2812, 659008.9375, 297025.6250, 362307.7500,
        237685.5625, 255554.0625, 302098.0938, 400785.8125, 599431.8125,
        363728.1562, 370824.5312, 395081.3438, 643852.0625, 309117.9062,
        346198.3125, 421168.0000, 487160.5000, 415385.4062, 318401.5000,
        314604.1875, 650324.1250, 330843.2188, 327223.1875, 433238.6250,
        403804.2188, 354085.9375, 472016.0000, 425797.0312, 270730.0625,
        344973.4062, 355769.0312, 363014.9062, 297497.8438, 444907.9062,
        362237.9375, 256740.3750, 181803.5625, 245193.4375, 384639.5312,
        536303.1250, 372766.4062, 394321.9688, 446363.1250, 500133.0625,
        512997.1875, 311662.0938, 383641.1562, 384592.8750, 510651.8750,
        588308.7500, 514531.5312, 405386.9062, 590927.4375, 350138.0312,
        321782.6562, 605426.5625, 461155.4688, 329964.2500, 381984.7500,
        255040.4375, 458248.0312, 508092.2188, 419126.4688, 540502.8125,
        403034.2812, 363487.7188, 469111.8125, 462700.8125, 548887.5000,
        297953.1875, 498907.5625, 276659.1250, 460662.4688, 579835.9375,
        469893.0000, 360042.0938, 324404.7812, 289127.9062, 354272.8438,
        366033.7500, 526856.8750, 311293.1875, 408002.3438, 355031.1875,
        471952.6562, 518505.9375, 371755.2188, 363761.6875, 512207.1562,
        324453.3438, 516761.2500, 564495.6875, 513395.8438, 350397.4375,
        630118.6250, 421429.0312, 432898.4688, 504556.9062, 540161.0000,
        354053.0000, 269440.6250, 452010.6250, 355888.2188, 303679.9062,
        477042.4062, 276935.5938, 334508.4375, 568171.8125, 291304.2188,
        413380.2188, 369578.5938, 269303.4375, 516807.2500, 307372.4688,
        363516.7188, 572359.0000, 417051.6875, 380037.7812, 302812.7812,
        483326.8438, 439915.4062, 331834.1250, 249128.1406, 408372.4062,
        419833.7812, 285404.1250, 250690.4062, 480516.8750, 293247.7500,
        462871.3750, 428047.5938, 601326.5000, 249306.7031, 378918.5312,
        528641.1250, 284732.2500, 250000.4688, 276801.8438, 305852.3438,
        343000.5625, 360338.2500, 429420.1250, 288351.3750, 448822.8125,
        301776.2500, 433126.9688, 416760.1875, 460788.3438, 336640.9688,
        451168.9688, 541877.8125, 288675.7812, 294606.0625, 291670.0312,
        239766.6562, 433280.0312, 319398.6250, 261118.5000, 447186.1250,
        502194.9688, 342057.4062, 288206.5000, 451326.0312, 419331.9688,
        453352.8125, 296911.9062, 414534.3438, 384817.1250, 336202.3438,
        501449.4062, 326612.8750, 440978.5000, 479590.8750, 263149.3438,
        565014.0625, 492511.4375, 423974.6875, 271025.4688, 329662.7188,
        286923.8750, 344933.2500, 427222.8750, 434781.4375, 287637.8125,
        432372.3438, 417729.8750, 445768.8750, 460610.5312, 507212.8125,
        358841.1875, 395827.9688, 468481.6562, 257013.4219, 366354.8750,
        585039.8750, 537111.5625, 264493.7188, 316414.6562, 528262.1250,
        328859.4688, 378783.9062, 188778.7656, 408215.0938, 277309.7500,
        310837.2812, 304987.0625, 264360.0625, 400764.9062, 245753.3750,
        358468.0312, 363959.8438, 438954.6875, 305680.4688, 457692.1875,
        338866.6875, 444268.3750, 545125.4375, 337473.6562, 346496.2812,
        332813.5312, 369990.9375, 432958.2500, 284785.0938, 698625.0625,
        410250.9375, 365908.3125, 341226.6562, 425551.5938, 327930.0000,
        311184.8125, 504076.5312, 429245.0312, 244685.0156, 513119.9375,
        260654.3906, 447361.7500, 367234.2812, 359491.4375, 270051.5938,
        368801.4375, 313104.2188, 357435.4688, 601545.1250, 339015.4688,
        323138.9688, 451111.0000, 476592.0312, 262114.8906, 264579.2500,
        305512.0000, 266054.1250, 401097.8438, 371953.6250, 489054.6875,
        533017.8750, 248071.8906, 204234.3438, 310289.2500, 397946.2812,
        305845.7188, 491041.4062, 326611.5938, 459310.9062, 542642.5000,
        474799.6875, 428560.6250, 450669.7500, 422752.4375, 377192.8125,
        432098.1562, 352196.5625, 349294.0625, 343943.0000, 691816.5625,
        397248.8125, 492609.3750, 301863.1875, 326109.3125, 274192.8125,
        294186.8438, 601463.9375, 241847.9688, 474317.8750, 529054.6250,
        512445.7188, 352114.5312, 381367.4062, 462288.6562, 407983.3750,
        713279.5625, 479581.2500, 560238.5625, 330520.8438, 349301.6250,
        335782.8750, 626910.4375, 538299.1250, 439589.6250, 246510.4375,
        343741.3750, 387747.5312, 531224.5625, 282939.6250, 284831.8125,
        368327.1875, 390773.1562, 278391.6875, 357094.5000, 346557.2500,
        279919.2188, 388938.3750, 324798.0938, 502800.7812, 270830.1250,
        371517.3438, 512607.6250, 629530.8125, 444180.1562, 307766.1875,
        579658.3125, 361467.9375, 342115.3438, 349202.9688, 387786.3125,
        439786.3750, 429464.4688, 291937.4688, 317051.4688, 250592.7656,
        417612.2812, 356742.2812, 612047.4375, 434909.8438, 350656.2812,
        341916.7500, 385214.4375, 248476.7656, 349118.3438, 395569.0312,
        311063.0938, 285274.0312, 418648.1875, 326686.0312, 298159.6562,
        426198.5625, 440916.9062, 240082.3438, 549802.4375, 479713.9375,
        517268.0938, 343926.2812, 398786.1562, 302768.5000, 414618.8438,
        348989.8125, 334813.7188, 418512.5000, 520980.9688, 310850.0000,
        283300.8438, 327169.5625, 571906.9375, 449240.4062, 501776.6562,
        475437.8750, 345385.7188, 308135.9062, 419540.2812, 232676.1406,
        414514.9062, 275909.7188, 265938.4062, 483969.0312, 561530.4375,
        398321.0625, 295568.8125, 325156.0938, 373213.0000, 469706.9375,
        380101.7812, 469693.2500, 408283.1250, 426420.7812, 290946.8750,
        341424.5000, 299754.5938, 438194.4688, 365745.0625, 269421.8438,
        447651.8750, 618284.3125, 402203.3125, 362699.0938, 398339.4375,
        397841.2500, 446784.5312, 453665.0000, 332683.5312, 423564.0938,
        508701.4375, 522581.1562, 427709.0312, 374048.8750, 249502.9844,
        450910.8438, 346380.8125, 425553.4375, 446764.0938, 422978.1250,
        400563.0625, 328322.2188, 401057.7812, 329591.6250, 292825.0312,
        391543.1562, 345826.7188, 392931.7188, 301494.9062, 506491.0312,
        460352.0000, 367218.6562, 352065.2500, 271338.2500, 300293.5000,
        354454.7500, 420001.7812, 444183.5000, 351098.0625, 416314.2812,
        495304.0312, 433819.2500, 368886.9688, 417439.9375, 339970.4375,
        577569.1875, 369256.4062, 353221.6875, 215290.6562, 296110.4062,
        347380.3125, 421004.3750, 334135.0938, 435483.8750, 357442.5625,
        382095.7812, 350892.7500, 326701.0938, 257044.7656, 368744.1875,
        423744.1875, 366023.7188, 312218.3125, 288214.8750, 331431.7188,
        302056.7812, 321766.0312, 391284.6875, 283455.6250, 362603.6562,
        544758.4375, 262430.9688, 442294.1250, 342370.1250, 346926.8438,
        373470.7812, 387092.7188, 628254.0000, 456349.2188, 267308.5000,
        243499.7500, 344148.3125, 379940.8438, 526613.1250, 382238.5938,
        506669.4062, 332926.9062, 346599.2500, 444507.6875, 389116.8125,
        362951.8125, 362078.5938, 311472.5312, 310518.3750, 498096.6875,
        528089.1875, 330557.7812])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 955704.6250, 1023140.3750,  751879.4375, 1163493.8750,  643793.2500,
        1164188.8750,  572427.1875,  458341.8750, 1478980.8750, 1148371.6250,
        1366544.3750,  616301.4375, 1269882.8750,  917204.5625,  725450.3750,
         599352.3750,  723708.0000, 1121058.2500,  545660.4375, 1105369.1250,
         837954.9375, 1246213.1250,  645037.8750,  732946.6875, 1360137.6250,
        1309646.8750,  759221.0000,  800332.9375,  817420.3750, 1044803.8750,
         770049.9375,  740056.1875,  714019.0000,  862422.0625,  884810.5000,
         839091.9375,  547772.1250,  995002.9375,  832834.8750,  878018.3750,
         952884.8125,  646276.3750, 1032103.8750,  566233.2500,  884203.7500,
         708891.6250,  927294.0625,  979272.5000,  945197.1875,  814645.8750,
         917792.3750,  799794.3125,  777836.0625,  589787.3750,  598344.7500,
         899619.3125,  931411.3125,  938035.4375, 1153123.6250, 1143433.2500,
         573124.8750,  952890.1875,  742818.0625, 1142801.8750, 1175892.7500,
         732055.8125,  675017.5000,  832619.1250,  915342.7500, 1123055.0000,
         593841.2500,  882481.6250,  731261.6250,  606552.6875, 1186684.8750,
         747190.7500,  730917.3750, 1118781.3750, 1044380.3125,  889003.6250,
         616110.7500, 1180173.5000,  952751.9375,  705105.5625,  971809.8750,
         689723.6250,  921398.8125,  755554.9375, 1133978.2500,  922465.3125,
        1222521.2500, 1617230.2500,  943419.7500, 1132927.7500, 1059852.2500,
         764407.2500, 1187414.7500, 1079121.2500,  517083.9688,  594599.3125,
         641421.8125,  741988.0000,  737776.3125, 1024670.0000, 1020491.4375,
         786276.5000,  755125.6875, 1192447.6250,  642742.8750,  621570.6250,
         954095.8750,  618981.5000, 1124773.3750,  615890.8750,  741612.6250,
         894869.0000, 1105843.7500,  705807.1875,  846893.1875, 1009344.5625,
         718214.8750, 1000501.1875, 1242243.1250,  730856.8125,  719515.3750,
         656786.2500, 1094285.3750, 1017308.2500, 1036116.1250, 1107619.0000,
         888313.0625,  512611.1562, 1280680.1250,  831338.5000, 1128224.7500,
         759099.7500,  705858.8125,  982208.0000,  659712.2500, 1476290.0000,
        1305827.6250,  893363.8750,  938189.7500, 1015868.3750,  855299.9375,
        1088101.5000, 1013509.5625,  986140.8125,  911794.0625,  656273.2500,
         597026.2500,  761651.6250,  856309.0625,  954698.5625,  691260.5625,
        1056215.7500,  953687.0000,  564998.2500,  959026.4375,  562555.0625,
         779039.8750,  847165.6875,  777280.1250,  895532.7500,  810028.0000,
        1044592.1250,  853333.1875, 1055568.1250,  847278.9375,  627003.6875,
         822572.1875, 1769215.5000,  814336.8750,  925125.3750,  656954.8125,
         562295.8750,  812986.8750,  898863.9375,  892809.3750, 1071186.7500,
         871317.6875,  851507.7500,  742078.4375,  798952.7500,  998077.5000,
         906665.5625,  682484.9375,  945902.0625,  913232.2500,  677192.3125,
        1809968.2500,  937999.9375, 1085821.5000,  825898.1250,  631976.4375,
         691310.1875, 1119778.6250,  845977.0000, 1304475.2500,  516247.7812,
         863487.6250,  947337.6875, 1107222.2500,  727948.5000,  731986.1875,
         891367.3125,  761915.1250,  942434.4375,  898846.2500,  798583.1875,
         785023.1875,  726408.2500,  690406.3125,  806799.6875,  892066.0000,
         786967.3125,  587315.5000,  880725.5000, 1052673.8750,  863092.6875,
         951228.5000, 1472150.2500,  871814.6250,  987370.6250,  512943.3438,
         767976.7500,  744034.8750,  798421.8125, 1021972.3125,  913729.9375,
         844687.5000,  851058.1250, 1209563.7500,  994513.3125,  749803.6250,
         955022.3125,  948689.7500, 1126122.0000, 1019922.5625,  739968.8125,
         884197.9375, 1165551.5000,  700122.1875,  995820.8750,  658862.4375,
         860010.8750, 1058179.0000,  703710.5625,  819370.9375,  879626.4375,
        1191419.1250,  715389.4375,  593331.9375, 1165476.2500,  913045.7500,
         860287.3750,  554171.5625,  851688.0625,  740654.9375,  873297.8750,
         597082.1250,  975849.1250, 1218747.3750,  901973.2500, 1406884.1250,
        1044382.1250, 1363706.1250, 1230132.3750, 1051956.1250, 1113497.3750,
         793596.4375,  867006.6250,  996913.8750, 1095861.3750,  698787.6250,
         679043.8125,  655143.6250, 1262503.6250,  871538.1875,  690834.9375,
         702062.5625,  637865.1875,  952833.8125, 1293580.0000,  798126.7500,
         855681.9375,  800094.4375,  629703.5000,  897447.0625,  955486.8750,
         791759.0625,  994624.1875, 1048475.3125,  774013.1875,  836301.8750,
        1029699.1875, 1375870.8750, 1068691.2500,  863128.4375, 1088753.1250,
         989475.6250,  982896.4375, 1165839.8750,  537443.2500, 1007566.3750,
         618041.5000,  460612.4688,  771067.2500,  815566.8750,  611231.5625,
        1332656.3750,  817309.0625,  828332.5625,  918532.8750,  625399.3125,
         933445.8750,  834859.8125, 1114117.5000,  958574.1250,  794438.3125,
        1000821.8750, 1300807.1250,  827805.5625,  724423.6875,  666526.2500,
         721929.3125, 1330513.3750,  871521.3125,  807276.4375,  926803.0625,
         741347.3125,  641374.5000,  774478.6875,  697040.3125,  811924.7500,
        1241187.5000,  706462.1875, 1048353.6875,  899781.7500,  945385.6875,
         834090.0625,  993025.7500,  748853.5625,  811968.8125,  697625.1250,
         624079.0000, 1380704.5000,  818612.1250, 1024978.0000,  570022.0000,
         817338.5625,  945600.6875,  872154.8750, 1028660.9375,  690960.7500,
         827398.0625,  750962.8750, 1032380.9375, 1148777.5000,  843008.1250,
         999973.3125,  965474.0000, 1074393.3750,  655297.6875, 1236629.0000,
        1036058.0625, 1326868.0000, 1433471.5000,  676611.2500,  732808.1875,
        1180452.1250,  918793.6250,  857497.9375, 1062022.5000,  831219.5000,
        1052125.5000, 1155274.8750,  814905.4375,  797477.7500, 1166010.3750,
         850287.0625,  821433.8750,  840365.1875,  768198.7500,  900032.2500,
         940621.5000,  998530.3125, 1039498.1250,  688985.8750,  589115.1875,
         764671.6250,  959779.6875,  800123.7500, 1131552.3750, 1029991.5000,
         933856.5000,  801674.3125,  776898.8750,  540885.5000, 1221690.3750,
         803361.6250,  898987.1875,  826723.0000,  856713.1875,  812287.2500,
         815767.2500,  767336.0625,  725590.0000,  923080.3750,  645726.1875,
         856162.1875,  622063.1250,  732755.0625,  781831.0000,  975483.3125,
         847341.5000,  725455.3750,  843980.0625,  584530.8125,  869798.6875,
         939910.9375,  659577.9375,  729040.2500,  752968.8750, 1045779.0625,
         989640.8750,  836327.4375, 1032974.2500,  796118.0000,  600614.6875,
         874612.8750,  886444.0625,  489132.3438,  972200.7500,  954910.8750,
        1172514.8750, 1101946.2500,  659108.3125,  642455.5000,  911338.4375,
         799878.5625,  686339.6250,  650925.6250,  795166.9375,  629694.8125,
         770511.4375,  835583.1250,  554933.5625, 1049149.5000,  970292.7500,
        1022046.2500,  710617.4375, 1080602.3750,  881860.7500,  896190.7500,
         850682.0625,  746208.7500,  500084.0312,  815377.1250,  825859.8750,
        1218072.6250,  627514.5625, 1194077.2500,  521990.1875, 1441413.1250,
         712997.4375,  714700.0625,  622554.3125,  807793.4375,  843572.0000,
         976455.3750, 1011506.4375, 1134483.7500, 1274966.2500,  994683.3125,
         957066.6875, 1137259.8750,  907881.5625,  519673.6875,  814546.8750,
         808423.0625,  702658.6875, 1100259.8750,  837951.3750,  747407.8125,
         768969.3750,  893443.5625, 1044793.0000,  803021.4375, 1015322.9375,
         935475.5000, 1256925.6250,  852509.7500,  872820.4375,  608648.2500,
         600107.9375,  819318.3125,  739515.8125, 1103512.8750,  891736.9375,
         734760.4375, 1153789.7500,  914749.7500,  673005.1875, 1170769.8750,
         675698.8125,  993674.7500,  780759.9375, 1099280.6250, 1051034.2500,
         786763.5625,  745562.6250])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([76685.6719,  1192.3740,  4507.1069,  ...,  3401.4666,  4711.3652,
         1089.5573])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([13177.6064, 11576.5977, 11697.4941, 18168.5742,  9933.1074, 11190.1172,
         9081.0781, 15300.2285, 13980.7412, 11830.0869, 11573.9180, 11303.4482,
        15723.6475, 12297.8516, 15226.2344, 10077.0205, 11554.7119, 10627.8008,
        17207.3770, 14164.4248, 13624.0830, 16365.7148, 13403.3691, 10968.4785,
        19417.7520, 13722.6309, 15937.4600, 15559.1104, 16512.9316, 10830.7539,
        24836.7891, 13025.0723, 11056.0176, 11616.4912, 17580.8105, 11580.5645,
        11579.4414, 14183.1074, 14751.0205, 11836.0928, 15968.5488, 15170.5195,
        10200.7520, 13494.3125, 18336.9023, 14030.1279, 13690.5410, 10802.7139,
        12481.6309, 12669.8213, 14443.4180, 10968.5352, 13176.4824, 13404.2803,
        11598.4893, 11281.6143, 12188.8467,  9720.4600, 15161.6807, 16957.0449,
        14069.0439, 12639.8975, 14851.4570, 11998.2432, 13334.4014, 12140.7891,
        14544.7910, 13549.8066, 10079.9707, 12509.3994,  8955.9609, 16976.5703,
        12689.5312, 12453.1738, 15758.2852, 19490.9375, 12360.2861, 14141.7480,
        11074.3232, 17917.6816, 11690.9434, 15225.2793, 17343.7520, 12510.7061,
        12864.7285, 14277.9355, 10144.8906, 17491.4160, 10226.2139, 10734.7822,
        13868.4131, 10870.8262, 14081.8096, 12280.8730,  8887.4902, 16926.8379,
        13920.7207, 14470.8467, 12935.8604, 20370.2383, 11633.2217, 11954.4150,
        19112.4492, 14074.0068, 12195.8975, 13410.1895, 12962.2588, 16261.8262,
        11930.4326, 15496.0137, 13527.4453, 10581.8779, 13294.3906, 11972.1445,
        14305.7852, 12353.8525, 17955.7031, 16259.4082,  9790.9795, 11400.8730,
        13005.6094, 11862.4668, 10660.3955, 14462.7002, 13821.4023, 14832.0840,
        14591.6641,  8903.6885, 12985.6465, 12341.6553, 17981.9863, 14206.4639,
        15558.0264, 12023.8779, 18503.0586, 12925.1963, 15457.0684, 10785.2256,
        15518.7422, 11241.8555, 11450.3535, 10528.9668, 12575.0547, 17521.1328,
        11802.1992, 13759.0029, 10560.0098, 12149.8818, 12398.7109, 12856.8721,
        11904.1416, 12549.9697, 16573.8555, 16559.2344, 12506.5342,  9139.8965,
        12169.8652, 10770.7812,  8766.8750, 13595.1328, 15424.0371, 15785.4912,
        15903.8408, 17165.2891,  9226.4912, 13781.1826, 13651.0967, 12293.1191,
        13254.9619, 12333.6289, 17792.2031, 13769.8496, 13106.6650, 13647.9336,
        15514.9365, 14029.7188, 12930.2158, 12050.6758, 10371.2158, 13655.0605,
        15385.5186, 12098.6758, 16239.3369, 13145.7842, 13927.3350, 12382.0566,
        10353.7109, 13858.1182, 11058.4326, 11188.9844, 18182.3340, 14756.6787,
         9872.9092, 11324.7236, 19462.7188, 13172.9014, 19084.1367, 17150.6348,
        15416.0840, 14920.6328, 11538.0264, 11409.1387, 12534.3076, 14761.3936,
        13660.6230, 13790.4873, 12619.5947, 15427.6494, 13673.2676, 15857.3027,
        13401.6924, 11682.3574, 11739.7158, 12448.8955, 12306.4443, 16494.2930,
        10075.1660, 13715.2744, 12449.0059, 17594.1973, 14207.3340, 17372.0645,
         9188.9219, 18077.8008, 13278.7490, 15202.1484, 15296.6777, 18390.8320,
        12063.0859, 13203.3740, 13089.2197, 13116.5342, 10118.2314, 13217.4043,
        14788.5996,  9916.2480, 12366.1191, 14464.4395, 15614.0977, 15346.4004,
         9149.0791, 13441.5156, 17909.1172, 11970.3281, 14553.6699, 10169.8555,
        15552.1562, 16195.4521, 13359.0977,  9059.1396, 14279.8223, 12843.3535,
        16480.1113, 21790.0488, 13128.0537, 14012.2988, 12255.1055,  9723.4932,
        14695.7461, 11826.6211, 14218.6514, 14339.6104, 14333.5078,  9882.7061,
        10764.1484, 15847.8301, 10755.7588, 11354.2578, 16692.8887, 12836.0420,
        18653.1387, 15345.1963, 16695.2070, 13985.4766, 11697.3467, 16229.8936,
        10900.2822, 16022.6914, 15275.4072, 12735.8711, 12264.2998, 11550.7119,
        14273.6631, 14956.1133, 11035.0908, 12507.4814, 14870.3135, 16276.4092,
        10010.8301, 10526.1680, 18396.1562, 12268.4941, 16707.7949, 11095.9180,
        10587.8203, 10894.2188, 12063.7227, 11466.6748, 14309.3906, 15825.9141,
        17312.8223, 13659.4150, 11674.5186, 14261.0527, 10639.7773, 18996.4336,
        15089.9268, 12923.0859, 10912.8213, 12847.5889, 13674.0459, 12966.2881,
        15695.9521, 10584.6387, 16122.6738, 10065.6348, 14485.6934, 13713.5508,
        20878.1426, 14727.0312, 16387.8105, 11200.4170, 14449.8799, 15125.0928,
        13846.1465, 11212.2568, 13985.7051, 13215.8193, 10860.1660, 13681.9473,
        12835.5088, 15838.5400, 13237.0645, 13509.1377, 17403.3418, 13767.1699,
        16373.2773, 16727.4922, 12293.3955, 18356.4355, 17979.4160, 14388.9521,
        11131.8584, 14123.1680, 11213.9785, 18203.5371, 11649.3438, 11859.0078,
        14427.9912, 10678.6660, 10033.7012, 12654.1689, 10816.6797, 12826.5713,
        12870.3633, 14739.2891, 11853.0029, 15101.7832, 14548.0645, 13057.7256,
        12525.2480,  9566.6592, 13555.3965, 11890.8418, 19485.5918, 14805.4268,
        11485.3164, 12673.9326, 11704.7500, 15332.0332, 11821.7451, 14618.7676,
        14224.2568, 11534.3311, 12424.8213, 17499.4766, 13496.9072, 13292.0723,
        11426.8252, 15312.0264, 17242.0879, 13501.1172, 14332.0928, 11203.5957,
        17451.6094, 14752.7480, 14605.4951,  9855.1816, 16133.9883,  9346.6348,
        12407.3594, 10690.4775, 11245.1494, 12229.7559, 14228.9619, 16314.4863,
        12109.5898, 13633.5557, 13556.8906, 17095.8184, 11490.7266, 12494.6543,
        15519.1602, 14643.4766, 10870.6240, 12095.6260, 21008.7500, 17307.4336,
         9023.3818, 15032.9141, 12873.2051, 14066.4443, 20536.4707, 15226.8447,
        13437.0820, 11677.5107, 15142.3135, 14308.7979, 14974.7295, 13489.6943,
        10402.3398, 16033.0654, 15287.8213, 14120.4590, 10271.2520, 12448.8066,
        14888.6924,  8789.3438, 16232.7793, 13733.2354, 13822.1465,  9383.6299,
        14945.0635, 12072.2295, 13587.9512, 13911.6133, 10407.5381, 11384.0801,
        11532.0244, 17393.1777, 10288.5645, 12528.8027, 17966.7324, 13411.0732,
        16110.8027, 14960.4385, 16335.9014, 10371.3477, 12424.5352, 14785.3867,
        14708.7588, 14698.1191, 15174.0156, 15638.0762, 11621.1992,  9729.7119,
        10522.0664, 15137.7637, 14741.3672, 12721.4033, 16840.9062, 12786.9492,
         8808.0332, 12678.1582,  9798.9092, 16625.0078, 13250.2158, 16403.8789,
        10084.4336, 12527.6182,  7900.8262, 12743.6006, 10321.5430,  9055.9844,
        13469.5391, 10389.1484,  9184.0000, 13447.2891, 11421.6660, 12602.6758,
        11913.6582, 16821.1152, 12123.4141, 14525.9043, 15372.3613, 14185.9688,
        14383.2803, 10796.0107, 12019.6230, 15144.4111, 10272.6602, 15119.7920,
        12378.3096, 12028.7529, 15806.3457, 10816.1455, 15597.6904, 10045.7930,
        10989.8340, 12413.2129, 11429.0479,  8947.2910, 13781.1045, 15462.8721,
        11932.6152, 12293.8115, 13457.4697, 10926.0352, 14928.2217, 14773.5430,
        11987.9795, 11521.4141])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([47967.8125, 39115.0000, 28035.2695, 40225.6211, 37873.6016, 36714.6602,
        41835.4180, 29940.3770, 37364.8477, 39284.1953, 52538.2734, 34612.5312,
        38905.5859, 29896.0410, 30200.7988, 27828.3516, 28532.1875, 27311.6270,
        43742.6445, 35061.2773, 39629.7578, 39093.1641, 27874.5293, 36352.7031,
        30273.0488, 56852.5859, 45442.5391, 33312.4219, 30940.4648, 34186.8047,
        29392.1113, 24954.7793, 48161.5117, 34867.2852, 40430.4961, 40548.8516,
        34427.6406, 44972.3516, 29371.6973, 41014.4180, 51750.5703, 28092.8574,
        35280.3867, 41503.9102, 36605.7070, 48663.8242, 36011.4922, 44022.4023,
        33054.4023, 35838.2461, 33087.2812, 40993.8008, 30231.0840, 33621.4453,
        44584.8828, 28020.7207, 45739.3672, 23116.9824, 34986.7227, 48820.5469,
        34190.3008, 36528.5391, 45524.4570, 36151.2930, 38630.9023, 42236.3711,
        48527.7305, 31760.0352, 25038.6621, 29032.9219, 37205.3125, 44074.2578,
        28021.5645, 39055.4570, 44362.8672, 34120.2812, 46931.5664, 32673.6465,
        37099.2578, 31786.8613, 35953.6523, 31836.0820, 30368.4805, 41443.9023,
        34297.1172, 47307.4961, 47647.1562, 50438.0156, 33559.7578, 26641.0547,
        46387.4102, 43774.8633, 35898.0898, 28614.0234, 41208.0156, 38434.7773,
        31100.9199, 36445.9961, 27875.3672, 27033.3086, 27955.4668, 32795.3828,
        35262.7266, 57659.4570, 32303.4766, 44207.1914, 29922.0918, 39320.3906,
        33178.6211, 35141.7266, 38545.6406, 41803.1953, 46171.9453, 39086.2422,
        27424.3320, 36494.1055, 32643.8594, 46715.2969, 30461.1328, 25423.2461,
        43541.7852, 31489.7852, 33872.0625, 34963.5703, 39045.1719, 45367.2617,
        31905.5625, 43960.4219, 38798.4961, 38471.0039, 33602.2695, 39580.4414,
        31099.1016, 40113.8203, 38790.5352, 35744.2930, 35136.5586, 33886.5117,
        35184.7383, 42601.7070, 37658.3086, 37183.5938, 24824.1309, 41407.8750,
        31748.3320, 28932.4883, 38055.0312, 47254.7852, 28614.5352, 34929.3750,
        35661.3242, 30891.8477, 29854.4062, 28941.8809, 37941.2930, 34569.2695,
        33968.8672, 49284.4961, 40965.3672, 48483.5156, 30340.0762, 38211.2500,
        32027.4883, 26297.2266, 33163.2266, 37427.9570, 44133.8945, 40548.7070,
        42551.8594, 29687.0703, 48694.1836, 28985.5605, 50727.3906, 36772.4648,
        31477.7969, 29655.4727, 31068.6777, 41386.9375, 33615.5742, 38266.3281,
        42384.8438, 31207.0156, 37594.7852, 50780.5273, 46722.8242, 47332.6484,
        37513.5820, 27755.1523, 38968.2461, 51015.1562, 30604.4844, 42715.5625,
        49745.3203, 56947.0938, 43271.3789, 23281.1367, 38682.7305, 27886.8750,
        37578.3086, 44802.5430, 38328.6367, 35098.6523, 29667.5137, 46391.9570,
        45120.9102, 30026.7559, 32700.0195, 36032.5273, 33158.2852, 29852.1191,
        44338.9062, 34639.6250, 28449.4629, 27937.3301, 36377.9336, 38640.8398,
        37070.2578, 32739.2324, 39295.3320, 35706.6602, 44967.4570, 39190.3711,
        41403.6953, 45739.9727, 27918.0820, 39775.7812, 28402.4531, 32423.0234,
        42136.2734, 40232.2070, 30437.1191, 39258.2422, 32166.2129, 45879.8867,
        39592.8398, 33153.3359, 42487.5820, 26364.9590, 41551.5781, 38970.0898,
        51162.7695, 49613.8867, 31578.9688, 37683.1719, 39653.2852, 43558.3008,
        29590.1348, 27877.8848, 43557.6562, 32352.1113, 34332.0039, 54426.9609,
        38502.9102, 31078.3691, 28257.2598, 37541.5391, 43248.2383, 32811.9805,
        45293.6680, 36840.9492, 35782.4883, 41405.2148, 26543.0352, 27349.3457,
        53528.8672, 26757.7402, 28887.3086, 27771.5020, 34505.3516, 33638.2461,
        33672.3047, 32790.9336, 37753.7773, 35727.3281, 35324.1172, 45616.7305,
        35930.3242, 37493.7422, 42066.2734, 31577.6738, 45220.5391, 48289.2773,
        35518.6367, 38622.3320, 36687.1758, 35136.0508, 41770.7422, 39045.8242,
        32187.2852, 30434.3809, 37928.0117, 41035.5156, 33580.6562, 31760.8887,
        32838.8359, 26126.1445, 31427.9258, 36904.1211, 29599.5078, 36960.2773,
        26528.5684, 40990.2500, 32114.9941, 28744.6641, 38797.4141, 32876.7461,
        36602.0391, 30015.0508, 27437.6113, 36469.1328, 39503.4297, 38237.9531,
        43492.2383, 34263.8203, 35244.0859, 35190.7148, 47679.6172, 45415.9141,
        33250.6641, 36089.1836, 49864.7891, 31408.2090, 36996.3711, 40240.2695,
        31509.0723, 43224.5195, 42488.9688, 38478.9727, 49755.8281, 52042.0625,
        33105.8984, 28314.1348, 27543.6484, 39587.2617, 47110.0234, 43378.7148,
        32097.0039, 24279.7422, 47864.6914, 36470.0938, 37850.2344, 52134.4102,
        25541.4238, 38271.5625, 36981.6289, 46161.3750, 38967.3828, 37904.3750,
        35998.2461, 30749.3652, 35390.1875, 36685.5742, 34057.4609, 32665.3887,
        36295.7695, 31711.5547, 33542.1328, 26289.8281, 44890.7266, 27115.4531,
        34151.1719, 40601.1289, 34499.8945, 29634.7598, 33587.0430, 37033.2148,
        31833.8105, 44150.3203, 44625.2031, 33889.3789, 49809.6953, 34292.4648,
        33310.9766, 28856.1543, 37695.6133, 46245.9844, 33482.9062, 38580.6133,
        31263.2148, 28026.1348, 26635.7969, 39019.1719, 46629.1758, 24653.5977,
        28988.5957, 45045.8906, 48428.3477, 28287.5469, 39155.1641, 37638.8320,
        36672.4375, 34477.5508, 42232.8242, 39900.5000, 40739.0430, 36053.1836,
        39501.7695, 46550.8047, 36262.8047, 34215.2930, 33139.9062, 42907.8477,
        41989.1875, 25209.9277, 33337.3359, 35888.6562, 47986.1289, 40255.6055,
        36678.4766, 33703.7109, 27839.6680, 38342.3164, 35067.8711, 29121.6035,
        28180.2676, 30587.1582, 37708.8984, 34238.4609, 45903.7227, 37765.8086,
        45415.9023, 54758.0156, 32427.1699, 31728.7051, 47447.2148, 46695.2109,
        43401.0859, 42350.8906, 46610.3633, 40817.4727, 34937.1875, 26351.4980,
        41730.7500, 39090.7031, 30129.4590, 38476.9023, 35014.5195, 34891.1250,
        39437.6953, 46924.9883, 42528.9023, 40215.3125, 49071.9141, 31139.4922,
        40461.1055, 28163.8320, 36392.5039, 30572.4883, 40367.4414, 35396.3633,
        45362.6797, 31238.9707, 41707.6133, 42975.8047, 50044.4805, 35629.8789,
        31391.9531, 31401.8359, 39329.3711, 32065.1758, 48507.0469, 38479.1992,
        35776.5547, 29430.7344, 31581.1348, 40210.0664, 38649.4180, 33582.2422,
        34152.9609, 34929.2773, 33602.9102, 41283.5273, 36973.7031, 32822.9062,
        41057.3750, 33080.7656, 53042.5586, 26600.7676, 42096.5430, 27916.9258,
        53837.2383, 43414.9766, 43089.8477, 36182.3828, 36928.8281, 33697.7344,
        28516.3906, 44825.7031, 38464.3164, 32101.4785, 38114.7422, 33174.5117,
        46256.0664, 30984.8125, 36290.8281, 31224.2676, 32599.8340, 50977.7070,
        38215.4648, 39487.9961, 28479.9844, 33613.7422, 42999.1094, 48134.4805,
        27560.5176, 36724.8867, 32125.5273, 39464.1328, 28566.8125, 37880.8359,
        28605.4922, 27680.5449])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1291.2964,   30.1408,  201.5727,  ...,    6.7101,  192.7522,
          84.8660])
Pruning mask statistics:
Layer: conv1 | Type of mask: <class 'dict'>
Failed to compute pruning mask: 'weight'
