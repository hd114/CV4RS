Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    29135 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Loading BEN data for train...
    237871 patches indexed
    237871 filtered patches indexed
    1000 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
==================================================
ROUND 1/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0530
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1224

==================================================
ROUND 2/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0928
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0599

==================================================
ROUND 3/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1611
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0604

==================================================
ROUND 4/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0253 | recall: 0.0374 | f1-score: 0.0302 | support: 1282 | mAP: 0.0587
macro     precision: 0.0516 | recall: 0.0158 | f1-score: 0.0242 | support: 1282 | mAP: 0.1340

==================================================
ROUND 5/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0660
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1477

==================================================
ROUND 6/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0191 | recall: 0.0437 | f1-score: 0.0266 | support: 1282 | mAP: 0.0706
macro     precision: 0.1046 | recall: 0.0742 | f1-score: 0.0323 | support: 1282 | mAP: 0.1387

==================================================
ROUND 7/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0974
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0827

==================================================
ROUND 8/20
==================================================
Finale Anzahl der Pruning-Patches: 10
Anzahl eindeutiger Klassen: 11
Klassenverteilung in den Pruning-Patches (Häufigkeiten): {'Marine waters': 7, 'Transitional woodland, shrub': 6, 'Mixed forest': 5, 'Coniferous forest': 4, 'Land principally occupied by agriculture, with significant areas of natural vegetation': 2, 'Inland waters': 1, 'Broad-leaved forest': 1, 'Arable land': 1, 'Urban fabric': 1, 'Industrial or commercial units': 1, 'Coastal wetlands': 1}
[INFO] Erstelle Prune Loader...
[INFO] 10 Patches nach Filterung übrig.
[SUCCESS] Prune Loader erfolgreich erstellt.
Starting relevance computation and pruning mask generation.
Type of components_relevances: <class 'collections.OrderedDict'>
Layer: conv1
Total layer relevance: 4.1730939254878604e+24
--------------------------------------------------
Layer: encoder.4.0.conv1
Total layer relevance: 2.2425600125169526e+24
--------------------------------------------------
Layer: encoder.4.0.conv2
Total layer relevance: 1.5206657063971553e+24
--------------------------------------------------
Layer: encoder.4.0.conv3
Total layer relevance: 2.1425705891869146e+23
--------------------------------------------------
Layer: encoder.4.0.downsample.0
Total layer relevance: 2.1381660687513462e+23
--------------------------------------------------
Layer: encoder.4.1.conv1
Total layer relevance: 3.4788871599055857e+21
--------------------------------------------------
Layer: encoder.4.1.conv2
Total layer relevance: 2.506775610188456e+21
--------------------------------------------------
Layer: encoder.4.1.conv3
Total layer relevance: 5.270703029556326e+20
--------------------------------------------------
Layer: encoder.4.2.conv1
Total layer relevance: 7.34345432357174e+19
--------------------------------------------------
Layer: encoder.4.2.conv2
Total layer relevance: 4.615574331347855e+19
--------------------------------------------------
Layer: encoder.4.2.conv3
Total layer relevance: 1.056164481362559e+19
--------------------------------------------------
Layer: encoder.5.0.conv1
Total layer relevance: 1.3810487268936253e+18
--------------------------------------------------
Layer: encoder.5.0.conv2
Total layer relevance: 1.0114961342893916e+18
--------------------------------------------------
Layer: encoder.5.0.conv3
Total layer relevance: 1.968216179830948e+17
--------------------------------------------------
Layer: encoder.5.0.downsample.0
Total layer relevance: 1.975674304641106e+17
--------------------------------------------------
Layer: encoder.5.1.conv1
Total layer relevance: 6188043924930560.0
--------------------------------------------------
Layer: encoder.5.1.conv2
Total layer relevance: 5117496482856960.0
--------------------------------------------------
Layer: encoder.5.1.conv3
Total layer relevance: 1710234233995264.0
--------------------------------------------------
Layer: encoder.5.2.conv1
Total layer relevance: 423078303629312.0
--------------------------------------------------
Layer: encoder.5.2.conv2
Total layer relevance: 334982887243776.0
--------------------------------------------------
Layer: encoder.5.2.conv3
Total layer relevance: 51862325690368.0
--------------------------------------------------
Layer: encoder.5.3.conv1
Total layer relevance: 3967562809344.0
--------------------------------------------------
Layer: encoder.5.3.conv2
Total layer relevance: 3386437533696.0
--------------------------------------------------
Layer: encoder.5.3.conv3
Total layer relevance: 1033309716480.0
--------------------------------------------------
Layer: encoder.6.0.conv1
Total layer relevance: 371943211008.0
--------------------------------------------------
Layer: encoder.6.0.conv2
Total layer relevance: 411333165056.0
--------------------------------------------------
Layer: encoder.6.0.conv3
Total layer relevance: 35363528704.0
--------------------------------------------------
Layer: encoder.6.0.downsample.0
Total layer relevance: 35589148672.0
--------------------------------------------------
Layer: encoder.6.1.conv1
Total layer relevance: 1358488064.0
--------------------------------------------------
Layer: encoder.6.1.conv2
Total layer relevance: 1437459968.0
--------------------------------------------------
Layer: encoder.6.1.conv3
Total layer relevance: 260575456.0
--------------------------------------------------
Layer: encoder.6.2.conv1
Total layer relevance: 60529184.0
--------------------------------------------------
Layer: encoder.6.2.conv2
Total layer relevance: 81070000.0
--------------------------------------------------
Layer: encoder.6.2.conv3
Total layer relevance: 19501316.0
--------------------------------------------------
Layer: encoder.6.3.conv1
Total layer relevance: 5250541.0
--------------------------------------------------
Layer: encoder.6.3.conv2
Total layer relevance: 9054821.0
--------------------------------------------------
Layer: encoder.6.3.conv3
Total layer relevance: 1431037.75
--------------------------------------------------
Layer: encoder.6.4.conv1
Total layer relevance: 542382.625
--------------------------------------------------
Layer: encoder.6.4.conv2
Total layer relevance: 836770.5
--------------------------------------------------
Layer: encoder.6.4.conv3
Total layer relevance: 126527.203125
--------------------------------------------------
Layer: encoder.6.5.conv1
Total layer relevance: 60463.2578125
--------------------------------------------------
Layer: encoder.6.5.conv2
Total layer relevance: 112128.2421875
--------------------------------------------------
Layer: encoder.6.5.conv3
Total layer relevance: 30379.1640625
--------------------------------------------------
Layer: encoder.7.0.conv1
Total layer relevance: 13786.705078125
--------------------------------------------------
Layer: encoder.7.0.conv2
Total layer relevance: 54437.6171875
--------------------------------------------------
Layer: encoder.7.0.conv3
Total layer relevance: 14003.484375
--------------------------------------------------
Layer: encoder.7.0.downsample.0
Total layer relevance: 14753.353515625
--------------------------------------------------
Layer: encoder.7.1.conv1
Total layer relevance: 1276.40673828125
--------------------------------------------------
Layer: encoder.7.1.conv2
Total layer relevance: 4111.40283203125
--------------------------------------------------
Layer: encoder.7.1.conv3
Total layer relevance: 1620.294921875
--------------------------------------------------
Layer: encoder.7.2.conv1
Total layer relevance: 711.0263061523438
--------------------------------------------------
Layer: encoder.7.2.conv2
Total layer relevance: 1537.14697265625
--------------------------------------------------
Layer: encoder.7.2.conv3
Total layer relevance: 733.822265625
--------------------------------------------------
--------------------------------------------------
Global Pruning Mask
Pruning-rate: 0.9
Layer: conv1		% of pruned neurons: 0.00%
Layer: encoder.4.0.conv1		% of pruned neurons: 0.00%
Layer: encoder.4.0.conv2		% of pruned neurons: 0.00%
Layer: encoder.4.0.conv3		% of pruned neurons: 0.00%
Layer: encoder.4.0.downsample.0		% of pruned neurons: 0.00%
Layer: encoder.4.1.conv1		% of pruned neurons: 0.00%
Layer: encoder.4.1.conv2		% of pruned neurons: 0.00%
Layer: encoder.4.1.conv3		% of pruned neurons: 0.00%
Layer: encoder.4.2.conv1		% of pruned neurons: 0.00%
Layer: encoder.4.2.conv2		% of pruned neurons: 0.00%
Layer: encoder.4.2.conv3		% of pruned neurons: 0.00%
Layer: encoder.5.0.conv1		% of pruned neurons: 0.00%
Layer: encoder.5.0.conv2		% of pruned neurons: 0.00%
Layer: encoder.5.0.conv3		% of pruned neurons: 26.56%
Layer: encoder.5.0.downsample.0		% of pruned neurons: 25.59%
Layer: encoder.5.1.conv1		% of pruned neurons: 31.25%
Layer: encoder.5.1.conv2		% of pruned neurons: 39.06%
Layer: encoder.5.1.conv3		% of pruned neurons: 99.02%
Layer: encoder.5.2.conv1		% of pruned neurons: 100.00%
Layer: encoder.5.2.conv2		% of pruned neurons: 100.00%
Layer: encoder.5.2.conv3		% of pruned neurons: 100.00%
Layer: encoder.5.3.conv1		% of pruned neurons: 100.00%
Layer: encoder.5.3.conv2		% of pruned neurons: 100.00%
Layer: encoder.5.3.conv3		% of pruned neurons: 100.00%
Layer: encoder.6.0.conv1		% of pruned neurons: 100.00%
Layer: encoder.6.0.conv2		% of pruned neurons: 100.00%
Layer: encoder.6.0.conv3		% of pruned neurons: 100.00%
Layer: encoder.6.0.downsample.0		% of pruned neurons: 100.00%
Layer: encoder.6.1.conv1		% of pruned neurons: 100.00%
Layer: encoder.6.1.conv2		% of pruned neurons: 100.00%
Layer: encoder.6.1.conv3		% of pruned neurons: 100.00%
Layer: encoder.6.2.conv1		% of pruned neurons: 100.00%
Layer: encoder.6.2.conv2		% of pruned neurons: 100.00%
Layer: encoder.6.2.conv3		% of pruned neurons: 100.00%
Layer: encoder.6.3.conv1		% of pruned neurons: 100.00%
Layer: encoder.6.3.conv2		% of pruned neurons: 100.00%
Layer: encoder.6.3.conv3		% of pruned neurons: 100.00%
Layer: encoder.6.4.conv1		% of pruned neurons: 100.00%
Layer: encoder.6.4.conv2		% of pruned neurons: 100.00%
Layer: encoder.6.4.conv3		% of pruned neurons: 100.00%
Layer: encoder.6.5.conv1		% of pruned neurons: 100.00%
Layer: encoder.6.5.conv2		% of pruned neurons: 100.00%
Layer: encoder.6.5.conv3		% of pruned neurons: 100.00%
Layer: encoder.7.0.conv1		% of pruned neurons: 100.00%
Layer: encoder.7.0.conv2		% of pruned neurons: 100.00%
Layer: encoder.7.0.conv3		% of pruned neurons: 100.00%
Layer: encoder.7.0.downsample.0		% of pruned neurons: 100.00%
Layer: encoder.7.1.conv1		% of pruned neurons: 100.00%
Layer: encoder.7.1.conv2		% of pruned neurons: 100.00%
Layer: encoder.7.1.conv3		% of pruned neurons: 100.00%
Layer: encoder.7.2.conv1		% of pruned neurons: 100.00%
Layer: encoder.7.2.conv2		% of pruned neurons: 100.00%
Layer: encoder.7.2.conv3		% of pruned neurons: 100.00%
Sendeing pruning mask to clients...
[INFO] Pruner and pruning mask received and stored.
[INFO] Pruner and pruning mask received and stored.
[INFO] Pruner and pruning mask received and stored.
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.3147
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0675

==================================================
ROUND 9/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.3140
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0667

==================================================
ROUND 10/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.3110
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0638

==================================================
ROUND 11/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.1907
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0617

==================================================
ROUND 12/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0914
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0611

==================================================
ROUND 13/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0810
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0716

==================================================
ROUND 14/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0739
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0608

==================================================
ROUND 15/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0746
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0609

==================================================
ROUND 16/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0724
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0608

==================================================
ROUND 17/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0726
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0615

==================================================
ROUND 18/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0719
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0608

==================================================
ROUND 19/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0709
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0701

==================================================
ROUND 20/20
==================================================
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
Epoch 1/3
----------
Epoch 2/3
----------
Epoch 3/3
----------
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0708
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 1282 | mAP: 0.0608

[{'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}]
