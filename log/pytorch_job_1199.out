Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Starting Round 1/5...
----------
Training and communication for Round 1...
Epoch 1/1
----------
Communication round completed.
Bug fix for empty classification report.
Validation completed.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.4692
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.2983

Applying pruning after round 1...
Layer: conv1.weight | Mask shape: torch.Size([64, 10, 7, 7]) | Pruned: 25088/31360 (0.80)
Layer: encoder.0.weight | Mask shape: torch.Size([64, 10, 7, 7]) | Pruned: 25088/31360 (0.80)
Layer: encoder.1.weight | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.1.bias | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.1.running_mean | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.1.running_var | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.conv1.weight | Mask shape: torch.Size([64, 64, 1, 1]) | Pruned: 3276/4096 (0.80)
Layer: encoder.4.0.bn1.weight | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.bn1.bias | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.bn1.running_mean | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.bn1.running_var | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.conv2.weight | Mask shape: torch.Size([64, 64, 3, 3]) | Pruned: 29491/36864 (0.80)
Layer: encoder.4.0.bn2.weight | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.bn2.bias | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.bn2.running_mean | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.bn2.running_var | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.0.conv3.weight | Mask shape: torch.Size([256, 64, 1, 1]) | Pruned: 13107/16384 (0.80)
Layer: encoder.4.0.bn3.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.0.bn3.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.0.bn3.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.0.bn3.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.0.downsample.0.weight | Mask shape: torch.Size([256, 64, 1, 1]) | Pruned: 13107/16384 (0.80)
Layer: encoder.4.0.downsample.1.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.0.downsample.1.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.0.downsample.1.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.0.downsample.1.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.1.conv1.weight | Mask shape: torch.Size([64, 256, 1, 1]) | Pruned: 13107/16384 (0.80)
Layer: encoder.4.1.bn1.weight | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.bn1.bias | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.bn1.running_mean | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.bn1.running_var | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.conv2.weight | Mask shape: torch.Size([64, 64, 3, 3]) | Pruned: 29491/36864 (0.80)
Layer: encoder.4.1.bn2.weight | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.bn2.bias | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.bn2.running_mean | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.bn2.running_var | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.1.conv3.weight | Mask shape: torch.Size([256, 64, 1, 1]) | Pruned: 13107/16384 (0.80)
Layer: encoder.4.1.bn3.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.1.bn3.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.1.bn3.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.1.bn3.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.2.conv1.weight | Mask shape: torch.Size([64, 256, 1, 1]) | Pruned: 13107/16384 (0.80)
Layer: encoder.4.2.bn1.weight | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.bn1.bias | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.bn1.running_mean | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.bn1.running_var | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.conv2.weight | Mask shape: torch.Size([64, 64, 3, 3]) | Pruned: 29491/36864 (0.80)
Layer: encoder.4.2.bn2.weight | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.bn2.bias | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.bn2.running_mean | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.bn2.running_var | Mask shape: torch.Size([64]) | Pruned: 51/64 (0.80)
Layer: encoder.4.2.conv3.weight | Mask shape: torch.Size([256, 64, 1, 1]) | Pruned: 13107/16384 (0.80)
Layer: encoder.4.2.bn3.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.2.bn3.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.2.bn3.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.4.2.bn3.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.5.0.conv1.weight | Mask shape: torch.Size([128, 256, 1, 1]) | Pruned: 26214/32768 (0.80)
Layer: encoder.5.0.bn1.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.bn1.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.bn1.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.bn1.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.conv2.weight | Mask shape: torch.Size([128, 128, 3, 3]) | Pruned: 117964/147456 (0.80)
Layer: encoder.5.0.bn2.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.bn2.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.bn2.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.bn2.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.0.conv3.weight | Mask shape: torch.Size([512, 128, 1, 1]) | Pruned: 52428/65536 (0.80)
Layer: encoder.5.0.bn3.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.0.bn3.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.0.bn3.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.0.bn3.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.0.downsample.0.weight | Mask shape: torch.Size([512, 256, 1, 1]) | Pruned: 104857/131072 (0.80)
Layer: encoder.5.0.downsample.1.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.0.downsample.1.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.0.downsample.1.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.0.downsample.1.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.1.conv1.weight | Mask shape: torch.Size([128, 512, 1, 1]) | Pruned: 52428/65536 (0.80)
Layer: encoder.5.1.bn1.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.bn1.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.bn1.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.bn1.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.conv2.weight | Mask shape: torch.Size([128, 128, 3, 3]) | Pruned: 117964/147456 (0.80)
Layer: encoder.5.1.bn2.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.bn2.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.bn2.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.bn2.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.1.conv3.weight | Mask shape: torch.Size([512, 128, 1, 1]) | Pruned: 52428/65536 (0.80)
Layer: encoder.5.1.bn3.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.1.bn3.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.1.bn3.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.1.bn3.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.2.conv1.weight | Mask shape: torch.Size([128, 512, 1, 1]) | Pruned: 52428/65536 (0.80)
Layer: encoder.5.2.bn1.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.bn1.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.bn1.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.bn1.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.conv2.weight | Mask shape: torch.Size([128, 128, 3, 3]) | Pruned: 117964/147456 (0.80)
Layer: encoder.5.2.bn2.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.bn2.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.bn2.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.bn2.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.2.conv3.weight | Mask shape: torch.Size([512, 128, 1, 1]) | Pruned: 52428/65536 (0.80)
Layer: encoder.5.2.bn3.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.2.bn3.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.2.bn3.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.2.bn3.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.3.conv1.weight | Mask shape: torch.Size([128, 512, 1, 1]) | Pruned: 52428/65536 (0.80)
Layer: encoder.5.3.bn1.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.bn1.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.bn1.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.bn1.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.conv2.weight | Mask shape: torch.Size([128, 128, 3, 3]) | Pruned: 117964/147456 (0.80)
Layer: encoder.5.3.bn2.weight | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.bn2.bias | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.bn2.running_mean | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.bn2.running_var | Mask shape: torch.Size([128]) | Pruned: 102/128 (0.80)
Layer: encoder.5.3.conv3.weight | Mask shape: torch.Size([512, 128, 1, 1]) | Pruned: 52428/65536 (0.80)
Layer: encoder.5.3.bn3.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.3.bn3.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.3.bn3.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.5.3.bn3.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.6.0.conv1.weight | Mask shape: torch.Size([256, 512, 1, 1]) | Pruned: 104857/131072 (0.80)
Layer: encoder.6.0.bn1.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.bn1.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.bn1.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.bn1.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.conv2.weight | Mask shape: torch.Size([256, 256, 3, 3]) | Pruned: 471859/589824 (0.80)
Layer: encoder.6.0.bn2.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.bn2.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.bn2.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.bn2.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.0.conv3.weight | Mask shape: torch.Size([1024, 256, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.0.bn3.weight | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.0.bn3.bias | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.0.bn3.running_mean | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.0.bn3.running_var | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.0.downsample.0.weight | Mask shape: torch.Size([1024, 512, 1, 1]) | Pruned: 419430/524288 (0.80)
Layer: encoder.6.0.downsample.1.weight | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.0.downsample.1.bias | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.0.downsample.1.running_mean | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.0.downsample.1.running_var | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.1.conv1.weight | Mask shape: torch.Size([256, 1024, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.1.bn1.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.bn1.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.bn1.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.bn1.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.conv2.weight | Mask shape: torch.Size([256, 256, 3, 3]) | Pruned: 471859/589824 (0.80)
Layer: encoder.6.1.bn2.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.bn2.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.bn2.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.bn2.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.1.conv3.weight | Mask shape: torch.Size([1024, 256, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.1.bn3.weight | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.1.bn3.bias | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.1.bn3.running_mean | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.1.bn3.running_var | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.2.conv1.weight | Mask shape: torch.Size([256, 1024, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.2.bn1.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.bn1.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.bn1.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.bn1.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.conv2.weight | Mask shape: torch.Size([256, 256, 3, 3]) | Pruned: 471859/589824 (0.80)
Layer: encoder.6.2.bn2.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.bn2.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.bn2.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.bn2.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.2.conv3.weight | Mask shape: torch.Size([1024, 256, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.2.bn3.weight | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.2.bn3.bias | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.2.bn3.running_mean | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.2.bn3.running_var | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.3.conv1.weight | Mask shape: torch.Size([256, 1024, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.3.bn1.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.bn1.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.bn1.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.bn1.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.conv2.weight | Mask shape: torch.Size([256, 256, 3, 3]) | Pruned: 471859/589824 (0.80)
Layer: encoder.6.3.bn2.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.bn2.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.bn2.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.bn2.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.3.conv3.weight | Mask shape: torch.Size([1024, 256, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.3.bn3.weight | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.3.bn3.bias | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.3.bn3.running_mean | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.3.bn3.running_var | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.4.conv1.weight | Mask shape: torch.Size([256, 1024, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.4.bn1.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.bn1.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.bn1.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.bn1.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.conv2.weight | Mask shape: torch.Size([256, 256, 3, 3]) | Pruned: 471859/589824 (0.80)
Layer: encoder.6.4.bn2.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.bn2.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.bn2.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.bn2.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.4.conv3.weight | Mask shape: torch.Size([1024, 256, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.4.bn3.weight | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.4.bn3.bias | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.4.bn3.running_mean | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.4.bn3.running_var | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.5.conv1.weight | Mask shape: torch.Size([256, 1024, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.5.bn1.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.bn1.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.bn1.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.bn1.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.conv2.weight | Mask shape: torch.Size([256, 256, 3, 3]) | Pruned: 471859/589824 (0.80)
Layer: encoder.6.5.bn2.weight | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.bn2.bias | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.bn2.running_mean | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.bn2.running_var | Mask shape: torch.Size([256]) | Pruned: 204/256 (0.80)
Layer: encoder.6.5.conv3.weight | Mask shape: torch.Size([1024, 256, 1, 1]) | Pruned: 209715/262144 (0.80)
Layer: encoder.6.5.bn3.weight | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.5.bn3.bias | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.5.bn3.running_mean | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.6.5.bn3.running_var | Mask shape: torch.Size([1024]) | Pruned: 819/1024 (0.80)
Layer: encoder.7.0.conv1.weight | Mask shape: torch.Size([512, 1024, 1, 1]) | Pruned: 419430/524288 (0.80)
Layer: encoder.7.0.bn1.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.bn1.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.bn1.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.bn1.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.conv2.weight | Mask shape: torch.Size([512, 512, 3, 3]) | Pruned: 1887436/2359296 (0.80)
Layer: encoder.7.0.bn2.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.bn2.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.bn2.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.bn2.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.0.conv3.weight | Mask shape: torch.Size([2048, 512, 1, 1]) | Pruned: 838860/1048576 (0.80)
Layer: encoder.7.0.bn3.weight | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.0.bn3.bias | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.0.bn3.running_mean | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.0.bn3.running_var | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.0.downsample.0.weight | Mask shape: torch.Size([2048, 1024, 1, 1]) | Pruned: 1677721/2097152 (0.80)
Layer: encoder.7.0.downsample.1.weight | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.0.downsample.1.bias | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.0.downsample.1.running_mean | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.0.downsample.1.running_var | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.1.conv1.weight | Mask shape: torch.Size([512, 2048, 1, 1]) | Pruned: 838860/1048576 (0.80)
Layer: encoder.7.1.bn1.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.bn1.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.bn1.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.bn1.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.conv2.weight | Mask shape: torch.Size([512, 512, 3, 3]) | Pruned: 1887436/2359296 (0.80)
Layer: encoder.7.1.bn2.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.bn2.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.bn2.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.bn2.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.1.conv3.weight | Mask shape: torch.Size([2048, 512, 1, 1]) | Pruned: 838860/1048576 (0.80)
Layer: encoder.7.1.bn3.weight | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.1.bn3.bias | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.1.bn3.running_mean | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.1.bn3.running_var | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.2.conv1.weight | Mask shape: torch.Size([512, 2048, 1, 1]) | Pruned: 838860/1048576 (0.80)
Layer: encoder.7.2.bn1.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.bn1.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.bn1.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.bn1.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.conv2.weight | Mask shape: torch.Size([512, 512, 3, 3]) | Pruned: 1887436/2359296 (0.80)
Layer: encoder.7.2.bn2.weight | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.bn2.bias | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.bn2.running_mean | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.bn2.running_var | Mask shape: torch.Size([512]) | Pruned: 409/512 (0.80)
Layer: encoder.7.2.conv3.weight | Mask shape: torch.Size([2048, 512, 1, 1]) | Pruned: 838860/1048576 (0.80)
Layer: encoder.7.2.bn3.weight | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.2.bn3.bias | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.2.bn3.running_mean | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: encoder.7.2.bn3.running_var | Mask shape: torch.Size([2048]) | Pruned: 1638/2048 (0.80)
Layer: FC.weight | Mask shape: torch.Size([19, 2048]) | Pruned: 31129/38912 (0.80)
Layer: FC.bias | Mask shape: torch.Size([19]) | Pruned: 15/19 (0.80)
Pruned state_dict successfully loaded.
Pruning mask reapplied successfully.
End of Round 1/5. Moving to next round...
Starting Round 2/5...
----------
Pruning mask reapplied successfully for this round.
Training and communication for Round 2...
Epoch 1/1
----------
Communication round completed.
Bug fix for empty classification report.
Validation completed.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.5438
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.3126

End of Round 2/5. Moving to next round...
Starting Round 3/5...
----------
Pruning mask reapplied successfully for this round.
Training and communication for Round 3...
Epoch 1/1
----------
Communication round completed.
Bug fix for empty classification report.
