Lines that potentially need to be canonized 309
/home/paul/projects/CV4RS-main/pxp/prune.py
Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    7150 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    4263 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    7180 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    3248 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initializing LRP Pruning...
LRP initialized successfully.
=== Round 1/3 ===
Training and communication for Round 1...
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Starting validation after Round 1...
Bug fix for empty classification report.
micro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.1714
macro     precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 0 | mAP: 0.1385

Führe LRP-Pruning in Runde 1 durch...
Führe LRP-Pruning in Runde 1 durch...
[DEBUG] Starte Berechnung der LRP-Pruning-Maske für Land: Finland
[DEBUG] Pruning-Rate: 0.3
Erstelle DataLoader für Land: Finland
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
[DEBUG] Dataloader für 'Finland' geladen mit Batch-Größe 16 und 4 Arbeitern.
[DEBUG] Starte Berechnung der Relevanzwerte...
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Unexpected input shape for LRP: torch.Size([16]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 16])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 16
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Grad output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([5])
Unexpected input shape for LRP: torch.Size([5]). Adding dummy dimensions.
Modified input shape for LRP: torch.Size([1, 1, 5])
Adjusting input channels from 1 to 10
Adjusting input batch size from 1 to match targets batch size 5
Input shape before relevance computation: torch.Size([5, 10, 1, 5])
Model output shape: torch.Size([5, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([5, 19]) to 5 for one-hot encoding.
Targets shape: torch.Size([5])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([5, 19])
Grad output shape: torch.Size([5, 19])
Relevance computation successful. Relevance shape: torch.Size([5, 10, 1, 5])
[DEBUG] Relevanzwerte berechnet. Anzahl der Layer mit Relevanzwerten: 53
[DEBUG] Überprüfe Struktur und Inhalt von global_concept_maps vor Pruning-Maske...
[DEBUG] Layer 'conv1' enthält Tensor: Shape = torch.Size([64])
[DEBUG] Layer 'encoder.4.0.conv1' enthält Tensor: Shape = torch.Size([64])
[DEBUG] Layer 'encoder.4.0.conv2' enthält Tensor: Shape = torch.Size([64])
[DEBUG] Layer 'encoder.4.0.conv3' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.4.0.downsample.0' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.4.1.conv1' enthält Tensor: Shape = torch.Size([64])
[DEBUG] Layer 'encoder.4.1.conv2' enthält Tensor: Shape = torch.Size([64])
[DEBUG] Layer 'encoder.4.1.conv3' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.4.2.conv1' enthält Tensor: Shape = torch.Size([64])
[DEBUG] Layer 'encoder.4.2.conv2' enthält Tensor: Shape = torch.Size([64])
[DEBUG] Layer 'encoder.4.2.conv3' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.5.0.conv1' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.0.conv2' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.0.conv3' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.5.0.downsample.0' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.5.1.conv1' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.1.conv2' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.1.conv3' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.5.2.conv1' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.2.conv2' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.2.conv3' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.5.3.conv1' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.3.conv2' enthält Tensor: Shape = torch.Size([128])
[DEBUG] Layer 'encoder.5.3.conv3' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.6.0.conv1' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.0.conv2' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.0.conv3' enthält Tensor: Shape = torch.Size([1024])
[DEBUG] Layer 'encoder.6.0.downsample.0' enthält Tensor: Shape = torch.Size([1024])
[DEBUG] Layer 'encoder.6.1.conv1' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.1.conv2' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.1.conv3' enthält Tensor: Shape = torch.Size([1024])
[DEBUG] Layer 'encoder.6.2.conv1' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.2.conv2' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.2.conv3' enthält Tensor: Shape = torch.Size([1024])
[DEBUG] Layer 'encoder.6.3.conv1' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.3.conv2' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.3.conv3' enthält Tensor: Shape = torch.Size([1024])
[DEBUG] Layer 'encoder.6.4.conv1' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.4.conv2' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.4.conv3' enthält Tensor: Shape = torch.Size([1024])
[DEBUG] Layer 'encoder.6.5.conv1' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.5.conv2' enthält Tensor: Shape = torch.Size([256])
[DEBUG] Layer 'encoder.6.5.conv3' enthält Tensor: Shape = torch.Size([1024])
[DEBUG] Layer 'encoder.7.0.conv1' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.7.0.conv2' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.7.0.conv3' enthält Tensor: Shape = torch.Size([2048])
[DEBUG] Layer 'encoder.7.0.downsample.0' enthält Tensor: Shape = torch.Size([2048])
[DEBUG] Layer 'encoder.7.1.conv1' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.7.1.conv2' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.7.1.conv3' enthält Tensor: Shape = torch.Size([2048])
[DEBUG] Layer 'encoder.7.2.conv1' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.7.2.conv2' enthält Tensor: Shape = torch.Size([512])
[DEBUG] Layer 'encoder.7.2.conv3' enthält Tensor: Shape = torch.Size([2048])
[DEBUG] Initialisiere GlobalPruningOperations...
[DEBUG] Ziel-Layer: ['conv1', 'encoder.4.0.conv1', 'encoder.4.0.conv2', 'encoder.4.0.conv3', 'encoder.4.0.downsample.0', 'encoder.4.1.conv1', 'encoder.4.1.conv2', 'encoder.4.1.conv3', 'encoder.4.2.conv1', 'encoder.4.2.conv2', 'encoder.4.2.conv3', 'encoder.5.0.conv1', 'encoder.5.0.conv2', 'encoder.5.0.conv3', 'encoder.5.0.downsample.0', 'encoder.5.1.conv1', 'encoder.5.1.conv2', 'encoder.5.1.conv3', 'encoder.5.2.conv1', 'encoder.5.2.conv2', 'encoder.5.2.conv3', 'encoder.5.3.conv1', 'encoder.5.3.conv2', 'encoder.5.3.conv3', 'encoder.6.0.conv1', 'encoder.6.0.conv2', 'encoder.6.0.conv3', 'encoder.6.0.downsample.0', 'encoder.6.1.conv1', 'encoder.6.1.conv2', 'encoder.6.1.conv3', 'encoder.6.2.conv1', 'encoder.6.2.conv2', 'encoder.6.2.conv3', 'encoder.6.3.conv1', 'encoder.6.3.conv2', 'encoder.6.3.conv3', 'encoder.6.4.conv1', 'encoder.6.4.conv2', 'encoder.6.4.conv3', 'encoder.6.5.conv1', 'encoder.6.5.conv2', 'encoder.6.5.conv3', 'encoder.7.0.conv1', 'encoder.7.0.conv2', 'encoder.7.0.conv3', 'encoder.7.0.downsample.0', 'encoder.7.1.conv1', 'encoder.7.1.conv2', 'encoder.7.1.conv3', 'encoder.7.2.conv1', 'encoder.7.2.conv2', 'encoder.7.2.conv3']
[DEBUG] Starte Generierung der globalen Pruning-Maske...
[DEBUG] Validierung von global_concept_maps vor Beginn der Maskengenerierung...
[DEBUG] Layer 'conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.0.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.0.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.0.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.0.downsample.0' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.1.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.1.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.1.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.2.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.2.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.4.2.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.0.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.0.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.0.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.0.downsample.0' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.1.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.1.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.1.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.2.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.2.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.2.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.3.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.3.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.5.3.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.0.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.0.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.0.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.0.downsample.0' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.1.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.1.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.1.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.2.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.2.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.2.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.3.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.3.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.3.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.4.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.4.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.4.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.5.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.5.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.6.5.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.0.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.0.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.0.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.0.downsample.0' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.1.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.1.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.1.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.2.conv1' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.2.conv2' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Layer 'encoder.7.2.conv3' hat Typ: <class 'torch.Tensor'>.
[DEBUG] Überprüfe Layer 'conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.0.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.0.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.0.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.0.downsample.0' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.1.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.1.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.1.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.2.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.2.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.4.2.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.0.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.0.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.0.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.0.downsample.0' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.1.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.1.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.1.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.2.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.2.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.2.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.3.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.3.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.5.3.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.0.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.0.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.0.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.0.downsample.0' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.1.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.1.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.1.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.2.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.2.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.2.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.3.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.3.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.3.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.4.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.4.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.4.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.5.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.5.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.6.5.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.0.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.0.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.0.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.0.downsample.0' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.1.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.1.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.1.conv3' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.2.conv1' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.2.conv2' in global_concept_maps...
[DEBUG] Überprüfe Layer 'encoder.7.2.conv3' in global_concept_maps...
[DEBUG] Überprüfung abgeschlossen. Alle relevanten Layer sind jetzt Tensoren.
[DEBUG] Intervallindizes initialisiert: OrderedDict([('conv1', (0, 63)), ('encoder.4.0.conv1', (64, 127)), ('encoder.4.0.conv2', (128, 191)), ('encoder.4.0.conv3', (192, 447)), ('encoder.4.0.downsample.0', (448, 703)), ('encoder.4.1.conv1', (704, 767)), ('encoder.4.1.conv2', (768, 831)), ('encoder.4.1.conv3', (832, 1087)), ('encoder.4.2.conv1', (1088, 1151)), ('encoder.4.2.conv2', (1152, 1215)), ('encoder.4.2.conv3', (1216, 1471)), ('encoder.5.0.conv1', (1472, 1599)), ('encoder.5.0.conv2', (1600, 1727)), ('encoder.5.0.conv3', (1728, 2239)), ('encoder.5.0.downsample.0', (2240, 2751)), ('encoder.5.1.conv1', (2752, 2879)), ('encoder.5.1.conv2', (2880, 3007)), ('encoder.5.1.conv3', (3008, 3519)), ('encoder.5.2.conv1', (3520, 3647)), ('encoder.5.2.conv2', (3648, 3775)), ('encoder.5.2.conv3', (3776, 4287)), ('encoder.5.3.conv1', (4288, 4415)), ('encoder.5.3.conv2', (4416, 4543)), ('encoder.5.3.conv3', (4544, 5055)), ('encoder.6.0.conv1', (5056, 5311)), ('encoder.6.0.conv2', (5312, 5567)), ('encoder.6.0.conv3', (5568, 6591)), ('encoder.6.0.downsample.0', (6592, 7615)), ('encoder.6.1.conv1', (7616, 7871)), ('encoder.6.1.conv2', (7872, 8127)), ('encoder.6.1.conv3', (8128, 9151)), ('encoder.6.2.conv1', (9152, 9407)), ('encoder.6.2.conv2', (9408, 9663)), ('encoder.6.2.conv3', (9664, 10687)), ('encoder.6.3.conv1', (10688, 10943)), ('encoder.6.3.conv2', (10944, 11199)), ('encoder.6.3.conv3', (11200, 12223)), ('encoder.6.4.conv1', (12224, 12479)), ('encoder.6.4.conv2', (12480, 12735)), ('encoder.6.4.conv3', (12736, 13759)), ('encoder.6.5.conv1', (13760, 14015)), ('encoder.6.5.conv2', (14016, 14271)), ('encoder.6.5.conv3', (14272, 15295)), ('encoder.7.0.conv1', (15296, 15807)), ('encoder.7.0.conv2', (15808, 16319)), ('encoder.7.0.conv3', (16320, 18367)), ('encoder.7.0.downsample.0', (18368, 20415)), ('encoder.7.1.conv1', (20416, 20927)), ('encoder.7.1.conv2', (20928, 21439)), ('encoder.7.1.conv3', (21440, 23487)), ('encoder.7.2.conv1', (23488, 23999)), ('encoder.7.2.conv2', (24000, 24511)), ('encoder.7.2.conv3', (24512, 26559))])
[DEBUG] Generierte Pruning-Indizes überprüft:
  Layer: conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.4.0.conv3 | Indizes: torch.Size([0])
  Layer: encoder.4.0.downsample.0 | Indizes: torch.Size([0])
  Layer: encoder.4.1.conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.4.1.conv3 | Indizes: torch.Size([0])
  Layer: encoder.4.2.conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.2.conv2 | Indizes: torch.Size([0])
  Layer: encoder.4.2.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.0.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.0.downsample.0 | Indizes: torch.Size([0])
  Layer: encoder.5.1.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.1.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.2.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.2.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.2.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.3.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.3.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.3.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.0.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.0.downsample.0 | Indizes: torch.Size([0])
  Layer: encoder.6.1.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.1.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.2.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.2.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.2.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.3.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.3.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.3.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.4.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.4.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.4.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.5.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.5.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.5.conv3 | Indizes: torch.Size([0])
  Layer: encoder.7.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.7.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.7.0.conv3 | Indizes: torch.Size([1506])
  Layer: encoder.7.0.downsample.0 | Indizes: torch.Size([1349])
  Layer: encoder.7.1.conv1 | Indizes: torch.Size([3])
  Layer: encoder.7.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.7.1.conv3 | Indizes: torch.Size([2038])
  Layer: encoder.7.2.conv1 | Indizes: torch.Size([512])
  Layer: encoder.7.2.conv2 | Indizes: torch.Size([512])
  Layer: encoder.7.2.conv3 | Indizes: torch.Size([2048])
[DEBUG] Verwende 'subsequent_layer_pruning': Both
[DEBUG] Generiere Pruning-Maske für Layer 'conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.2.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.2.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.3.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.3.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.3.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.2.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.3.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.3.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.3.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.4.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.4.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.4.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.5.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.5.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.5.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.2.conv3'...
[DEBUG] Globale Pruning-Maske generiert für 53 Layer
[DEBUG] Layer: conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.0.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.0.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.0.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.0.downsample.0 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.1.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.1.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.1.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.2.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.2.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.4.2.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.0.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.0.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.0.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.0.downsample.0 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.1.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.1.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.1.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.2.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.2.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.2.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.3.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.3.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.5.3.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.0.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.0.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.0.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.0.downsample.0 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.1.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.1.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.1.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.2.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.2.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.2.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.3.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.3.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.3.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.4.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.4.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.4.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.5.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.5.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.6.5.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.0.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.0.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.0.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.0.downsample.0 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.1.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.1.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.1.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.2.conv1 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.2.conv2 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer: encoder.7.2.conv3 | Masken-Typen: ['Conv2d', 'BatchNorm2d']
[DEBUG] Registriere Forward Hooks für Pruning...
[DEBUG] Starte Anwendung der Pruning-Maske auf 53 Layer...
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.2.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.2.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.3.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.3.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.3.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.2.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.3.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.3.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.3.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.4.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.4.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.4.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.5.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.5.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.5.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.2.conv3'.
[DEBUG] Forward Hooks registriert: Keine Hooks
[DEBUG] Überprüfe Struktur von global_concept_maps nach Hook-Registrierung...
[DEBUG] Layer 'conv1' hat Tensor mit Shape torch.Size([64]).
[DEBUG] Layer 'encoder.4.0.conv1' hat Tensor mit Shape torch.Size([64]).
[DEBUG] Layer 'encoder.4.0.conv2' hat Tensor mit Shape torch.Size([64]).
[DEBUG] Layer 'encoder.4.0.conv3' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.4.0.downsample.0' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.4.1.conv1' hat Tensor mit Shape torch.Size([64]).
[DEBUG] Layer 'encoder.4.1.conv2' hat Tensor mit Shape torch.Size([64]).
[DEBUG] Layer 'encoder.4.1.conv3' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.4.2.conv1' hat Tensor mit Shape torch.Size([64]).
[DEBUG] Layer 'encoder.4.2.conv2' hat Tensor mit Shape torch.Size([64]).
[DEBUG] Layer 'encoder.4.2.conv3' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.5.0.conv1' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.0.conv2' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.0.conv3' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.5.0.downsample.0' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.5.1.conv1' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.1.conv2' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.1.conv3' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.5.2.conv1' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.2.conv2' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.2.conv3' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.5.3.conv1' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.3.conv2' hat Tensor mit Shape torch.Size([128]).
[DEBUG] Layer 'encoder.5.3.conv3' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.6.0.conv1' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.0.conv2' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.0.conv3' hat Tensor mit Shape torch.Size([1024]).
[DEBUG] Layer 'encoder.6.0.downsample.0' hat Tensor mit Shape torch.Size([1024]).
[DEBUG] Layer 'encoder.6.1.conv1' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.1.conv2' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.1.conv3' hat Tensor mit Shape torch.Size([1024]).
[DEBUG] Layer 'encoder.6.2.conv1' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.2.conv2' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.2.conv3' hat Tensor mit Shape torch.Size([1024]).
[DEBUG] Layer 'encoder.6.3.conv1' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.3.conv2' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.3.conv3' hat Tensor mit Shape torch.Size([1024]).
[DEBUG] Layer 'encoder.6.4.conv1' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.4.conv2' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.4.conv3' hat Tensor mit Shape torch.Size([1024]).
[DEBUG] Layer 'encoder.6.5.conv1' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.5.conv2' hat Tensor mit Shape torch.Size([256]).
[DEBUG] Layer 'encoder.6.5.conv3' hat Tensor mit Shape torch.Size([1024]).
[DEBUG] Layer 'encoder.7.0.conv1' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.7.0.conv2' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.7.0.conv3' hat Tensor mit Shape torch.Size([2048]).
[DEBUG] Layer 'encoder.7.0.downsample.0' hat Tensor mit Shape torch.Size([2048]).
[DEBUG] Layer 'encoder.7.1.conv1' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.7.1.conv2' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.7.1.conv3' hat Tensor mit Shape torch.Size([2048]).
[DEBUG] Layer 'encoder.7.2.conv1' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.7.2.conv2' hat Tensor mit Shape torch.Size([512]).
[DEBUG] Layer 'encoder.7.2.conv3' hat Tensor mit Shape torch.Size([2048]).
[DEBUG] Layer 'conv1' nach Pruning-Maske erfolgreich gefunden: Conv2d(10, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)).
[DEBUG] Validierung von global_concept_maps vor Beginn der Maskengenerierung...
[DEBUG] Layer 'conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.0.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.0.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.0.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.0.downsample.0' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.1.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.1.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.1.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.2.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.2.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.4.2.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.0.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.0.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.0.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.0.downsample.0' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.1.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.1.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.1.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.2.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.2.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.2.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.3.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.3.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.5.3.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.0.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.0.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.0.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.0.downsample.0' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.1.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.1.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.1.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.2.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.2.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.2.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.3.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.3.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.3.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.4.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.4.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.4.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.5.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.5.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.6.5.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.0.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.0.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.0.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.0.downsample.0' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.1.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.1.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.1.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.2.conv1' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.2.conv2' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Layer 'encoder.7.2.conv3' enthält Keys: ['Conv2d', 'BatchNorm2d']
[DEBUG] Überprüfe Layer 'conv1' in global_concept_maps...
[DEBUG] Layer 'conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.0.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.4.0.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.0.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.4.0.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.0.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.4.0.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.0.downsample.0' in global_concept_maps...
[DEBUG] Layer 'encoder.4.0.downsample.0' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.1.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.4.1.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.1.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.4.1.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.1.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.4.1.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.2.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.4.2.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.2.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.4.2.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.4.2.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.4.2.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.0.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.5.0.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.0.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.5.0.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.0.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.5.0.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.0.downsample.0' in global_concept_maps...
[DEBUG] Layer 'encoder.5.0.downsample.0' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.1.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.5.1.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.1.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.5.1.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.1.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.5.1.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.2.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.5.2.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.2.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.5.2.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.2.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.5.2.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.3.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.5.3.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.3.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.5.3.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.5.3.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.5.3.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.0.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.6.0.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.0.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.6.0.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.0.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.6.0.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.0.downsample.0' in global_concept_maps...
[DEBUG] Layer 'encoder.6.0.downsample.0' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.1.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.6.1.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.1.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.6.1.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.1.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.6.1.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.2.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.6.2.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.2.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.6.2.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.2.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.6.2.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.3.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.6.3.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.3.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.6.3.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.3.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.6.3.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.4.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.6.4.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.4.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.6.4.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.4.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.6.4.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.5.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.6.5.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.5.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.6.5.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.6.5.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.6.5.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.0.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.7.0.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.0.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.7.0.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.0.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.7.0.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.0.downsample.0' in global_concept_maps...
[DEBUG] Layer 'encoder.7.0.downsample.0' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.1.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.7.1.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.1.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.7.1.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.1.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.7.1.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.2.conv1' in global_concept_maps...
[DEBUG] Layer 'encoder.7.2.conv1' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.2.conv2' in global_concept_maps...
[DEBUG] Layer 'encoder.7.2.conv2' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfe Layer 'encoder.7.2.conv3' in global_concept_maps...
[DEBUG] Layer 'encoder.7.2.conv3' enthält verschachtelte Struktur. Verwende 'weight' von 'Conv2d'.
[DEBUG] Überprüfung abgeschlossen. Alle relevanten Layer sind jetzt Tensoren.
[DEBUG] Intervallindizes initialisiert: OrderedDict([('conv1', (0, 63)), ('encoder.4.0.conv1', (64, 127)), ('encoder.4.0.conv2', (128, 191)), ('encoder.4.0.conv3', (192, 447)), ('encoder.4.0.downsample.0', (448, 703)), ('encoder.4.1.conv1', (704, 767)), ('encoder.4.1.conv2', (768, 831)), ('encoder.4.1.conv3', (832, 1087)), ('encoder.4.2.conv1', (1088, 1151)), ('encoder.4.2.conv2', (1152, 1215)), ('encoder.4.2.conv3', (1216, 1471)), ('encoder.5.0.conv1', (1472, 1599)), ('encoder.5.0.conv2', (1600, 1727)), ('encoder.5.0.conv3', (1728, 2239)), ('encoder.5.0.downsample.0', (2240, 2751)), ('encoder.5.1.conv1', (2752, 2879)), ('encoder.5.1.conv2', (2880, 3007)), ('encoder.5.1.conv3', (3008, 3519)), ('encoder.5.2.conv1', (3520, 3647)), ('encoder.5.2.conv2', (3648, 3775)), ('encoder.5.2.conv3', (3776, 4287)), ('encoder.5.3.conv1', (4288, 4415)), ('encoder.5.3.conv2', (4416, 4543)), ('encoder.5.3.conv3', (4544, 5055)), ('encoder.6.0.conv1', (5056, 5311)), ('encoder.6.0.conv2', (5312, 5567)), ('encoder.6.0.conv3', (5568, 6591)), ('encoder.6.0.downsample.0', (6592, 7615)), ('encoder.6.1.conv1', (7616, 7871)), ('encoder.6.1.conv2', (7872, 8127)), ('encoder.6.1.conv3', (8128, 9151)), ('encoder.6.2.conv1', (9152, 9407)), ('encoder.6.2.conv2', (9408, 9663)), ('encoder.6.2.conv3', (9664, 10687)), ('encoder.6.3.conv1', (10688, 10943)), ('encoder.6.3.conv2', (10944, 11199)), ('encoder.6.3.conv3', (11200, 12223)), ('encoder.6.4.conv1', (12224, 12479)), ('encoder.6.4.conv2', (12480, 12735)), ('encoder.6.4.conv3', (12736, 13759)), ('encoder.6.5.conv1', (13760, 14015)), ('encoder.6.5.conv2', (14016, 14271)), ('encoder.6.5.conv3', (14272, 15295)), ('encoder.7.0.conv1', (15296, 15807)), ('encoder.7.0.conv2', (15808, 16319)), ('encoder.7.0.conv3', (16320, 18367)), ('encoder.7.0.downsample.0', (18368, 20415)), ('encoder.7.1.conv1', (20416, 20927)), ('encoder.7.1.conv2', (20928, 21439)), ('encoder.7.1.conv3', (21440, 23487)), ('encoder.7.2.conv1', (23488, 23999)), ('encoder.7.2.conv2', (24000, 24511)), ('encoder.7.2.conv3', (24512, 26559))])
[DEBUG] Generierte Pruning-Indizes überprüft:
  Layer: conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.4.0.conv3 | Indizes: torch.Size([0])
  Layer: encoder.4.0.downsample.0 | Indizes: torch.Size([0])
  Layer: encoder.4.1.conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.4.1.conv3 | Indizes: torch.Size([0])
  Layer: encoder.4.2.conv1 | Indizes: torch.Size([0])
  Layer: encoder.4.2.conv2 | Indizes: torch.Size([0])
  Layer: encoder.4.2.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.0.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.0.downsample.0 | Indizes: torch.Size([0])
  Layer: encoder.5.1.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.1.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.2.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.2.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.2.conv3 | Indizes: torch.Size([0])
  Layer: encoder.5.3.conv1 | Indizes: torch.Size([0])
  Layer: encoder.5.3.conv2 | Indizes: torch.Size([0])
  Layer: encoder.5.3.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.0.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.0.downsample.0 | Indizes: torch.Size([0])
  Layer: encoder.6.1.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.1.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.2.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.2.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.2.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.3.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.3.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.3.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.4.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.4.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.4.conv3 | Indizes: torch.Size([0])
  Layer: encoder.6.5.conv1 | Indizes: torch.Size([0])
  Layer: encoder.6.5.conv2 | Indizes: torch.Size([0])
  Layer: encoder.6.5.conv3 | Indizes: torch.Size([0])
  Layer: encoder.7.0.conv1 | Indizes: torch.Size([0])
  Layer: encoder.7.0.conv2 | Indizes: torch.Size([0])
  Layer: encoder.7.0.conv3 | Indizes: torch.Size([0])
  Layer: encoder.7.0.downsample.0 | Indizes: torch.Size([0])
  Layer: encoder.7.1.conv1 | Indizes: torch.Size([0])
  Layer: encoder.7.1.conv2 | Indizes: torch.Size([0])
  Layer: encoder.7.1.conv3 | Indizes: torch.Size([0])
  Layer: encoder.7.2.conv1 | Indizes: torch.Size([0])
  Layer: encoder.7.2.conv2 | Indizes: torch.Size([0])
  Layer: encoder.7.2.conv3 | Indizes: torch.Size([0])
[WARNING] 'subsequent_layer_pruning' war bool: True. Konvertiere zu 'Conv2d'.
[DEBUG] Verwende 'subsequent_layer_pruning': Conv2d
[DEBUG] Generiere Pruning-Maske für Layer 'conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.4.2.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.2.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.3.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.3.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.5.3.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.2.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.3.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.3.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.3.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.4.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.4.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.4.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.5.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.5.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.6.5.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.0.downsample.0'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.1.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.1.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.1.conv3'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.2.conv1'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.2.conv2'...
[DEBUG] Generiere Pruning-Maske für Layer 'encoder.7.2.conv3'...
LRP Pruning applied successfully in Round 1.
[WARNING] No pruning hooks to remove.
=== Round 2/3 ===
Applying pruning mask for round 2...
[DEBUG] Starte Anwendung der Pruning-Maske auf 53 Layer...
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.4.2.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.2.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.3.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.3.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.5.3.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.2.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.3.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.3.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.3.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.4.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.4.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.4.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.5.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.5.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.6.5.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.0.downsample.0'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.1.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.1.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.1.conv3'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.2.conv1'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.2.conv2'.
[DEBUG] Pruning-Maske erfolgreich angewendet auf Layer 'encoder.7.2.conv3'.
Training and communication for Round 2...
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Epoch 1/2
----------
Updated learning rate: [0.0002]
Epoch 2/2
----------
Updated learning rate: [0.0002]
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
Starting validation after Round 2...
NaN or Inf detected in logits in batch 0
Logits stats - max: nan, min: nan, mean: nan
Error during training or validation: Invalid logits detected.
