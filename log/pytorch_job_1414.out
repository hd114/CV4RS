Lines that potentially need to be canonized 309
Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created

=== Modellarchitektur ===
conv1: weight shape = torch.Size([64, 10, 7, 7])
encoder.1: weight shape = torch.Size([64])
encoder.1: bias shape = torch.Size([64])
encoder.4.0.conv1: weight shape = torch.Size([64, 64, 1, 1])
encoder.4.0.bn1: weight shape = torch.Size([64])
encoder.4.0.bn1: bias shape = torch.Size([64])
encoder.4.0.conv2: weight shape = torch.Size([64, 64, 3, 3])
encoder.4.0.bn2: weight shape = torch.Size([64])
encoder.4.0.bn2: bias shape = torch.Size([64])
encoder.4.0.conv3: weight shape = torch.Size([256, 64, 1, 1])
encoder.4.0.bn3: weight shape = torch.Size([256])
encoder.4.0.bn3: bias shape = torch.Size([256])
encoder.4.0.downsample.0: weight shape = torch.Size([256, 64, 1, 1])
encoder.4.0.downsample.1: weight shape = torch.Size([256])
encoder.4.0.downsample.1: bias shape = torch.Size([256])
encoder.4.1.conv1: weight shape = torch.Size([64, 256, 1, 1])
encoder.4.1.bn1: weight shape = torch.Size([64])
encoder.4.1.bn1: bias shape = torch.Size([64])
encoder.4.1.conv2: weight shape = torch.Size([64, 64, 3, 3])
encoder.4.1.bn2: weight shape = torch.Size([64])
encoder.4.1.bn2: bias shape = torch.Size([64])
encoder.4.1.conv3: weight shape = torch.Size([256, 64, 1, 1])
encoder.4.1.bn3: weight shape = torch.Size([256])
encoder.4.1.bn3: bias shape = torch.Size([256])
encoder.4.2.conv1: weight shape = torch.Size([64, 256, 1, 1])
encoder.4.2.bn1: weight shape = torch.Size([64])
encoder.4.2.bn1: bias shape = torch.Size([64])
encoder.4.2.conv2: weight shape = torch.Size([64, 64, 3, 3])
encoder.4.2.bn2: weight shape = torch.Size([64])
encoder.4.2.bn2: bias shape = torch.Size([64])
encoder.4.2.conv3: weight shape = torch.Size([256, 64, 1, 1])
encoder.4.2.bn3: weight shape = torch.Size([256])
encoder.4.2.bn3: bias shape = torch.Size([256])
encoder.5.0.conv1: weight shape = torch.Size([128, 256, 1, 1])
encoder.5.0.bn1: weight shape = torch.Size([128])
encoder.5.0.bn1: bias shape = torch.Size([128])
encoder.5.0.conv2: weight shape = torch.Size([128, 128, 3, 3])
encoder.5.0.bn2: weight shape = torch.Size([128])
encoder.5.0.bn2: bias shape = torch.Size([128])
encoder.5.0.conv3: weight shape = torch.Size([512, 128, 1, 1])
encoder.5.0.bn3: weight shape = torch.Size([512])
encoder.5.0.bn3: bias shape = torch.Size([512])
encoder.5.0.downsample.0: weight shape = torch.Size([512, 256, 1, 1])
encoder.5.0.downsample.1: weight shape = torch.Size([512])
encoder.5.0.downsample.1: bias shape = torch.Size([512])
encoder.5.1.conv1: weight shape = torch.Size([128, 512, 1, 1])
encoder.5.1.bn1: weight shape = torch.Size([128])
encoder.5.1.bn1: bias shape = torch.Size([128])
encoder.5.1.conv2: weight shape = torch.Size([128, 128, 3, 3])
encoder.5.1.bn2: weight shape = torch.Size([128])
encoder.5.1.bn2: bias shape = torch.Size([128])
encoder.5.1.conv3: weight shape = torch.Size([512, 128, 1, 1])
encoder.5.1.bn3: weight shape = torch.Size([512])
encoder.5.1.bn3: bias shape = torch.Size([512])
encoder.5.2.conv1: weight shape = torch.Size([128, 512, 1, 1])
encoder.5.2.bn1: weight shape = torch.Size([128])
encoder.5.2.bn1: bias shape = torch.Size([128])
encoder.5.2.conv2: weight shape = torch.Size([128, 128, 3, 3])
encoder.5.2.bn2: weight shape = torch.Size([128])
encoder.5.2.bn2: bias shape = torch.Size([128])
encoder.5.2.conv3: weight shape = torch.Size([512, 128, 1, 1])
encoder.5.2.bn3: weight shape = torch.Size([512])
encoder.5.2.bn3: bias shape = torch.Size([512])
encoder.5.3.conv1: weight shape = torch.Size([128, 512, 1, 1])
encoder.5.3.bn1: weight shape = torch.Size([128])
encoder.5.3.bn1: bias shape = torch.Size([128])
encoder.5.3.conv2: weight shape = torch.Size([128, 128, 3, 3])
encoder.5.3.bn2: weight shape = torch.Size([128])
encoder.5.3.bn2: bias shape = torch.Size([128])
encoder.5.3.conv3: weight shape = torch.Size([512, 128, 1, 1])
encoder.5.3.bn3: weight shape = torch.Size([512])
encoder.5.3.bn3: bias shape = torch.Size([512])
encoder.6.0.conv1: weight shape = torch.Size([256, 512, 1, 1])
encoder.6.0.bn1: weight shape = torch.Size([256])
encoder.6.0.bn1: bias shape = torch.Size([256])
encoder.6.0.conv2: weight shape = torch.Size([256, 256, 3, 3])
encoder.6.0.bn2: weight shape = torch.Size([256])
encoder.6.0.bn2: bias shape = torch.Size([256])
encoder.6.0.conv3: weight shape = torch.Size([1024, 256, 1, 1])
encoder.6.0.bn3: weight shape = torch.Size([1024])
encoder.6.0.bn3: bias shape = torch.Size([1024])
encoder.6.0.downsample.0: weight shape = torch.Size([1024, 512, 1, 1])
encoder.6.0.downsample.1: weight shape = torch.Size([1024])
encoder.6.0.downsample.1: bias shape = torch.Size([1024])
encoder.6.1.conv1: weight shape = torch.Size([256, 1024, 1, 1])
encoder.6.1.bn1: weight shape = torch.Size([256])
encoder.6.1.bn1: bias shape = torch.Size([256])
encoder.6.1.conv2: weight shape = torch.Size([256, 256, 3, 3])
encoder.6.1.bn2: weight shape = torch.Size([256])
encoder.6.1.bn2: bias shape = torch.Size([256])
encoder.6.1.conv3: weight shape = torch.Size([1024, 256, 1, 1])
encoder.6.1.bn3: weight shape = torch.Size([1024])
encoder.6.1.bn3: bias shape = torch.Size([1024])
encoder.6.2.conv1: weight shape = torch.Size([256, 1024, 1, 1])
encoder.6.2.bn1: weight shape = torch.Size([256])
encoder.6.2.bn1: bias shape = torch.Size([256])
encoder.6.2.conv2: weight shape = torch.Size([256, 256, 3, 3])
encoder.6.2.bn2: weight shape = torch.Size([256])
encoder.6.2.bn2: bias shape = torch.Size([256])
encoder.6.2.conv3: weight shape = torch.Size([1024, 256, 1, 1])
encoder.6.2.bn3: weight shape = torch.Size([1024])
encoder.6.2.bn3: bias shape = torch.Size([1024])
encoder.6.3.conv1: weight shape = torch.Size([256, 1024, 1, 1])
encoder.6.3.bn1: weight shape = torch.Size([256])
encoder.6.3.bn1: bias shape = torch.Size([256])
encoder.6.3.conv2: weight shape = torch.Size([256, 256, 3, 3])
encoder.6.3.bn2: weight shape = torch.Size([256])
encoder.6.3.bn2: bias shape = torch.Size([256])
encoder.6.3.conv3: weight shape = torch.Size([1024, 256, 1, 1])
encoder.6.3.bn3: weight shape = torch.Size([1024])
encoder.6.3.bn3: bias shape = torch.Size([1024])
encoder.6.4.conv1: weight shape = torch.Size([256, 1024, 1, 1])
encoder.6.4.bn1: weight shape = torch.Size([256])
encoder.6.4.bn1: bias shape = torch.Size([256])
encoder.6.4.conv2: weight shape = torch.Size([256, 256, 3, 3])
encoder.6.4.bn2: weight shape = torch.Size([256])
encoder.6.4.bn2: bias shape = torch.Size([256])
encoder.6.4.conv3: weight shape = torch.Size([1024, 256, 1, 1])
encoder.6.4.bn3: weight shape = torch.Size([1024])
encoder.6.4.bn3: bias shape = torch.Size([1024])
encoder.6.5.conv1: weight shape = torch.Size([256, 1024, 1, 1])
encoder.6.5.bn1: weight shape = torch.Size([256])
encoder.6.5.bn1: bias shape = torch.Size([256])
encoder.6.5.conv2: weight shape = torch.Size([256, 256, 3, 3])
encoder.6.5.bn2: weight shape = torch.Size([256])
encoder.6.5.bn2: bias shape = torch.Size([256])
encoder.6.5.conv3: weight shape = torch.Size([1024, 256, 1, 1])
encoder.6.5.bn3: weight shape = torch.Size([1024])
encoder.6.5.bn3: bias shape = torch.Size([1024])
encoder.7.0.conv1: weight shape = torch.Size([512, 1024, 1, 1])
encoder.7.0.bn1: weight shape = torch.Size([512])
encoder.7.0.bn1: bias shape = torch.Size([512])
encoder.7.0.conv2: weight shape = torch.Size([512, 512, 3, 3])
encoder.7.0.bn2: weight shape = torch.Size([512])
encoder.7.0.bn2: bias shape = torch.Size([512])
encoder.7.0.conv3: weight shape = torch.Size([2048, 512, 1, 1])
encoder.7.0.bn3: weight shape = torch.Size([2048])
encoder.7.0.bn3: bias shape = torch.Size([2048])
encoder.7.0.downsample.0: weight shape = torch.Size([2048, 1024, 1, 1])
encoder.7.0.downsample.1: weight shape = torch.Size([2048])
encoder.7.0.downsample.1: bias shape = torch.Size([2048])
encoder.7.1.conv1: weight shape = torch.Size([512, 2048, 1, 1])
encoder.7.1.bn1: weight shape = torch.Size([512])
encoder.7.1.bn1: bias shape = torch.Size([512])
encoder.7.1.conv2: weight shape = torch.Size([512, 512, 3, 3])
encoder.7.1.bn2: weight shape = torch.Size([512])
encoder.7.1.bn2: bias shape = torch.Size([512])
encoder.7.1.conv3: weight shape = torch.Size([2048, 512, 1, 1])
encoder.7.1.bn3: weight shape = torch.Size([2048])
encoder.7.1.bn3: bias shape = torch.Size([2048])
encoder.7.2.conv1: weight shape = torch.Size([512, 2048, 1, 1])
encoder.7.2.bn1: weight shape = torch.Size([512])
encoder.7.2.bn1: bias shape = torch.Size([512])
encoder.7.2.conv2: weight shape = torch.Size([512, 512, 3, 3])
encoder.7.2.bn2: weight shape = torch.Size([512])
encoder.7.2.bn2: bias shape = torch.Size([512])
encoder.7.2.conv3: weight shape = torch.Size([2048, 512, 1, 1])
encoder.7.2.bn3: weight shape = torch.Size([2048])
encoder.7.2.bn3: bias shape = torch.Size([2048])
FC: weight shape = torch.Size([19, 2048])
FC: bias shape = torch.Size([19])

=== State Dict Parameter ===
