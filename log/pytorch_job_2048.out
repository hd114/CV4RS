Lines that potentially need to be canonized 309
Using device: cuda:0
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    8209 filtered patches indexed
    8209 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7150 filtered patches indexed
    7150 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 15549 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    4263 filtered patches indexed
    4263 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    7180 filtered patches indexed
    7180 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 13683 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    3248 filtered patches indexed
    3248 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Pre-filtered 59999 patches based on country and season (split ignored)
Loading BEN data for test...
    119825 patches indexed
    15720 filtered patches indexed
    15720 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Initializing LRP Pruning...
LRP initialized successfully.
=== Round 1/5 ===
Training and communication for Round 1...
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
[INFO] Performing LRP Pruning in Round 1...
[INFO] Computing LRP pruning mask...
Erstelle DataLoader f√ºr Land: Finland
Pre-filtered 30767 patches based on country and season (split ignored)
Loading BEN data for train...
    237871 patches indexed
    14805 filtered patches indexed
    14805 patches indexed considering max_len
Merged metadata with snow/cloud metadata
Loaded 549488 labels
Loaded 549488 keys
Loaded mapping created
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([16])
Input shape before relevance computation: torch.Size([16, 10, 1, 16])
Model output shape: torch.Size([16, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([16, 19]) to 16 for one-hot encoding.
Targets shape: torch.Size([16])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([16, 19])
Relevance computation successful. Relevance shape: torch.Size([16, 10, 1, 16])
Input shape before model forward: torch.Size([5])
Input shape before relevance computation: torch.Size([5, 10, 1, 5])
Model output shape: torch.Size([5, 19])
Converting targets from torch.float64 to torch.long
Reducing targets shape from torch.Size([5, 19]) to 5 for one-hot encoding.
Targets shape: torch.Size([5])
Eye matrix shape: torch.Size([19, 19])
Output shape: torch.Size([5, 19])
Relevance computation successful. Relevance shape: torch.Size([5, 10, 1, 5])
[INFO] Relevance maps computed for 53 layers.
[DEBUG] Layer: conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.0.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.0.downsample.0
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.1.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.1.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.1.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.4.2.conv1
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.2.conv2
  Mask shape: torch.Size([64]), Non-zero elements: 64
[DEBUG] Layer: encoder.4.2.conv3
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.5.0.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.0.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.0.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.0.downsample.0
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.1.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.1.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.1.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.2.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.2.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.2.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.5.3.conv1
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.3.conv2
  Mask shape: torch.Size([128]), Non-zero elements: 128
[DEBUG] Layer: encoder.5.3.conv3
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.6.0.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.0.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.0.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.0.downsample.0
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.1.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.1.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.1.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.2.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.2.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.2.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.3.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.3.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.3.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.4.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.4.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.4.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.6.5.conv1
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.5.conv2
  Mask shape: torch.Size([256]), Non-zero elements: 256
[DEBUG] Layer: encoder.6.5.conv3
  Mask shape: torch.Size([1024]), Non-zero elements: 1024
[DEBUG] Layer: encoder.7.0.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.0.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.0.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.0.downsample.0
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.1.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.1.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.1.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 2048
[DEBUG] Layer: encoder.7.2.conv1
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.2.conv2
  Mask shape: torch.Size([512]), Non-zero elements: 512
[DEBUG] Layer: encoder.7.2.conv3
  Mask shape: torch.Size([2048]), Non-zero elements: 1835
Layer: conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.0529e+17, 4.2269e+17, 7.6661e+17, 4.6773e+17, 1.5379e+17, 3.6967e+17,
        5.8021e+17, 2.1197e+17, 3.1957e+17, 2.6826e+17, 3.1428e+17, 3.3486e+17,
        5.6384e+17, 4.5546e+17, 3.5575e+17, 4.7456e+17, 6.2424e+17, 4.2471e+17,
        4.8655e+17, 7.2450e+17, 4.6875e+17, 1.6990e+17, 2.7591e+17, 5.2187e+17,
        2.2664e+17, 4.9400e+17, 3.3683e+17, 5.7885e+17, 3.2158e+17, 2.1429e+17,
        3.9735e+17, 5.3235e+17, 4.1727e+17, 3.2876e+17, 6.0918e+17, 6.9718e+17,
        3.1859e+17, 5.4654e+17, 6.8489e+17, 4.7180e+17, 3.2686e+17, 3.5484e+17,
        2.9590e+17, 1.8833e+17, 3.3431e+17, 6.2901e+17, 3.3590e+17, 3.7412e+17,
        3.8847e+17, 7.7980e+17, 2.9700e+17, 8.2598e+17, 5.3677e+17, 3.6874e+17,
        8.6870e+17, 2.2803e+17, 2.6131e+17, 3.0357e+17, 2.1887e+17, 4.1600e+17,
        6.6930e+17, 3.0135e+17, 7.1175e+17, 3.5953e+17])
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.8480e+17, 2.5297e+17, 1.4122e+17, 1.4026e+17, 2.1099e+17, 1.6120e+17,
        1.7889e+17, 2.9935e+17, 1.0793e+17, 2.3407e+17, 1.8383e+17, 1.1580e+17,
        2.7824e+17, 1.7451e+17, 1.1798e+17, 1.7156e+17, 1.6083e+17, 1.8183e+17,
        2.1141e+17, 2.2409e+17, 4.8960e+17, 2.4975e+17, 3.6749e+17, 1.2552e+17,
        6.7577e+16, 1.2538e+17, 9.8315e+16, 1.6745e+17, 1.6804e+17, 2.5147e+17,
        2.0840e+17, 1.3898e+17, 2.9593e+17, 2.3599e+17, 9.5932e+16, 1.2489e+17,
        2.2664e+17, 2.3020e+17, 4.1493e+17, 2.7361e+17, 1.6085e+17, 2.5032e+17,
        2.1974e+17, 1.0284e+17, 3.4021e+17, 1.3642e+17, 2.4059e+17, 1.6189e+17,
        1.8683e+17, 3.9224e+17, 9.5484e+16, 1.3339e+17, 2.2703e+17, 2.0783e+17,
        4.9720e+17, 2.7672e+17, 2.4496e+17, 2.0559e+17, 1.3112e+17, 7.5310e+16,
        1.6669e+17, 1.8925e+17, 4.1396e+17, 1.4661e+17])
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.8343e+17, 1.7383e+17, 3.6162e+17, 1.2021e+17, 1.2929e+17, 2.0108e+17,
        1.7230e+17, 5.6637e+17, 3.3676e+17, 1.8838e+17, 1.9787e+17, 1.1906e+17,
        1.9169e+17, 1.3247e+17, 1.9119e+17, 3.4798e+17, 1.9183e+17, 3.2722e+17,
        1.6592e+17, 2.2887e+17, 4.4826e+17, 2.6234e+17, 1.9250e+17, 2.1470e+17,
        2.6861e+17, 2.3616e+17, 1.8031e+17, 2.7847e+17, 1.8893e+17, 2.5912e+17,
        1.5459e+17, 2.6645e+17, 2.1625e+17, 1.8021e+17, 1.7761e+17, 3.2574e+17,
        2.4741e+17, 2.8085e+17, 3.5382e+17, 2.2041e+17, 2.7821e+17, 2.1459e+17,
        3.3373e+17, 2.1250e+17, 1.2164e+17, 2.1572e+17, 4.1010e+17, 2.8317e+17,
        1.9668e+17, 2.8379e+17, 2.7241e+17, 1.7285e+17, 1.3830e+17, 7.7900e+16,
        1.2911e+17, 3.3457e+17, 3.3419e+17, 1.6468e+17, 2.9305e+17, 2.5136e+17,
        1.5617e+17, 3.4631e+17, 1.5021e+17, 4.9420e+17])
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.1942e+16, 2.0347e+16, 3.9052e+15, 3.7234e+15, 7.4907e+16, 2.2246e+15,
        2.4000e+15, 1.6575e+16, 5.9231e+15, 6.3561e+16, 2.9957e+15, 2.6114e+16,
        1.1587e+16, 1.8681e+16, 1.9128e+15, 1.9573e+16, 2.1615e+15, 1.1402e+16,
        1.7442e+15, 7.1072e+15, 1.5930e+14, 1.0993e+16, 3.3040e+16, 2.9377e+16,
        1.2042e+15, 1.5305e+15, 1.1439e+16, 1.6923e+15, 2.2795e+16, 6.6171e+16,
        4.0259e+16, 2.9318e+15, 3.3985e+16, 6.0232e+15, 2.4524e+17, 3.3559e+16,
        3.6573e+15, 5.6008e+15, 1.9756e+15, 1.9634e+16, 1.4644e+16, 1.6104e+15,
        2.2285e+15, 1.9462e+16, 1.0686e+16, 4.7609e+17, 1.1545e+15, 1.7822e+15,
        4.0391e+15, 9.4837e+14, 2.4295e+15, 1.5059e+16, 7.5548e+16, 5.6196e+16,
        5.5023e+16, 4.0747e+15, 6.5042e+16, 5.5039e+15, 5.9837e+15, 2.8223e+16,
        9.0039e+16, 2.6142e+15, 2.8556e+16, 2.8103e+16, 3.0073e+15, 4.0586e+15,
        9.0882e+15, 1.4410e+16, 7.7077e+15, 1.3642e+16, 9.7334e+15, 1.4365e+16,
        6.6682e+15, 1.7948e+15, 1.6714e+15, 5.6350e+15, 2.6252e+15, 3.8691e+16,
        4.0911e+15, 2.3393e+15, 8.9967e+15, 1.1982e+16, 2.3563e+16, 2.0298e+15,
        2.0764e+15, 1.1584e+15, 3.7029e+15, 4.1583e+15, 4.2531e+16, 5.0732e+16,
        1.5996e+15, 3.0525e+16, 1.1842e+15, 1.4579e+15, 2.0250e+16, 1.5259e+15,
        7.8359e+16, 1.1589e+15, 3.7961e+16, 9.0819e+14, 5.7790e+16, 1.1430e+15,
        1.9823e+15, 6.0298e+15, 2.0272e+16, 2.2061e+15, 1.0000e+15, 3.3662e+15,
        1.9444e+16, 1.2570e+16, 2.3609e+15, 2.6959e+16, 2.9044e+15, 1.6582e+16,
        8.9510e+15, 9.3275e+15, 2.5103e+15, 3.8074e+15, 7.5598e+15, 2.5809e+16,
        7.2076e+14, 3.5423e+16, 1.3227e+16, 1.3729e+15, 5.0200e+15, 1.4098e+15,
        3.8440e+15, 1.5504e+16, 2.9796e+15, 3.4318e+15, 5.2360e+15, 7.6745e+15,
        1.6140e+16, 1.4273e+16, 2.8709e+15, 7.2106e+15, 2.4783e+15, 2.9980e+15,
        7.0074e+15, 1.4417e+15, 4.0324e+16, 4.7841e+15, 6.8586e+16, 2.3386e+14,
        1.6015e+16, 1.8443e+15, 9.3520e+14, 6.4174e+15, 7.8988e+15, 9.3343e+15,
        1.4547e+15, 2.2927e+15, 1.3010e+16, 3.2884e+15, 2.9404e+16, 3.6042e+16,
        4.5806e+15, 2.0474e+15, 8.9144e+15, 1.3813e+15, 1.7172e+15, 4.0510e+15,
        5.3261e+15, 1.1132e+16, 6.8804e+15, 3.7728e+15, 3.3185e+16, 4.6328e+15,
        1.0452e+15, 6.4857e+16, 1.2240e+15, 6.5899e+15, 5.2654e+15, 4.3373e+15,
        9.4130e+15, 2.9301e+15, 2.6732e+15, 3.3537e+15, 1.3663e+16, 2.3492e+15,
        8.0187e+15, 3.7316e+15, 1.2517e+16, 2.9494e+16, 7.6373e+15, 6.2377e+15,
        6.1547e+16, 5.4440e+16, 1.0863e+15, 7.6257e+15, 2.3162e+16, 2.8101e+15,
        3.6652e+15, 4.8801e+14, 2.0630e+15, 2.9274e+15, 1.0986e+16, 2.8644e+16,
        4.9388e+14, 2.2168e+15, 7.7136e+15, 7.6922e+15, 8.7252e+14, 2.6276e+15,
        1.8276e+16, 1.4943e+15, 1.6461e+15, 6.6419e+15, 3.1260e+15, 4.9749e+15,
        1.6982e+15, 7.6852e+15, 1.8548e+16, 2.2474e+15, 1.0547e+16, 4.5280e+15,
        1.1747e+16, 4.7228e+15, 2.1255e+16, 5.2730e+16, 1.4081e+15, 1.3959e+15,
        1.1790e+15, 1.2406e+15, 1.1167e+16, 2.9183e+17, 1.2480e+16, 1.9715e+16,
        2.0222e+15, 1.0662e+17, 7.3547e+15, 2.4747e+16, 3.9313e+15, 4.3802e+15,
        2.1050e+15, 6.8796e+15, 2.6306e+15, 1.5328e+15, 1.5528e+16, 1.4286e+16,
        1.3604e+15, 3.2016e+15, 2.2735e+15, 1.1679e+15, 7.2502e+15, 5.4159e+14,
        5.4505e+14, 4.9081e+14, 7.3912e+15, 1.3130e+16, 3.4165e+15, 3.2269e+15,
        4.0927e+16, 2.7761e+15, 9.7607e+15, 8.2946e+15])
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.9441e+16, 2.2929e+16, 3.4122e+15, 2.0212e+15, 7.3025e+16, 3.8324e+15,
        3.0919e+15, 1.3058e+16, 1.0016e+16, 6.2715e+16, 1.9400e+15, 2.4623e+16,
        1.4179e+16, 1.8663e+16, 8.2976e+15, 2.2590e+16, 5.1456e+15, 1.3476e+16,
        2.4840e+15, 1.1915e+16, 1.8456e+15, 1.5071e+16, 3.7973e+16, 2.5731e+16,
        2.4045e+15, 3.1294e+15, 7.5726e+15, 2.2919e+15, 2.2761e+16, 7.1596e+16,
        4.4019e+16, 4.1036e+15, 2.8752e+16, 4.2821e+15, 2.4959e+17, 3.3514e+16,
        8.2006e+15, 6.5744e+15, 1.6715e+15, 2.6110e+16, 1.5308e+16, 2.3363e+15,
        8.1808e+15, 1.2807e+16, 1.1062e+16, 4.7916e+17, 1.8205e+15, 1.3010e+15,
        3.0288e+15, 5.1468e+15, 4.6807e+15, 2.3808e+16, 7.9575e+16, 5.1235e+16,
        5.1619e+16, 9.3716e+15, 5.8857e+16, 4.0973e+15, 1.0591e+16, 3.4734e+16,
        8.2227e+16, 1.1089e+15, 2.6775e+16, 2.8813e+16, 1.0015e+15, 5.9335e+15,
        4.7216e+15, 1.8444e+16, 8.2997e+15, 8.2865e+15, 1.4135e+16, 1.6505e+16,
        8.4558e+15, 2.5939e+15, 1.3259e+16, 4.5665e+15, 5.3019e+15, 3.8647e+16,
        2.4608e+15, 8.8315e+15, 9.2075e+15, 2.3512e+16, 2.7362e+16, 1.6637e+15,
        8.6274e+15, 1.6177e+15, 2.3738e+15, 5.3964e+15, 5.6515e+16, 5.1570e+16,
        8.5886e+14, 3.7971e+16, 2.0489e+15, 3.1022e+15, 3.3352e+16, 3.7815e+15,
        8.0654e+16, 3.8900e+15, 3.1937e+16, 1.7946e+15, 4.8429e+16, 4.3080e+15,
        4.5855e+15, 1.0731e+16, 3.8455e+16, 6.1471e+15, 2.6921e+15, 2.0322e+15,
        1.3742e+16, 1.4842e+16, 2.5762e+15, 3.0202e+16, 1.7231e+15, 1.6049e+16,
        9.5808e+15, 1.1513e+16, 8.7143e+15, 3.2188e+15, 5.9487e+15, 2.6067e+16,
        5.7232e+15, 3.5096e+16, 7.0464e+15, 5.2536e+15, 6.0673e+15, 2.4275e+15,
        4.7565e+15, 1.1595e+16, 2.0931e+15, 9.7955e+15, 6.5057e+15, 6.4048e+15,
        1.7543e+16, 1.5929e+16, 3.0513e+15, 8.8000e+15, 2.8721e+15, 2.6193e+15,
        9.7070e+15, 5.2907e+15, 3.7754e+16, 2.3388e+15, 7.4752e+16, 2.1819e+15,
        1.4657e+16, 1.8319e+15, 8.8750e+14, 4.1552e+15, 1.9222e+16, 6.6617e+15,
        6.9017e+15, 8.2307e+14, 3.5674e+15, 8.3629e+15, 2.9494e+16, 4.0087e+16,
        4.3175e+15, 2.1284e+15, 1.0452e+16, 5.0336e+15, 4.1156e+15, 4.9555e+15,
        4.1799e+15, 2.3846e+16, 8.5814e+15, 2.6020e+15, 4.0486e+16, 9.2646e+15,
        2.8213e+15, 6.8869e+16, 2.5196e+15, 7.6622e+15, 3.2052e+15, 3.8249e+15,
        1.3950e+16, 4.1544e+15, 2.7017e+15, 5.5976e+15, 5.3359e+15, 2.0670e+15,
        1.5980e+16, 4.9220e+15, 2.1968e+16, 2.3596e+16, 6.7288e+15, 8.6744e+15,
        5.8593e+16, 5.1850e+16, 6.6347e+15, 5.3237e+15, 2.3753e+16, 5.6948e+15,
        2.6003e+15, 2.1152e+15, 2.9040e+15, 4.7737e+15, 1.5524e+16, 3.2435e+16,
        3.0856e+15, 1.7353e+15, 1.1582e+16, 7.3886e+15, 1.7896e+15, 1.4910e+15,
        1.2695e+16, 1.2940e+16, 1.4591e+15, 9.7338e+15, 6.6900e+15, 4.1866e+15,
        6.3338e+15, 3.3509e+15, 6.4861e+15, 4.4839e+15, 9.2778e+15, 1.1309e+16,
        1.4381e+16, 4.5661e+15, 2.0158e+16, 5.6200e+16, 1.1358e+15, 2.7010e+15,
        2.5125e+15, 3.6742e+15, 1.1608e+16, 2.9617e+17, 9.5673e+15, 1.4460e+16,
        3.5676e+15, 1.0158e+17, 1.8743e+15, 2.2444e+16, 7.0016e+15, 5.3444e+15,
        4.3583e+15, 9.6703e+15, 4.6307e+15, 8.5018e+15, 8.1579e+15, 1.1720e+16,
        3.9105e+15, 6.6333e+15, 1.1519e+15, 6.7921e+15, 1.4204e+16, 5.4641e+15,
        2.9545e+15, 2.2332e+15, 3.8137e+15, 1.4637e+16, 8.7547e+15, 7.5601e+15,
        4.1743e+16, 4.4750e+15, 6.1935e+15, 9.0620e+15])
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1147e+16, 1.3953e+16, 5.3123e+15, 2.1795e+16, 1.4158e+16, 6.2738e+15,
        2.3567e+15, 1.7561e+16, 8.9927e+15, 8.9362e+15, 1.0838e+16, 6.4656e+15,
        1.2873e+16, 3.4140e+15, 5.9303e+15, 6.6192e+15, 2.7698e+15, 1.1270e+16,
        1.0458e+16, 2.1608e+16, 1.4185e+16, 8.7450e+15, 7.5763e+15, 1.5867e+16,
        1.3691e+16, 3.8036e+15, 3.0629e+15, 2.1242e+16, 1.7171e+16, 5.5972e+15,
        6.1285e+15, 1.2734e+16, 6.8213e+15, 4.7461e+15, 1.0324e+16, 1.8008e+16,
        8.2088e+15, 8.6735e+15, 5.3082e+15, 7.7141e+15, 7.8860e+15, 3.7434e+15,
        9.0361e+15, 3.6198e+15, 7.8549e+15, 3.2913e+15, 8.4172e+15, 9.1175e+15,
        4.4509e+15, 3.2592e+15, 3.3294e+15, 1.1515e+16, 4.8540e+15, 3.2302e+15,
        5.4236e+15, 5.2786e+15, 9.0744e+15, 5.4516e+15, 1.2177e+16, 6.9271e+15,
        8.6337e+15, 6.3197e+15, 1.5843e+16, 6.7232e+15])
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1484e+16, 1.0942e+16, 8.1858e+15, 8.4865e+15, 1.1335e+16, 1.0643e+16,
        1.0095e+16, 2.7340e+16, 1.6617e+16, 6.1878e+15, 1.7177e+16, 4.0883e+15,
        2.8842e+15, 3.4187e+15, 4.1373e+15, 7.7609e+15, 1.8274e+16, 4.0868e+15,
        3.3143e+15, 2.2839e+16, 1.0372e+16, 9.0295e+15, 7.4384e+15, 7.0740e+15,
        1.1249e+16, 1.1403e+16, 9.4050e+15, 6.1171e+15, 6.8475e+15, 2.7133e+16,
        1.1188e+16, 8.9403e+15, 8.2184e+15, 1.2249e+16, 4.4700e+15, 1.0420e+16,
        2.6522e+16, 1.5991e+16, 3.6081e+15, 2.3100e+16, 1.1710e+16, 1.6743e+16,
        1.3977e+16, 8.0207e+15, 1.3499e+16, 7.6787e+15, 6.5687e+15, 1.7331e+16,
        2.2767e+16, 5.7056e+15, 1.3096e+16, 6.1240e+15, 1.8083e+16, 8.1745e+15,
        6.9155e+15, 3.5137e+15, 1.2191e+16, 1.3739e+16, 5.5476e+15, 9.1765e+15,
        1.3089e+16, 1.2069e+16, 2.5701e+16, 2.1672e+16])
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.1410e+15, 5.8409e+14, 1.6307e+15, 2.9272e+14, 8.1874e+14, 9.9713e+13,
        7.5112e+14, 1.2791e+15, 5.9459e+14, 4.1534e+14, 3.3046e+14, 7.1680e+14,
        1.6756e+14, 7.4798e+14, 2.0655e+13, 4.1026e+14, 4.3301e+14, 1.3280e+14,
        1.7335e+14, 8.6429e+13, 6.9640e+13, 4.1815e+14, 1.9227e+14, 4.8568e+14,
        1.4457e+15, 3.8209e+14, 3.8182e+14, 1.5148e+14, 6.0245e+14, 1.2161e+15,
        4.1480e+14, 3.7959e+13, 1.1784e+15, 3.5511e+14, 2.6144e+14, 8.9867e+13,
        3.1826e+13, 2.8206e+14, 4.6883e+14, 4.6196e+14, 5.2417e+14, 6.4885e+14,
        1.9718e+14, 1.5616e+14, 9.3257e+13, 7.4805e+13, 4.4603e+14, 3.0371e+14,
        8.2126e+13, 2.0902e+14, 2.3068e+13, 6.0075e+14, 1.3987e+14, 2.1895e+14,
        9.1075e+14, 1.9492e+14, 5.7427e+14, 4.8493e+14, 1.0875e+14, 4.3546e+14,
        6.3656e+14, 5.0086e+14, 5.4070e+14, 2.8220e+14, 1.9216e+14, 1.0387e+15,
        2.5373e+15, 5.4206e+14, 1.6453e+14, 3.1692e+14, 2.5385e+14, 1.2733e+14,
        3.6586e+14, 1.2389e+15, 3.8538e+14, 6.6798e+14, 3.8352e+13, 9.1329e+13,
        2.2731e+14, 4.5782e+14, 6.7809e+14, 2.7671e+14, 7.4376e+14, 3.1782e+14,
        3.1037e+14, 8.9657e+13, 2.8309e+14, 1.2450e+15, 1.0097e+16, 3.2323e+14,
        3.7335e+14, 1.8580e+14, 3.0254e+14, 1.1353e+13, 5.8156e+15, 9.4708e+14,
        1.5316e+14, 1.8122e+14, 1.1977e+14, 8.1184e+13, 3.5606e+15, 3.1405e+14,
        3.1190e+14, 1.6338e+15, 2.4440e+16, 5.6031e+14, 1.7940e+14, 4.2398e+14,
        4.3889e+14, 3.0185e+14, 3.4706e+14, 3.5262e+14, 2.4218e+13, 6.5857e+14,
        2.1713e+14, 3.8149e+14, 4.3607e+14, 3.1356e+14, 1.7432e+14, 1.0287e+15,
        9.2090e+13, 8.1586e+14, 7.0622e+14, 5.1338e+14, 1.3986e+15, 5.2434e+14,
        3.7682e+14, 1.7120e+14, 1.9457e+14, 4.3802e+14, 7.4132e+14, 5.9462e+14,
        2.7499e+14, 1.7884e+14, 5.6323e+14, 2.7294e+14, 9.5695e+14, 2.4945e+14,
        7.7340e+13, 4.2074e+14, 4.2738e+14, 1.7199e+14, 2.7086e+14, 7.9393e+14,
        5.8848e+14, 2.1911e+14, 3.6497e+14, 8.8341e+13, 1.3979e+14, 7.1045e+14,
        9.9225e+13, 6.6051e+14, 8.0300e+14, 1.4489e+14, 2.7215e+14, 2.5393e+15,
        7.6145e+14, 1.4969e+14, 1.5907e+14, 2.4607e+14, 1.6061e+14, 3.8988e+14,
        7.6281e+13, 1.6714e+16, 1.2615e+14, 5.3874e+13, 2.4244e+15, 6.8787e+13,
        2.8020e+14, 1.7488e+14, 1.1390e+14, 4.9533e+14, 4.3275e+14, 9.8641e+14,
        1.4083e+14, 1.8865e+14, 1.0586e+13, 4.9644e+13, 2.6466e+14, 5.2735e+14,
        5.8378e+15, 3.4935e+14, 7.5013e+14, 2.7564e+14, 3.7357e+14, 1.0825e+14,
        3.3980e+14, 2.1831e+14, 1.1187e+14, 6.8225e+13, 2.1891e+14, 1.8057e+14,
        6.4688e+14, 3.8544e+13, 4.7865e+14, 3.1492e+14, 1.5447e+14, 4.5792e+14,
        1.8469e+14, 2.6655e+13, 3.0231e+14, 5.0591e+15, 6.2805e+14, 1.1662e+14,
        3.1427e+14, 5.2994e+13, 7.4840e+14, 2.1182e+15, 9.0928e+14, 3.5769e+14,
        1.4794e+14, 2.5551e+14, 3.1606e+16, 1.4189e+14, 5.9079e+14, 1.3683e+14,
        2.0169e+14, 4.7237e+13, 2.9082e+14, 5.2412e+14, 9.7961e+13, 1.8831e+14,
        2.5158e+14, 5.0513e+14, 3.0691e+14, 1.1876e+14, 3.0418e+14, 4.0136e+14,
        4.5312e+14, 5.9754e+14, 5.7784e+14, 2.0297e+14, 2.4151e+14, 4.2252e+14,
        3.4212e+14, 3.9195e+14, 2.7329e+14, 5.6437e+14, 1.1107e+16, 4.5439e+14,
        9.6449e+14, 1.6999e+14, 2.5556e+15, 4.8050e+14, 3.9259e+14, 3.9738e+14,
        3.5823e+14, 5.6200e+13, 1.1605e+14, 1.7916e+14, 7.2012e+13, 3.9339e+14,
        3.7805e+14, 3.9692e+13, 6.4757e+14, 5.1608e+14])
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3993e+15, 6.5764e+14, 4.1223e+14, 9.7380e+14, 1.3021e+15, 7.1484e+14,
        4.2797e+14, 2.5684e+14, 2.0932e+14, 1.5478e+14, 7.5154e+14, 2.6672e+15,
        5.7806e+14, 1.0157e+15, 2.3158e+15, 5.4041e+14, 1.1815e+15, 2.2529e+15,
        6.3638e+14, 1.2755e+15, 6.9026e+14, 6.1377e+14, 1.0984e+15, 9.4297e+14,
        3.6737e+14, 9.6248e+14, 7.3265e+14, 1.2962e+15, 4.3304e+14, 2.6559e+14,
        3.7063e+14, 1.9036e+15, 1.1594e+15, 5.7607e+14, 1.6516e+15, 9.1782e+14,
        8.4069e+14, 2.4704e+14, 8.3644e+14, 1.2351e+15, 1.3506e+15, 7.6912e+14,
        1.0826e+15, 1.5269e+15, 7.7167e+14, 8.8030e+14, 3.3325e+14, 6.8296e+14,
        9.2718e+14, 8.3567e+14, 4.7974e+14, 1.9776e+15, 6.1733e+14, 3.0678e+14,
        9.3606e+14, 8.9395e+14, 9.4981e+14, 2.8228e+14, 5.1568e+14, 9.6417e+14,
        3.8698e+14, 1.7616e+15, 1.2368e+15, 9.7735e+14])
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.0645e+14, 1.2773e+15, 1.2139e+15, 9.1366e+14, 1.7407e+15, 7.6688e+14,
        1.4919e+15, 4.1298e+14, 1.2788e+15, 2.3140e+15, 1.9759e+15, 3.8471e+14,
        2.4427e+14, 3.0499e+14, 6.6905e+14, 1.0786e+15, 9.6851e+14, 3.4030e+14,
        2.1408e+15, 1.5647e+15, 6.8792e+14, 9.8448e+14, 5.9995e+14, 9.5170e+14,
        5.9663e+14, 3.1059e+14, 6.6349e+14, 1.6013e+15, 4.8507e+14, 5.0822e+14,
        9.8928e+14, 1.3058e+15, 5.9200e+14, 1.3631e+15, 1.2982e+15, 2.8724e+14,
        1.2207e+15, 4.5391e+14, 1.9457e+15, 1.1708e+15, 1.6885e+14, 5.0551e+14,
        1.0545e+15, 1.3196e+15, 6.4535e+14, 4.4658e+14, 8.4955e+14, 1.2377e+15,
        1.8637e+15, 2.3294e+15, 2.5813e+15, 6.2930e+14, 6.3083e+14, 2.9855e+15,
        1.1732e+15, 3.3312e+14, 3.7742e+14, 7.6622e+14, 9.5505e+14, 7.2863e+14,
        4.0517e+14, 5.8324e+14, 3.4741e+14, 1.0663e+15])
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.4545e+14, 2.6466e+14, 7.4626e+13, 4.5972e+13, 1.8990e+13, 2.4801e+14,
        4.4698e+13, 2.7719e+13, 1.9394e+14, 1.9592e+14, 7.0474e+13, 3.2627e+14,
        1.9470e+14, 1.3473e+14, 3.4008e+12, 1.3509e+14, 3.7880e+13, 6.1605e+13,
        5.6927e+13, 1.3814e+14, 1.4864e+14, 1.0089e+13, 1.2023e+14, 4.0795e+14,
        6.6663e+14, 2.1875e+14, 1.2067e+14, 2.6576e+13, 1.1442e+14, 3.7332e+13,
        4.3079e+14, 2.1498e+12, 2.9808e+13, 2.0852e+14, 4.3405e+14, 3.0956e+13,
        4.2038e+12, 1.3459e+14, 4.3899e+13, 4.8156e+12, 1.0648e+14, 5.3415e+13,
        4.9540e+14, 1.2964e+14, 2.6756e+13, 1.0175e+13, 3.2519e+14, 1.5986e+14,
        1.6718e+13, 4.9977e+13, 1.2031e+13, 7.4409e+13, 1.8447e+13, 9.7692e+13,
        2.5417e+14, 1.6376e+13, 9.7986e+13, 1.3599e+14, 1.5687e+14, 4.3250e+14,
        2.7164e+14, 1.0190e+14, 7.4315e+13, 3.6484e+14, 7.9549e+13, 2.6197e+13,
        7.4057e+13, 3.6427e+14, 1.0135e+13, 9.2362e+13, 7.6496e+13, 1.6723e+13,
        2.1456e+13, 2.1596e+13, 1.3551e+14, 2.2609e+14, 8.8477e+12, 1.2384e+14,
        7.5293e+13, 5.7015e+13, 2.5256e+13, 2.7998e+14, 1.8275e+14, 1.0603e+14,
        1.0341e+14, 2.0954e+13, 2.0108e+13, 1.4066e+13, 7.6531e+13, 1.5855e+14,
        1.0398e+14, 9.8502e+13, 1.5459e+13, 1.2350e+13, 1.8719e+15, 9.1228e+13,
        2.3803e+14, 7.7414e+12, 3.3174e+14, 2.6451e+13, 1.6280e+14, 6.0550e+13,
        2.1780e+14, 3.8707e+13, 1.2180e+14, 6.6685e+13, 2.0620e+14, 1.8964e+13,
        1.4404e+14, 1.9432e+13, 4.2725e+13, 8.1680e+13, 1.9580e+13, 9.0884e+13,
        1.4092e+14, 1.1242e+14, 8.6634e+13, 1.1523e+14, 2.5400e+13, 3.8776e+14,
        1.7099e+14, 4.5263e+14, 3.4151e+14, 1.9578e+14, 2.0171e+14, 1.5716e+14,
        7.4246e+13, 6.2094e+13, 1.1680e+14, 2.2213e+14, 1.0964e+15, 1.5000e+14,
        3.1230e+14, 1.3990e+14, 2.8197e+13, 1.9721e+14, 4.7006e+14, 4.1168e+13,
        2.7406e+13, 1.5338e+14, 1.3826e+14, 6.7182e+13, 8.6712e+14, 3.3914e+13,
        3.9106e+14, 2.1237e+14, 3.3342e+13, 3.0580e+12, 3.2526e+13, 5.8833e+13,
        4.4014e+13, 1.4994e+14, 5.5599e+13, 1.4334e+13, 2.3972e+14, 1.8529e+14,
        2.1483e+13, 3.6697e+12, 3.4842e+14, 1.7535e+13, 8.7715e+13, 3.2928e+14,
        8.0062e+13, 1.2116e+14, 2.3267e+14, 1.3338e+14, 1.8598e+15, 2.5936e+13,
        3.2467e+14, 8.5745e+13, 3.2961e+13, 1.1575e+13, 6.3690e+13, 2.7652e+13,
        4.4504e+13, 2.0706e+13, 1.4323e+13, 2.7181e+13, 1.1721e+14, 2.0506e+14,
        1.4375e+13, 2.0379e+14, 4.6314e+14, 1.6482e+14, 2.0894e+14, 1.1260e+14,
        1.6617e+14, 1.5540e+14, 8.4093e+13, 2.9325e+13, 1.6946e+14, 1.9858e+14,
        2.9629e+14, 2.5564e+14, 2.1884e+14, 8.3354e+12, 7.8469e+13, 5.5908e+13,
        1.0780e+13, 1.6800e+13, 3.2263e+14, 1.7651e+13, 1.4648e+14, 6.8022e+12,
        3.3881e+13, 4.2967e+12, 3.3150e+14, 1.6395e+14, 6.1085e+13, 1.7588e+13,
        7.8811e+12, 1.0450e+14, 1.8425e+14, 1.6136e+14, 2.3407e+14, 3.0005e+14,
        7.1410e+13, 6.6888e+13, 6.1693e+13, 1.7487e+14, 5.6988e+13, 5.5972e+13,
        2.2647e+13, 8.6310e+13, 2.4390e+14, 3.6856e+13, 7.9364e+13, 8.7662e+13,
        2.4315e+14, 2.7114e+14, 9.1280e+13, 9.5767e+12, 7.6057e+13, 2.7645e+14,
        2.2396e+14, 1.8270e+14, 8.9806e+13, 7.5217e+13, 5.8364e+14, 3.0421e+14,
        7.1184e+14, 1.9502e+14, 1.2335e+14, 2.1472e+14, 3.8536e+14, 4.4664e+13,
        1.0791e+14, 5.9508e+12, 1.0217e+14, 1.1247e+13, 3.8537e+14, 3.6728e+14,
        8.3028e+13, 8.1575e+13, 4.1577e+13, 4.2492e+13])
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.2713e+13, 1.0398e+14, 1.9562e+14, 1.3034e+14, 1.2310e+14, 9.2324e+13,
        1.5201e+14, 9.5853e+13, 5.7387e+13, 8.0209e+13, 1.9785e+14, 1.3447e+14,
        1.7743e+14, 3.8997e+14, 4.3159e+14, 3.2305e+14, 2.8940e+14, 3.9826e+14,
        2.8926e+14, 1.3972e+14, 2.1970e+14, 1.7127e+14, 7.6057e+13, 1.8909e+14,
        1.1082e+14, 2.2338e+14, 3.1910e+13, 8.1105e+13, 3.5438e+14, 1.0410e+14,
        3.5156e+14, 1.3040e+14, 8.4860e+13, 1.9632e+14, 2.2884e+14, 5.0958e+13,
        1.4311e+14, 2.7171e+14, 1.0040e+14, 1.1703e+14, 7.2754e+14, 2.8334e+14,
        1.7063e+14, 4.1780e+14, 8.3031e+13, 3.8464e+13, 5.9237e+13, 3.1461e+14,
        1.0990e+14, 1.6917e+14, 2.0635e+14, 1.8627e+14, 4.3717e+13, 1.3629e+14,
        2.7309e+14, 1.1789e+14, 5.4996e+14, 2.5533e+14, 4.1566e+13, 3.2720e+14,
        1.8483e+13, 1.1895e+14, 7.3372e+13, 2.2365e+13, 2.8543e+14, 9.8753e+13,
        3.5170e+13, 7.6572e+13, 4.7927e+14, 1.9600e+14, 3.0961e+14, 1.3123e+14,
        2.6507e+14, 6.6248e+13, 3.6898e+13, 1.0192e+14, 1.1061e+14, 2.2693e+14,
        3.9810e+14, 1.4879e+14, 8.3128e+13, 8.9243e+13, 8.5529e+13, 1.9617e+14,
        2.2184e+14, 4.4152e+14, 1.1043e+14, 5.2561e+14, 8.4258e+13, 3.8915e+14,
        3.1009e+14, 2.0076e+14, 1.1066e+14, 2.7579e+13, 4.5540e+14, 1.7287e+14,
        9.9912e+13, 1.1948e+14, 5.6856e+13, 2.4119e+14, 1.3220e+14, 1.4699e+14,
        7.1826e+13, 2.4036e+14, 3.5445e+13, 5.3595e+13, 1.5666e+14, 2.5202e+14,
        3.5516e+14, 1.4346e+14, 3.2969e+13, 2.0668e+14, 1.7403e+14, 8.1653e+13,
        2.0627e+14, 1.2985e+14, 3.7890e+14, 3.3393e+14, 1.8422e+14, 2.3766e+14,
        9.5873e+13, 2.7439e+14, 9.7107e+13, 1.7668e+14, 1.7224e+14, 3.7435e+14,
        3.7356e+13, 3.7578e+13])
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3870e+14, 1.2269e+14, 8.8516e+13, 3.1811e+14, 2.2523e+14, 1.7141e+14,
        3.5377e+14, 7.0050e+13, 3.4643e+14, 3.6292e+14, 4.4825e+14, 3.5876e+14,
        1.0672e+14, 3.6618e+14, 3.1026e+14, 1.4357e+14, 4.6139e+14, 1.5266e+14,
        9.3903e+13, 2.6088e+14, 2.7277e+14, 6.9724e+14, 6.8140e+13, 2.4796e+14,
        4.6642e+14, 3.0812e+14, 4.7809e+14, 6.3445e+14, 3.5463e+14, 1.1968e+14,
        4.7208e+13, 2.7605e+14, 3.8924e+14, 2.5541e+14, 3.2714e+14, 2.0053e+14,
        1.9601e+14, 1.8046e+14, 1.7507e+14, 2.4880e+14, 4.1259e+14, 2.6947e+14,
        3.0112e+14, 4.0841e+14, 4.3184e+14, 1.6599e+14, 3.3099e+14, 2.2152e+14,
        1.1501e+14, 3.1155e+14, 2.9173e+14, 8.0334e+14, 1.4409e+13, 9.8627e+13,
        4.4569e+14, 4.0585e+14, 5.4141e+13, 2.4743e+14, 3.9457e+13, 9.0393e+13,
        2.7142e+14, 2.2643e+14, 3.0001e+13, 2.6859e+14, 2.9684e+14, 6.0871e+14,
        4.8694e+13, 3.0282e+14, 1.8600e+14, 1.1643e+14, 4.7158e+14, 4.6553e+14,
        1.0244e+15, 2.8508e+14, 2.7620e+14, 2.2865e+14, 9.8620e+13, 2.3742e+14,
        4.2485e+14, 5.5660e+13, 4.4016e+14, 1.6252e+14, 2.7214e+14, 2.4945e+14,
        2.4561e+14, 1.6644e+14, 1.4584e+14, 1.3161e+14, 4.3406e+14, 2.1841e+14,
        3.0900e+14, 2.6347e+14, 1.7678e+14, 2.1800e+14, 4.7045e+14, 2.2778e+14,
        2.7468e+14, 5.2146e+14, 4.1203e+14, 4.2215e+14, 7.9490e+13, 4.0825e+14,
        7.0791e+13, 2.5013e+14, 4.4940e+14, 3.6730e+14, 8.9912e+13, 2.5688e+14,
        3.0877e+14, 7.7611e+13, 1.7452e+14, 1.6015e+14, 6.3804e+14, 2.5922e+14,
        3.0994e+14, 1.7955e+14, 3.9409e+14, 4.3231e+14, 5.5805e+14, 1.5975e+14,
        4.8909e+14, 3.2402e+14, 2.9408e+14, 1.3409e+14, 1.2844e+14, 1.9922e+14,
        1.7537e+14, 8.9370e+13])
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.4585e+13, 4.4754e+12, 1.0435e+12, 1.7954e+13, 6.5373e+12, 1.8023e+13,
        4.8489e+12, 7.1042e+12, 4.6444e+12, 2.0141e+12, 4.2976e+13, 5.2722e+13,
        3.7150e+12, 6.8497e+12, 2.0830e+12, 3.0804e+12, 2.3572e+12, 5.0324e+12,
        1.5499e+12, 3.9020e+12, 1.3512e+14, 4.0248e+12, 4.5509e+12, 8.0463e+12,
        1.1326e+13, 1.8422e+13, 3.6323e+13, 3.3019e+13, 8.2329e+12, 2.5795e+13,
        4.5276e+12, 1.5149e+14, 2.5735e+12, 1.4203e+13, 2.0101e+13, 8.6356e+12,
        1.5379e+14, 3.5824e+13, 9.9374e+12, 1.3639e+13, 3.5450e+12, 1.3748e+12,
        3.0098e+12, 6.0363e+12, 4.8236e+11, 8.2055e+12, 5.8649e+13, 1.3257e+12,
        6.4171e+12, 2.7747e+12, 7.0590e+12, 3.2212e+12, 2.0167e+13, 2.5469e+13,
        1.5272e+12, 1.6033e+12, 2.4974e+12, 1.7474e+12, 1.3846e+15, 4.1589e+13,
        2.5183e+12, 4.7381e+13, 1.2016e+13, 1.8321e+14, 5.0738e+11, 6.1258e+13,
        1.6858e+13, 5.0522e+11, 1.1758e+12, 9.9265e+12, 2.5869e+13, 1.5054e+12,
        2.8375e+13, 2.6789e+12, 9.2608e+12, 7.6967e+11, 1.5259e+13, 3.1237e+12,
        1.5649e+12, 2.6265e+13, 2.7698e+11, 2.3264e+14, 2.1602e+12, 1.2486e+13,
        1.1703e+12, 5.5604e+12, 6.5146e+12, 9.9542e+12, 1.1488e+13, 4.1994e+12,
        6.8866e+12, 6.8477e+13, 2.0480e+13, 1.4702e+13, 1.5708e+13, 9.3912e+13,
        1.2529e+13, 2.3258e+13, 8.2584e+12, 9.9003e+12, 5.7445e+12, 1.3323e+13,
        4.6212e+12, 1.5133e+13, 3.2982e+13, 7.0162e+12, 9.6154e+11, 2.5095e+13,
        5.4106e+13, 1.8903e+12, 4.5622e+12, 7.7116e+12, 4.9394e+11, 5.5353e+12,
        1.4718e+14, 5.1626e+13, 8.2170e+12, 1.4777e+13, 3.4460e+12, 5.3600e+11,
        5.2056e+12, 1.8634e+13, 2.0944e+12, 5.7615e+12, 4.6632e+13, 2.1378e+12,
        1.9261e+14, 3.7830e+13, 6.5317e+12, 1.5158e+11, 1.9495e+12, 2.4278e+13,
        6.9471e+12, 3.0073e+13, 1.0360e+12, 7.5436e+12, 3.9413e+11, 4.4948e+12,
        1.2063e+13, 2.0953e+12, 4.0872e+12, 1.4292e+13, 1.6570e+13, 3.8298e+12,
        1.5461e+13, 3.3323e+12, 6.8681e+12, 7.2967e+11, 2.3943e+11, 7.2668e+12,
        2.0224e+13, 2.7286e+13, 4.3208e+12, 2.0616e+12, 5.2715e+12, 7.9983e+12,
        7.8783e+13, 5.9086e+12, 9.8790e+11, 1.1262e+12, 2.2308e+12, 7.1898e+12,
        1.4341e+12, 8.9013e+11, 7.9262e+12, 2.2258e+13, 9.8599e+12, 1.4442e+14,
        2.7662e+12, 5.2075e+12, 4.0516e+12, 9.9892e+13, 2.5979e+12, 1.1404e+13,
        1.2336e+12, 6.0260e+13, 1.0254e+12, 1.5415e+13, 4.6973e+12, 1.9606e+13,
        2.8800e+12, 2.1799e+12, 1.1439e+14, 4.9131e+12, 5.5010e+12, 3.5081e+12,
        5.8238e+12, 3.6144e+11, 7.0074e+11, 8.7697e+12, 7.1248e+12, 1.3364e+13,
        2.7897e+13, 1.4607e+12, 6.7306e+11, 1.1069e+12, 5.2652e+12, 5.4044e+12,
        6.9440e+12, 1.5866e+13, 1.5519e+13, 1.0452e+12, 3.0385e+12, 6.3980e+12,
        5.3074e+12, 1.2742e+13, 6.8688e+12, 2.0749e+12, 2.6488e+12, 5.7249e+13,
        1.5943e+13, 3.9190e+12, 8.0162e+13, 7.9916e+11, 8.2372e+13, 8.3834e+12,
        1.5977e+13, 2.8265e+12, 6.7001e+12, 9.2129e+12, 4.7973e+12, 6.4741e+12,
        7.8663e+12, 1.1739e+13, 8.2821e+12, 6.6919e+12, 1.2423e+13, 2.3322e+12,
        8.6464e+12, 5.4024e+12, 2.7138e+13, 3.3824e+12, 3.8216e+12, 1.5243e+13,
        1.3499e+13, 2.3278e+13, 4.6947e+11, 4.9917e+12, 2.2267e+12, 1.5213e+13,
        2.6429e+12, 6.0811e+12, 1.6994e+14, 1.1857e+13, 2.5712e+12, 1.3732e+14,
        4.5702e+13, 1.6859e+11, 1.5391e+13, 4.9874e+12, 6.7689e+12, 7.4823e+12,
        3.7872e+12, 1.6528e+14, 2.3184e+12, 2.7688e+12, 5.0430e+12, 4.1657e+12,
        1.0121e+12, 1.5539e+12, 3.6280e+12, 1.8164e+12, 9.7706e+12, 1.6010e+13,
        1.8704e+13, 1.3725e+13, 5.3520e+12, 2.3055e+12, 3.5791e+12, 2.1038e+14,
        6.6551e+12, 3.2597e+13, 5.2795e+12, 1.1386e+13, 2.3487e+13, 7.7629e+12,
        5.8074e+11, 4.8450e+12, 2.4167e+12, 8.2304e+12, 2.6451e+13, 3.1104e+13,
        5.2221e+12, 9.8874e+12, 1.6466e+13, 7.2972e+11, 1.9197e+13, 3.0525e+12,
        6.6679e+11, 1.1686e+12, 1.7614e+13, 1.5260e+13, 3.1267e+12, 3.0444e+12,
        1.1698e+13, 7.7374e+12, 1.2463e+14, 2.0460e+13, 1.4409e+13, 6.0625e+12,
        5.7346e+12, 6.0472e+12, 2.6847e+13, 3.0301e+12, 1.5794e+13, 1.6078e+13,
        2.2693e+12, 1.8844e+13, 1.3591e+12, 2.7205e+12, 1.2459e+13, 7.0542e+12,
        1.3420e+12, 3.4543e+13, 3.3849e+12, 1.8639e+11, 1.1594e+12, 1.4435e+14,
        2.0397e+12, 1.1804e+14, 4.1229e+12, 5.2533e+12, 1.9304e+13, 5.8181e+12,
        2.6431e+13, 2.3874e+13, 3.0782e+13, 1.9310e+12, 9.2975e+12, 1.1839e+13,
        2.3562e+12, 1.0208e+14, 2.8480e+12, 1.1579e+12, 4.2599e+12, 2.8186e+12,
        2.0207e+14, 1.4751e+12, 6.7219e+12, 8.4467e+11, 1.9298e+12, 1.7672e+13,
        4.5313e+12, 2.1117e+13, 3.4756e+13, 3.3459e+12, 3.9870e+12, 4.0482e+11,
        4.1274e+12, 3.4092e+12, 3.6836e+12, 7.3317e+13, 6.9627e+11, 3.9280e+12,
        1.7704e+12, 3.4443e+12, 3.8749e+13, 4.6422e+12, 3.4506e+13, 8.9773e+12,
        1.7536e+12, 1.2336e+13, 1.1746e+13, 1.1743e+12, 1.3168e+13, 2.5640e+12,
        2.1119e+13, 3.6873e+13, 1.3622e+14, 2.5641e+12, 2.2890e+13, 1.1560e+13,
        1.2907e+13, 5.2648e+12, 2.2949e+13, 3.2749e+12, 2.4484e+13, 6.4815e+12,
        2.6325e+13, 4.8626e+13, 4.5955e+12, 1.1004e+13, 4.4615e+12, 2.3057e+13,
        1.1031e+13, 1.5317e+13, 1.8239e+12, 7.6348e+11, 1.1986e+12, 9.6683e+11,
        1.0374e+13, 3.2199e+14, 1.6366e+13, 6.9293e+12, 3.1135e+12, 5.8877e+13,
        3.1605e+12, 1.0391e+12, 1.9415e+12, 5.4147e+12, 3.9707e+13, 4.6143e+12,
        1.8056e+12, 2.4911e+12, 1.9067e+12, 1.2140e+13, 1.7806e+13, 6.7739e+11,
        3.9440e+13, 9.7639e+12, 2.0433e+12, 4.8663e+12, 4.3786e+12, 4.7739e+12,
        3.5750e+12, 1.4023e+13, 6.3242e+11, 4.1859e+12, 1.5814e+12, 2.0726e+12,
        5.0531e+12, 3.5219e+12, 1.4851e+15, 6.2389e+12, 2.5734e+13, 3.2780e+12,
        1.2067e+13, 8.4305e+12, 2.0937e+12, 8.7190e+11, 1.2454e+12, 2.2127e+13,
        2.3611e+13, 7.3156e+13, 2.7661e+12, 1.6568e+12, 1.2078e+13, 1.2952e+13,
        2.3342e+13, 1.0617e+12, 5.3378e+12, 1.1720e+13, 4.9855e+13, 2.0000e+12,
        4.1610e+12, 2.6031e+12, 2.1918e+13, 1.3995e+12, 1.3316e+13, 1.7026e+13,
        2.1605e+12, 3.3870e+12, 8.9165e+12, 1.7069e+13, 1.6208e+12, 3.9259e+12,
        3.4681e+13, 7.6279e+11, 2.5513e+13, 4.0749e+12, 9.2724e+12, 9.0335e+11,
        1.6648e+12, 2.0567e+12, 6.2174e+12, 4.3828e+12, 4.0174e+12, 5.3188e+12,
        5.3284e+12, 6.5835e+11, 3.2350e+13, 1.9941e+12, 4.4394e+12, 2.3744e+12,
        4.7888e+12, 2.7638e+12, 3.7858e+12, 4.7084e+12, 4.5636e+13, 2.7828e+12,
        2.9323e+13, 1.6439e+13, 5.4388e+13, 3.3005e+12, 3.1097e+12, 1.1862e+12,
        1.0607e+12, 1.2846e+14, 1.0583e+13, 1.3290e+12, 1.7027e+13, 8.5512e+12,
        2.5341e+12, 6.0833e+13, 2.1851e+13, 7.0484e+12, 1.6789e+14, 9.0468e+12,
        2.0069e+11, 8.0930e+11, 8.6651e+12, 2.1111e+13, 6.6283e+12, 2.6060e+13,
        3.5210e+12, 8.5002e+11, 4.1871e+12, 4.5507e+12, 3.2608e+12, 2.6312e+13,
        7.4450e+12, 6.5807e+11])
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.8721e+13, 1.6738e+13, 1.9474e+13, 1.5407e+12, 2.1593e+13, 1.4394e+13,
        7.8488e+12, 4.8965e+12, 6.7933e+12, 1.0438e+13, 5.0927e+13, 8.9118e+13,
        1.0156e+13, 5.8154e+12, 2.6406e+12, 1.8206e+13, 1.3481e+13, 1.5088e+13,
        5.5464e+12, 1.4615e+13, 1.0501e+14, 1.3492e+13, 2.6990e+13, 8.4282e+12,
        3.7667e+13, 2.9521e+12, 6.7231e+13, 2.9885e+13, 2.2516e+13, 6.0715e+13,
        1.8541e+13, 1.7603e+14, 1.6825e+13, 2.4527e+13, 2.0546e+12, 2.3847e+13,
        1.5375e+14, 5.7720e+13, 1.6077e+13, 3.1539e+13, 5.1255e+12, 2.2330e+13,
        3.0485e+12, 2.6415e+13, 6.7327e+12, 1.9454e+13, 3.1134e+13, 2.9459e+12,
        1.3475e+13, 5.9451e+12, 3.0054e+13, 3.1500e+12, 8.3510e+12, 2.7680e+13,
        3.1022e+12, 2.0557e+12, 2.4736e+12, 9.6409e+11, 1.4273e+15, 4.8830e+13,
        1.1109e+13, 3.3598e+13, 2.0901e+13, 1.9626e+14, 4.1785e+12, 4.7653e+13,
        4.9205e+12, 1.5263e+12, 7.1985e+11, 1.3125e+13, 2.9165e+13, 3.1808e+12,
        5.1393e+13, 7.3795e+12, 2.2800e+13, 2.5245e+12, 2.6956e+13, 8.2346e+12,
        1.8893e+12, 6.2136e+12, 6.1537e+12, 2.2889e+14, 1.5657e+12, 9.7198e+12,
        6.4628e+12, 2.9372e+12, 1.8854e+13, 2.0174e+13, 2.3677e+12, 5.8460e+12,
        2.5835e+13, 5.4806e+13, 3.1868e+13, 4.9093e+12, 4.4551e+13, 1.2247e+14,
        3.4689e+13, 3.4135e+13, 2.4511e+12, 5.6869e+12, 1.0766e+13, 2.0790e+13,
        5.9763e+12, 8.6938e+12, 2.7556e+13, 1.7267e+13, 4.7488e+12, 3.2298e+13,
        3.3734e+13, 3.2652e+13, 1.4062e+13, 8.1663e+12, 1.3215e+13, 2.2667e+13,
        9.9013e+13, 3.7313e+13, 8.2985e+12, 7.0305e+12, 1.9500e+13, 7.7494e+12,
        2.3412e+13, 8.2811e+12, 1.0155e+13, 8.2467e+12, 5.8392e+13, 1.8342e+12,
        1.9311e+14, 2.8945e+13, 3.2016e+12, 1.2838e+12, 1.7482e+12, 3.4902e+13,
        2.7361e+13, 2.0184e+13, 5.3405e+12, 3.8543e+13, 7.2964e+12, 1.1939e+13,
        7.2957e+12, 8.2793e+12, 1.0445e+13, 1.5367e+12, 8.4382e+12, 7.0034e+12,
        4.1227e+13, 2.2835e+12, 8.1648e+12, 1.0040e+12, 6.4605e+12, 1.0567e+13,
        9.2387e+11, 3.0809e+13, 4.6209e+12, 6.9879e+12, 6.5729e+12, 4.0438e+13,
        7.7948e+13, 9.2324e+12, 1.5065e+13, 2.1327e+13, 3.0864e+12, 1.3849e+13,
        1.1151e+13, 4.1135e+12, 2.3211e+12, 3.4989e+13, 9.6590e+12, 1.5513e+14,
        2.5323e+13, 7.6972e+12, 9.4395e+12, 8.8275e+13, 1.4386e+13, 1.6730e+13,
        2.5589e+12, 2.9098e+14, 4.3890e+12, 8.6167e+12, 2.3846e+13, 1.5067e+13,
        3.1234e+12, 2.9086e+12, 1.2523e+14, 4.7894e+12, 2.6175e+13, 1.8304e+13,
        1.7056e+13, 4.9355e+12, 2.8527e+12, 1.5495e+13, 6.7893e+12, 5.5006e+12,
        5.7424e+13, 1.3906e+12, 1.9012e+12, 4.1437e+12, 1.6364e+13, 2.8956e+12,
        1.4628e+13, 1.1770e+13, 1.6694e+13, 5.3946e+12, 1.6830e+13, 6.5549e+12,
        5.0572e+12, 3.9301e+12, 1.3007e+13, 8.4278e+12, 9.9528e+12, 4.5527e+13,
        2.6951e+13, 3.3466e+13, 6.1093e+13, 2.4604e+13, 8.9246e+13, 5.6797e+12,
        1.8197e+13, 8.9479e+12, 2.2890e+12, 1.7119e+13, 2.0821e+12, 1.1332e+13,
        1.4564e+13, 1.6557e+13, 1.1522e+13, 2.5900e+12, 2.7897e+12, 4.1491e+12,
        8.0700e+12, 2.2245e+13, 4.4949e+13, 1.1168e+13, 2.7181e+12, 1.1301e+13,
        4.0865e+13, 1.2627e+13, 3.8612e+12, 4.6814e+12, 1.5434e+13, 9.7786e+12,
        1.0478e+13, 9.0040e+12, 1.5070e+14, 2.7225e+13, 1.0422e+13, 1.5777e+14,
        4.0613e+13, 1.7866e+12, 2.3134e+13, 4.3850e+12, 4.1474e+12, 1.2346e+12,
        2.0362e+12, 1.6399e+14, 1.9542e+13, 8.0930e+12, 1.1102e+13, 1.5338e+13,
        3.5934e+12, 2.0650e+13, 2.1503e+12, 3.4672e+12, 1.7010e+13, 1.9516e+13,
        6.1611e+13, 1.7047e+12, 1.2160e+13, 1.0998e+12, 1.9486e+13, 2.3345e+14,
        1.3566e+13, 2.0763e+13, 6.6704e+12, 1.3886e+13, 1.7472e+13, 1.4511e+13,
        4.5194e+12, 9.9498e+12, 5.0633e+12, 2.4230e+13, 5.8089e+12, 3.0703e+13,
        2.5083e+13, 1.8275e+13, 1.6504e+13, 1.0950e+12, 2.0061e+13, 4.8149e+12,
        5.8393e+12, 1.2800e+12, 2.2626e+13, 1.3086e+13, 2.1674e+12, 1.6322e+13,
        1.7590e+13, 1.9643e+13, 1.1091e+14, 3.8859e+12, 3.0767e+13, 2.6901e+12,
        1.1753e+13, 2.3629e+13, 1.2775e+13, 1.5816e+12, 1.1523e+13, 2.9867e+13,
        7.3159e+12, 7.1696e+12, 6.5922e+11, 9.7226e+12, 1.2367e+12, 4.0588e+12,
        1.1242e+13, 2.5801e+13, 1.0741e+13, 1.1202e+12, 9.3570e+11, 9.3393e+13,
        4.4373e+12, 1.5498e+14, 1.2668e+13, 5.3105e+12, 1.6347e+13, 7.8803e+12,
        4.7647e+13, 4.4937e+12, 1.3928e+13, 1.6631e+13, 8.0617e+12, 6.6767e+12,
        3.9701e+12, 1.0824e+14, 3.7541e+12, 1.9520e+13, 9.9827e+12, 7.5325e+12,
        1.9387e+14, 8.6632e+12, 1.0159e+13, 1.5929e+12, 6.3285e+12, 3.3085e+13,
        3.3173e+13, 1.8664e+13, 2.1378e+13, 1.2088e+13, 3.8075e+12, 8.5437e+12,
        9.8595e+11, 6.5980e+12, 1.4289e+13, 9.1412e+13, 2.1614e+12, 1.1874e+13,
        2.5836e+12, 3.3440e+12, 4.6434e+13, 9.8161e+12, 4.3509e+13, 2.1294e+13,
        1.0678e+13, 3.0930e+12, 2.3642e+13, 8.6618e+11, 4.2133e+13, 1.0036e+13,
        3.1102e+13, 3.6897e+13, 1.0767e+14, 2.7076e+13, 2.0988e+13, 1.1427e+13,
        7.3364e+12, 8.7491e+12, 3.0628e+13, 1.1266e+13, 5.2336e+13, 2.2471e+13,
        1.4786e+13, 3.9915e+13, 2.0918e+13, 4.5695e+12, 2.0589e+13, 1.6903e+13,
        2.3769e+13, 1.1471e+12, 1.9829e+12, 5.1473e+12, 7.3999e+12, 1.6670e+13,
        2.7731e+13, 3.4043e+14, 1.1723e+13, 2.9991e+13, 3.4401e+12, 6.5253e+13,
        2.1971e+12, 1.2437e+13, 1.2569e+13, 2.3569e+13, 2.4744e+13, 3.9078e+12,
        2.5025e+13, 1.9259e+12, 6.3689e+12, 3.3725e+12, 9.3697e+12, 1.4564e+13,
        9.8071e+12, 6.2038e+12, 2.7339e+12, 9.3857e+12, 6.5827e+12, 1.3294e+13,
        6.6186e+12, 3.9097e+12, 2.2874e+12, 1.4987e+13, 2.3530e+12, 1.1952e+12,
        2.5060e+13, 1.3599e+12, 1.4740e+15, 1.7477e+13, 5.5882e+13, 5.1251e+12,
        5.6219e+12, 1.7481e+13, 3.3044e+12, 1.1194e+13, 1.0576e+13, 1.5746e+13,
        4.0038e+13, 6.3379e+13, 7.8962e+12, 9.8548e+12, 1.3796e+13, 1.1314e+13,
        1.4815e+13, 2.1658e+12, 8.6181e+12, 1.4546e+13, 5.2371e+13, 1.4774e+13,
        9.8207e+12, 8.6085e+12, 2.1954e+12, 1.0197e+13, 9.8734e+12, 1.5933e+13,
        2.0951e+13, 8.1520e+12, 1.4480e+13, 7.8439e+12, 1.1060e+12, 2.9014e+13,
        5.0937e+13, 1.5539e+12, 2.1752e+13, 1.8063e+13, 1.3169e+13, 1.1608e+13,
        4.9649e+12, 7.0253e+12, 8.2631e+12, 5.2984e+12, 8.0863e+12, 1.7423e+13,
        1.7818e+13, 3.3361e+12, 1.8795e+13, 6.1854e+12, 7.9546e+12, 4.9561e+12,
        2.0091e+13, 5.3290e+12, 1.8895e+13, 1.7821e+13, 5.2496e+13, 1.6355e+13,
        5.9531e+13, 2.8337e+13, 6.4915e+13, 9.0907e+12, 2.1711e+13, 1.4203e+13,
        4.5546e+12, 1.5899e+14, 2.8093e+13, 4.4559e+12, 2.6389e+13, 5.8070e+12,
        5.9720e+12, 4.7483e+13, 8.8960e+12, 4.5609e+12, 1.5678e+14, 1.0630e+13,
        8.8599e+11, 1.2412e+13, 5.5369e+12, 1.3650e+13, 1.3007e+13, 2.3957e+13,
        1.5542e+13, 1.6875e+13, 4.7312e+12, 6.9382e+12, 8.7941e+12, 3.2393e+13,
        8.7477e+12, 1.8086e+13])
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([7.1233e+12, 4.6343e+12, 3.4021e+12, 1.9456e+13, 1.8110e+13, 2.9153e+13,
        1.8210e+13, 9.5480e+12, 9.3501e+12, 2.3512e+13, 3.9044e+13, 7.8452e+12,
        2.9909e+13, 2.7522e+13, 8.8818e+12, 4.1546e+13, 2.3764e+12, 1.1021e+13,
        3.4502e+13, 6.4378e+13, 2.4249e+12, 2.2088e+13, 7.7267e+12, 1.0284e+13,
        2.4691e+13, 2.1606e+13, 1.2297e+13, 1.2615e+13, 7.9454e+12, 1.0778e+13,
        6.2677e+12, 5.3742e+12, 4.4304e+13, 4.5140e+13, 6.8245e+12, 1.4577e+13,
        5.0326e+13, 9.1052e+12, 5.3759e+13, 2.3857e+13, 1.8953e+13, 2.6440e+13,
        7.6283e+12, 1.4134e+13, 3.0316e+13, 1.9818e+13, 2.2563e+13, 4.8162e+12,
        2.1927e+13, 7.8233e+13, 6.1883e+12, 3.0575e+13, 4.8147e+13, 8.5392e+12,
        9.4982e+12, 2.9517e+12, 3.3165e+13, 3.3080e+12, 4.0366e+13, 4.3189e+12,
        1.4191e+13, 1.7793e+13, 1.8227e+13, 1.2550e+13, 2.8183e+13, 5.0419e+12,
        1.1253e+13, 2.6566e+13, 4.2998e+13, 1.3305e+13, 4.5799e+13, 2.4101e+12,
        2.6483e+13, 1.6520e+13, 2.4396e+13, 2.4894e+12, 4.1725e+13, 7.6533e+12,
        1.6014e+13, 2.5129e+13, 3.5532e+13, 1.9732e+13, 6.4344e+13, 2.5097e+13,
        9.7642e+12, 4.9971e+13, 1.9153e+12, 2.6737e+13, 2.2841e+13, 7.3511e+12,
        3.7879e+13, 1.2141e+13, 1.1325e+12, 2.6871e+13, 2.0219e+12, 4.7288e+13,
        1.1778e+13, 1.1396e+13, 5.4173e+12, 4.7739e+13, 1.5043e+13, 3.1883e+13,
        5.6407e+13, 6.5615e+13, 2.0502e+13, 4.1959e+13, 5.3085e+13, 9.0733e+12,
        1.0429e+13, 1.0344e+13, 6.7597e+12, 5.9722e+13, 5.4057e+13, 5.4033e+13,
        1.5239e+13, 6.4230e+12, 9.6817e+13, 2.8519e+13, 1.8345e+13, 3.3010e+13,
        1.0568e+13, 1.4616e+13, 5.8831e+13, 1.3486e+13, 4.2240e+13, 5.5444e+13,
        6.9892e+11, 4.1071e+13])
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.4148e+13, 3.7004e+13, 2.3814e+13, 9.6257e+13, 4.3256e+13, 1.1487e+13,
        5.1126e+13, 4.7992e+13, 2.9632e+13, 1.9805e+13, 5.8311e+13, 5.7476e+12,
        5.5378e+13, 8.4398e+13, 5.0754e+13, 7.2314e+13, 3.5713e+13, 1.2847e+13,
        2.4528e+12, 5.0979e+13, 1.3649e+13, 3.9981e+13, 1.2285e+14, 2.2352e+13,
        4.7688e+13, 1.3315e+13, 9.1732e+13, 5.7226e+13, 1.2365e+12, 5.9981e+13,
        4.7202e+12, 8.8450e+12, 5.2651e+13, 3.8828e+13, 1.0451e+13, 2.2823e+13,
        1.7737e+13, 1.3892e+13, 4.9281e+13, 9.8190e+12, 2.7155e+13, 3.3156e+13,
        4.4193e+13, 7.2926e+13, 2.3379e+13, 5.7134e+13, 3.5467e+13, 1.0261e+14,
        9.8638e+13, 3.9546e+13, 1.3234e+13, 3.4954e+13, 5.5101e+13, 3.6884e+13,
        1.3704e+13, 2.2134e+13, 3.2019e+13, 8.2984e+13, 4.4088e+13, 5.2730e+13,
        1.7654e+13, 2.0870e+13, 5.4889e+12, 3.2138e+13, 3.4881e+13, 3.8588e+13,
        2.7495e+13, 2.1579e+13, 1.9869e+13, 3.8012e+13, 3.8378e+13, 3.6117e+13,
        5.5096e+13, 2.7501e+13, 3.3632e+13, 7.2754e+13, 4.2268e+13, 2.8634e+13,
        5.0548e+12, 9.3328e+12, 3.8959e+13, 2.4508e+13, 3.0318e+13, 1.7869e+13,
        1.9122e+13, 2.5960e+13, 3.9661e+13, 3.5899e+13, 9.1283e+13, 4.5934e+13,
        2.1937e+13, 1.7675e+13, 4.8156e+13, 1.7918e+13, 3.0982e+13, 2.6009e+13,
        1.0732e+14, 1.9317e+13, 5.7145e+13, 1.6506e+13, 4.6542e+13, 3.9678e+13,
        2.2967e+13, 5.4465e+12, 2.6087e+13, 1.0432e+14, 2.9156e+13, 6.8454e+13,
        5.8975e+13, 3.3777e+13, 6.4831e+13, 2.4935e+13, 2.7948e+13, 8.7953e+13,
        1.4157e+13, 2.8598e+13, 5.6240e+13, 4.9414e+13, 1.8201e+13, 5.7398e+13,
        2.3921e+12, 6.7652e+13, 3.0852e+13, 8.6173e+12, 5.6167e+13, 2.7887e+13,
        2.5822e+13, 6.1639e+12])
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.2186e+11, 5.0277e+11, 1.0517e+11, 1.0063e+12, 3.7338e+11, 6.3583e+11,
        6.5975e+11, 3.0418e+12, 1.1926e+12, 4.7553e+10, 1.1223e+11, 2.7140e+11,
        1.9545e+12, 1.3333e+11, 1.7151e+11, 2.1087e+11, 7.7972e+11, 1.1511e+13,
        4.8218e+11, 6.3032e+11, 3.7849e+11, 1.0061e+12, 1.1648e+12, 1.2155e+12,
        2.4367e+11, 7.8849e+11, 7.0642e+11, 7.6162e+11, 7.5173e+11, 5.8564e+11,
        4.8816e+11, 1.3256e+11, 1.0566e+12, 1.6328e+11, 1.1181e+12, 8.0055e+11,
        2.4963e+11, 1.1776e+11, 5.9302e+11, 1.5057e+11, 1.3930e+12, 8.5867e+11,
        4.1299e+10, 8.8254e+11, 4.5879e+10, 4.9390e+11, 6.1856e+12, 7.1471e+11,
        2.5583e+12, 5.2574e+11, 2.7060e+11, 5.4789e+11, 2.2772e+12, 1.0628e+12,
        1.1359e+11, 4.2177e+11, 6.7135e+10, 5.7925e+11, 3.3778e+12, 6.8519e+11,
        8.1581e+10, 1.7477e+11, 3.7042e+11, 1.1083e+11, 6.3924e+11, 4.2356e+11,
        1.0198e+12, 8.6301e+10, 6.7390e+11, 3.9973e+11, 1.5848e+11, 7.1851e+10,
        4.0393e+11, 8.2133e+11, 3.1365e+11, 2.7555e+11, 2.6893e+11, 3.9613e+11,
        3.3342e+11, 8.3911e+10, 1.7848e+12, 9.8774e+11, 6.7367e+10, 5.9822e+11,
        7.5094e+10, 3.2235e+12, 6.0140e+11, 3.4616e+11, 9.2824e+11, 6.0104e+10,
        2.9081e+11, 8.0731e+11, 3.9131e+11, 4.0876e+11, 7.1967e+11, 5.9089e+11,
        1.5560e+11, 3.9800e+11, 4.9469e+11, 1.7237e+11, 1.3248e+13, 3.1340e+11,
        4.7760e+11, 2.9569e+11, 3.3495e+11, 4.7292e+11, 2.4760e+11, 4.4279e+11,
        1.4667e+11, 6.0434e+11, 1.4309e+12, 5.0306e+11, 8.5584e+10, 2.3258e+11,
        2.4267e+14, 1.8047e+11, 3.3870e+11, 5.3885e+11, 1.7531e+12, 2.7484e+11,
        3.0044e+11, 2.3149e+11, 1.7581e+11, 5.0057e+11, 4.2659e+11, 9.0501e+10,
        6.7521e+11, 4.3235e+11, 6.3487e+11, 1.0460e+12, 1.0968e+12, 1.1270e+12,
        7.6854e+11, 8.7280e+10, 4.7906e+11, 1.5889e+12, 1.0807e+11, 8.0076e+11,
        5.6775e+11, 2.4103e+10, 3.8864e+11, 7.3805e+11, 7.4824e+11, 1.7630e+11,
        9.0581e+10, 7.8717e+11, 3.7271e+11, 2.2910e+11, 8.1017e+11, 1.4954e+11,
        2.9015e+11, 3.3548e+11, 1.3158e+12, 4.6637e+11, 1.2551e+11, 4.4863e+10,
        1.6016e+11, 1.5778e+10, 4.9813e+11, 9.4213e+11, 2.9978e+11, 3.3822e+11,
        5.5385e+11, 9.5894e+10, 1.0457e+12, 2.8063e+11, 3.2220e+11, 3.6380e+11,
        6.6848e+11, 4.8006e+11, 4.5618e+11, 4.1141e+11, 1.4294e+11, 1.9006e+12,
        3.1548e+11, 2.2515e+14, 9.8184e+10, 3.5173e+11, 7.0894e+10, 3.7214e+11,
        4.7840e+11, 1.6690e+11, 2.3566e+12, 5.4915e+11, 3.6475e+11, 8.9316e+11,
        9.6042e+11, 1.5438e+11, 1.2540e+10, 9.8094e+11, 1.2793e+11, 3.2209e+11,
        1.4490e+11, 6.3636e+11, 4.9759e+11, 3.7886e+11, 5.1612e+10, 5.5413e+11,
        6.6782e+10, 1.4982e+12, 4.7090e+11, 1.5937e+11, 3.6581e+11, 1.2199e+11,
        1.5344e+12, 1.2458e+13, 2.7249e+11, 1.0763e+12, 2.2889e+12, 4.7892e+11,
        4.8472e+11, 8.8000e+11, 6.0429e+12, 2.0986e+11, 7.8055e+11, 2.6079e+11,
        8.0707e+11, 1.8540e+11, 1.2877e+11, 1.4710e+11, 1.9155e+11, 8.2004e+11,
        1.9895e+11, 3.1603e+11, 1.0406e+12, 3.2743e+10, 1.4623e+11, 5.5701e+10,
        1.0038e+12, 9.2894e+11, 1.2920e+12, 6.5393e+12, 4.3622e+11, 1.8342e+12,
        4.3883e+11, 6.9804e+11, 6.5698e+10, 9.8781e+11, 4.0299e+11, 1.4346e+11,
        2.6531e+11, 9.7892e+10, 9.5212e+11, 7.1553e+11, 2.5344e+12, 2.5874e+11,
        1.3867e+11, 1.1018e+11, 4.3028e+11, 1.1117e+11, 5.0922e+11, 5.1219e+11,
        2.0782e+11, 6.9323e+11, 5.6089e+10, 1.6038e+12, 6.5700e+11, 4.9275e+12,
        2.1169e+11, 5.1848e+11, 1.1029e+12, 6.2484e+10, 1.9620e+12, 1.7346e+11,
        2.4018e+11, 9.6837e+10, 4.6926e+11, 4.2694e+12, 2.4677e+10, 8.8045e+11,
        1.7576e+11, 6.6259e+11, 9.8621e+11, 4.9020e+11, 3.6786e+11, 1.2045e+12,
        7.9834e+11, 6.1693e+11, 5.6809e+11, 1.6809e+11, 4.0260e+11, 1.7655e+12,
        2.4580e+11, 1.5325e+12, 1.3255e+11, 1.1682e+11, 2.8269e+12, 3.8178e+11,
        2.3083e+11, 7.3461e+11, 2.5955e+11, 6.9358e+11, 8.1256e+11, 1.8759e+11,
        3.3440e+11, 7.6862e+11, 7.0791e+10, 4.2004e+11, 1.1304e+12, 8.7806e+11,
        8.8103e+11, 2.4647e+11, 1.1363e+11, 1.2224e+12, 5.9172e+11, 6.5270e+11,
        1.3083e+11, 1.5936e+11, 3.9060e+10, 5.5122e+11, 1.5493e+12, 1.2341e+11,
        2.5644e+11, 2.4777e+12, 7.7838e+11, 4.2921e+11, 6.7502e+11, 4.4459e+11,
        3.6160e+11, 7.2392e+11, 3.5408e+10, 2.0310e+11, 1.3134e+12, 4.0094e+11,
        6.0963e+10, 3.6937e+13, 1.1135e+12, 7.7603e+11, 2.6804e+11, 5.3109e+11,
        7.4009e+11, 3.5993e+11, 1.8896e+12, 3.0093e+11, 4.2383e+11, 3.7917e+11,
        2.6491e+11, 4.5348e+11, 1.7976e+12, 6.0976e+12, 2.9298e+10, 7.7175e+10,
        4.7760e+11, 6.9214e+11, 6.6592e+11, 4.3443e+11, 1.1244e+11, 1.6660e+11,
        6.7773e+11, 1.9600e+11, 7.4482e+10, 1.0204e+11, 3.4246e+12, 7.2808e+11,
        2.1030e+11, 1.8325e+13, 1.5306e+11, 1.9646e+11, 3.6066e+10, 3.2766e+11,
        5.5207e+11, 3.6760e+11, 5.0806e+11, 7.0318e+10, 6.9391e+11, 4.8017e+11,
        3.6211e+11, 6.1793e+11, 1.6337e+11, 4.8299e+11, 3.7943e+12, 7.0538e+11,
        8.2173e+11, 6.2888e+11, 2.9625e+11, 7.5089e+11, 3.3168e+11, 1.7495e+11,
        1.0112e+12, 3.2187e+12, 5.6343e+12, 1.7982e+11, 2.3662e+12, 2.9100e+12,
        3.2135e+11, 1.3125e+11, 2.2211e+10, 7.6687e+10, 7.7234e+11, 4.9362e+11,
        7.0643e+11, 6.0203e+11, 4.2029e+11, 3.6511e+12, 6.8888e+11, 2.1380e+12,
        2.0913e+11, 1.1836e+11, 1.2252e+12, 4.7343e+11, 1.9001e+12, 4.0979e+11,
        2.8222e+11, 6.6661e+11, 4.1386e+11, 4.4302e+11, 1.0472e+11, 1.4424e+11,
        3.6820e+10, 2.1282e+11, 9.5755e+11, 2.0295e+11, 2.7921e+12, 1.0307e+12,
        8.4292e+11, 3.2676e+11, 5.8965e+11, 7.7227e+10, 5.1205e+11, 1.3531e+11,
        8.1444e+11, 6.2976e+11, 1.1834e+12, 1.7116e+11, 5.2647e+10, 1.5781e+11,
        1.0860e+12, 3.1494e+11, 7.1579e+11, 2.1670e+11, 2.2893e+11, 2.6537e+11,
        9.8572e+11, 1.4821e+11, 1.2005e+12, 1.9095e+11, 1.6112e+12, 2.3527e+11,
        1.8601e+13, 4.6078e+11, 1.4155e+11, 3.8723e+11, 4.6580e+11, 5.9514e+11,
        8.6832e+11, 1.3250e+12, 1.1362e+11, 6.4991e+11, 5.5182e+11, 5.1427e+11,
        4.4675e+12, 6.0844e+11, 2.4926e+11, 6.1660e+11, 2.5762e+12, 1.0634e+11,
        3.0060e+11, 4.7180e+11, 9.2680e+10, 1.2803e+12, 5.7208e+11, 7.0615e+11,
        5.2477e+11, 1.3860e+12, 1.1308e+12, 2.0244e+11, 8.2913e+11, 1.6612e+12,
        6.5651e+11, 1.5227e+11, 1.7432e+11, 1.4891e+12, 4.2639e+11, 1.6836e+12,
        4.6343e+10, 1.1515e+11, 5.0345e+11, 1.4092e+11, 1.7614e+12, 7.1665e+11,
        1.6758e+13, 7.3300e+11, 2.4800e+11, 8.6795e+11, 1.7224e+11, 1.3553e+11,
        1.9692e+11, 1.9919e+12, 1.5193e+11, 6.8234e+11, 8.6228e+11, 3.1804e+11,
        1.5951e+11, 8.2207e+11, 1.0999e+12, 3.6776e+11, 1.0173e+12, 2.4781e+12,
        5.7104e+11, 1.3045e+11, 2.0878e+12, 5.4749e+10, 9.3925e+11, 9.7211e+11,
        5.9298e+11, 1.4272e+12, 7.4466e+11, 5.9174e+11, 7.6556e+12, 5.2878e+11,
        1.2034e+12, 1.4999e+11])
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.0852e+12, 2.1108e+11, 1.3514e+12, 1.2412e+12, 1.1033e+12, 2.2180e+12,
        8.7122e+11, 1.4591e+12, 4.2210e+11, 1.6817e+11, 2.4754e+12, 5.1057e+11,
        1.1451e+12, 1.4882e+12, 2.5834e+12, 3.5437e+12, 2.4257e+12, 1.7038e+12,
        2.6260e+12, 1.8256e+12, 2.7603e+12, 1.5330e+12, 5.6607e+11, 2.0349e+12,
        1.6351e+12, 1.5626e+12, 1.5974e+11, 1.1716e+12, 1.1220e+12, 6.5110e+11,
        1.4746e+12, 4.0191e+12, 1.0796e+12, 1.1316e+12, 2.1213e+11, 1.8414e+12,
        2.5353e+12, 1.5840e+12, 2.7039e+12, 6.3122e+11, 1.0324e+12, 1.3786e+12,
        1.0988e+12, 6.5792e+11, 4.9811e+11, 4.6667e+11, 2.0465e+12, 6.9045e+11,
        1.6784e+12, 2.1250e+12, 6.7559e+11, 4.1827e+12, 1.0199e+12, 4.8194e+11,
        1.5956e+12, 7.2641e+11, 4.9823e+11, 1.8303e+12, 2.7489e+12, 2.4107e+12,
        1.5546e+12, 2.5996e+12, 6.9679e+11, 1.1044e+12, 1.0654e+12, 1.1393e+12,
        3.0728e+11, 4.0455e+12, 1.3221e+12, 7.1359e+11, 1.8888e+12, 1.7244e+12,
        2.0008e+12, 1.9332e+12, 2.1881e+12, 3.1527e+12, 6.1169e+11, 1.8850e+12,
        1.1375e+12, 7.5931e+11, 2.9006e+12, 2.0253e+12, 1.6291e+12, 2.3131e+12,
        1.9728e+12, 5.2056e+11, 2.0402e+12, 9.6556e+11, 9.8730e+11, 2.1863e+12,
        1.2141e+12, 2.6317e+12, 1.1162e+12, 2.8556e+12, 1.2595e+12, 2.0670e+12,
        1.3326e+12, 7.4636e+11, 1.0520e+12, 7.6544e+11, 9.3277e+11, 6.6732e+11,
        1.6136e+12, 1.2630e+12, 1.5743e+12, 2.9454e+12, 1.2720e+12, 9.1843e+11,
        2.8938e+12, 2.0650e+12, 2.5352e+11, 7.2293e+11, 4.6710e+11, 1.6975e+12,
        2.0639e+12, 3.4220e+12, 1.5674e+12, 9.7473e+11, 6.7082e+11, 1.8166e+12,
        1.2055e+12, 5.7537e+11, 1.9347e+12, 1.6761e+12, 7.8779e+11, 4.3175e+12,
        8.5066e+11, 1.7565e+12])
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([4.7006e+11, 8.2643e+11, 1.0076e+12, 8.9780e+11, 2.1066e+12, 3.5466e+12,
        3.1110e+12, 7.1347e+12, 2.0157e+12, 2.9881e+12, 9.0358e+12, 4.3322e+12,
        4.3576e+12, 6.1480e+11, 4.0009e+12, 4.9021e+12, 5.8782e+12, 1.0466e+12,
        2.4482e+11, 1.6079e+12, 2.5778e+12, 8.3025e+12, 5.7578e+12, 4.6477e+12,
        3.1643e+12, 2.3418e+12, 3.1521e+12, 1.2947e+12, 8.6356e+12, 3.8755e+12,
        2.6220e+12, 4.2987e+12, 1.0946e+12, 3.8255e+12, 2.9762e+12, 2.3893e+12,
        6.9566e+12, 2.8823e+12, 1.1607e+12, 5.0942e+12, 2.5900e+12, 5.7260e+11,
        4.5805e+12, 2.4422e+12, 3.0271e+12, 1.6899e+12, 1.7756e+12, 1.5482e+12,
        1.0955e+12, 2.8107e+11, 4.1051e+11, 3.6140e+12, 1.5705e+12, 6.6219e+11,
        2.0783e+12, 2.1141e+11, 3.1195e+12, 2.4034e+12, 4.9425e+12, 1.2000e+12,
        3.9961e+12, 3.1678e+12, 3.3006e+12, 2.2391e+12, 5.8929e+12, 1.4638e+12,
        2.5848e+12, 3.8039e+12, 4.7729e+12, 2.7227e+12, 2.0492e+12, 5.4585e+12,
        6.8624e+11, 3.6446e+12, 1.9327e+12, 5.3967e+12, 2.8726e+12, 1.4401e+12,
        1.6749e+12, 1.4224e+12, 5.5374e+12, 1.3036e+12, 1.1702e+12, 2.6090e+12,
        4.2701e+12, 1.1077e+12, 9.2905e+12, 1.5336e+12, 2.4462e+12, 2.6234e+12,
        6.7324e+11, 7.7786e+11, 6.0261e+11, 1.1504e+12, 1.2446e+12, 6.1543e+12,
        3.3218e+11, 2.3505e+12, 6.5817e+11, 2.4741e+11, 3.2071e+11, 2.8085e+12,
        1.0083e+12, 7.0438e+11, 1.5546e+12, 2.0603e+12, 2.6988e+12, 1.3664e+12,
        4.9215e+12, 1.9967e+12, 8.1292e+11, 2.9130e+12, 4.8605e+11, 3.5071e+12,
        2.5387e+12, 5.9927e+12, 5.1188e+12, 4.5080e+12, 2.4671e+12, 1.5204e+12,
        1.1647e+12, 6.6216e+12, 1.1771e+12, 4.5772e+12, 2.2721e+12, 7.4552e+12,
        2.5937e+12, 4.1051e+12])
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.2231e+11, 2.1328e+10, 1.9910e+10, 2.8973e+10, 1.1424e+11, 2.4997e+11,
        1.3220e+11, 2.0919e+11, 8.9159e+10, 4.5494e+09, 9.7759e+10, 5.3762e+11,
        3.8163e+11, 1.1361e+11, 4.0147e+09, 2.1208e+11, 9.2303e+09, 1.2120e+13,
        3.5027e+11, 1.6684e+11, 1.8572e+11, 7.5640e+10, 8.8946e+10, 2.5031e+11,
        5.1328e+09, 4.1298e+10, 7.1870e+10, 5.5073e+10, 3.4943e+10, 4.4648e+10,
        1.3390e+11, 4.8002e+11, 2.7273e+11, 1.2067e+11, 9.8491e+09, 8.3076e+10,
        4.2258e+10, 1.6243e+10, 5.6921e+10, 1.8307e+10, 4.2895e+10, 2.5721e+10,
        6.7277e+10, 5.4663e+10, 7.8212e+10, 2.0589e+10, 2.1747e+10, 1.1091e+12,
        2.2472e+11, 2.1219e+11, 5.3801e+10, 7.8584e+11, 2.2723e+10, 1.3536e+11,
        1.0644e+10, 2.3952e+11, 4.5247e+10, 2.3432e+11, 1.3438e+11, 2.5126e+11,
        1.7028e+11, 1.1800e+11, 1.4354e+11, 1.7133e+11, 1.3465e+10, 1.1618e+10,
        6.5461e+09, 5.3503e+10, 1.4573e+11, 6.3013e+10, 3.0189e+11, 1.2907e+10,
        1.9527e+11, 4.8333e+10, 5.9463e+10, 7.1074e+11, 8.5953e+10, 2.1039e+11,
        1.3840e+10, 7.6169e+10, 1.8995e+11, 9.7016e+10, 4.9513e+10, 2.7148e+10,
        7.0572e+10, 2.0221e+11, 2.5519e+10, 3.4583e+09, 1.9008e+11, 2.3136e+11,
        3.6874e+11, 1.1432e+11, 7.0364e+10, 9.6180e+10, 3.0122e+11, 2.0273e+11,
        1.7687e+11, 2.4207e+11, 9.9652e+10, 1.9331e+10, 2.1540e+11, 2.7667e+10,
        2.2868e+11, 3.8340e+10, 4.7053e+09, 3.3832e+11, 1.4504e+11, 8.9332e+10,
        1.5260e+11, 2.4865e+11, 1.7943e+11, 1.4109e+11, 1.0438e+09, 1.1774e+10,
        1.8742e+11, 7.8772e+10, 2.0807e+10, 1.9602e+10, 8.0167e+10, 3.0014e+11,
        3.9940e+11, 1.2629e+11, 2.9303e+09, 3.0625e+10, 9.5852e+11, 5.1409e+10,
        3.1632e+11, 6.3946e+09, 1.7208e+11, 8.3563e+11, 2.7422e+11, 1.3366e+11,
        1.2026e+11, 5.9699e+10, 4.2285e+10, 1.3819e+10, 1.4368e+11, 2.7920e+11,
        1.4558e+10, 9.9353e+10, 1.3235e+11, 1.5649e+11, 4.2384e+10, 1.5736e+11,
        7.9016e+10, 4.5555e+10, 6.9963e+10, 1.3376e+10, 2.0142e+10, 1.5043e+10,
        1.9713e+11, 3.0925e+11, 3.0217e+11, 5.7247e+10, 1.1521e+10, 1.2455e+10,
        1.9837e+10, 2.7137e+09, 3.1224e+11, 8.0681e+10, 3.7963e+10, 1.4525e+11,
        1.0938e+11, 2.5677e+10, 4.6541e+10, 2.3974e+11, 9.3269e+11, 4.5888e+10,
        1.2121e+11, 5.7446e+10, 8.6758e+10, 3.4727e+11, 1.0367e+11, 1.7632e+11,
        1.2020e+11, 7.9467e+12, 1.0955e+11, 1.9199e+10, 3.6366e+10, 1.3332e+11,
        3.2832e+11, 1.2256e+12, 1.7248e+12, 3.2723e+11, 4.5368e+11, 3.4324e+11,
        1.4975e+11, 3.1325e+09, 5.4130e+10, 1.2571e+11, 5.5321e+10, 6.8537e+10,
        1.3006e+11, 3.8182e+10, 2.5983e+09, 2.4718e+11, 2.9714e+10, 8.1970e+10,
        1.3620e+09, 1.4502e+11, 1.0221e+11, 1.4054e+10, 1.3019e+10, 3.5161e+11,
        4.0610e+10, 9.8131e+10, 1.2995e+11, 3.4298e+10, 4.5391e+10, 5.2314e+10,
        7.7643e+09, 7.3285e+10, 6.0393e+12, 4.5171e+10, 3.6304e+11, 3.0261e+11,
        3.7375e+11, 8.3310e+10, 6.4165e+10, 2.0450e+11, 1.6829e+10, 8.3077e+10,
        8.2999e+09, 5.7821e+10, 2.2711e+11, 2.7439e+10, 1.4453e+09, 1.7502e+09,
        2.8673e+11, 7.2524e+09, 3.0895e+11, 1.7835e+11, 2.6345e+11, 4.7287e+10,
        4.3497e+10, 1.2863e+11, 2.5242e+10, 5.1432e+10, 2.4459e+10, 4.0582e+09,
        2.3477e+11, 7.3565e+10, 1.0259e+13, 9.2010e+10, 1.2381e+11, 9.2571e+10,
        4.7224e+10, 3.5094e+11, 2.7653e+09, 1.1557e+09, 6.2119e+10, 2.5747e+11,
        8.7969e+11, 6.5554e+10, 5.5674e+10, 7.4288e+11, 3.2925e+11, 8.7725e+10,
        2.5172e+11, 1.6512e+11, 1.4809e+11, 1.4691e+10, 1.4790e+12, 4.3903e+11,
        2.7319e+10, 1.4477e+11, 1.8274e+10, 4.0053e+12, 5.0023e+09, 3.7213e+11,
        4.8093e+10, 3.8168e+11, 8.6707e+10, 7.1834e+10, 1.9207e+11, 1.2313e+11,
        1.1283e+11, 1.7375e+11, 2.1494e+12, 1.5138e+10, 8.0569e+10, 5.0626e+10,
        1.6021e+11, 1.3768e+10, 4.3442e+10, 7.1250e+09, 6.2849e+11, 2.4300e+11,
        2.2815e+11, 1.5524e+11, 3.3953e+11, 3.6283e+10, 1.1033e+11, 6.7748e+10,
        2.2401e+11, 3.0472e+11, 6.6278e+10, 5.8962e+11, 1.1172e+11, 1.5165e+11,
        5.2088e+10, 1.6515e+11, 2.3796e+09, 1.5485e+11, 1.5503e+11, 1.5257e+11,
        7.5181e+10, 5.2795e+10, 2.3512e+10, 1.7435e+10, 1.4432e+12, 3.1060e+10,
        4.7877e+10, 1.0779e+11, 6.0574e+10, 9.9762e+10, 2.4315e+11, 3.4087e+10,
        4.6145e+10, 1.1565e+11, 2.4607e+09, 1.1702e+11, 5.9225e+11, 1.0559e+11,
        9.8437e+10, 1.5514e+11, 9.2416e+10, 1.2086e+11, 2.8167e+09, 1.7656e+11,
        2.3777e+11, 3.1640e+10, 1.4021e+11, 1.0497e+11, 1.6318e+11, 4.3191e+10,
        1.1304e+11, 1.8083e+11, 3.6128e+10, 4.0467e+10, 6.8153e+09, 4.3872e+11,
        1.3562e+10, 1.1709e+10, 2.6172e+11, 1.3454e+10, 9.9731e+10, 2.0182e+10,
        5.5217e+10, 1.1985e+10, 4.0755e+10, 4.3572e+10, 7.9092e+10, 1.2568e+11,
        1.7598e+10, 8.7252e+10, 1.1490e+11, 2.3592e+10, 2.9807e+11, 2.2575e+10,
        4.9811e+11, 7.9347e+11, 3.4423e+11, 2.7641e+10, 9.1356e+10, 1.8171e+11,
        8.2760e+10, 1.2177e+10, 1.9604e+11, 2.6781e+10, 2.6766e+10, 5.6272e+10,
        9.6660e+10, 6.6487e+10, 4.5618e+11, 1.9814e+11, 2.8165e+10, 8.2529e+10,
        4.3743e+10, 3.9297e+10, 1.3016e+09, 2.0579e+10, 2.8453e+10, 8.6034e+10,
        2.1999e+10, 9.7019e+10, 1.9198e+10, 4.2188e+09, 2.4277e+09, 3.7221e+09,
        2.2745e+11, 1.3665e+11, 1.6956e+11, 3.2279e+11, 1.0981e+11, 1.8343e+11,
        2.4089e+09, 1.0044e+10, 1.2898e+11, 3.8313e+09, 5.0348e+10, 2.5216e+11,
        4.4216e+09, 5.9251e+10, 1.1020e+10, 2.2267e+11, 2.4352e+10, 1.6379e+11,
        4.5466e+10, 1.1752e+11, 5.6705e+10, 3.1673e+10, 3.1159e+12, 3.2104e+10,
        4.9943e+11, 1.6977e+10, 8.8214e+10, 4.8404e+10, 1.1638e+11, 4.4388e+10,
        8.1866e+10, 1.7226e+11, 1.8014e+11, 3.0652e+11, 5.1528e+10, 5.5384e+10,
        2.8496e+11, 5.5982e+10, 1.9258e+11, 6.4753e+10, 2.5729e+10, 4.9380e+10,
        4.8281e+10, 3.6534e+11, 1.1220e+11, 4.4998e+10, 5.5610e+11, 8.0613e+10,
        1.9478e+11, 6.2421e+09, 3.4018e+11, 4.2430e+11, 3.0453e+10, 1.8023e+10,
        2.3503e+10, 1.3327e+11, 1.4196e+11, 1.4851e+11, 1.3436e+11, 2.3801e+10,
        3.6121e+11, 2.4970e+11, 7.5978e+11, 7.1066e+10, 3.8770e+10, 8.1601e+10,
        3.1208e+11, 8.6530e+10, 8.5219e+10, 1.0043e+12, 7.4968e+09, 3.3853e+10,
        1.1556e+11, 5.7399e+10, 8.2288e+10, 6.3455e+11, 2.1258e+10, 8.5602e+10,
        3.2080e+10, 6.1047e+09, 1.6792e+11, 3.1566e+11, 2.7308e+11, 1.9492e+11,
        1.0364e+11, 3.6491e+09, 1.0280e+11, 5.1560e+10, 3.2926e+10, 3.2197e+11,
        8.3951e+12, 4.2163e+10, 7.8518e+10, 9.8382e+10, 7.7360e+10, 8.8112e+09,
        2.2184e+10, 7.6392e+11, 5.7098e+10, 6.4550e+10, 2.7361e+11, 3.0081e+11,
        4.9039e+10, 2.4001e+11, 2.1961e+11, 2.4776e+11, 6.8507e+11, 1.0954e+12,
        9.1049e+10, 2.6032e+10, 4.1485e+11, 2.5767e+11, 1.3499e+11, 2.2851e+10,
        3.1094e+11, 3.3260e+11, 2.3124e+10, 2.7086e+11, 2.1420e+10, 5.1428e+09,
        1.1962e+10, 1.5275e+11])
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6788e+10, 1.3032e+11, 4.1341e+11, 1.1600e+12, 2.4915e+11, 2.9535e+11,
        1.4313e+12, 4.1472e+10, 2.2009e+11, 5.6920e+11, 4.2497e+11, 1.5773e+11,
        8.7301e+11, 7.2076e+11, 2.0022e+11, 2.3591e+11, 7.1368e+11, 5.1180e+11,
        5.0336e+11, 5.4569e+11, 3.0632e+11, 3.3105e+11, 8.2778e+11, 3.6110e+11,
        4.3933e+11, 6.9952e+11, 4.8565e+11, 1.1728e+12, 1.3368e+12, 3.2262e+10,
        1.1888e+10, 6.6881e+11, 2.4798e+11, 1.5782e+12, 1.1279e+12, 6.9093e+11,
        8.5149e+11, 3.7507e+11, 2.1750e+11, 9.7173e+10, 7.2095e+11, 4.9740e+11,
        1.0000e+11, 7.1124e+11, 1.1526e+11, 2.6580e+11, 3.4348e+11, 6.4806e+11,
        1.0416e+12, 7.0673e+10, 8.6244e+11, 8.4274e+11, 3.1205e+11, 6.8635e+11,
        5.0696e+11, 9.2919e+11, 3.9168e+11, 2.9123e+11, 1.0017e+12, 9.4562e+11,
        4.3895e+11, 7.0211e+10, 1.3614e+11, 3.7458e+10, 1.5242e+11, 3.1088e+11,
        1.3515e+11, 2.9302e+11, 3.4890e+11, 9.0375e+11, 7.3788e+11, 2.2959e+10,
        4.2905e+11, 6.1277e+10, 8.0473e+11, 2.9778e+11, 2.1789e+11, 4.3911e+10,
        3.5330e+11, 1.7162e+11, 4.6540e+11, 1.2243e+12, 1.7777e+12, 5.1810e+11,
        1.0488e+12, 1.0075e+12, 2.2508e+11, 6.7361e+11, 1.5890e+11, 1.7817e+11,
        2.5420e+11, 1.9386e+12, 2.6274e+11, 1.1568e+12, 8.8573e+11, 2.8631e+11,
        7.4367e+11, 1.8645e+11, 6.1672e+09, 3.4307e+11, 9.2730e+11, 1.3332e+11,
        1.0180e+12, 9.8353e+11, 8.6576e+11, 1.0835e+12, 5.6591e+10, 4.8684e+10,
        3.1419e+11, 4.9062e+11, 5.6827e+11, 5.2469e+11, 5.7067e+11, 1.1076e+12,
        4.1646e+11, 3.2764e+11, 3.7425e+11, 1.7154e+11, 3.6888e+11, 1.3578e+12,
        3.1416e+11, 1.6666e+12, 2.0381e+11, 1.4345e+11, 8.4203e+11, 3.0753e+11,
        2.7812e+11, 3.0987e+11])
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.0365e+12, 1.5070e+12, 2.6263e+11, 1.8210e+12, 5.5952e+11, 1.0603e+12,
        6.9201e+11, 9.0775e+11, 1.3239e+12, 1.6745e+12, 1.2266e+12, 1.9865e+12,
        8.2837e+11, 1.7531e+12, 1.1733e+12, 5.2442e+11, 1.0081e+12, 1.5234e+12,
        1.5993e+12, 3.4503e+11, 2.6034e+12, 5.8817e+11, 1.1222e+12, 7.4559e+10,
        2.5242e+11, 1.4387e+12, 1.4858e+11, 2.0344e+12, 6.0233e+11, 1.2330e+12,
        4.7652e+11, 2.3270e+12, 1.3409e+11, 2.8818e+12, 1.8415e+12, 1.1679e+12,
        7.0304e+11, 1.6559e+11, 1.2261e+11, 3.4568e+10, 2.5939e+12, 1.9603e+12,
        7.6170e+11, 1.6845e+12, 3.6480e+11, 8.4127e+11, 5.5143e+11, 1.1793e+12,
        1.0324e+12, 3.5930e+11, 1.0938e+11, 4.4481e+11, 2.6475e+11, 2.2985e+12,
        7.2415e+11, 3.1594e+11, 5.5944e+11, 5.0395e+11, 1.8208e+11, 5.7762e+11,
        1.3076e+12, 3.3462e+11, 1.9449e+12, 2.2499e+12, 2.4166e+12, 1.1708e+12,
        7.3219e+11, 2.1907e+12, 2.6503e+12, 1.3848e+12, 8.3613e+11, 9.3834e+11,
        1.7299e+11, 6.8963e+11, 1.1321e+12, 9.4202e+11, 2.3495e+12, 7.5434e+11,
        3.2874e+11, 7.2007e+11, 2.1389e+12, 5.0549e+10, 9.0399e+11, 1.4252e+12,
        9.1591e+11, 9.7215e+11, 3.2803e+11, 1.4803e+12, 1.0691e+12, 6.3344e+11,
        3.7621e+11, 7.9953e+10, 4.2227e+12, 5.9042e+11, 4.9832e+11, 8.4505e+11,
        1.0909e+12, 1.0082e+11, 5.5518e+11, 1.2352e+12, 7.7042e+11, 5.6119e+11,
        1.8683e+11, 1.3357e+12, 6.1628e+11, 3.7944e+11, 2.5204e+11, 4.2188e+11,
        1.4064e+12, 5.0526e+11, 8.5713e+11, 1.0442e+12, 6.8869e+11, 1.4716e+12,
        5.2122e+11, 8.2121e+11, 2.9163e+11, 2.2852e+12, 6.8798e+11, 1.5960e+12,
        3.5741e+12, 8.6369e+11, 2.0480e+12, 1.8274e+12, 1.2620e+12, 3.8429e+11,
        1.3456e+11, 1.2224e+12])
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([3.2826e+08, 1.9898e+10, 1.7246e+08, 2.1044e+09, 1.7162e+09, 4.7869e+08,
        4.5564e+08, 5.5313e+08, 2.4245e+09, 6.7806e+08, 4.4374e+08, 3.4012e+11,
        1.6218e+09, 2.1710e+09, 7.5749e+08, 1.5002e+09, 2.5494e+08, 2.7395e+09,
        1.3164e+09, 8.6608e+09, 5.6872e+09, 1.8980e+09, 6.2103e+09, 9.2878e+09,
        5.4419e+08, 2.3021e+09, 2.3147e+10, 2.2729e+09, 1.5575e+09, 6.9792e+08,
        1.2210e+09, 4.2510e+08, 7.0271e+09, 2.9556e+08, 8.8413e+08, 7.4980e+09,
        2.1278e+09, 2.9049e+09, 1.9772e+09, 1.4673e+08, 5.6796e+08, 4.2884e+09,
        1.4796e+09, 9.4089e+08, 1.7714e+08, 7.0689e+08, 5.4527e+08, 3.2515e+09,
        3.0189e+09, 3.8753e+09, 2.1253e+09, 1.9632e+09, 9.6126e+08, 1.8827e+09,
        2.1444e+08, 2.9879e+09, 3.0010e+09, 5.0944e+09, 9.8119e+09, 4.3307e+09,
        3.0798e+09, 2.5206e+09, 5.0318e+08, 1.8581e+09, 4.6931e+07, 1.0650e+08,
        1.2249e+09, 9.7654e+08, 2.6129e+09, 2.5748e+09, 1.1715e+09, 1.4510e+09,
        1.0641e+10, 2.6223e+09, 7.0050e+09, 3.9603e+09, 8.1473e+08, 3.4937e+09,
        3.5314e+08, 2.9322e+09, 1.0757e+09, 8.1995e+08, 8.8856e+08, 1.3616e+09,
        4.7575e+08, 8.6949e+08, 6.2327e+09, 1.0486e+08, 1.1862e+09, 5.3457e+08,
        5.6059e+09, 1.8295e+09, 2.0039e+09, 8.5241e+08, 2.2195e+08, 4.0927e+08,
        4.5210e+08, 5.1299e+09, 5.9147e+09, 1.2920e+08, 1.4593e+09, 5.5148e+08,
        1.8811e+09, 2.4088e+09, 2.1268e+09, 1.5192e+09, 2.0412e+09, 1.9909e+09,
        4.7068e+08, 2.0002e+09, 1.3675e+09, 2.2311e+09, 4.1187e+08, 1.2728e+08,
        4.1022e+09, 1.7875e+08, 1.6798e+09, 1.1390e+09, 1.3384e+10, 1.2748e+09,
        3.9688e+09, 1.7692e+09, 2.0052e+10, 5.8919e+09, 7.3670e+09, 1.2028e+09,
        4.7595e+09, 1.8146e+08, 1.3759e+09, 4.4122e+09, 4.1540e+09, 2.1901e+09,
        9.7694e+08, 3.1362e+09, 1.7269e+08, 6.8505e+09, 8.6685e+09, 3.4348e+08,
        7.0790e+09, 1.1806e+08, 4.2003e+09, 7.0398e+09, 4.7935e+08, 4.3565e+09,
        1.4573e+09, 7.7606e+09, 3.8628e+08, 1.5402e+10, 1.4000e+09, 7.5143e+09,
        7.4833e+08, 7.1283e+09, 6.4776e+09, 4.9414e+09, 7.8602e+08, 6.3812e+08,
        5.4612e+09, 3.4767e+08, 2.4566e+09, 8.1763e+08, 9.7359e+07, 2.3980e+08,
        2.9832e+09, 1.8487e+09, 5.8241e+08, 1.0536e+10, 9.7141e+09, 4.6769e+08,
        3.0971e+09, 1.6471e+09, 2.0873e+08, 2.7544e+08, 3.5189e+08, 2.7784e+08,
        2.0765e+10, 4.9006e+09, 1.5210e+09, 1.2575e+08, 5.8325e+09, 1.5834e+09,
        5.6067e+09, 8.9356e+08, 8.0934e+09, 1.5782e+09, 3.9212e+09, 5.0231e+09,
        5.0208e+08, 2.5102e+08, 7.0846e+08, 7.7191e+09, 3.4006e+08, 4.3519e+09,
        6.6122e+09, 2.2820e+09, 1.0699e+09, 9.8297e+09, 3.0509e+08, 2.0018e+08,
        7.0457e+08, 2.4259e+09, 9.2866e+08, 8.1175e+08, 6.4215e+08, 2.3964e+09,
        1.7739e+09, 5.6137e+09, 1.7222e+09, 2.8526e+08, 4.6974e+08, 1.3390e+09,
        1.1515e+09, 1.7970e+08, 3.3921e+09, 1.1813e+09, 7.0487e+08, 6.3506e+09,
        4.6320e+09, 1.3706e+10, 4.5638e+09, 1.4678e+10, 1.2254e+10, 8.4489e+09,
        1.2883e+09, 2.6841e+08, 2.6688e+08, 2.0059e+08, 1.0640e+08, 4.7891e+08,
        9.5403e+09, 4.7859e+09, 1.0247e+09, 2.6143e+09, 2.7329e+09, 2.4056e+08,
        5.2171e+08, 3.4685e+09, 9.3408e+08, 9.6579e+08, 1.2597e+09, 2.4158e+09,
        3.1017e+09, 1.1393e+08, 9.3326e+12, 2.5418e+09, 4.3195e+09, 4.8894e+08,
        4.6976e+08, 9.2174e+08, 8.8092e+08, 1.1300e+09, 1.8338e+09, 1.5727e+09,
        2.1672e+09, 1.3270e+08, 5.1961e+07, 1.6513e+10, 1.7228e+09, 1.5488e+09,
        8.4158e+09, 1.1437e+09, 8.0917e+08, 3.1425e+08, 1.3844e+09, 7.9351e+08,
        2.8606e+08, 5.0336e+08, 2.6138e+08, 4.6918e+09, 2.3486e+08, 9.4326e+08,
        2.7066e+09, 2.3360e+09, 1.2839e+09, 2.6531e+09, 3.8665e+08, 7.8171e+08,
        7.3243e+09, 9.1555e+08, 4.8988e+09, 1.5900e+09, 2.4421e+09, 3.8768e+09,
        1.8009e+09, 5.5546e+09, 1.1167e+10, 2.5818e+08, 8.3038e+09, 3.8590e+10,
        2.4813e+09, 6.0422e+08, 7.1990e+08, 8.9128e+09, 5.2638e+09, 3.6443e+09,
        2.7108e+09, 1.0384e+09, 1.3140e+09, 2.8868e+09, 9.0199e+09, 1.0656e+09,
        1.3840e+09, 9.4332e+09, 3.9575e+08, 1.4595e+09, 1.7334e+09, 5.7804e+09,
        7.7515e+08, 7.4898e+09, 2.6263e+09, 8.1547e+08, 1.2708e+10, 6.6569e+08,
        1.0464e+10, 4.7856e+08, 8.8298e+09, 2.9464e+09, 5.2434e+09, 1.3245e+09,
        5.6611e+08, 2.3463e+08, 5.7049e+08, 6.8461e+09, 8.3495e+08, 2.2678e+09,
        3.4507e+09, 4.6343e+09, 1.5750e+09, 5.4834e+09, 3.5637e+08, 3.0448e+08,
        3.1534e+09, 1.2471e+08, 2.3740e+09, 1.3571e+09, 5.8501e+09, 2.0396e+08,
        6.4406e+09, 1.1374e+10, 3.5353e+09, 2.1839e+09, 1.5311e+08, 1.5134e+09,
        3.8495e+09, 1.5920e+09, 5.9433e+09, 3.3058e+08, 8.6364e+08, 6.5035e+08,
        5.3170e+08, 2.7537e+09, 7.7857e+07, 1.6978e+08, 5.0433e+09, 5.4201e+08,
        5.9221e+08, 8.7129e+08, 9.0852e+08, 3.2608e+08, 1.0719e+08, 3.2745e+10,
        1.0530e+09, 2.0458e+09, 1.8918e+09, 5.2115e+08, 2.5887e+09, 1.6817e+09,
        1.1191e+09, 2.7539e+08, 5.0222e+09, 4.6942e+08, 4.7458e+08, 1.5889e+10,
        1.5702e+09, 2.6911e+10, 7.9189e+09, 2.1936e+09, 4.5441e+08, 1.0744e+09,
        3.3403e+08, 3.4689e+10, 1.0683e+09, 2.9859e+09, 3.7439e+09, 9.5528e+09,
        2.2343e+09, 1.2282e+09, 5.7123e+07, 3.9533e+08, 7.7615e+08, 1.9031e+08,
        6.2277e+08, 2.7738e+09, 5.7579e+08, 1.5870e+09, 1.5105e+09, 1.1587e+09,
        5.7329e+08, 4.9685e+07, 7.2219e+08, 1.6655e+10, 3.0863e+09, 2.5018e+09,
        1.5392e+08, 4.4501e+08, 5.1802e+09, 4.8755e+09, 2.8352e+08, 8.0603e+07,
        6.3984e+09, 1.2247e+09, 7.2597e+09, 3.6023e+08, 2.8697e+09, 7.2603e+08,
        5.4325e+07, 9.9411e+08, 9.7599e+08, 1.7825e+09, 6.3635e+09, 4.3664e+08,
        7.4594e+08, 8.6183e+09, 1.2844e+09, 1.2516e+09, 7.3736e+08, 6.2190e+09,
        9.1037e+08, 1.3824e+09, 6.1621e+09, 4.3446e+08, 1.3598e+09, 4.6661e+07,
        1.8155e+09, 2.7074e+09, 3.8774e+09, 3.2194e+08, 4.4041e+09, 1.9487e+10,
        4.2928e+09, 7.7132e+08, 2.5120e+10, 3.5449e+09, 1.4498e+09, 1.0154e+09,
        8.8024e+09, 6.8177e+08, 2.5323e+09, 3.5049e+09, 7.0604e+08, 2.2380e+09,
        2.5787e+09, 2.2208e+09, 2.6759e+11, 1.1419e+09, 5.5039e+09, 2.1786e+08,
        1.7018e+08, 2.2654e+08, 1.5934e+10, 4.4208e+09, 6.0748e+08, 7.9561e+08,
        2.0474e+08, 8.9529e+09, 1.2132e+09, 3.0323e+09, 7.2750e+08, 5.2932e+09,
        4.2754e+08, 2.0368e+08, 2.1083e+10, 1.0059e+10, 4.3437e+09, 9.3521e+08,
        1.3379e+08, 1.2132e+08, 1.5568e+09, 1.1068e+09, 2.1867e+09, 4.0954e+08,
        9.9035e+09, 3.9137e+08, 4.0069e+09, 2.1478e+08, 7.5015e+09, 9.3607e+08,
        4.4999e+08, 4.4932e+09, 1.6934e+09, 2.9658e+08, 5.9703e+09, 2.5490e+09,
        3.3520e+08, 1.5857e+09, 2.3000e+08, 7.6306e+08, 8.5856e+09, 1.2596e+09,
        1.7386e+09, 2.1758e+08, 6.1958e+09, 2.4822e+09, 7.6914e+09, 7.1030e+08,
        3.6876e+09, 4.9880e+09, 5.4378e+09, 2.1402e+09, 1.8519e+10, 2.9923e+09,
        6.8049e+08, 3.2257e+09])
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([6.9719e+09, 3.3206e+09, 2.1107e+09, 1.0494e+10, 2.8448e+09, 5.3072e+09,
        1.3036e+09, 3.9184e+09, 3.5679e+09, 1.0627e+10, 3.0803e+09, 2.9583e+09,
        1.3020e+10, 7.9702e+09, 1.1385e+09, 1.0360e+10, 4.2400e+09, 7.5254e+09,
        5.1521e+09, 6.5294e+09, 1.8998e+09, 2.8084e+09, 2.4392e+09, 1.5358e+09,
        8.6902e+08, 1.7971e+09, 7.1191e+09, 2.4023e+09, 4.2001e+09, 2.5880e+09,
        1.2539e+09, 4.3734e+09, 5.7518e+09, 8.8027e+09, 2.6485e+09, 9.9078e+09,
        5.1422e+09, 8.9375e+08, 3.7382e+09, 2.5807e+09, 9.6615e+08, 7.0970e+09,
        5.0696e+09, 7.2315e+09, 1.8266e+09, 5.7229e+09, 7.6740e+09, 4.7825e+09,
        2.9029e+09, 7.7618e+09, 4.9176e+09, 5.1481e+09, 2.2615e+09, 4.1282e+09,
        4.4734e+09, 1.4201e+09, 1.8641e+09, 8.9168e+08, 4.2932e+09, 6.2719e+08,
        5.5622e+08, 6.5812e+09, 4.0025e+09, 4.9590e+09, 5.5204e+08, 4.4842e+08,
        2.2865e+09, 4.7209e+09, 4.9791e+08, 1.9357e+09, 3.2997e+09, 1.0161e+10,
        1.1117e+09, 7.0973e+09, 1.1125e+09, 2.1111e+09, 1.3698e+09, 3.4674e+09,
        5.5418e+09, 2.8950e+09, 6.5617e+09, 2.9576e+09, 1.5617e+09, 5.1307e+09,
        3.1917e+09, 2.6929e+09, 1.7348e+09, 5.6020e+09, 2.3986e+09, 5.1797e+09,
        3.8944e+09, 4.2627e+09, 1.9344e+09, 5.6045e+09, 8.5456e+08, 5.3970e+09,
        6.2116e+09, 8.7956e+09, 2.7991e+09, 5.7348e+09, 1.1856e+10, 3.8775e+09,
        4.0041e+09, 4.8697e+09, 5.3206e+08, 3.3871e+09, 1.3041e+10, 7.8047e+09,
        4.7959e+09, 9.5375e+08, 4.7006e+09, 7.4761e+09, 8.4933e+09, 1.8901e+09,
        5.8418e+09, 1.9938e+09, 4.9007e+09, 6.2378e+08, 7.2554e+09, 1.9418e+09,
        4.5173e+09, 2.7008e+09, 7.0669e+09, 5.0311e+09, 1.0492e+10, 4.0880e+09,
        4.9516e+09, 9.3856e+09, 9.6419e+08, 5.4671e+09, 7.1441e+09, 1.8289e+09,
        1.2463e+10, 8.3597e+09, 9.4349e+09, 2.7357e+09, 4.9805e+09, 8.8277e+09,
        7.9719e+09, 3.4053e+09, 9.5846e+09, 8.2167e+09, 3.7225e+09, 6.4241e+09,
        2.4677e+09, 2.5044e+09, 3.4197e+09, 9.0107e+09, 1.0035e+10, 6.6344e+09,
        2.4567e+09, 2.9608e+09, 4.0691e+08, 2.3119e+09, 2.1787e+09, 7.2569e+09,
        3.3218e+09, 3.2589e+09, 1.1141e+10, 7.4211e+09, 1.3277e+10, 6.2923e+09,
        1.8385e+09, 6.8321e+09, 1.5583e+09, 7.4604e+09, 5.3145e+09, 9.0450e+09,
        4.4973e+09, 6.6396e+09, 2.2252e+09, 1.7447e+09, 4.7108e+09, 7.6482e+09,
        2.6815e+09, 8.9286e+08, 1.8567e+09, 1.6220e+09, 1.4354e+09, 1.3179e+10,
        4.1311e+09, 2.3437e+09, 4.6241e+09, 1.5793e+09, 4.1385e+09, 1.3757e+09,
        5.4286e+09, 1.1383e+10, 8.6477e+09, 6.3818e+08, 6.8798e+08, 2.1836e+09,
        7.4428e+09, 2.5754e+09, 4.7495e+09, 4.8648e+08, 1.4597e+09, 8.7803e+09,
        4.4564e+09, 9.0368e+08, 6.6712e+09, 2.1727e+09, 9.1064e+09, 4.3698e+09,
        5.5984e+09, 1.3438e+09, 5.2206e+09, 3.0880e+09, 3.1898e+09, 3.1633e+09,
        4.5903e+09, 9.7142e+08, 9.8287e+08, 3.0527e+09, 2.9421e+09, 3.0859e+09,
        6.3195e+09, 3.1236e+09, 4.9064e+09, 1.1620e+10, 4.0393e+09, 4.8863e+09,
        4.2609e+09, 6.1667e+09, 4.0221e+09, 7.3311e+09, 2.1773e+09, 5.0024e+09,
        1.0636e+09, 9.4590e+09, 2.4627e+09, 4.7346e+09, 4.7403e+09, 4.7940e+09,
        6.3044e+09, 4.8707e+09, 6.3886e+09, 5.1684e+09, 2.9848e+09, 1.5251e+09,
        3.2722e+09, 6.4865e+09, 4.0985e+09, 3.0350e+09, 4.4463e+09, 6.0384e+09,
        1.9042e+09, 5.2788e+09, 5.3515e+09, 2.4559e+09, 3.9180e+09, 4.0688e+09,
        9.2655e+09, 5.4386e+09, 5.4526e+09, 3.7595e+09])
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([5.1513e+09, 1.3375e+10, 2.7207e+10, 7.0748e+09, 5.9005e+09, 4.7940e+09,
        2.4630e+09, 3.5190e+09, 1.8889e+09, 5.7652e+09, 2.3295e+10, 7.8558e+09,
        4.3490e+09, 7.6788e+09, 1.7229e+10, 1.2698e+10, 2.2257e+09, 8.3260e+09,
        4.9318e+09, 3.2800e+09, 8.1135e+09, 1.5522e+10, 1.0359e+10, 1.7433e+10,
        1.8081e+10, 4.2725e+09, 1.9572e+10, 2.7534e+09, 3.8905e+09, 1.6592e+10,
        1.9444e+10, 1.3066e+10, 4.1039e+09, 9.9070e+09, 1.0065e+10, 1.4988e+10,
        6.8330e+09, 8.0812e+08, 1.8601e+10, 1.0773e+10, 3.7762e+09, 5.7647e+09,
        2.4827e+09, 6.5927e+09, 1.2961e+10, 6.7816e+09, 6.0744e+09, 6.3575e+09,
        4.9853e+09, 5.3190e+09, 4.3718e+09, 1.1319e+10, 2.4155e+09, 1.1488e+10,
        1.2401e+10, 1.9045e+10, 1.2287e+10, 2.5554e+09, 3.4290e+09, 1.5788e+10,
        3.7978e+09, 9.6816e+09, 1.4197e+10, 1.3697e+10, 4.9934e+09, 9.3704e+09,
        4.3028e+09, 5.8397e+09, 3.3935e+09, 6.4662e+09, 1.6067e+10, 1.5205e+10,
        6.0539e+09, 3.9126e+09, 2.6106e+09, 2.9745e+09, 2.8058e+09, 8.7328e+09,
        4.6373e+09, 1.1373e+10, 1.1544e+10, 1.3517e+10, 2.3136e+10, 3.2263e+10,
        9.8370e+09, 2.6560e+09, 1.2594e+10, 4.8510e+09, 3.6944e+09, 2.9448e+09,
        2.8908e+09, 5.4075e+09, 1.6943e+09, 3.7288e+09, 1.4751e+10, 8.8811e+09,
        2.0963e+10, 8.1334e+09, 1.4709e+10, 2.5507e+09, 2.5758e+09, 1.2719e+10,
        1.7137e+10, 5.8290e+09, 1.1874e+10, 6.4415e+09, 1.3275e+10, 1.2037e+09,
        6.1274e+09, 9.4855e+09, 6.3596e+09, 1.6879e+10, 1.3359e+10, 1.7579e+10,
        5.4110e+09, 4.3735e+09, 1.6091e+10, 5.4623e+09, 5.2699e+09, 9.0839e+09,
        5.9590e+09, 6.0532e+09, 8.5762e+09, 1.4341e+09, 4.4810e+09, 1.6445e+10,
        5.9898e+09, 1.0458e+10, 5.4212e+09, 5.2687e+09, 5.4246e+09, 1.2755e+10,
        2.1240e+10, 7.0233e+09, 1.3768e+10, 6.5451e+09, 8.3731e+09, 1.3200e+10,
        1.8474e+10, 3.2343e+09, 7.5594e+09, 7.3164e+09, 2.1283e+10, 2.8301e+09,
        2.5893e+09, 1.1035e+10, 1.5429e+10, 1.2644e+10, 1.3974e+10, 6.1956e+09,
        1.2961e+09, 6.6005e+09, 7.7633e+09, 1.1543e+10, 8.1092e+09, 4.4684e+09,
        9.9882e+09, 1.1576e+10, 3.1645e+09, 2.8095e+10, 3.7299e+09, 1.7356e+10,
        1.8598e+09, 3.9075e+09, 3.1985e+10, 1.6221e+10, 1.8244e+10, 3.2425e+09,
        1.4119e+10, 1.7912e+10, 7.3090e+09, 1.1789e+10, 9.3528e+09, 1.1998e+10,
        1.3305e+10, 2.1228e+10, 9.2458e+09, 5.6964e+09, 1.6640e+10, 7.6363e+09,
        2.0131e+10, 2.7798e+09, 1.0794e+10, 1.6671e+10, 8.0558e+09, 1.8283e+10,
        2.8253e+09, 2.0011e+09, 1.7314e+10, 1.0895e+10, 1.7737e+09, 3.9181e+09,
        2.0862e+10, 1.1797e+10, 5.6951e+09, 5.5328e+09, 9.6904e+09, 5.3148e+09,
        1.7022e+10, 9.9865e+09, 2.5859e+09, 1.6083e+10, 7.6508e+09, 1.5838e+09,
        7.5315e+09, 1.1434e+10, 2.0951e+10, 1.2935e+10, 1.5804e+10, 9.2125e+09,
        5.9571e+09, 9.6589e+09, 7.3665e+09, 6.3279e+09, 1.1266e+10, 4.1208e+09,
        5.6147e+09, 5.7870e+09, 2.5259e+09, 2.7050e+09, 7.9397e+09, 1.9231e+10,
        3.0040e+09, 1.1789e+10, 1.2271e+10, 1.8187e+10, 1.3581e+10, 7.3321e+09,
        4.8350e+09, 2.3920e+10, 3.9220e+09, 1.4905e+10, 4.1518e+09, 1.6166e+10,
        5.1039e+09, 5.6987e+09, 4.9447e+09, 1.4623e+09, 1.1759e+10, 3.9954e+09,
        1.3516e+10, 8.4466e+09, 8.7797e+09, 3.6590e+09, 5.1895e+09, 4.7018e+09,
        1.5616e+10, 1.7683e+10, 1.1466e+10, 2.9567e+09, 1.3122e+10, 1.4054e+10,
        7.8515e+09, 2.2687e+09, 1.7401e+09, 1.2207e+10])
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.3366e+07, 2.5271e+07, 1.7975e+07,  ..., 2.2907e+08, 1.6572e+07,
        8.9640e+06])
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2.5667e+07, 3.7344e+07, 2.0635e+07,  ..., 2.6940e+08, 1.8007e+07,
        2.0657e+07])
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([9.5531e+07, 6.5743e+07, 7.4813e+07, 5.9393e+07, 9.5052e+07, 5.7566e+07,
        7.6471e+07, 7.2765e+07, 6.8240e+07, 6.0690e+07, 7.5481e+07, 5.5871e+07,
        1.4046e+08, 1.1630e+08, 6.8962e+07, 3.0118e+07, 5.4368e+07, 5.0365e+07,
        3.8879e+07, 4.8639e+07, 3.7517e+07, 4.5523e+07, 1.4498e+08, 4.7981e+07,
        9.1312e+07, 5.4655e+07, 3.7605e+07, 4.0544e+07, 7.8089e+07, 1.1908e+08,
        4.9577e+07, 3.0265e+07, 5.1378e+07, 7.1988e+07, 4.0106e+07, 8.6864e+07,
        4.0918e+07, 4.0596e+07, 6.9222e+07, 3.6022e+07, 4.6487e+07, 4.4415e+07,
        7.4350e+07, 1.0083e+08, 1.1733e+08, 1.1322e+08, 5.9955e+07, 1.1904e+08,
        6.0624e+07, 4.8054e+07, 9.0299e+07, 6.7811e+07, 1.3200e+08, 6.0429e+07,
        2.7309e+07, 5.3107e+07, 2.8771e+07, 5.9177e+07, 1.2028e+08, 7.9184e+07,
        7.5139e+07, 1.3378e+08, 8.4558e+07, 5.7390e+07, 4.6586e+07, 9.5528e+07,
        4.4715e+07, 6.4297e+07, 5.4625e+07, 3.9735e+07, 7.2121e+07, 4.3644e+07,
        3.3679e+07, 8.0726e+07, 7.7581e+07, 1.0857e+08, 2.2241e+07, 5.7320e+07,
        3.6612e+07, 6.2888e+07, 8.4635e+07, 7.5033e+07, 5.8490e+07, 9.5510e+07,
        6.0464e+07, 6.2248e+07, 5.0505e+07, 9.6966e+07, 6.8737e+07, 3.9294e+07,
        7.7603e+07, 1.0828e+08, 5.3938e+07, 4.3191e+07, 7.2755e+07, 5.9343e+07,
        2.4910e+07, 4.5763e+07, 6.0575e+07, 7.5919e+07, 4.0272e+07, 5.2028e+07,
        4.9386e+07, 1.0643e+08, 5.1749e+07, 7.9921e+07, 4.6559e+07, 5.3986e+07,
        5.1595e+07, 7.7233e+07, 3.5148e+07, 4.1754e+07, 2.5366e+07, 7.5193e+07,
        6.9242e+07, 1.7195e+08, 6.4933e+07, 1.1513e+08, 4.0594e+07, 6.4457e+07,
        6.2896e+07, 6.8170e+07, 4.0413e+07, 7.1980e+07, 7.9995e+07, 4.7300e+07,
        4.7333e+07, 5.8905e+07, 4.7027e+07, 1.1167e+08, 6.0616e+07, 3.9552e+07,
        7.7559e+07, 3.2967e+07, 4.0481e+07, 8.9119e+07, 9.2177e+07, 7.4958e+07,
        6.7783e+07, 5.7480e+07, 1.1893e+08, 6.4795e+07, 3.1163e+07, 5.5700e+07,
        9.6018e+07, 2.6555e+07, 5.5457e+07, 5.4099e+07, 4.3212e+07, 4.8335e+07,
        4.7980e+07, 2.8575e+07, 5.9025e+07, 2.6919e+07, 5.3146e+07, 2.3448e+07,
        8.2949e+07, 5.0916e+07, 5.1976e+07, 7.1841e+07, 7.3615e+07, 6.7641e+07,
        6.0916e+07, 3.7726e+07, 5.7337e+07, 3.8728e+07, 6.4429e+07, 2.1430e+07,
        2.5171e+07, 2.9126e+07, 5.6278e+07, 8.1630e+07, 1.3272e+08, 5.9336e+07,
        1.1676e+08, 6.0361e+07, 8.2676e+07, 3.6715e+07, 6.8246e+07, 1.4981e+08,
        4.0960e+07, 6.5516e+07, 4.1877e+07, 3.2588e+07, 5.2311e+07, 3.8832e+07,
        7.4526e+07, 7.6923e+07, 3.4397e+07, 6.0291e+07, 3.6617e+07, 1.6179e+08,
        4.6098e+07, 5.0290e+07, 1.0117e+08, 9.0783e+07, 3.7549e+07, 1.5518e+08,
        8.6809e+07, 6.0314e+07, 3.9833e+07, 5.7696e+07, 6.5529e+07, 4.6255e+07,
        6.6712e+07, 5.1944e+07, 5.1541e+07, 3.2076e+07, 9.8046e+07, 3.0910e+07,
        4.3848e+07, 4.3564e+07, 5.0022e+07, 4.1497e+07, 1.2864e+08, 8.7341e+07,
        7.1611e+07, 3.8130e+07, 3.7507e+07, 6.7164e+07, 2.7035e+07, 7.1381e+07,
        2.9840e+07, 5.2004e+07, 6.4034e+07, 3.3920e+07, 5.6984e+07, 5.7041e+07,
        6.6278e+07, 6.2254e+07, 5.6679e+07, 8.0110e+07, 8.1806e+07, 7.4722e+07,
        5.4622e+07, 4.7165e+07, 1.1966e+08, 5.7088e+07, 4.3308e+07, 3.1029e+07,
        6.3050e+07, 9.3538e+07, 8.4283e+07, 1.6062e+07, 4.1941e+07, 4.3430e+07,
        4.9348e+07, 3.0795e+07, 4.9919e+07, 4.0849e+07, 6.7527e+07, 7.6219e+07,
        6.1509e+07, 6.4384e+07, 7.7996e+07, 8.8959e+07])
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1.6653e+08, 2.6671e+08, 1.5721e+08, 8.0088e+07, 2.1419e+08, 2.9055e+08,
        1.1584e+08, 3.7656e+08, 2.5988e+08, 1.2952e+08, 2.0249e+08, 5.9992e+07,
        2.2262e+08, 2.6839e+08, 1.1811e+08, 2.2010e+08, 1.6029e+08, 1.3691e+08,
        8.4892e+07, 2.1575e+08, 6.0901e+07, 1.9471e+08, 1.6959e+08, 1.5924e+08,
        1.1196e+08, 1.5082e+08, 2.1510e+08, 3.0779e+08, 7.1591e+07, 2.1688e+08,
        1.9604e+08, 2.2657e+08, 2.0654e+08, 2.1535e+08, 2.6468e+08, 1.7998e+08,
        1.8920e+08, 1.5631e+08, 1.7349e+08, 1.9894e+08, 9.7520e+07, 9.1644e+07,
        1.1829e+08, 1.1268e+08, 1.9908e+08, 2.6397e+08, 2.2961e+08, 1.6000e+08,
        2.6086e+08, 8.4094e+07, 1.9657e+08, 2.4525e+08, 2.9647e+08, 7.3766e+07,
        3.3216e+08, 9.1527e+07, 2.3337e+08, 2.0155e+08, 1.6836e+08, 2.2646e+08,
        7.5553e+07, 1.4959e+08, 3.5037e+08, 1.9389e+08, 4.1902e+07, 9.7775e+07,
        1.3004e+08, 2.8737e+08, 3.9097e+08, 2.2364e+08, 9.3365e+07, 1.8897e+08,
        2.5619e+08, 1.9541e+08, 3.3054e+08, 1.3902e+08, 1.0563e+08, 1.5355e+08,
        1.7158e+08, 7.6345e+07, 1.7833e+08, 1.2688e+08, 1.2833e+08, 1.8016e+08,
        1.4346e+08, 1.4160e+08, 3.3071e+08, 1.3302e+08, 1.7489e+08, 7.8411e+07,
        1.8119e+08, 1.0332e+08, 1.1274e+08, 1.1112e+08, 1.8072e+08, 1.1749e+08,
        2.7841e+08, 8.6256e+07, 2.4120e+08, 1.6700e+08, 3.5322e+08, 1.3557e+08,
        1.7743e+08, 2.7482e+08, 2.1030e+08, 3.3331e+08, 1.4016e+08, 2.7701e+08,
        2.2244e+08, 8.9961e+07, 1.3901e+08, 2.0896e+08, 1.6986e+08, 1.3556e+08,
        1.3696e+08, 1.7236e+08, 7.7837e+07, 1.3862e+08, 1.0111e+08, 2.2003e+08,
        2.4634e+08, 1.3999e+08, 1.9308e+08, 6.0650e+07, 1.7941e+08, 1.8868e+08,
        3.2731e+08, 1.3222e+08, 1.5253e+08, 8.6548e+07, 2.3482e+08, 2.3459e+08,
        2.1567e+08, 3.1267e+08, 1.9562e+08, 1.3250e+08, 2.1990e+08, 3.7425e+08,
        1.0790e+08, 1.8525e+08, 6.2403e+07, 5.8032e+07, 1.6537e+08, 7.9058e+07,
        3.0251e+08, 2.0485e+08, 1.3102e+08, 2.4352e+08, 2.2040e+08, 2.1051e+08,
        3.3338e+08, 1.0894e+08, 1.0656e+08, 1.4575e+08, 2.0480e+08, 1.3286e+08,
        1.4505e+08, 1.9726e+08, 9.9347e+07, 1.4349e+08, 1.6773e+08, 2.4819e+08,
        1.4827e+08, 1.4931e+08, 1.8187e+08, 9.7048e+07, 1.4368e+08, 2.1924e+08,
        1.3599e+08, 2.9418e+08, 2.3097e+08, 1.1104e+08, 8.6415e+07, 2.5004e+08,
        1.4968e+08, 3.0721e+08, 1.9642e+08, 1.1313e+08, 1.4791e+08, 3.1783e+08,
        9.6558e+07, 8.8024e+07, 1.8264e+08, 1.9557e+08, 1.4463e+08, 1.6618e+08,
        1.6664e+08, 1.8194e+08, 2.6891e+08, 1.7834e+08, 1.2037e+08, 2.1437e+08,
        1.3069e+08, 1.3998e+08, 1.8036e+08, 2.3241e+08, 3.3840e+08, 3.9235e+08,
        1.9875e+08, 1.5664e+08, 2.0096e+08, 1.2559e+08, 2.9049e+08, 8.9443e+07,
        1.5417e+08, 2.3449e+08, 1.0980e+08, 2.3862e+08, 1.3320e+08, 1.3835e+08,
        1.8347e+08, 1.8541e+08, 1.4217e+08, 1.5218e+08, 1.1828e+08, 1.6984e+08,
        1.7767e+08, 2.3080e+08, 9.3655e+07, 1.5179e+08, 8.1685e+07, 2.3236e+08,
        9.1672e+07, 2.0739e+08, 1.0912e+08, 1.6085e+08, 2.3076e+08, 6.3755e+07,
        2.5876e+08, 2.6134e+08, 1.9136e+08, 1.3940e+08, 1.3912e+08, 1.2380e+08,
        2.0124e+08, 1.1676e+08, 1.2616e+08, 1.1490e+08, 1.5052e+08, 1.6646e+08,
        1.1194e+08, 8.4141e+07, 2.2462e+08, 2.1494e+08, 3.0901e+08, 1.6038e+08,
        1.3908e+08, 1.2856e+08, 9.4163e+07, 2.6948e+08, 2.4472e+08, 1.3833e+08,
        2.1251e+08, 1.3925e+08, 2.7604e+08, 2.0027e+08])
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 866285.0000, 5075939.5000,   55456.1016,  ..., 1034686.8750,
        2131376.0000, 5109315.5000])
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 4870596.0000,  6143692.5000,  3836105.0000,  6196442.0000,
         5678420.5000,  5210095.5000,  3541545.5000,  7607295.5000,
         4297793.0000,  6240003.0000,  5152277.5000,  3487898.0000,
         2416845.7500,  4530127.0000,  5096269.0000,  3589747.5000,
         4961807.0000,  4411603.0000,  3877552.0000, 13182507.0000,
         7207257.0000,  3580197.5000,  5374502.0000,  5989743.0000,
         3104414.7500,  3470249.2500,  6455263.0000,  7651390.0000,
         7127805.5000,  6719135.0000,  5050503.5000,  6457386.5000,
         4787078.5000,  7419376.0000,  6631399.5000,  8271180.5000,
         7402802.0000,  6552971.0000,  6243762.0000,  2905044.0000,
         4530658.0000,  6024685.5000,  4352239.0000,  7200335.5000,
         3226127.5000,  5499612.0000,  4354032.5000,  4422123.0000,
         6720609.5000,  3152137.0000,  3290291.5000,  4419386.0000,
         7681232.5000,  8557603.0000,  3102185.5000,  4720772.0000,
         4688068.5000,  5688565.5000,  6009967.0000,  3814492.5000,
         6956667.0000,  3955062.7500,  8762728.0000,  2853495.7500,
         6833549.5000,  4781263.5000,  4349327.0000,  4041345.0000,
         3442949.0000,  5040892.5000, 10163808.0000,  6456446.0000,
         9793615.0000,  7914512.0000,  5837929.0000,  2898654.0000,
         6634979.0000,  2422281.5000,  4629143.5000, 13448609.0000,
         3622873.5000,  7909704.5000,  4179617.5000,  7342160.5000,
         4504388.5000,  7123009.5000,  5647644.5000,  5849828.0000,
         6329157.0000,  4232307.5000,  6057329.5000,  3227299.2500,
         6173666.0000,  3002272.2500,  6712090.5000,  3546189.2500,
        12362562.0000,  5945504.5000,  6247278.0000,  2781308.7500,
         8499399.0000,  3932270.5000,  2795355.2500,  7022993.5000,
         8806835.0000,  6629903.0000,  6002508.0000,  4302657.0000,
         3394200.5000,  4294411.5000,  3017649.2500,  7354802.5000,
         5989607.0000,  7437763.0000,  8357561.0000,  3937602.0000,
         4875486.0000,  4410743.5000,  4394711.0000,  7735904.5000,
         9794324.0000,  4641285.0000,  6526835.5000,  5095472.5000,
         8662430.0000,  5758526.0000,  4201491.5000,  5384010.5000,
        10363184.0000,  6380093.0000,  8227037.5000,  6156342.5000,
         3305717.5000,  6555038.0000,  2960427.7500,  9893232.0000,
         2629624.5000,  4112522.5000,  6779191.5000,  3962356.0000,
         6173855.5000,  3747928.2500,  3810818.0000,  6421463.5000,
         5971508.5000,  7780367.5000,  3665513.2500,  8737229.0000,
         6658849.0000,  4796166.5000,  4583186.0000,  5101169.5000,
         6581409.5000,  5697977.0000,  3973717.2500,  5239768.5000,
         7214011.5000,  2870217.0000,  5166377.0000,  5971271.5000,
         3220568.5000,  4096409.5000,  5797526.0000,  6025831.0000,
         3929865.2500,  3110003.5000,  6367237.5000,  6014423.0000,
         5462283.0000,  3397369.5000, 10642879.0000,  6471382.0000,
         4523345.5000,  6452946.5000,  7618322.5000,  8537866.0000,
         9310330.0000,  4269884.0000,  5543409.0000,  4981463.5000,
         3351646.7500,  3947513.0000,  2418975.2500,  2353230.7500,
         2710763.7500,  5572457.5000, 10013863.0000,  5690478.0000,
         6202064.5000,  4493789.5000,  7086097.0000,  6628790.0000,
         4073106.5000,  3925822.5000,  3631397.5000,  6091628.5000,
         6282028.5000, 10008219.0000,  4616609.0000,  3268110.7500,
         3119189.5000,  6495590.0000, 11303490.0000,  3757416.0000,
         5299459.5000,  3715174.2500,  4745244.0000,  2385860.2500,
         7152850.5000,  4761974.5000,  6668686.0000,  2938861.2500,
         6638202.5000,  5364819.5000,  2910045.7500,  3546092.2500,
         6681696.0000,  4990631.0000,  3148797.2500,  6817661.5000,
         6997880.5000,  4492726.5000,  3683630.5000,  5585342.5000,
         4016051.7500,  4309390.5000,  5947374.5000,  6499078.0000,
         8726488.0000,  5776612.5000,  9219741.0000,  4880949.0000,
         4678680.5000,  6514292.0000,  2684863.2500,  8919181.0000,
         3368795.7500,  5987006.5000,  6221802.5000, 12145765.0000,
         5100901.5000,  7156396.0000,  2538222.7500,  6077956.0000,
         5450000.5000,  8434382.0000,  4927172.5000,  6656764.0000,
         5153869.5000,  3240668.0000,  9165429.0000,  3538666.0000,
         6098060.5000,  9935609.0000,  8374350.5000,  7617409.0000])
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([28112648., 25864754., 23721510., 12989060., 16591698., 34834488.,
        24588498., 18662180., 16955384., 14360815., 26461680., 14654749.,
        23766078., 12438020., 16934374., 27717334., 19686962., 21965752.,
        20619722., 17780934., 28187696., 31069900., 24134028., 16209903.,
        14092271., 27173726., 20147284., 29496448., 18084884., 37101040.,
        24425576., 18737120., 20965814., 13215712., 13182989., 15552707.,
        20616758., 18096604., 22535368., 17655642., 23856608., 16302814.,
        28147022., 11335892., 13389805., 24577182., 38666864., 22704058.,
        25980476., 20526192., 10076447., 13579912., 12598143., 15360024.,
        20589518., 14602489., 11761210., 15983737., 10934757., 17861786.,
        22312826., 22113482., 23743098., 19182908., 12472563., 15309688.,
        22156188., 22082194., 11234285., 18146856., 29865734., 35440972.,
        17284990., 13439099., 21836834., 12229349., 21571042., 12367694.,
        22905374., 22930608.,  6854977., 23555144.,  9857993., 30289750.,
        18163304., 18917348., 33546758., 12679613., 11542833., 14906343.,
        12246081., 26243894., 21549586., 16904282., 26079498., 11550402.,
        10137041., 13325839., 15480510., 20977558., 18338370., 12568590.,
        10914009., 16288188., 14643097., 16104615.,  8298092., 13520527.,
        16430365., 27421204.,  9755064., 11017234., 10629503., 30148768.,
        24351748., 22178710., 26654894.,  9080575., 13302386., 20100592.,
        19695280.,  8319983., 17665540., 12312387., 17232932., 29315954.,
        17089978., 29673750., 11788354., 15577852., 14601797., 19699970.,
        22050100., 19440038., 16084980., 12626214., 13229188., 23314850.,
        22443536., 11015745., 25301600., 22563096., 26533108., 24487212.,
        15576915., 24766830., 26791048., 21756664., 25453986., 18884496.,
        15660984., 23102822., 27379000., 24095268., 15982846., 17016844.,
        32461034., 15150375., 13801226., 17147014., 17326862., 20768416.,
        15101517., 25863308., 26388866., 26103838., 28343358., 16439928.,
        17299828., 11348491., 11478160., 22499338., 16099612., 17387742.,
        17483484., 42824208.,  8693135., 22408040., 18541724., 13410264.,
        16238283., 10119555., 22286928., 15327483., 21967146., 30063268.,
        22379496., 23619566., 35056284.,  8484859., 15051522., 17644602.,
        24510810., 29491348., 21769648., 12455605., 16648955., 15153047.,
        19266864.,  9376529., 18952770., 14921192., 19833526., 10864720.,
        25940482., 34472960., 20597122., 21712016., 16097669., 15558096.,
        21237010., 12327915., 10403385., 16744193., 13070295., 10037588.,
         8557695., 20089332., 14044363., 13764891., 12826515., 25902202.,
        26085626., 17134720., 20443842., 10866664.,  9644087., 14042697.,
        16644702., 25320072., 18995754., 33124350., 17067676., 22280902.,
        15727092., 16790884., 10651953.,  9228272., 14667353., 24705928.,
        17261302., 18037974., 14843530., 13359603., 26751778., 18280260.,
        14010887., 19552318., 14839922., 16905118., 17728662., 21473872.,
        16384941., 12613748., 20889472., 22112282.])
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 120460.2422, 3818140.7500,   52709.0312,  ...,   79714.9609,
         288648.0938,  157876.1406])
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 624244.6875, 1008408.5000, 1396883.3750,  718150.5000,  852096.8750,
        1203091.8750, 1019581.6875, 1220062.7500, 1125597.7500, 1205633.0000,
        1122536.8750,  661336.6250,  701269.4375, 1760561.1250, 2202038.5000,
        1102099.2500, 1405706.0000,  739426.8125,  542364.3750, 1441152.2500,
         614456.0000,  532429.3750, 1112543.6250,  864754.2500,  735118.1875,
        3148866.5000,  668890.9375,  893622.9375, 1805864.0000, 1574294.7500,
         823095.3125,  638509.3125, 2107035.2500,  837951.5000,  537055.8125,
        2629278.7500,  650756.0625, 1239450.5000, 1048289.8750,  591095.6875,
         613970.4375,  588794.9375, 1648369.0000,  961225.7500,  534448.1875,
        2140063.0000, 1712832.3750,  777117.6250, 1428720.3750, 1466939.7500,
        2406687.2500, 2342815.5000,  689234.1250,  991908.3750,  583463.9375,
        1626524.3750,  877650.8125, 1065989.2500, 1898497.1250, 1554934.6250,
        2438582.7500, 1245498.7500,  600813.1875, 2249210.2500,  947258.0000,
         933390.1250, 1827047.3750, 1083577.2500, 2313884.7500,  888498.5000,
         684216.1250, 1021555.3750,  713898.7500, 1306036.3750, 2337530.5000,
         781037.5000, 1413281.7500, 2166925.7500, 1364546.1250, 1201745.0000,
        1433447.1250,  412680.8750,  814906.8750, 2618486.7500, 3124749.7500,
        1742080.3750, 1327268.8750, 1112601.1250, 1498388.1250, 1190787.3750,
        2338618.5000,  489395.3750, 1369935.7500, 1919411.8750,  714054.6875,
         994526.4375,  721226.1875, 3563949.0000,  643902.9375, 1271852.7500,
        1906502.2500, 1543628.3750, 1007024.8125,  672813.3750, 1843312.0000,
        1373668.0000, 1035646.2500, 2684463.2500, 1520157.7500, 1550791.3750,
        2207768.0000, 1372025.1250,  743959.6250, 1346601.5000,  728931.9375,
         823616.6250, 1000541.4375,  680238.0625,  775057.1250, 2796544.2500,
         879321.6250, 1332896.5000,  734086.5625,  905109.0000,  783469.6875,
        2054968.7500, 1548315.3750,  641957.6250,  849854.9375, 1504566.3750,
        2070338.0000, 1200220.1250, 1456234.6250, 1731037.5000, 1997354.0000,
         587477.9375, 1261995.8750,  989582.9375, 2921011.5000, 1177768.8750,
         721452.6250,  953049.4375, 3140576.2500, 1344535.8750,  752733.6250,
         900271.3125, 1745901.7500, 2176171.7500,  910553.3125,  956325.0625,
        1342003.2500,  796800.0625, 1055415.1250,  975408.6875, 2640204.0000,
        1338264.2500,  903658.7500,  831233.6875,  869233.8750, 1195613.0000,
        1676824.7500,  895965.1875,  741923.6250, 1124898.0000, 1533589.5000,
         710810.1875,  878336.0625,  667617.8125, 2460675.2500,  717515.0625,
        1524977.1250,  823482.8750, 2192920.5000,  832017.8750, 1603521.1250,
        1115618.5000, 1486185.8750, 2492090.7500,  684644.6875,  776316.6875,
         762787.6250,  718965.8125, 1573516.6250, 1104048.8750, 1800701.7500,
        1464805.5000, 1109387.3750, 2326410.7500, 1156899.5000, 1584054.8750,
         832605.8125, 1665731.0000,  879385.5625, 1745673.5000,  784432.3125,
        1137834.1250, 1249502.1250, 2079198.3750, 2344826.0000, 1339778.5000,
         832920.6250, 1355052.7500, 2747076.0000, 1360551.3750,  967667.5625,
        1796458.0000, 1816025.5000, 1440222.0000, 1475019.8750, 1131514.3750,
         781703.7500,  951823.0625,  648002.4375,  914317.9375,  954039.2500,
        1107411.2500,  489886.5938, 1011240.8125, 1188464.0000, 2060978.8750,
         624366.5625,  925714.9375,  688837.8125,  776113.8125, 2035305.2500,
        1373982.8750, 2085039.1250, 1593302.7500, 2610617.2500, 1336157.7500,
        1400628.7500, 1268227.1250, 1284137.6250, 2175171.0000, 2434473.7500,
        2483787.5000, 2329134.2500,  804635.6250, 1158177.8750,  803840.8125,
         798382.8125, 1239558.6250, 2324584.0000, 1705185.2500, 1622808.5000,
        1007494.8750,  492327.7500, 1907668.6250,  990938.5000, 1104717.3750,
         845018.8750, 1429944.2500, 2517039.7500, 2185910.2500, 1446708.0000,
        2394018.0000])
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 5646155.0000,  2614505.7500,  3548736.2500,  2178006.0000,
        10528048.0000,  5627767.5000,  5197968.5000,  6926192.0000,
         8651666.0000,  3533351.2500,  8098213.5000,  5511871.5000,
         3991208.0000,  4393693.5000,  2807304.0000,  3525749.2500,
         9074198.0000,  3328364.0000, 10388056.0000,  3362675.2500,
         5390473.0000,  4335099.0000,  3767554.5000,  4217802.5000,
         3067911.2500,  3406629.7500,  4154394.5000,  2958441.0000,
         2658729.7500,  5536346.5000,  5564307.5000,  2061307.0000,
         1787742.7500,  2764971.5000,  4874741.0000,  2593670.7500,
        10273915.0000,  2660211.2500,  4953666.5000,  5903906.5000,
         3527286.7500,  2649944.2500,  3424925.7500,  9433040.0000,
         7588239.5000,  4393393.5000,  3106475.7500,  3672012.5000,
         5267936.0000,  2802218.7500,  9953628.0000,  3455311.2500,
         3144649.5000,  2879153.0000,  3183550.0000,  2320702.2500,
         9604885.0000,  4211718.5000,  3166698.0000,  6039212.5000,
         5591373.0000,  5301711.0000,  7476098.5000,  2767728.7500,
         2657228.7500,  3608258.0000,  9390190.0000,  7038907.0000,
         7043329.0000,  6648620.0000,  9160725.0000,  2004734.5000,
         7454878.0000,  9161915.0000,  7840083.5000,  3009786.0000,
         3451146.0000,  5587484.5000,  7482360.5000,  3181091.7500,
         2463440.7500,  3835672.2500,  6305053.5000,  6492506.0000,
         9059592.0000,  9914170.0000,  3001172.2500,  4285471.5000,
         2857478.7500,  2880794.5000,  2425293.5000,  3169620.0000,
         5679288.5000,  2601347.7500,  4544765.5000,  7156273.0000,
         2588894.5000,  5595109.5000,  4437216.5000,  2943316.2500,
         4477231.5000,  4041985.7500,  3039256.2500,  4723289.5000,
         8369737.0000,  7110211.5000,  4444222.5000,  4838245.0000,
         2789786.7500,  4628653.0000,  6637388.0000, 13079078.0000,
         2634223.0000,  2749182.2500,  2510579.5000,  3603349.2500,
         4305253.0000,  5113106.5000,  7146876.5000,  4910758.5000,
         5070886.5000,  2128314.0000,  2783958.2500,  2929996.2500,
         2561931.7500,  4703762.5000,  4508936.5000,  7966164.0000,
         3341187.2500,  5742292.5000,  8983535.0000,  2837050.2500,
         2104350.7500,  2154672.5000,  6920216.0000,  4145824.2500,
         9975542.0000,  6165452.0000,  4126480.7500,  9875249.0000,
         3507271.5000,  2828488.2500,  2928481.2500,  3166731.5000,
         2048235.5000,  6251579.0000,  6383121.0000,  3033810.7500,
        10031912.0000,  1841024.7500,  4509315.0000,  7374898.0000,
         4961941.0000,  7140971.5000,  6527573.0000,  4127877.5000,
         4982055.5000,  4380839.0000,  4505343.0000,  2492724.0000,
         4064782.7500, 12475084.0000,  6638142.5000,  3033633.2500,
         5210000.5000,  2866219.5000,  6830981.5000,  4020773.5000,
         9803931.0000,  3081055.0000,  5181894.5000,  2884442.2500,
         5161622.0000,  3175408.2500,  6613103.0000,  3623586.2500,
         5858714.5000,  6418735.0000,  5323544.5000,  7660300.5000,
         3073616.0000,  5918267.5000,  8947284.0000,  3000181.0000,
         5523851.5000,  3670023.7500,  5833423.0000,  6152435.0000,
         4600861.5000,  6503814.0000, 10795817.0000,  7166850.0000,
         4343636.5000,  3975931.2500,  8248604.5000,  6497432.0000,
         3886428.5000,  4375270.0000,  6794438.0000,  2733121.5000,
         4958375.0000,  3448232.2500,  5748089.5000,  3092763.0000,
         4073175.7500,  3123938.2500,  1655682.5000,  6878768.0000,
         6408124.0000,  3889074.7500,  6541477.5000,  3179133.5000,
         5076676.0000,  3839753.5000,  7706179.0000, 10371752.0000,
         4953733.0000,  3406355.0000,  2184956.0000,  2735816.5000,
         4806937.0000,  4573199.5000,  5028185.0000,  5626459.5000,
         7258104.0000,  5523230.0000,  2303250.0000,  3576206.0000,
         2547936.5000, 11565498.0000,  6977845.0000,  2570409.5000,
         3332030.7500,  2403448.2500,  2913128.2500,  3227296.7500,
         7700506.0000,  4845992.5000,  2897051.7500,  3558398.0000,
         3264372.5000,  8478602.0000,  5227766.0000,  8366840.0000,
         6069352.5000,  5683521.0000,  8922161.0000,  2905306.5000,
         3834958.0000,  8008856.0000,  2982094.2500,  8260699.5000,
         5944090.5000,  3524560.7500,  3708623.2500,  3751084.5000])
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([114642.2969,  38312.1719,  18035.8711,  ...,   2123.5762,
         18684.2461,  73978.0938])
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([309107.1562, 267111.0625, 274837.8438, 277190.9062, 348598.6250,
        233354.2500, 213610.9844, 211212.6562, 156626.5625, 158739.5156,
        250849.2656, 200661.2031, 204364.4219, 248008.9062, 219715.1719,
        293305.0000, 283968.1562, 239899.2812, 205324.0469, 214637.3594,
        195877.7500, 244753.8906, 282224.7812, 254175.7344, 221163.2188,
        284716.0938, 152539.7656, 306062.9688, 274926.4688, 254641.1094,
        228130.1406, 165219.9531, 239704.4219, 276071.9375, 293697.1250,
        300842.4062, 222481.7656, 199947.6875, 210838.0312, 123359.8047,
        227036.7344, 255703.3281, 238645.7812, 180914.8125, 276084.6875,
        276349.7500, 224012.6094, 228598.4219, 202983.4531, 229176.1094,
        216520.7188, 307989.6875, 176304.7188, 258216.2500, 224997.0469,
        221528.3125, 247334.5312, 190368.3125, 253416.1562, 341057.0938,
        160653.9688, 323391.5000, 287798.3125, 203703.2188, 309293.4062,
        306946.4062, 249452.5625, 239132.9062, 205594.1875, 175655.3281,
        244272.5938, 308991.8750, 280454.5000, 291513.4062, 298054.6562,
        245183.1719, 193184.0156, 342678.4375, 332576.9062, 278817.0625,
        235560.6562, 211449.8594, 226232.1875, 269226.9375, 230304.7188,
        218149.6562, 263932.3438, 303198.4375, 267655.9375, 159691.1250,
        337215.3125, 258334.2812, 179398.6562, 274553.6875, 180650.1094,
        199162.1406, 232572.5156, 175623.4531, 278103.2188, 229136.9375,
        209916.5938, 160990.2812, 263655.5625, 250471.5625, 169002.2188,
        204291.7812, 251753.8906, 225157.0000, 217942.8125, 216134.7656,
        293698.6562, 265994.7188, 247889.1562, 282108.9062, 238556.9688,
        306138.7812, 312081.9062, 338596.8438, 374844.1250, 168651.7188,
        244809.9531, 224839.9375, 215428.2188, 176784.1250, 171490.9688,
        196442.4688, 304783.2500, 293703.0938, 159182.3438, 251780.4688,
        193452.8125, 199246.1250, 232001.6719, 362347.8125, 324290.5312,
        275718.9062, 191419.5781, 253435.1562, 202164.3594, 216422.8750,
        205123.2969, 170705.6406, 179278.6250, 269046.5312, 240261.5312,
        220176.6250, 301306.4688, 233958.8750, 211267.7188, 228001.0312,
        273539.6250, 219893.2188, 231128.9062, 227666.7969, 369218.6875,
        231238.9531, 212586.2188, 231405.0625, 265178.8125, 197751.0625,
        290560.0000, 326018.3125, 253556.1562, 252102.1094, 281649.5625,
        277105.2500, 246069.4531, 201942.9062, 287904.6875, 305119.6250,
        296243.2500, 259127.3594, 314432.8125, 237848.9844, 206598.7500,
        218349.7344, 230218.1250, 227477.2344, 244906.6875, 357377.4375,
        179171.7500, 198478.4531, 284967.4062, 275380.5000, 219535.2031,
        239003.9062, 241407.9688, 275175.3125, 306040.1250, 184032.4531,
        387497.9688, 188010.2344, 181031.2812, 290447.1562, 301977.9375,
        272237.6875, 181754.6094, 243661.0625, 190397.4531, 256312.6719,
        331216.1250, 204933.2500, 359403.6562, 264365.0625, 131300.1562,
        245056.9844, 289328.3125, 284518.7188, 341455.6250, 209035.9062,
        298798.1875, 322844.5000, 230999.9688, 296194.4375, 258977.5312,
        217248.5000, 160669.4688, 300922.3750, 173509.5938, 302653.6562,
        307642.7188, 240558.2656, 196723.4844, 213362.5938, 245487.3750,
        333349.5312, 300427.7500, 362220.2500, 252977.7656, 351305.1562,
        158054.0000, 247580.4688, 239867.8594, 227150.1562, 329577.5938,
        262681.8438, 265057.2812, 200866.4844, 182342.2812, 142509.7500,
        301208.2500, 196603.9844, 251497.5938, 341160.7188, 251420.0000,
        218280.4688, 300080.0312, 271635.9688, 255549.3750, 273446.3750,
        263770.1875, 197821.4062, 249561.8125, 251339.5625, 265570.0312,
        187985.1406])
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1163679.3750,  851517.3125,  836890.0625, 1210036.8750, 1134636.5000,
        1007240.1250, 1023971.9375, 1024207.2500,  932390.5625, 1369512.6250,
        1300710.7500,  824857.8125, 1157025.2500,  900526.7500,  942474.8750,
        1074719.7500,  913158.9375,  966947.1875,  803099.3125, 1426422.0000,
        1184934.8750,  818193.6875, 1518608.6250, 1228278.1250, 1236118.0000,
        1228313.0000,  895422.2500, 1029203.0000,  907888.3750,  716993.6875,
         586272.2500, 1137192.7500,  888570.8750,  766887.5625,  965919.3125,
         938393.1250,  908885.6250,  817486.4375, 1005457.4375, 1375684.2500,
        1001381.1250,  984092.1250,  999243.9375, 1119151.7500,  697549.9375,
         964667.3125, 1125296.5000, 1113475.6250,  742645.1250, 1117084.0000,
         783517.3750, 1310741.8750,  817764.6250,  856965.7500, 1012809.3125,
         629425.1875,  948976.1875,  857098.2500, 1235392.6250,  768244.2500,
        1067727.3750,  845128.4375, 1545153.2500, 1003638.5625, 1319456.1250,
        1237811.6250, 1201232.2500,  716537.5625,  812085.1875,  960955.2500,
         949305.6875, 1088939.1250, 1522849.8750,  739110.4375,  672605.0625,
         907771.1250,  907056.2500,  859669.0625, 1159129.0000,  979145.4375,
        1131628.7500,  922464.8750, 1338985.3750,  967784.3750, 1173336.2500,
        1008019.8750, 1145318.2500, 1080352.8750, 1197077.7500, 1157222.2500,
         843389.0625,  923168.8750, 1015392.5625, 1179808.8750, 1094009.5000,
         939841.3125, 1021962.1875, 1338796.2500,  931309.6250,  815575.9375,
        1236406.6250, 1014366.6875,  828286.8750, 1036346.3125, 1187557.5000,
         978252.7500, 1008254.5625, 1078108.2500, 1020961.3125,  903090.8750,
         871463.0000, 1383336.1250,  810721.4375,  995842.6875, 1045199.3750,
         762919.8125, 1049726.3750,  950359.4375, 1270422.3750, 1222889.5000,
        1259964.2500, 1074195.8750, 1552798.5000,  935987.5625,  953191.4375,
        1082416.0000,  993904.1875,  767167.1250, 1053134.1250,  950131.0000,
         884231.5000,  851910.3125,  840418.1250,  838096.7500,  892255.6875,
        1105091.0000,  618004.2500,  740848.9375, 1094653.3750,  761652.9375,
         704501.1250,  999388.6250, 1552328.3750,  767426.6875,  695866.7500,
         542821.8750,  992865.6250, 1299829.7500,  787769.0000, 1096296.8750,
        1021786.6875,  827671.3125, 1036720.7500, 1034764.9375,  692654.1875,
         959574.4375,  646641.8750,  934863.6250,  914730.5000, 1250263.5000,
         928703.7500,  877728.5000, 1202819.5000, 1049466.2500, 1023625.9375,
         922444.1250, 1029771.9375,  756178.7500, 1182273.6250, 1201163.6250,
         867353.4375,  965051.0000, 1051560.8750, 1156669.6250,  804941.4375,
        1080842.1250, 1368104.7500,  836362.1875, 1210564.1250,  946442.6250,
         943559.1250,  813986.7500,  961059.2500,  766813.4375,  882009.1875,
         872374.6250,  860568.0000, 1090779.5000,  629675.8125, 1248729.2500,
         955665.6250,  682671.6250,  769620.9375, 1041015.5625,  731386.5625,
        1353383.2500,  581744.1250, 1001119.3750,  982947.6250,  764414.1250,
         788764.3125,  876763.3125, 1339702.8750, 1039316.4375,  735264.1250,
         962116.9375, 1303743.8750, 1500231.8750,  946702.8125,  578002.8125,
         778946.4375,  951463.3750,  745724.3750, 1015780.0000, 1074010.8750,
        1091914.8750,  802827.7500,  777900.6250,  994855.7500,  938259.0625,
        1087251.5000,  952652.1250,  730586.6875,  838795.1875,  751421.1875,
         883702.1250, 1205521.6250, 1100845.0000, 1067000.1250,  968168.3125,
         695185.7500,  674886.4375,  634581.1875, 1204585.6250,  850164.5625,
        1043393.3750,  824736.8750, 1224628.7500, 1501162.5000, 1078810.3750,
        1071960.8750,  686405.0625, 1103245.2500,  764760.8750,  696663.3125,
         851438.0625, 1107645.0000, 1068361.1250,  865653.1875, 1142888.2500,
        1111710.1250,  879625.6875,  891103.0625,  941762.1250, 1330250.6250,
        1057427.8750])
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([16933.2891, 15290.5293,  9023.9414,  ...,  5134.2139, 11435.5605,
        14645.1260])
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 91097.4219, 141325.4219, 103205.0234, 136118.7031, 124504.7656,
        150261.7344, 106332.0234, 143174.0000, 120069.7109, 174620.1719,
        115345.1641, 139667.2188, 127122.9219, 155638.0938, 132804.8438,
        119843.9766, 109161.1484, 108211.5781,  85277.9375,  86417.8438,
         85956.1406, 108781.5859, 126439.9453, 129587.9844, 112831.6484,
        134307.3438,  97951.6719, 103264.4141, 117486.9219, 105849.7812,
         73915.1875,  88489.8516, 121017.7344, 107516.4688, 110126.0938,
        114049.3594, 163382.3750, 101374.1016, 104520.9062, 166158.2969,
         97524.7422, 143735.0469,  92813.3984, 137337.7031,  79156.8594,
         69886.6016, 106567.5625,  88933.4922, 150763.0781, 111934.1406,
        103238.6484, 155355.9531, 111059.2188, 137014.7031, 121474.3516,
         87875.4375, 120465.4297,  64253.3867, 125421.7109, 120047.2578,
         70308.4141, 116105.3047, 150078.8594, 168885.8594,  87533.1484,
         96135.0625, 135093.4688,  94398.9766,  92088.3906, 121321.9531,
        161174.1562, 169991.4531, 149529.8125,  79669.0469, 131392.1094,
        108751.1484, 162550.7031,  85252.2656,  77952.7500,  89944.6484,
        178828.1250, 120088.0234, 163148.4062, 106140.2031, 110696.0234,
         69700.7969,  92042.1875, 141373.5156, 138348.9531,  92129.5156,
         89199.8672, 139376.2031, 207908.8906,  78880.4219,  63645.4883,
         71542.3359, 121289.0234, 100235.2656, 153331.2188, 112295.9297,
        136348.3594,  65444.7891, 100657.5312, 124501.9219, 138861.0938,
        121094.2422, 141922.7969, 124189.8516, 115088.8906, 106650.3828,
        145310.9844,  64861.6641,  95926.6484,  76632.4922,  99594.2266,
         93081.9219, 124271.6562, 144165.9219, 127873.1094,  74613.3594,
        101679.3125,  94721.0078, 120600.3750, 100390.0859,  73316.3516,
         61002.5938,  89887.5703, 150133.9375,  89547.6641,  83507.0938,
         92464.5625, 109139.7500,  98881.4766, 121079.0547,  74625.9531,
        154874.5469, 117796.0234,  89336.2656, 121372.4922,  58786.2109,
         90492.7031, 104586.3125, 119109.9375, 157346.6406, 129429.4375,
        134039.1094,  90590.8906,  80188.4375, 108046.5938, 138933.6719,
         94003.0625,  92866.6719, 123831.3047, 100889.6094,  78631.7109,
         79104.8438,  77711.5547, 135607.5000,  81911.5547,  83708.7422,
        107641.0391,  57287.5625,  93947.0625,  80648.4062,  88123.1953,
        108144.5547,  70250.5000, 122596.3516,  83676.6875,  98889.4453,
        161009.9219, 103647.9531, 102648.0156, 115193.1641,  74343.7578,
        102168.4219,  97279.3516,  79760.6641,  63123.2695,  89158.0000,
        111605.5781, 138859.5781, 131388.6875, 169027.8594, 113466.9766,
        129772.4141, 125871.1797, 118095.0156, 149455.9062, 107377.0000,
        138136.8125,  64082.3672, 196073.5000,  65248.7188, 154752.5781,
         79112.4141,  93216.7188, 112859.9688,  85940.4375, 209480.8125,
        108299.6953, 197125.2500, 152496.7500, 124500.4453, 109418.9062,
        118030.7031, 167284.2344, 103756.3047, 150877.7344, 103325.1953,
         97036.1406,  76662.1797, 142808.8438,  95716.1328, 115116.9141,
        105252.2500, 137769.7500, 130665.0938,  86786.7969, 108386.0078,
        115284.4453, 183538.0938,  91877.3750, 115285.3281, 100698.4297,
         96749.0781,  69014.5312,  76668.1094, 132435.1406,  93427.7422,
         89966.2109, 138252.7812,  98147.7891, 123631.3594, 143606.1875,
         93698.6562, 102314.9688, 151862.5625,  91897.1172,  79878.1328,
        121973.0703,  60509.8555, 142912.1719, 121381.9844, 108303.7344,
        106800.3125, 145281.7500,  93601.5312, 143698.9844, 115287.9453,
        127120.6484, 133760.1094, 112980.4688,  84302.2734,  77466.3281,
        104256.1172])
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 477710.9688,  389662.2812,  646275.6875,  557101.6875,  698384.1875,
         726975.5000,  360267.3125,  601277.8125,  370560.9688,  372459.8438,
         642281.4375,  530322.6250,  725029.1250,  406114.6875,  625745.9375,
         590911.7500,  528445.3750,  436926.0625,  551353.4375,  329863.9375,
         371638.6562,  562414.2500,  694913.0000,  437487.2812,  533226.8125,
         382658.5000,  761607.1250,  520094.5000,  443917.0000,  413148.4375,
         312981.4062,  748751.5000,  295696.0000,  381427.5938,  445680.5625,
         378846.9688,  419453.3750,  404497.5000,  457605.6562,  725922.4375,
         662665.0000,  680132.7500,  260409.5781,  471776.2500,  592843.5625,
         573960.9375,  253261.6875,  472139.0312,  767107.0625,  713841.9375,
         654744.9375,  824776.8125,  402110.2188,  536776.3750,  588628.9375,
         645988.5625,  762628.8125,  442469.3438,  455990.9062,  727014.0000,
         569444.5625,  614116.5625,  438747.6875,  764644.2500,  483605.1562,
         451696.8438,  664081.6875,  690917.8750,  373305.9375,  534922.6875,
         498126.4062,  766974.1250,  442644.6250,  741986.5625,  565101.0000,
         571680.6875,  337586.6875,  749363.2500,  372441.7812,  545757.6875,
         771013.1250,  671728.1875,  402528.4062,  602640.0625,  579330.0625,
         442134.1250,  381097.9375,  606173.9375,  670787.4375,  453060.9062,
         411812.6250,  431999.6562,  675278.2500,  482014.2500,  745097.0000,
         542764.8750,  381635.5625,  389689.7188,  329254.0312,  443164.3438,
         437553.0938,  815937.3750,  501810.2500,  621975.8125,  371702.2500,
         855315.8125,  582968.5000,  251904.9844,  495512.5000,  641820.1250,
        1071207.7500,  312263.3750,  892701.0000,  510726.8438,  513124.4688,
         652097.8750,  791671.4375,  650282.1875,  539962.5000,  399632.9375,
         639209.0625,  439430.3125,  476691.3438,  580045.6250,  846569.8125,
         658588.1875,  630443.0625,  761920.1250,  432654.6250,  571741.1250,
         353322.1875,  342616.8125,  350087.5625,  657574.3125,  449357.3750,
         649886.1250,  322524.4062,  608630.0625,  393160.4375,  563368.3750,
         895499.1250,  499292.8750,  429295.6250,  410266.7812,  390902.9375,
         709808.9375,  435512.3750,  526899.8750,  343743.5000,  732825.8750,
         341454.5625,  588108.5000,  461909.3125,  702179.9375,  597134.3750,
         680063.4375,  561500.4375,  597339.1875,  505884.9062,  545738.4375,
         437108.8750,  471673.8438,  340080.3125,  386787.9375,  420797.7812,
         673174.9375,  319149.8438,  576364.3750,  517588.4688,  555981.2500,
         804239.3125,  448967.7500,  446589.5312,  548749.5000,  361095.8750,
         495715.5625,  874896.7500,  688668.2500,  569669.0625,  593751.3125,
         515301.6875,  782118.9375,  411382.5625,  424049.8750,  320287.3125,
         564172.1250,  501403.2500,  688210.0000,  395998.2188,  463306.5312,
         625678.3125,  431591.5000,  536600.1875,  900075.0625,  844886.0000,
         612442.3750,  608662.4375,  425186.5312,  435479.3125,  618203.4375,
         750784.3125,  658552.5625,  532330.8750,  558880.1875,  482987.5938,
         705591.4375,  469907.2188,  709606.8125,  526065.1875,  671649.6250,
         703759.1250,  499669.5312,  417852.8438,  952371.2500,  424552.6250,
         536574.0625,  629153.6875,  491396.5938,  633989.1875,  497037.1875,
         525031.3750,  733031.5625,  821467.0000,  654532.2500,  451803.8750,
         357043.7812,  446339.0938,  409920.2500,  391325.9688,  636807.0000,
         480168.8438,  571613.5000,  727210.2500,  462557.7500,  535050.9375,
         584894.0000,  617923.3750,  328558.3438,  381584.3438,  416876.9062,
         349537.3125,  407592.5625,  432692.6562,  617791.3125,  563786.0625,
         452048.3125,  504083.5625,  673731.8125,  776390.5625,  503346.3438,
         507471.1875,  625747.9375,  608014.3750,  611605.7500,  585275.3750,
         500572.5000])
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([10440.3652,  5501.7866,  4147.2227,  ...,  5524.3457,  4026.6189,
        17969.4551])
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 7740.4009,  7374.4351, 11510.6104,  7601.6128,  7968.9956, 12849.7930,
        16594.8457, 16742.7773, 14220.5820, 11571.7637, 17506.7031, 11327.4912,
         9073.6768,  8318.3760,  9853.4854, 13390.8350,  9635.5908, 12335.8369,
        12461.4795, 18936.1387,  6540.7705,  8403.2207, 14125.8164, 11555.1572,
        16538.9141, 15298.5400,  6044.2861,  7520.2119, 15819.8086, 13022.5127,
         9218.6201, 11935.5557, 17284.0859, 11511.5244, 14373.8828, 12305.6611,
        12717.5439,  9754.5293, 20054.1113, 15963.1426, 10267.9912, 11990.6875,
        15560.6182, 16403.0664, 12667.9160,  9969.4648,  9434.2754, 15599.4102,
         9220.2549,  9976.4609, 15051.5391,  7160.2578, 10170.6162,  7771.1011,
        11897.0742, 13589.1191,  6911.1572,  7251.5923,  8887.0410, 12694.2852,
        18703.8047, 14670.9688, 12936.7598, 12570.3291,  7184.2524, 11241.9629,
        13866.5488,  9825.7686, 16489.3438,  9082.3447, 12220.8311, 10381.3389,
        10290.2236,  8394.8623, 28034.6465, 14897.9697,  7564.7158, 20521.9492,
        12530.8047,  6164.7671,  8953.6807, 10707.5957, 12096.8301, 20074.0469,
        11511.1592,  7435.8037, 11398.1523, 11682.4453, 14955.4561, 12288.5293,
        16444.6133, 12842.9004, 15151.7549,  9862.6846, 10496.3652, 12801.4746,
        15000.9219,  7749.4302, 21978.2617,  5574.9951,  9313.6094, 12362.9521,
         9874.5479,  8870.0859, 11110.4160, 11219.3818,  7373.5303,  9492.8916,
        12533.5615, 12524.0645,  6658.0923, 10505.6328,  8696.7383, 15220.4795,
         7961.2280, 11432.3516, 12766.0615, 10907.2998, 15030.8281, 11990.2158,
        10766.9434, 10399.4072,  6014.5078, 11301.6064,  6340.6050,  9804.1914,
        12964.3555,  7788.8433,  9361.4619, 10971.3564,  9731.0879, 13118.5752,
         8962.5996, 14921.8994, 11208.8447, 11836.9355, 18393.1191, 17445.3809,
         6823.3486, 10551.7461, 11442.8965, 11032.8623, 12294.3789, 10821.8965,
         7638.1602,  8277.8301, 16137.7285, 10245.0713, 13982.5273, 16156.3975,
        10749.1895, 13902.5459, 13894.5000, 11900.1143, 10902.4365, 12431.2744,
         9308.2881, 15005.3721, 11795.5732, 15059.9209, 12485.2012, 12522.8701,
         8274.7832,  7687.7354, 11891.3096,  9417.4678, 15955.8691,  7082.7734,
        12143.0342, 20607.0898, 17282.3340, 10461.3447, 10230.3877,  8014.9204,
        15273.3047,  8260.0107, 12760.6064, 10228.8799, 12570.8555, 13405.8701,
         6960.6108,  8232.7441, 12871.5820, 10142.9375, 18399.6484, 11290.0234,
        16024.8291, 10122.1768, 10413.7900,  6763.5986, 13490.9131, 19055.3457,
        15963.8203, 12008.7656,  8543.4590, 12865.8301, 11568.4326,  8880.7910,
         7257.6304, 16870.7812, 12665.7803, 10088.7080,  9584.0645, 12965.1602,
        10332.6445, 11154.6562,  9193.6562, 11222.1201, 12139.1602,  8986.2930,
        18273.9453, 10465.8818,  6696.9810, 17410.7793, 15389.4404, 10935.6240,
         6994.1592,  9000.5186, 14508.6191, 10789.8896, 22418.1719, 16840.2656,
         6061.1118, 16332.1094,  8261.6953,  8584.6416,  9022.6084, 13099.2441,
        10426.0117,  8515.8809, 13634.7549, 16054.8701, 10564.4150, 15613.8164,
        16601.3672,  6976.2974, 18382.5410,  6114.0986,  6993.5278, 16440.2949,
        22615.4512, 10702.4004,  6169.7739, 11555.9453,  9385.4375, 17351.6387,
         7693.0269,  8891.9775, 11035.3633, 16282.2988,  8002.3843, 15575.0654,
        10752.8828,  8329.8955, 13467.8125,  6103.6777, 14399.4941, 14867.2646,
        11020.9834,  8773.4619, 15146.9131, 14471.4082,  7676.3950, 10279.3311,
        14573.4150, 13434.1465, 10266.3066, 11620.7510,  9293.5713,  8230.8291,
        16787.8672,  5911.3691, 10880.0010, 11611.9717, 12447.1162,  8995.5518,
         9611.1240, 19729.4824, 10314.9258, 14960.0068, 10255.6729, 12578.6914,
         8377.9707,  9368.5449,  9803.5107,  9726.1797, 12023.4932,  9750.6514,
        10832.2236, 11261.1553,  9392.0830,  8933.2441,  9767.5830, 18939.2324,
         9933.7363,  7233.3613, 12126.7295, 16019.4355,  8309.9404, 15213.8779,
        12939.1133, 11039.8018,  8909.0039,  7102.7822, 13835.6924, 10424.5879,
        15532.9902, 14373.3584, 16319.9033, 14584.4033,  7914.1738, 17108.0137,
        17866.7949, 15190.9141, 12910.4785, 11271.9863, 14393.6074, 14252.1836,
        16999.0605, 14353.6191,  6813.0396,  9409.8945,  7195.1465, 14678.8096,
        14271.2539, 17043.6719,  9950.9971, 12038.5986, 11077.4473, 15316.1641,
        10559.2441, 13749.8594,  5285.4888, 11242.7314, 14458.8047, 12555.1367,
        10626.2568, 11767.1836,  8754.1768, 13999.6504, 12386.1660, 14534.8301,
        11540.2363, 21588.6504, 13772.8896,  7780.1851,  8334.5088,  9252.9951,
         9578.0244,  9668.9043,  6466.4585, 10291.9766, 16298.7588, 11409.6064,
        17820.3418, 15614.1387,  9189.7070, 11929.6758, 12316.7334,  7340.3472,
        10819.7646,  8078.4067, 10103.9561, 14238.0977, 10788.6514, 14134.0312,
        11481.3721, 11888.8408,  7367.9980, 12367.7383, 11241.7500,  8059.2622,
         9579.7832, 10634.3516, 16243.0117, 17665.0273, 16173.1504, 14483.5693,
        14835.6973, 13368.9951, 12447.8945, 12573.6318, 18349.6328, 20216.6621,
        14338.3877, 10932.4268, 13124.3682, 21188.6602, 17676.1660, 13889.7646,
        17309.1094, 12389.6006, 15209.0371, 14566.4541,  7605.9829, 13126.8594,
         7685.4448,  7176.9150,  7263.5327,  8830.8643, 11282.4082, 11508.9297,
         7533.9775, 21992.4434,  9658.8213,  9683.3252,  9053.6348, 19469.7695,
        10923.3984,  6035.1929, 17823.7988, 10596.7031, 11339.7920, 14786.0967,
        18363.1953,  6975.0684, 15289.4951, 18142.2090,  7631.2925, 11719.0879,
         9523.1680, 10299.9258,  6335.4517, 12483.2656, 13509.3525, 19080.2949,
         9458.6270, 12416.4170, 13569.3604, 13603.5625, 11666.3486, 13183.0996,
        15202.6553, 10005.8350,  6388.3237, 10736.2256,  9718.5439, 12069.8486,
        11222.2139, 19293.4316, 16694.0879,  9950.4893, 10059.2090, 11752.7236,
         8439.7393,  7357.3667, 13970.6826, 10840.1768, 12718.1719, 10079.2012,
        10323.8330, 21770.7305, 16683.5039,  6931.1987, 12662.7861, 13399.7812,
        15625.8447, 10898.5479, 12965.4805, 13756.2617, 11667.8535, 13414.3652,
         8058.3789, 12228.0713,  9299.9824,  7148.2119, 14926.0195,  8204.9678,
         8865.4932,  8590.7734, 11137.3594, 15320.2803, 14867.5088, 14171.2627,
         7574.1812, 12990.8818,  9057.0049, 17749.7871, 10807.3623,  8897.2061,
        10575.2539, 10697.3457,  6947.9692, 14417.1904, 11670.0742, 13179.7109,
        13651.7236, 12788.4443, 20026.2148,  8464.8330,  9355.7266,  6434.1240,
        15010.1523,  9469.6826, 15019.4834,  7660.6782, 15850.5430, 22122.7324,
        14432.5312, 14467.1660, 12045.5957,  7677.7847, 11693.8418, 12511.6104,
        17971.5117, 19661.9414, 15515.5049,  5244.7432,  8273.1123,  6889.2188,
        15718.5664, 11630.7158])
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 86908.4531, 137399.1094, 134285.1250, 220959.7188, 192430.5000,
         76683.4141, 109866.1016,  77138.0938, 159391.4062,  91758.5391,
         73486.0859, 107213.7969, 108879.7734, 118240.5469,  58672.4102,
         78081.4609, 128374.2031, 224671.0312,  85025.6094, 146127.5312,
        108965.4844, 131858.8438,  63650.1328,  85072.7266, 119354.1484,
        155611.3906, 130586.9062, 163355.2188, 112110.1562,  98224.2266,
        123483.5000,  87904.4219,  91323.8125, 122361.0703,  88290.4219,
        162520.5156,  89364.3672, 148317.8125, 108368.6719,  77831.0156,
        105275.6328, 140359.9531, 177120.2344, 107591.1406, 108395.9297,
        133765.4375, 139409.4688,  81113.5547, 118161.5391, 119350.8594,
         85071.5312,  99014.0000, 119709.6328, 117623.2500, 106164.3672,
        114335.8125, 160150.6562,  98043.1719,  87733.3281,  90089.8828,
        130627.2578, 132137.6562, 100218.5938,  68108.8203,  94790.4219,
         63207.2617, 211203.4375, 112144.8359, 157159.0312, 116262.4531,
        126122.1719,  82178.3672,  70823.3438, 163410.1875, 103466.7734,
         83794.4766, 105136.0000,  84433.1641, 106315.7656,  91142.4062,
         82716.8281, 104325.0078,  93646.8047, 117446.7109, 105976.9062,
         82667.4219, 163173.9844,  68463.2812, 173314.4219, 115583.1875,
         97076.5938, 160400.4062, 117262.7031, 135278.7344,  99970.6328,
        129577.6328,  69275.4844, 107343.7891,  89327.6953,  89637.5938,
         63094.3203, 131610.6406, 128396.2500,  93105.8672, 104960.8984,
        148380.6406, 108178.1406, 125729.1406,  60625.3242,  98216.1953,
        150357.7969,  95660.2422, 126346.3906, 109940.6484,  94246.0625,
         66156.3984, 110377.2969, 127739.0547, 123452.8359, 139628.4219,
        178281.7344,  82560.6797, 110280.3672, 160729.4844,  59132.8359,
         84739.6016,  82617.6641, 168938.9531,  99866.2812, 129243.5234,
        146219.2812,  91604.9453, 132605.4375, 109858.3672,  97654.4062,
         87085.9531, 108214.5781, 143377.7031, 137176.3594, 100865.5391,
        208211.0312,  69536.9531, 105531.2422, 105756.8984, 102814.8750,
        156984.2188,  97867.6562,  86323.5625, 178247.4375, 123070.9219,
        167351.7656, 148400.2969,  83623.7109, 134438.2188,  94331.2422,
        139464.5781, 142988.4688,  87141.3984,  55961.2148,  90928.3203,
         91607.8672, 124838.3828, 119088.8750,  77372.4766,  88011.2031,
         81770.7812, 149899.1094,  73560.8984, 168740.6250, 108161.9766,
         95775.3359, 135994.1094, 139557.0938,  68039.6250, 113109.1719,
         90504.3359,  76615.0312, 136348.9062,  88364.8984,  76032.5781,
        112480.2188, 148971.3281,  74255.2344,  81078.7500, 111276.3750,
         92206.4375, 127744.6484,  75403.0000, 136958.8438,  48238.8867,
         92905.4922, 142487.3750,  86221.6719,  76566.7188, 103231.9688,
        117518.5469, 128256.9219, 128691.4531, 142568.9844,  92808.7734,
         76576.1797, 133590.0625, 100207.1484, 198934.2656,  61925.4297,
        146998.9531,  86604.7188, 109949.6719,  89150.0391,  93627.0703,
         94007.4219,  70921.8203, 150732.1250, 129018.2188, 104256.6797,
        137518.0625,  97731.4375, 143364.7344, 103712.1094, 170521.3594,
        127687.0547, 148470.8906, 104185.9609, 119484.9219, 106021.8906,
        198093.5156,  79443.4375,  62234.9648,  98211.5469,  95688.2969,
         86010.0078, 143720.2969, 117599.4531, 130430.6406,  63560.3477,
        136718.2188, 118284.2188,  57748.7734, 132565.2188, 129868.7812,
        155081.3906, 106830.6094,  49554.0781,  99112.3125, 108002.8906,
        139425.2969,  74985.2734, 158318.4531, 127924.3281, 125729.9219,
        123971.4844, 128503.0234, 101635.5625,  85457.4297, 125944.3516,
        116322.9688, 175371.2969, 109839.5703, 184722.5156, 101575.7969,
         82086.0156, 197809.6719, 111980.7969, 114819.3750, 137914.3125,
         95318.5547,  63029.8398,  79430.4219,  90253.2422, 129609.6797,
        119981.8047, 189648.9688, 114452.1172, 161826.3906, 136945.5625,
        133146.7031, 132178.1406,  74616.4453, 137313.0000, 180443.6719,
        148948.0000, 195314.4375, 123937.2344, 175506.0938, 178725.3125,
        134356.8125, 106083.1406, 137411.6875,  92657.9844, 100952.6016,
         61599.2617, 153986.8438, 114712.4219, 101492.4766, 122337.8516,
        106831.9922,  98844.0156, 157846.4062, 111218.8828, 186870.9219,
         82742.9922,  93646.7656, 149237.5469, 105808.1094, 106168.6953,
         66363.8594,  93152.0391, 103003.1953, 163933.7500,  89647.8750,
        149681.0000,  94233.6016, 116911.6641,  98086.9219, 167368.7344,
        108939.1406,  83291.9766, 143155.8750, 141885.5469, 122599.7969,
         88103.7109, 129280.7812, 151430.1719, 112142.4453, 109902.1484,
        136920.7188, 109546.8125, 165004.2188, 106172.5469, 119146.6328,
        113546.8516, 162813.9062, 103847.8672,  69981.1797, 183884.2188,
        127638.4375,  88702.1406, 105790.8516, 112544.4609,  85978.9375,
         92088.5078, 145295.7812, 138689.7031, 156601.8906,  86829.6562,
        116438.9453, 162488.5312, 137718.8906, 109224.7422, 138809.1562,
        102336.8438, 104590.2266, 143891.0312, 121345.1016, 164499.5000,
        108210.4531, 102849.4375, 102418.0234,  91039.8750, 111272.6328,
        111405.0547, 114111.5625, 128049.5938,  97745.8359, 126087.3516,
        129691.5703,  94000.7422, 143092.5781,  96040.8594, 157565.0938,
        154204.8281,  68464.0859, 177392.1562,  61123.2383, 156200.9375,
        142671.9219,  93964.5469, 147654.6875,  73373.6172,  99956.2031,
        170876.8906,  89902.5391, 127889.5938, 113476.5391,  91526.8828,
        118190.4297, 118889.1406,  98866.2812, 168876.6719, 131698.8594,
         98326.5156,  67386.1797, 100349.4219,  87116.6719,  67744.2422,
         77048.3281, 116058.6641, 117277.0781,  92781.7031,  84796.2188,
         58823.8438,  58973.0820,  81149.2422,  86595.4062, 210924.7500,
         97203.6094, 132976.7344,  89847.8750,  63699.5391, 144196.8750,
         86145.6953, 110148.1562, 161490.2188, 187868.9375,  88305.1094,
         96191.9844, 164407.6562, 144115.6719, 100580.4141, 138680.2656,
         93790.8828,  83877.3125, 173381.6250,  57143.0664, 114998.6406,
        153173.1250, 148902.5469,  76422.3438,  67505.8516, 107130.5547,
        123623.4688, 115518.8047,  67701.7188, 104800.3906, 130051.8828,
        144282.3750, 182022.0000, 152739.5000,  92801.7891, 102222.3047,
        216142.1719,  80041.1094,  86258.8672,  81694.8438,  87829.0156,
         75255.6406, 172523.7031,  78324.0625, 162597.4531, 150799.0625,
         75771.5312,  74572.0625, 181035.4219, 157631.4531, 121993.4219,
        169714.0625,  69341.3750, 111492.0859, 125020.0312,  88041.2500,
        110273.7422, 124603.2969, 105935.3828, 205401.4844,  96961.3672,
         77233.0000, 112759.2656, 192500.3438, 143809.4688, 120798.2344,
         98590.1953,  78723.0312, 136434.3750,  87721.2500,  70459.9766,
        104093.5547, 188505.7969, 158461.9375, 143954.3125, 160799.4688,
        127639.0234, 143559.7188,  91331.5703, 146904.6562, 113538.7109,
        120602.8281,  99270.7109, 123876.6875,  73451.5156, 161335.4062,
        156788.1719, 141763.0938, 111945.3906, 131632.2344, 129920.6406,
        138098.0312,  77396.3906, 122978.9062,  81208.1953, 175123.0000,
        113627.0234, 163537.2656,  91576.1953,  97396.5938,  80072.3594,
         87303.8203, 159553.8125,  87530.3594,  83291.6641, 104640.7500,
         85398.6641, 110671.9297])
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2339.9136, 3155.6636, 1279.5869,  ...,  184.5611,  265.4602,
        1896.2245])
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2185.1301, 4955.5122,  563.3434,  ...,  843.7219,  182.7095,
        1217.4857])
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([2530.1533, 2581.9458, 2305.9851, 2934.7837, 1946.5808, 2546.8474,
        2995.9771, 2097.6338, 2862.4922, 2147.2749, 2186.6279, 2387.2075,
        2229.8281, 2228.1252, 2833.9033, 2618.4990, 2050.0544, 3061.1323,
        2608.5676, 2252.1833, 3491.0959, 2662.5603, 2721.1987, 2261.2402,
        2094.7329, 2623.2295, 2336.2747, 2608.9138, 2534.3750, 3367.2302,
        2053.3171, 2237.7991, 2461.8010, 2803.1787, 3124.5903, 2425.6670,
        2166.1533, 1929.1321, 3447.0642, 2569.4700, 2653.3250, 2601.1631,
        2162.1023, 2955.8445, 2069.4954, 2734.6296, 2345.0759, 2385.5010,
        2231.5188, 2355.5105, 2375.4365, 2077.1914, 3194.6304, 2281.3374,
        2094.1150, 2840.9978, 3225.9661, 2454.3657, 2143.5776, 2011.7990,
        2840.0317, 2390.0093, 2196.4214, 2496.6873, 2242.1433, 2719.2507,
        2044.2085, 2226.4255, 2007.4493, 2791.0742, 2549.7852, 2170.1943,
        2068.0947, 2242.1206, 2429.0874, 2121.5842, 2233.4651, 2500.8904,
        2233.3660, 2138.5154, 3154.7078, 2605.6599, 2874.5237, 3019.6326,
        3291.7410, 2917.1313, 2470.9624, 2856.4231, 3004.9910, 2062.7373,
        1941.1991, 2976.6162, 2843.7444, 2403.0547, 1992.2985, 3091.5291,
        2648.3542, 2205.9043, 2492.7930, 2973.3127, 3196.6116, 3538.2563,
        2569.9192, 2097.2532, 1853.2031, 2222.8069, 2665.9956, 2137.1436,
        2530.9004, 2424.3677, 3078.1885, 2590.7351, 2445.8130, 2096.9661,
        2053.0811, 2586.6609, 2505.2881, 2668.9136, 2739.2041, 2718.8347,
        2797.0542, 2758.3259, 2276.5059, 2474.3816, 2243.0771, 2768.8047,
        1931.4019, 2220.7937, 2792.6624, 2599.9814, 1988.8414, 2752.3904,
        2320.8862, 2338.1736, 2536.4246, 2455.0828, 2506.7053, 2596.5359,
        2903.8560, 2208.4141, 2005.6738, 3002.8660, 1965.8851, 2209.8782,
        2837.8960, 2173.9204, 2320.1401, 2768.9309, 2182.0144, 2604.9070,
        2990.4121, 2187.0874, 2390.3376, 2391.9863, 2031.2498, 1978.4012,
        3116.3691, 2137.8433, 2452.3069, 2375.3792, 3414.2722, 3170.4246,
        2590.1248, 2874.3362, 2232.1885, 1923.5701, 2981.7466, 2780.3677,
        2792.7915, 2663.4500, 2470.9033, 2774.2996, 2493.2695, 2096.0718,
        3272.3223, 3153.0454, 2316.5312, 3067.9299, 3158.6187, 2689.0820,
        2357.6799, 2361.6074, 2428.7349, 2750.7134, 2147.3726, 2173.6460,
        2811.6299, 2354.5525, 2785.6021, 2650.6934, 1768.2379, 2692.7253,
        2271.9485, 2747.3989, 2166.2917, 2350.0564, 2353.9854, 2915.7383,
        3613.9304, 2033.5538, 2676.5720, 2690.3279, 2933.8945, 1957.1158,
        2061.6396, 2995.2996, 2055.4661, 2262.9951, 2296.1306, 2786.0947,
        2473.7139, 1846.8206, 2049.2683, 2205.6379, 2295.9084, 2537.9395,
        2205.4216, 1950.6335, 3180.2480, 2718.0715, 2654.2019, 2278.1804,
        2323.2363, 3277.3743, 2618.6836, 2439.6992, 2826.7795, 2241.3333,
        1987.9235, 2849.6575, 1943.1907, 2939.9897, 2246.8342, 2993.5203,
        2309.4160, 1587.9701, 2885.2888, 2914.9158, 2962.4136, 2895.7791,
        2221.1953, 2437.9832, 2310.5505, 2585.1272, 2240.5049, 2531.2151,
        2703.5547, 2274.0291, 2086.3752, 1698.3757, 1946.0480, 2659.7512,
        1830.7662, 3071.1982, 2826.9407, 2561.4424, 2615.5461, 3843.6536,
        2550.2947, 3006.0986, 2525.4238, 3228.9270, 3210.4202, 2546.0095,
        2683.7302, 2347.3936, 2577.8706, 2133.7380, 2328.7312, 2685.8660,
        2831.9001, 1869.9688, 2504.4790, 2235.2117, 3355.2305, 2363.6704,
        2684.5725, 3216.4858, 2286.3540, 2172.9500, 2372.1396, 2656.0439,
        2271.9807, 2160.5266, 3560.2390, 2174.4570, 2406.6384, 2088.7156,
        2245.5188, 2230.1223, 2369.3594, 2995.6223, 2105.1853, 2357.1370,
        2349.3347, 3237.6309, 2514.3491, 3073.3459, 1893.1135, 2197.3420,
        2236.9600, 2806.8689, 2372.9529, 2506.0579, 1862.2876, 3204.3137,
        2368.4827, 1780.6049, 2450.9417, 2711.3547, 2747.1489, 2072.6877,
        2548.0820, 2310.9436, 2677.2524, 3025.1960, 2350.9070, 2795.6973,
        2961.1868, 2866.8650, 1720.8779, 2882.9370, 2783.8706, 2761.8694,
        2475.1272, 2407.6836, 2897.7444, 3293.8533, 2491.6431, 2760.9163,
        3466.5051, 2881.1912, 2566.1169, 2133.5295, 2306.9685, 2129.7832,
        2504.0908, 3111.2883, 2693.4805, 2310.7319, 2436.8357, 2479.9160,
        2407.1609, 2418.6064, 2860.0190, 2553.1885, 3061.4973, 2604.1877,
        1970.5120, 2473.0581, 2516.9917, 1851.1290, 1937.7802, 1905.5366,
        2610.7598, 2443.1714, 2239.3638, 2299.0488, 2778.0129, 3194.7378,
        2478.7966, 2277.4473, 1666.2572, 2492.8442, 2103.1604, 2380.7634,
        2174.6965, 2330.7773, 2192.2964, 2181.5049, 2936.7732, 2612.0100,
        2322.6028, 2219.0659, 1979.5416, 2232.8281, 3091.1724, 2278.5974,
        2361.3340, 2346.4214, 2084.1819, 2668.5806, 2303.6208, 2349.0723,
        2643.1824, 2738.1079, 2130.5271, 2499.1013, 2158.8801, 2151.7231,
        2184.5618, 2285.5928, 2351.3936, 2356.9031, 2659.3232, 2273.9971,
        2151.3379, 3309.5256, 2203.0002, 2528.7275, 2474.8640, 1578.2317,
        1408.6836, 2172.9780, 2484.0920, 3199.0295, 2096.3311, 2247.3115,
        2462.4595, 1968.8831, 2200.0771, 1864.2356, 2054.1721, 3056.6094,
        3113.6846, 2175.6067, 1792.0687, 2459.1860, 2483.7903, 3219.6113,
        2894.0264, 2643.9644, 3584.6965, 1955.2378, 2423.3833, 2488.3389,
        3086.9099, 2242.5122, 2761.2700, 2913.2988, 2053.2703, 2370.1365,
        2272.7556, 2924.9211, 2253.4475, 2776.9487, 2131.6123, 2540.1394,
        2464.7566, 2355.6499, 3166.7776, 2472.6350, 2794.0515, 2710.6360,
        2072.3372, 2911.8918, 2947.8723, 2397.1794, 2567.3330, 2135.8872,
        2573.7383, 2770.5969, 2568.0811, 2684.3108, 2768.6687, 3140.0215,
        2732.9700, 2510.2332, 2768.2908, 2460.2163, 2639.8132, 2615.8823,
        2949.7957, 1888.2094, 3001.0356, 2700.2812, 3304.1970, 2374.0432,
        2434.1467, 2532.5952, 2956.3525, 2511.1567, 2242.1240, 3168.6758,
        2596.9185, 2269.2290, 2475.3335, 2044.1335, 2740.8794, 2252.3657,
        1822.3707, 2749.8567, 1972.0966, 2093.3540, 2282.7502, 2887.3450,
        3287.5068, 2412.0681, 2183.5283, 1991.5048, 2484.4929, 2173.2966,
        2349.5181, 1830.9196, 2600.1721, 3185.8557, 1897.2898, 2873.2097,
        2549.2109, 2023.4578, 2155.9688, 2546.9490, 1972.7300, 2323.4675,
        2152.4502, 3053.0959, 3167.5728, 3059.6279, 2702.7400, 3120.2954,
        2536.3882, 2644.8503])
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([23095.6777, 22417.3301, 22741.3281, 21422.8887, 14118.5801, 20023.0527,
        19161.5645, 20769.8867, 22344.0020, 26199.0703, 20049.9922, 23943.4980,
        23460.9121, 22540.9102, 23309.0469, 19798.8691, 17271.3652, 23000.7812,
        20357.7910, 25685.6914, 18329.0195, 16038.7676, 22902.0137, 16345.7568,
        20013.7715, 16488.3574, 26946.8340, 22714.7285, 21006.4609, 19104.3184,
        16101.8623, 13057.7402, 16536.7949, 24666.8789, 17566.1680, 20814.5234,
        26784.7988, 18733.6816, 13494.3809, 19841.9004, 23699.9434, 23138.5117,
        16699.3633, 19939.0527, 26001.8633, 23686.7383, 26485.5078, 15638.0654,
        16797.5898, 20216.0234, 22537.6348, 18403.2988, 21017.5742, 22287.4980,
        15503.9600, 20921.4160, 22268.4199, 21408.8164, 21660.4004, 14826.1113,
        24195.2168, 28901.9512, 23854.8613, 23844.6172, 23215.4043, 16335.8193,
        26050.0566, 22311.7246, 19437.0332, 24381.4082, 19971.5000, 22237.0547,
        16530.9727, 24035.3789, 19153.0977, 22219.5039, 21619.2539, 20155.8203,
        23689.8691, 26150.0234, 21559.6738, 14200.5283, 20043.6289, 20719.3613,
        29109.5957, 18230.6465, 17966.2129, 23391.1406, 19295.5312, 20857.5625,
        23612.7441, 25417.1035, 21183.8496, 27244.4629, 30316.5039, 20131.5020,
        24042.1543, 15767.1904, 20144.5801, 18509.2070, 20196.5059, 19410.7480,
        15216.0645, 20917.4102, 17879.7871, 22281.1641, 20767.6367, 27435.4492,
        25686.3691, 16016.7764, 18221.9199, 16051.6553, 16167.5811, 24378.3359,
        18256.4316, 16813.4844, 17840.6855, 18887.5879, 20383.6914, 23334.3535,
        18477.3789, 22248.9922, 21644.1934, 22369.8164, 19156.9023, 18264.9668,
        22545.1406, 21948.6602, 20315.5332, 25770.7266, 22721.6113, 17268.8262,
        19564.8262, 17093.4082, 14357.1924, 25200.0664, 24756.3535, 22224.0098,
        20317.5898, 19291.1094, 24017.1914, 22904.0527, 18165.2246, 17171.9883,
        20917.5332, 18794.6504, 22923.6582, 17272.5488, 21371.3535, 19089.6465,
        24552.3398, 18160.3516, 20046.1016, 23217.9980, 19229.5879, 17325.5801,
        22970.0117, 22793.5449, 23263.7148, 18187.3066, 22232.0762, 21855.3945,
        17128.2090, 20456.2227, 16806.3496, 22812.3848, 19624.9102, 19214.3145,
        17872.4375, 22622.8477, 16454.6270, 19343.9863, 26803.8730, 24095.3008,
        17966.0801, 22319.1055, 24083.2637, 22222.4336, 24691.4785, 26018.8848,
        21572.4648, 21106.0508, 23157.9453, 26770.9590, 19586.2617, 21333.6836,
        17654.0137, 16254.8311, 19795.5488, 21997.0938, 23896.5430, 15870.5537,
        21607.4727, 19789.0547, 18257.7383, 22287.8359, 18208.2031, 18054.8730,
        21194.6602, 18187.2188, 18499.9902, 17025.1309, 19942.7969, 19248.8301,
        22579.1797, 22579.7637, 23447.0156, 21124.1797, 25520.7910, 25930.6914,
        24960.9746, 20160.8730, 20937.8242, 27024.1504, 19068.7363, 23397.5684,
        21879.0312, 22303.8457, 26058.6055, 18643.4961, 20940.7207, 23739.9160,
        16567.0234, 21703.6934, 24442.0312, 17325.3457, 17971.1523, 29940.1094,
        25365.5762, 27032.7090, 24179.8691, 25977.9453, 18096.6602, 18259.1816,
        17473.7773, 21030.6875, 16846.8574, 19255.1680, 15954.0020, 17524.6523,
        15968.6436, 28782.9590, 23366.2539, 18196.2793, 21077.1055, 27260.2793,
        17348.7852, 23227.2715, 22708.4082, 19454.1348, 18901.3105, 20690.9746,
        21085.2637, 22894.0586, 15654.3760, 20857.0078, 18660.7207, 24551.3066,
        19852.3477, 17792.4023, 23138.7012, 21427.4492, 24027.4980, 17712.6660,
        19862.1641, 20080.8613, 17960.3105, 18913.8906, 20271.1094, 19952.7871,
        24597.8145, 22293.1270, 22480.6016, 15030.6885, 18926.7402, 21999.6934,
        20215.4238, 16298.5791, 19248.8789, 15944.7549, 25185.0566, 21179.2148,
        23431.5586, 22498.9473, 22156.6797, 22753.9863, 16613.1113, 18995.9570,
        20021.7266, 22030.5723, 15618.0713, 17352.8105, 21515.1914, 22105.1660,
        18626.7109, 21726.6406, 20909.5391, 15513.1426, 16238.2705, 22290.7871,
        17636.5469, 24679.7754, 17594.8750, 25109.2031, 21183.3398, 22596.6270,
        18378.6113, 23282.6699, 21867.1543, 20153.4961, 17717.6895, 17113.3457,
        20538.0117, 24757.9902, 18421.7246, 19165.1367, 18604.2559, 13299.7051,
        18636.7227, 24476.0254, 21259.6621, 20246.2598, 19821.9902, 17419.2715,
        21186.2246, 22944.6211, 19132.1328, 20590.9238, 28615.3984, 21710.8613,
        24780.4688, 24970.9180, 19285.2520, 24938.3848, 25542.1816, 18580.1836,
        15779.8223, 21277.1992, 27564.4883, 19362.8223, 22224.8105, 17730.5020,
        28969.3711, 22233.6504, 16646.3984, 21228.9648, 21351.9219, 22504.7070,
        24948.4531, 21766.4395, 19433.5039, 18352.8359, 19911.1504, 21022.8418,
        19326.8516, 26093.9688, 18020.1719, 34720.1445, 15340.6113, 14350.0322,
        21222.4277, 27037.8457, 18265.1504, 22080.8535, 27147.1934, 18525.0938,
        23396.6875, 22750.2969, 19015.2656, 24000.8906, 24326.2969, 22338.7207,
        17695.4863, 23044.3555, 20837.8262, 22799.3613, 19012.6348, 19084.1016,
        18221.3516, 20049.4609, 28472.1191, 19915.2930, 14369.7412, 22085.9785,
        20634.0371, 26771.9141, 15879.2627, 18612.3555, 22318.7012, 20430.0918,
        24587.5195, 20267.8789, 17954.1836, 22389.2285, 20994.3887, 24144.7383,
        14874.1406, 18375.4180, 23388.2832, 16410.6914, 23059.7422, 19221.7129,
        30117.3125, 20959.6504, 20599.4688, 16959.5469, 19638.6035, 18597.9102,
        18929.7441, 16678.7520, 19628.4453, 17143.0273, 23712.6504, 16683.8789,
        20340.6582, 26643.3711, 16109.8672, 21389.5371, 16878.5645, 24341.9961,
        22221.7930, 21329.8301, 19350.7559, 21884.4277, 20984.5176, 17768.4688,
        26027.1094, 19525.7402, 25544.5527, 22454.0391, 21743.6387, 19135.4258,
        22774.0195, 24612.5898, 22611.4492, 21093.0410, 19728.2754, 21929.1797,
        22976.9551, 22050.9648, 20775.4375, 22240.6465, 16552.5527, 20150.5176,
        21506.8770, 20949.1074, 32838.8945, 19273.8926, 31901.3516, 15339.7461,
        18768.3066, 19625.8457, 17520.0156, 24759.5254, 25287.9844, 22223.5918,
        17226.7988, 23971.9336, 16056.7666, 24470.4688, 21247.3262, 24013.9102,
        19078.6953, 15096.0098, 16600.8945, 19704.5293, 18140.9590, 20703.1738,
        17118.5801, 20612.1309, 22399.8848, 22747.1211, 17750.2969, 17333.5332,
        24003.6230, 25462.2871, 19833.9160, 20098.0605, 24460.2910, 19817.0781,
        21036.8340, 20681.0137, 27956.5352, 15885.4902, 16814.5137, 23922.4160,
        27742.2520, 22082.0430, 16575.0859, 17752.4043, 18537.3828, 15532.2402,
        22120.6016, 26235.5195, 20389.5898, 19486.7227, 24179.9922, 16439.5469,
        18049.7246, 21296.4902, 19275.6777, 18743.8848, 20191.5371, 21071.7344,
        25083.8848, 32295.6094, 22327.9023, 24239.4141, 20998.3027, 20303.9980,
        21571.4961, 19651.5957])
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([ 298.3011, 1546.0450,   69.3401,  ...,  710.9474,   14.0624,
          45.5800])
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([1777.3070, 1190.3577, 1185.0731, 1140.7402, 1220.0072, 1860.1503,
        1177.1128, 1179.9646, 1381.0598, 2196.8313, 1581.6714, 1816.1278,
        1760.3348, 1187.5023, 1911.4619, 1390.3423, 1354.7994, 1772.7509,
        1921.9269, 1395.7549, 1289.5188, 1154.2059, 1242.4966, 1720.4805,
        1945.6766, 1949.5624, 1475.9279, 1529.1368, 1923.9991,  970.7373,
        1813.0325, 1308.5559, 1215.8862, 1263.9830, 1159.7937, 1161.7706,
        1618.9667, 1622.9211, 1439.7119, 1370.7625, 1332.2573, 1381.7334,
        1125.3074, 1680.6560, 1434.0367, 1401.7609, 1423.1880, 1750.1399,
        1464.1241, 1349.0753, 1524.4186, 1261.7567, 1543.6461, 1798.5587,
        1367.2980, 1790.1488, 1086.5409, 1301.6017, 1188.6970, 1324.5085,
        1212.6445, 1215.1484, 1069.8381, 1333.7115, 1253.4308, 1583.1577,
        1437.9856, 1296.2238, 1327.5411, 1090.2614, 1359.2106, 2067.8003,
        1261.5669, 1253.0334, 1438.2059, 1220.5377, 1588.8535, 1397.3092,
        1558.0979, 1711.5897, 1208.6172, 1459.6100, 1471.7522, 1498.8109,
         992.6980, 1051.4979, 1648.1892, 1213.9752, 1377.3457, 1189.0007,
        1183.5931,  900.6333, 1697.8158, 1320.9987,  921.2288, 1596.0890,
        1549.9452, 1318.5449,  969.4232, 1290.6830, 1227.6160,  963.9333,
        1102.0217, 1313.0289, 1193.8699, 1690.4108,  955.8172, 1086.5917,
        1451.0922, 1509.7788, 1408.5044, 1090.9689, 1022.8776, 1002.9636,
        1640.5172, 1196.1975, 1371.6708, 1668.5835, 1406.1327, 1730.8142,
        1321.6001, 1069.3243, 1993.6705, 1070.5253, 1450.6394, 1711.1785,
         835.9678, 1661.2885, 1271.3513, 1530.1637, 1304.2845, 2025.9299,
        1535.7609, 1532.5653, 1324.5721, 1561.7714, 1563.7847, 1252.8284,
        1195.4017, 1334.9976, 1052.9855, 1430.8969, 1607.1976, 1412.3199,
        1118.1140, 1852.2638, 1608.4862,  882.2800, 1276.9103, 1241.5967,
        1595.8129, 1400.9917, 1188.8717, 1393.5332, 1116.8682, 1156.8369,
        1411.2246, 1586.4758, 1714.8240, 1194.6072, 1245.0078, 1131.6785,
        1087.5300, 1513.2632, 1123.1252,  980.8531, 1328.8590, 1220.3894,
        1361.3417, 1761.2869, 1852.6711, 1409.7296, 1227.3700, 1209.5902,
        1699.9978, 1454.8019, 1063.4994, 1515.4829, 1722.2762, 1778.6787,
        1224.8422, 1755.6321, 1105.0139, 1445.6466, 1369.4917, 1483.2595,
         947.2822, 1322.8910, 1086.1067, 1661.3682, 1252.8376, 1150.3983,
         862.4269, 1572.6266, 1271.7450, 1553.7384, 1210.1458, 1486.9166,
        1018.9878, 1214.0006, 1545.8079, 1184.3536, 1327.0536,  964.6745,
        1665.4235,  899.0728, 1637.3463, 1076.8024, 1226.6470, 1315.5356,
        1105.2451, 1409.4558, 2158.8806, 1155.8097, 1641.7448, 1464.2599,
        1941.2532, 1430.5647, 1678.7246, 1612.1617, 1028.7863, 1402.5504,
        1092.3594, 1114.7024, 1412.3582, 1436.5161, 1251.4520, 1319.2784,
        1200.5063, 1065.5154,  883.7809, 1048.1332, 1128.7477, 1430.8052,
         994.9359, 1424.5117, 1042.0380, 1381.5067, 1863.7108, 1505.7407,
        1322.8467, 1297.6021, 1175.6943, 1251.0900, 1209.7151, 1226.9277,
        1491.7719, 1079.5397, 1107.4382, 1484.2881, 1157.2231, 1523.3278,
        1088.1620, 1121.6934, 1327.1450, 1994.0190, 1715.9786, 1382.8866,
         861.5847, 1549.9413, 1347.8524, 1231.6344, 1749.6208, 1274.2241,
        1953.9060, 1259.4825, 1088.7067, 1587.1925, 1163.4561, 1333.6455,
        1159.3854, 1465.5131, 1412.3875, 1245.8868, 1677.9913, 1555.0675,
        1040.7233, 1088.2697, 1263.5872, 1224.0492, 1043.2213, 1458.8593,
        1412.3816, 1346.4850, 1448.3528, 1097.2678, 1366.6824, 1306.3098,
         867.2551, 1414.2881, 1340.7241,  953.2692, 1271.7196,  963.4501,
        1352.1150, 1326.4111, 1566.0498, 1872.6078, 1547.3203, 1794.8700,
        1114.7438, 1274.8965, 1180.7715, 2154.5830, 1475.4021, 1125.9703,
        1354.9879, 1734.9114, 1216.7819, 1276.3177, 1437.5415, 1083.7244,
        1607.0167, 1499.7650,  961.8359,  959.4128, 1405.1348, 1367.5400,
        1362.9365, 1400.0217,  996.8145, 2004.3191, 1331.8514, 1434.3219,
        1299.8680, 1506.1099, 1900.1012, 1649.0421, 1421.6329, 1624.3918,
         985.5445, 1680.3981, 1969.4485, 1693.5269, 1384.7196, 1523.8564,
        1524.9153, 1366.0870, 1164.3163, 1203.4077, 1354.9211, 1175.2423,
         910.7465, 1381.1117,  924.0864, 1749.9951,  953.0831, 1231.6299,
        1395.7130, 2075.4495, 1429.4850, 1938.7943, 1177.7351, 1449.7166,
        2221.0386, 1529.6581, 1813.7776, 1665.8705, 1063.6510, 1504.6062,
        1262.1547, 1283.3402, 1044.1732, 1292.5281, 1513.5792, 1144.3173,
        1540.7152, 1432.6669, 1204.8577, 1212.7869, 1177.4974, 1203.8270,
        1288.9430, 1142.9425, 1183.1886, 1004.1556, 1215.4818, 1374.5309,
        1116.4823, 1349.8792, 1547.4425, 1688.9900, 1802.2338, 1467.7405,
        1573.0457, 1492.0214, 1163.1888, 1356.4773, 1525.3075, 1763.3342,
        1638.1945, 1105.9707, 1308.9840, 1635.2106, 1146.2700, 1558.0765,
        1084.9280, 1120.2782, 1462.2703, 1285.7246, 1804.9069, 1150.7904,
         969.9800, 1887.0035, 1248.4711, 1663.2919, 1452.8888, 1318.6990,
        2365.2178, 1317.0018, 1411.1042, 1386.4308, 1258.9659, 1377.3236,
        1214.3822, 1216.8816, 1379.2134, 1049.6417,  913.0220, 1516.0022,
         935.5843, 1198.6475, 1314.2720, 1149.0575, 1039.2633, 1260.3108,
        1506.9490, 1426.8762, 1243.9093, 1705.5950, 1085.7292, 1366.6383,
        1298.2988, 1987.7078, 1430.9174, 1655.0470, 1021.0873, 1220.8661,
        1422.6874, 1922.5052, 1283.7253, 1795.9343, 1103.5675, 1858.3912,
        1237.1283, 1660.1149, 1680.3459, 1145.4255, 1919.2432, 1442.8284,
        1421.1040, 2229.3630, 1567.7865, 1307.1836, 1266.0763, 1223.4745,
        1266.7723, 1063.7770, 1277.5327, 1303.9001, 1347.0281, 1315.0769,
        1308.9746, 1585.2123, 1453.9736, 1482.0643, 1430.0458, 1517.0940,
        1436.3402, 1507.7395, 1611.5079, 1581.0825, 1142.8522, 1119.9321,
        1200.4998, 1472.1355, 1314.3011, 1537.2899, 1792.2631, 1259.5575,
         977.0677, 1449.1663,  884.3346, 1236.7896, 1410.6215, 1470.0634,
        1575.7144, 1164.0404,  990.1313, 1496.1560, 1308.6918, 1294.6072,
        1052.2616, 1293.3802, 1437.0472, 1899.7891, 1267.6677, 1295.3345,
        1066.8459, 1196.4856, 1198.4574, 1388.2443, 1411.2129, 1175.8201,
        1614.1058, 1689.0378, 1284.5289, 1421.6908, 1736.0470, 2295.2690,
         958.9112, 1217.0940])
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([11574.0469,  9295.2725, 13531.0264, 11768.1895, 15676.9395, 14093.3965,
         9944.3809, 14854.6123, 10998.5859, 13569.4229, 12299.6914, 16109.4189,
        12413.9053, 14926.7529, 12236.8906, 15338.7861, 21316.7656, 13120.0244,
        14199.0723, 10982.1299, 12815.8096, 12610.5205,  9563.8135, 13180.7676,
        11111.3447, 10467.6221, 17623.4531, 14004.8564, 14807.9531, 13179.8086,
        13814.3984, 11284.4209, 11372.8027, 14951.7529, 20751.6465, 13967.0186,
        15480.1396, 14204.3916, 14595.6787, 14855.5547, 15236.6553, 18372.3730,
        12684.5420, 15765.1084, 13489.4736, 17018.1270, 12871.9092, 10967.3799,
        16027.0928, 11295.6592, 14737.1006, 14672.1934,  9403.2305, 18378.3672,
        13887.4150, 12500.7881, 10400.9004, 13608.2178, 14068.1094, 14313.9141,
        12824.7920, 14810.2646, 12378.8174, 15767.3623, 10465.7861, 15656.2480,
        15291.2031, 15555.8672, 15792.4912, 14431.9268, 17788.2949, 15670.0889,
        12993.8027, 11854.9473, 11603.3252, 15098.6201, 12816.0576, 13851.5830,
        15495.9258, 10287.5791, 14032.7520, 14336.1709, 18398.5918, 11177.8506,
        12772.8633, 12365.0137, 14597.2939, 12838.2646, 10763.5820, 14590.2988,
        16678.0020, 13999.5342, 11869.4287, 11156.6309, 14212.2236, 12564.4561,
        14935.0625,  8786.7666, 14244.0166, 17298.4141, 16825.2246,  9470.5303,
        11820.3916, 15601.4033, 11362.8535, 17951.7617, 12541.7549, 12759.8711,
        16297.5938, 14241.3047, 13343.1523,  9490.4922, 10538.1250, 14906.9199,
        14045.7637, 12560.6299, 13137.6025, 22463.5547, 11161.0449,  7526.5337,
        16247.5098, 12396.4844, 11443.1631, 16131.8057, 10319.3516, 16008.5244,
        11697.7842, 12582.8232, 15474.5137, 11804.9609,  8632.2129, 16104.8096,
        14381.8633, 15512.7676, 11715.2988, 13141.8789, 16571.6016, 13807.5010,
        13464.3232, 17560.4688, 15650.7178, 14276.5947, 16715.5898, 12356.0254,
        13161.2012, 15467.7881,  9438.8848, 12629.4404, 15023.2051, 12686.5625,
        13116.9121, 16544.3887, 13392.2891, 11609.3652, 13981.5332, 13662.3936,
        14075.1562, 18253.4219, 12662.7305, 11649.2383, 13694.7979, 12041.9121,
        18561.3594,  9772.8779, 11380.6914, 15541.3711, 12298.1553,  8947.2412,
        15634.4092, 15462.2158, 14793.9395, 13407.1562, 12175.8018, 18759.6641,
        11564.8164, 14331.7773, 14618.0527, 14925.4043, 11834.9619, 10346.5889,
        15395.4951, 12280.5635,  9831.7461, 17101.3398, 11247.4814, 14039.4502,
        13969.6592, 11254.9141, 15419.3203, 14557.8291, 14305.2998, 13659.9639,
        14169.2656, 13126.6025, 12214.4453, 10352.4287, 16960.8711,  9321.2412,
         9894.3398,  9862.4990, 10402.0664, 20624.9395, 15969.8242, 13915.9775,
        11486.1045, 11254.7891, 20090.5137, 16361.1816, 19223.3770, 11365.7881,
        16248.2305, 11922.4600, 17664.8535, 14137.4248, 10330.5752,  9644.4922,
        12286.2109, 10686.5459, 14560.9092, 10680.6689, 15219.2324, 12120.8525,
        13101.2295, 12974.1465, 15259.2998, 16078.4922, 12620.1250, 12531.4258,
        10187.1367, 14088.9121, 11047.1836, 12353.6982,  8004.9434, 15334.9971,
        15811.3818, 18042.7500, 10707.2188, 13192.4541, 12507.0576, 14983.2012,
        13248.0430,  9719.8926, 12387.8535, 12115.6553, 12357.3711, 16126.6768,
        12390.1689, 17285.7168, 15659.2344, 13725.1406, 12555.0820, 11766.7656,
        10748.0088, 21701.7109, 15051.7344, 18028.6543, 15217.0098, 12228.4111,
        16895.0781, 11309.2881, 10935.7490, 14585.7295, 11665.3965, 11291.3135,
        16009.6748,  7958.9868, 10603.9062, 10386.0049, 13384.5693, 14327.5771,
        16259.0371, 17041.2812, 13225.9326,  8681.9043, 13051.8984, 13165.5312,
        15368.5537, 13777.2637, 15624.7012, 11058.9570, 13400.3203, 10297.2803,
        10881.9072, 12693.4463, 13312.5127, 13884.4268, 12097.5303, 12153.9121,
        11033.2227, 11805.6309, 15426.0186, 12644.9580, 14853.5576, 11522.6230,
        14100.5938, 15947.8936,  9304.4004, 14219.1289, 17689.5254, 11013.6523,
        11920.5889, 11152.3496, 10496.9697, 12209.9307, 11075.7314, 18633.4648,
        10332.7939, 13510.2988, 15318.8877, 20141.4609, 13567.4580, 16770.7539,
        16495.4375, 15664.5654, 13876.6006, 13975.0566,  8999.2158, 11222.5156,
        14145.0371, 20154.0762, 12638.1299, 13380.8057, 13685.5029, 17106.2891,
        13311.9932, 11274.7695, 15806.5166, 12687.8008, 13604.6553, 14706.9473,
        14536.9541, 12310.6377, 12618.4746, 14932.4551, 10641.4551, 15417.5254,
         8311.7217, 12865.4326, 12817.5449, 15371.6611, 13319.3359, 15612.8965,
        14339.5156,  9561.9717, 14389.6602, 13043.9033,  9695.0625, 12827.7119,
        10375.9600, 13413.3389, 16492.5879, 12196.7578, 15602.9453, 24697.2988,
        11957.7383, 13182.7422,  9879.5166, 16057.2227, 12934.7998, 14984.0898,
        20188.4922,  9893.6826, 15694.9521, 17144.8848, 12427.5029, 22928.1934,
        15590.3506, 15619.8457, 13395.4287, 13096.1846, 15164.1318, 11660.5371,
        15414.2998, 11872.8135, 10975.4180, 13218.4082, 13264.5791, 12640.3701,
        13035.3652, 13665.8037, 14007.7217, 15483.7314, 16679.2715, 12949.2754,
        13504.8359,  9438.4482, 12574.5713, 12465.3779, 12672.5039, 11748.4717,
        13902.0879, 16863.4180, 17809.5059, 12549.9883, 12505.9072, 16251.8428,
        16522.4102,  8679.7871, 12084.5762, 13892.2646, 13965.2305, 21568.3027,
        14339.1328, 11314.0039, 11425.5654, 16096.1621, 12657.0527, 13682.5889,
        11138.3242, 16055.7207,  9651.8564, 15054.9033, 16704.9219, 12539.0547,
        15979.8027, 12169.8506, 15657.9746, 12334.8330, 12996.0205, 14888.6699,
        14196.8779, 18745.6445, 11311.8545, 15250.0303, 18528.0352, 13028.4512,
        14605.0127, 15426.6768, 15703.1104, 13642.3535, 12020.4111, 12083.7305,
        15642.5957, 18840.1582, 16400.6582, 10702.2861, 11954.4170, 17456.1777,
        14052.8848, 14876.8877, 17117.2969, 13557.3574, 13566.0020,  8965.5166,
        11042.3018, 11169.4990, 14298.5078, 13385.1963, 14928.2109, 14428.4941,
        18186.3750, 11212.0859, 14617.3262, 15427.1641, 10982.5029,  9733.2754,
        18047.9668, 11421.6865, 18127.8750, 13231.3154, 13471.3438, 15209.6133,
        10999.5771, 10914.0801, 10927.7969, 16007.4492, 11159.4619, 11908.7451,
        16341.4590, 14584.9541, 11491.4844, 12999.6348,  9974.7080, 19864.9805,
        15415.7168, 15191.9512, 14573.7920, 10785.2939, 14791.5400, 18827.1562,
        14795.2832, 15395.6963, 13740.1387, 11554.0020, 10080.8994,  9453.5732,
        14363.0342, 13634.6182, 18641.2480, 14452.4004, 10047.9111,  9509.2715,
        14765.3428, 14399.4941, 12144.9902, 16175.5850, 14146.5732, 10083.3154,
        10829.3057, 12244.1914, 16591.8691, 11112.1523, 12060.1670, 10849.1855,
        11609.5762, 11686.5039, 12292.2715, 10918.1191, 17421.1582, 13360.2822,
        18648.1270, 12236.5488])
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'torch.Tensor'>
Content of relevance_map: tensor([237.0957,   0.0000,   1.1319,  ...,   0.0000,  50.0337, 565.8334])
[DEBUG] Global concept maps computed with 53 layers.
Layer: conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.4.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.4.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.4.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.4.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.4.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.4.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}
Layer: encoder.5.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.6.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.6.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.3.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0')}}
Layer: encoder.6.3.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.3.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.6.4.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.6.5.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.0.downsample.0
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.1.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.7.1.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.1.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}}
Layer: encoder.7.2.conv1
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}
Layer: encoder.7.2.conv2
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0')}}
Layer: encoder.7.2.conv3
Type of relevance_map: <class 'dict'>
Content of relevance_map: {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0')}}
[DEBUG] Global pruning mask: OrderedDict([('conv1', {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}), ('encoder.4.0.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}), ('encoder.4.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0')}}), ('encoder.4.0.conv3', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.0.downsample.0', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.1.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.1.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0')}}), ('encoder.4.1.conv3', {'Conv2d': {'weight': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.2.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}), ('encoder.4.2.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.4.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.0.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.0.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.0.downsample.0', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.1.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0.], device='cuda:0')}}), ('encoder.5.1.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.1.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.2.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}}), ('encoder.5.2.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0.], device='cuda:0')}}), ('encoder.5.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.5.3.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.3.conv2', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.], device='cuda:0')}}), ('encoder.5.3.conv3', {'Conv2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.0.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.0.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}}), ('encoder.6.0.downsample.0', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')}}), ('encoder.6.1.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.1.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.1.conv3', {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 1.], device='cuda:0')}}), ('encoder.6.2.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.], device='cuda:0')}}), ('encoder.6.2.conv2', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.2.conv3', {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.3.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1.], device='cuda:0')}}), ('encoder.6.3.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.3.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.4.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.4.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.], device='cuda:0')}}), ('encoder.6.4.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.5.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.5.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0')}}), ('encoder.6.5.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.0.downsample.0', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.1.conv1', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0')}}), ('encoder.7.1.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.1.conv3', {'Conv2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'bias': tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')}}), ('encoder.7.2.conv1', {'Conv2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0'), 'bias': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.], device='cuda:0')}}), ('encoder.7.2.conv2', {'Conv2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0'), 'bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0.], device='cuda:0')}}), ('encoder.7.2.conv3', {'Conv2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0')}, 'BatchNorm2d': {'weight': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0'), 'bias': tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0')}})])
Full profiling results saved to /home/paul/projects/CV4RS-main/pruning_callgraph.txt
=== Round 2/5 ===
Applying pruning mask for Round 2...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping 1D mask to match 4D weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
Training and communication for Round 2...
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
=== Round 3/5 ===
Applying pruning mask for Round 3...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
Training and communication for Round 3...
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
=== Round 4/5 ===
Applying pruning mask for Round 4...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
Training and communication for Round 4...
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
=== Round 5/5 ===
Applying pruning mask for Round 5...
[DEBUG] Validating mask for layer: conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: conv1
[DEBUG] Processing layer: conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Processing layer: conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 10, 7, 7])
[DEBUG] Layer weight shape: torch.Size([64, 10, 7, 7])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 10, 7, 7])
[INFO] Successfully applied mask to weight in layer: conv1
[DEBUG] Layer conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Processing layer: encoder.4.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv1
[DEBUG] Layer encoder.4.0.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv1
[DEBUG] Validating mask for layer: encoder.4.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Processing layer: encoder.4.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv2
[DEBUG] Layer encoder.4.0.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv2
[DEBUG] Validating mask for layer: encoder.4.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Processing layer: encoder.4.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.conv3
[DEBUG] Layer encoder.4.0.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.conv3
[DEBUG] Validating mask for layer: encoder.4.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Processing layer: encoder.4.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.0.downsample.0
[DEBUG] Layer encoder.4.0.downsample.0 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.0.downsample.0
[DEBUG] Validating mask for layer: encoder.4.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Processing layer: encoder.4.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv1
[DEBUG] Layer encoder.4.1.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv1
[DEBUG] Validating mask for layer: encoder.4.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Processing layer: encoder.4.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv2
[DEBUG] Layer encoder.4.1.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv2
[DEBUG] Validating mask for layer: encoder.4.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Processing layer: encoder.4.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.1.conv3
[DEBUG] Layer encoder.4.1.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.1.conv3
[DEBUG] Validating mask for layer: encoder.4.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Processing layer: encoder.4.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([64, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv1
[DEBUG] Layer encoder.4.2.conv1 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv1
[DEBUG] Validating mask for layer: encoder.4.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Processing layer: encoder.4.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([64, 64, 3, 3])
[DEBUG] Layer weight shape: torch.Size([64, 64, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([64, 64, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv2
[DEBUG] Layer encoder.4.2.conv2 has bias with shape torch.Size([64])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv2
[DEBUG] Validating mask for layer: encoder.4.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Processing layer: encoder.4.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 64, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 64, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 64, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.4.2.conv3
[DEBUG] Layer encoder.4.2.conv3 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.4.2.conv3
[DEBUG] Validating mask for layer: encoder.5.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Processing layer: encoder.5.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv1
[DEBUG] Layer encoder.5.0.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv1
[DEBUG] Validating mask for layer: encoder.5.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Processing layer: encoder.5.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv2
[DEBUG] Layer encoder.5.0.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv2
[DEBUG] Validating mask for layer: encoder.5.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Processing layer: encoder.5.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.conv3
[DEBUG] Layer encoder.5.0.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.conv3
[DEBUG] Validating mask for layer: encoder.5.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Processing layer: encoder.5.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.0.downsample.0
[DEBUG] Layer encoder.5.0.downsample.0 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.0.downsample.0
[DEBUG] Validating mask for layer: encoder.5.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Processing layer: encoder.5.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv1
[DEBUG] Layer encoder.5.1.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv1
[DEBUG] Validating mask for layer: encoder.5.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Processing layer: encoder.5.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv2
[DEBUG] Layer encoder.5.1.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv2
[DEBUG] Validating mask for layer: encoder.5.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Processing layer: encoder.5.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.1.conv3
[DEBUG] Layer encoder.5.1.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.1.conv3
[DEBUG] Validating mask for layer: encoder.5.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Processing layer: encoder.5.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv1
[DEBUG] Layer encoder.5.2.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv1
[DEBUG] Validating mask for layer: encoder.5.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Processing layer: encoder.5.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv2
[DEBUG] Layer encoder.5.2.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv2
[DEBUG] Validating mask for layer: encoder.5.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Processing layer: encoder.5.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.2.conv3
[DEBUG] Layer encoder.5.2.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.2.conv3
[DEBUG] Validating mask for layer: encoder.5.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Processing layer: encoder.5.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([128, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv1
[DEBUG] Layer encoder.5.3.conv1 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv1
[DEBUG] Validating mask for layer: encoder.5.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Processing layer: encoder.5.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([128, 128, 3, 3])
[DEBUG] Layer weight shape: torch.Size([128, 128, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([128, 128, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv2
[DEBUG] Layer encoder.5.3.conv2 has bias with shape torch.Size([128])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv2
[DEBUG] Validating mask for layer: encoder.5.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Processing layer: encoder.5.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 128, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 128, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 128, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.5.3.conv3
[DEBUG] Layer encoder.5.3.conv3 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.5.3.conv3
[DEBUG] Validating mask for layer: encoder.6.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Processing layer: encoder.6.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv1
[DEBUG] Layer encoder.6.0.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv1
[DEBUG] Validating mask for layer: encoder.6.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Processing layer: encoder.6.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv2
[DEBUG] Layer encoder.6.0.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv2
[DEBUG] Validating mask for layer: encoder.6.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Processing layer: encoder.6.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.conv3
[DEBUG] Layer encoder.6.0.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.conv3
[DEBUG] Validating mask for layer: encoder.6.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Processing layer: encoder.6.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.0.downsample.0
[DEBUG] Layer encoder.6.0.downsample.0 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.0.downsample.0
[DEBUG] Validating mask for layer: encoder.6.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Processing layer: encoder.6.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv1
[DEBUG] Layer encoder.6.1.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv1
[DEBUG] Validating mask for layer: encoder.6.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Processing layer: encoder.6.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv2
[DEBUG] Layer encoder.6.1.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv2
[DEBUG] Validating mask for layer: encoder.6.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Processing layer: encoder.6.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.1.conv3
[DEBUG] Layer encoder.6.1.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.1.conv3
[DEBUG] Validating mask for layer: encoder.6.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Processing layer: encoder.6.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv1
[DEBUG] Layer encoder.6.2.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv1
[DEBUG] Validating mask for layer: encoder.6.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Processing layer: encoder.6.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv2
[DEBUG] Layer encoder.6.2.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv2
[DEBUG] Validating mask for layer: encoder.6.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Processing layer: encoder.6.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.2.conv3
[DEBUG] Layer encoder.6.2.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.2.conv3
[DEBUG] Validating mask for layer: encoder.6.3.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Processing layer: encoder.6.3.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv1
[DEBUG] Layer encoder.6.3.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv1
[DEBUG] Validating mask for layer: encoder.6.3.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Processing layer: encoder.6.3.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv2
[DEBUG] Layer encoder.6.3.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv2
[DEBUG] Validating mask for layer: encoder.6.3.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Processing layer: encoder.6.3.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.3.conv3
[DEBUG] Layer encoder.6.3.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.3.conv3
[DEBUG] Validating mask for layer: encoder.6.4.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Processing layer: encoder.6.4.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv1
[DEBUG] Layer encoder.6.4.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv1
[DEBUG] Validating mask for layer: encoder.6.4.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Processing layer: encoder.6.4.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv2
[DEBUG] Layer encoder.6.4.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv2
[DEBUG] Validating mask for layer: encoder.6.4.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Processing layer: encoder.6.4.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.4.conv3
[DEBUG] Layer encoder.6.4.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.4.conv3
[DEBUG] Validating mask for layer: encoder.6.5.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Processing layer: encoder.6.5.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([256, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv1
[DEBUG] Layer encoder.6.5.conv1 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv1
[DEBUG] Validating mask for layer: encoder.6.5.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Processing layer: encoder.6.5.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([256, 256, 3, 3])
[DEBUG] Layer weight shape: torch.Size([256, 256, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([256, 256, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv2
[DEBUG] Layer encoder.6.5.conv2 has bias with shape torch.Size([256])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv2
[DEBUG] Validating mask for layer: encoder.6.5.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Processing layer: encoder.6.5.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([1024, 256, 1, 1])
[DEBUG] Layer weight shape: torch.Size([1024, 256, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([1024, 256, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.6.5.conv3
[DEBUG] Layer encoder.6.5.conv3 has bias with shape torch.Size([1024])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.6.5.conv3
[DEBUG] Validating mask for layer: encoder.7.0.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Processing layer: encoder.7.0.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv1
[DEBUG] Layer encoder.7.0.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv1
[DEBUG] Validating mask for layer: encoder.7.0.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Processing layer: encoder.7.0.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv2
[DEBUG] Layer encoder.7.0.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv2
[DEBUG] Validating mask for layer: encoder.7.0.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Processing layer: encoder.7.0.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.conv3
[DEBUG] Layer encoder.7.0.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.conv3
[DEBUG] Validating mask for layer: encoder.7.0.downsample.0
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Processing layer: encoder.7.0.downsample.0, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 1024, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 1024, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 1024, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.0.downsample.0
[DEBUG] Layer encoder.7.0.downsample.0 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.0.downsample.0
[DEBUG] Validating mask for layer: encoder.7.1.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Processing layer: encoder.7.1.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv1
[DEBUG] Layer encoder.7.1.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv1
[DEBUG] Validating mask for layer: encoder.7.1.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Processing layer: encoder.7.1.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv2
[DEBUG] Layer encoder.7.1.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv2
[DEBUG] Validating mask for layer: encoder.7.1.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Processing layer: encoder.7.1.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.1.conv3
[DEBUG] Layer encoder.7.1.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.1.conv3
[DEBUG] Validating mask for layer: encoder.7.2.conv1
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Processing layer: encoder.7.2.conv1, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 2048, 1, 1])
[DEBUG] Layer weight shape: torch.Size([512, 2048, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 2048, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv1
[DEBUG] Layer encoder.7.2.conv1 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv1
[DEBUG] Validating mask for layer: encoder.7.2.conv2
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Processing layer: encoder.7.2.conv2, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([512, 512, 3, 3])
[DEBUG] Layer weight shape: torch.Size([512, 512, 3, 3])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([512, 512, 3, 3])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv2
[DEBUG] Layer encoder.7.2.conv2 has bias with shape torch.Size([512])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv2
[DEBUG] Validating mask for layer: encoder.7.2.conv3
[WARNING] Mask for Conv2d is not a Tensor: <class 'dict'>
[WARNING] Mask for BatchNorm2d is not a Tensor: <class 'dict'>
[INFO] Applying pruning mask to layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: Conv2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
[DEBUG] Processing layer: encoder.7.2.conv3, Type: BatchNorm2d
[DEBUG] Original mask shape: torch.Size([2048, 512, 1, 1])
[DEBUG] Layer weight shape: torch.Size([2048, 512, 1, 1])
[INFO] Reshaping flat mask to match exact weight dimensions.
[DEBUG] Final adjusted mask shape: torch.Size([2048, 512, 1, 1])
[INFO] Successfully applied mask to weight in layer: encoder.7.2.conv3
[DEBUG] Layer encoder.7.2.conv3 has bias with shape torch.Size([2048])
[INFO] Adjusting bias mask to match bias dimensions.
[INFO] Successfully applied mask to bias in layer: encoder.7.2.conv3
Training and communication for Round 5...
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Epoch 1/2
----------
Epoch 2/2
----------
Skipping missing parameter: conv1.bias
Skipping missing parameter: encoder.0.bias
Skipping missing parameter: encoder.4.0.conv1.bias
Skipping missing parameter: encoder.4.0.conv2.bias
Skipping missing parameter: encoder.4.0.conv3.bias
Skipping missing parameter: encoder.4.0.downsample.0.bias
Skipping missing parameter: encoder.4.1.conv1.bias
Skipping missing parameter: encoder.4.1.conv2.bias
Skipping missing parameter: encoder.4.1.conv3.bias
Skipping missing parameter: encoder.4.2.conv1.bias
Skipping missing parameter: encoder.4.2.conv2.bias
Skipping missing parameter: encoder.4.2.conv3.bias
Skipping missing parameter: encoder.5.0.conv1.bias
Skipping missing parameter: encoder.5.0.conv2.bias
Skipping missing parameter: encoder.5.0.conv3.bias
Skipping missing parameter: encoder.5.0.downsample.0.bias
Skipping missing parameter: encoder.5.1.conv1.bias
Skipping missing parameter: encoder.5.1.conv2.bias
Skipping missing parameter: encoder.5.1.conv3.bias
Skipping missing parameter: encoder.5.2.conv1.bias
Skipping missing parameter: encoder.5.2.conv2.bias
Skipping missing parameter: encoder.5.2.conv3.bias
Skipping missing parameter: encoder.5.3.conv1.bias
Skipping missing parameter: encoder.5.3.conv2.bias
Skipping missing parameter: encoder.5.3.conv3.bias
Skipping missing parameter: encoder.6.0.conv1.bias
Skipping missing parameter: encoder.6.0.conv2.bias
Skipping missing parameter: encoder.6.0.conv3.bias
Skipping missing parameter: encoder.6.0.downsample.0.bias
Skipping missing parameter: encoder.6.1.conv1.bias
Skipping missing parameter: encoder.6.1.conv2.bias
Skipping missing parameter: encoder.6.1.conv3.bias
Skipping missing parameter: encoder.6.2.conv1.bias
Skipping missing parameter: encoder.6.2.conv2.bias
Skipping missing parameter: encoder.6.2.conv3.bias
Skipping missing parameter: encoder.6.3.conv1.bias
Skipping missing parameter: encoder.6.3.conv2.bias
Skipping missing parameter: encoder.6.3.conv3.bias
Skipping missing parameter: encoder.6.4.conv1.bias
Skipping missing parameter: encoder.6.4.conv2.bias
Skipping missing parameter: encoder.6.4.conv3.bias
Skipping missing parameter: encoder.6.5.conv1.bias
Skipping missing parameter: encoder.6.5.conv2.bias
Skipping missing parameter: encoder.6.5.conv3.bias
Skipping missing parameter: encoder.7.0.conv1.bias
Skipping missing parameter: encoder.7.0.conv2.bias
Skipping missing parameter: encoder.7.0.conv3.bias
Skipping missing parameter: encoder.7.0.downsample.0.bias
Skipping missing parameter: encoder.7.1.conv1.bias
Skipping missing parameter: encoder.7.1.conv2.bias
Skipping missing parameter: encoder.7.1.conv3.bias
Skipping missing parameter: encoder.7.2.conv1.bias
Skipping missing parameter: encoder.7.2.conv2.bias
Skipping missing parameter: encoder.7.2.conv3.bias
Training completed in 445.06 seconds.
[{'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}, {'0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '2': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '3': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '4': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '5': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '6': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '7': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '8': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '9': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '10': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '11': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '12': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '13': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '14': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '15': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '16': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '17': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, '18': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'micro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []}, 'ap_mic': [], 'ap_mac': []}]
